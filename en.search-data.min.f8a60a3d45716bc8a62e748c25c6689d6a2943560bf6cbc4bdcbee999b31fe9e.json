[{"id":0,"href":"/numerical_optimization/docs/lectures/machine_learning/perceptron/","title":"1. From Linear regression to perceptron","section":"III - Machine Learning problems","content":" From Linear regression to perceptron # Soon to be added.\n"},{"id":1,"href":"/numerical_optimization/docs/lectures/fundamentals/optimization_problems/","title":"1. Optimization problems","section":"I - Fundamentals","content":" Optimization problems # Unconstrained vs constrained # What we are interested in these lectures is to solve problems of the form :\n\\begin{equation} \\underset{\\mathbf{x}\\in\\mathbb{R}^d}{\\operatorname{(arg)min}} f(\\mathbf{x}), \\label{eq: optim general unconstrained} \\end{equation} where $\\mathbf{x}\\in\\mathbb{R}^d$ and $f:\\mathcal{D}_f \\mapsto \\mathbb{R} $ is a scalar-valued function with domain $\\mathcal{D}_f$. Under this formulation, the problem is said to be an unconstrained optimization problem.\nIf additionally, we add a set of equalities constraints functions: $$ \\{h_i : \\mathbb{R}^d \\mapsto \\mathbb{R} \\, /\\, 1 \\leq i \\leq N \\} $$ and inequalities constraints functions: $$ \\{g_j : \\mathbb{R}^d \\mapsto \\mathbb{R} \\, /\\, 1 \\leq j \\leq M \\} $$ and define the set $\\mathcal{S} = \\{\\mathbf{x} \\in \\mathbb{R}^d \\,/\\, \\forall\\,(i, j),\\, h_i(\\mathbf{x})=0,\\, g_j(\\mathbf{x})\\leq 0\\}$ and want to solve: \\begin{equation} \\underset{\\mathbf{x}\\in\\mathcal{S}}{\\operatorname{(arg)min}} f(\\mathbf{x}), \\label{eq: optim general constrained} \\end{equation} then the problem is said to be a constrained optimization problem.\nNote that here, the constraints and the function domain are not the same sets. Constraints usually stem from modelling of the problem whilst the function domain only characterizes for which values of $\\mathbf{x}$ it is possible to compute a value of the function.\nGlobal optimization vs local optimization # Figure 1.1: An example of multiple local minima\nIn the context of optimization, we can distinguish between global optimization and local optimization:\nGlobal optimization refers to the process of finding the best solution (minimum or maximum) across the entire search space. This means identifying the point where the function achieves its absolute minimum or maximum value, regardless of how many local minima or maxima exist. Local optimization, on the other hand, focuses on finding a solution that is optimal within a limited neighborhood of the search space. This means identifying a point where the function achieves a minimum or maximum value relative to nearby points, but not necessarily the absolute best solution across the entire space. Often, global optimization is not feasible unless the function is convex, or the search space is small enough. In practice, we often use local optimization methods to find a good enough solution, which may not be the global optimum. This is peculiarly true in machine learning, where the loss function is often non-convex and may have many local minima.\n"},{"id":2,"href":"/numerical_optimization/docs/lectures/advanced/unconstrained_newton/","title":"1. Unconstrained optimization : Second-order ","section":"II - Advanced problems","content":" Unconstrained optimization - Second-order methods # We have seen in the previous chapter that first-order methods, such as steepest descent, are often used to find a local minimum of a function $f(\\mathbf{x})$. However, these methods can be slow to converge, especially when the function has ill-conditioned Hessian or when the initial guess is far from the solution. Second-order methods, which use information about the curvature of the function, can provide faster convergence rates.\nSearch directions # Another important search direction-perhaps the most important one of all-is the Newton direction. This direction is derived from the second-order Taylor series approximation to $f\\left(\\mathbf{x}_k+\\mathbf{p}\\right)$, which is $$ f\\left(\\mathbf{x}_k+\\mathbf{p}\\right) \\approx f_k+\\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\mathbf{p}^{\\mathrm{T}} \\nabla^2 f_k \\mathbf{p} \\stackrel{\\text { def }}{=} m_k(\\mathbf{p}) $$\nAssuming for the moment that $\\nabla^2 f_k$ is positive definite, we obtain the Newton direction by finding the vector $\\mathbf{p}$ that minimizes $m_k(\\mathbf{p})$. By simply setting the derivative of $m_k(\\mathbf{p})$ to zero, we obtain the following explicit formula:\n\\begin{equation} \\mathbf{p}_k^{\\mathrm{N}}=-\\nabla^2 f_k^{-1} \\nabla f_k \\label{eq:newton_direction} \\end{equation}\nThe Newton direction is reliable when the difference between the true function $f\\left(\\mathbf{x}_k+ \\mathbf{p}\\right)$ and its quadratic model $m_k(\\mathbf{p})$ is not too large. By comparing \\eqref{eq:newton_direction} with traditional Taylor expansion, we see that the only difference between these functions is that the matrix $\\nabla^2 f\\left(\\mathbf{x}_k+t \\mathbf{p}\\right)$ in the third term of the expansion has been replaced by $\\nabla^2 f_k=\\nabla^2 f\\left(\\mathbf{x}_k\\right)$. If $\\nabla^2 f(\\cdot)$ is sufficiently smooth, this difference introduces a perturbation of only $O\\left(\\lVert\\mathbf{p}\\rVert^3\\right)$ into the expansion, so that when $\\lVert\\mathbf{p}\\rVert$ is small, the approximation $f\\left(\\mathbf{x}_k+\\mathbf{p}\\right) \\approx m_k(\\mathbf{p})$ is very accurate indeed.\nThe Newton direction can be used in a line search method when $\\nabla^2 f_k$ is positive definite, for in this case we have\n$$ \\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}=-\\mathbf{p}_k^{\\mathrm{N} \\mathrm{T}} \\nabla^2 f_k \\mathbf{p}_k^{\\mathrm{N}} \\leq-\\sigma_k\\lVert\\mathbf{p}_k^{\\mathrm{N}}\\rVert^2 $$\nfor some $\\sigma_k\u0026gt;0$. Unless the gradient $\\nabla f_k$ (and therefore the step $\\mathbf{p}_k^N$) is zero, we have that $\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}\u0026lt;0$, so the Newton direction is a descent direction. Unlike the steepest descent direction, there is a \u0026ldquo;natural\u0026rdquo; step length of 1 associated with the Newton direction. Most line search implementations of Newton\u0026rsquo;s method use the unit step $\\alpha=1$ where possible and adjust this step length only when it does not produce a satisfactory reduction in the value of $f$.\nWhen $\\nabla^2 f_k$ is not positive definite, the Newton direction may not even be defined, since $\\nabla^2 f_k^{-1}$ may not exist. Even when it is defined, it may not satisfy the descent property $\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}\u0026lt;0$, in which case it is unsuitable as a search direction. In these situations, line search methods modify the definition of $\\mathbf{p}_k$ to make it satisfy the downhill condition while retaining the benefit of the second-order information contained in $\\nabla^2 f_k$.\nMethods that use the Newton direction have a fast rate of local convergence, typically quadratic. When a neighborhood of the solution is reached, convergence to high accuracy often occurs in just a few iterations. The main drawback of the Newton direction is the need for the Hessian $\\nabla^2 f(\\mathbf{x})$. Explicit computation of this matrix of second derivatives is sometimes, though not always, a cumbersome, error-prone, and expensive process.\nQuasi-Newton search directions provide an attractive alternative in that they do not require computation of the Hessian and yet still attain a superlinear rate of convergence. In place of the true Hessian $\\nabla^2 f_k$, they use an approximation $\\mathbf{B}_k$, which is updated after each step to take account of the additional knowledge gained during the step. The updates make use of the fact that changes in the gradient $\\mathbf{g}$ provide information about the second derivative of $f$ along the search direction. By using the expression from our statement of Taylor\u0026rsquo;s theorem, we have by adding and subtracting the term $\\nabla^2 f(\\mathbf{x}) \\mathbf{p}$ that\n$$ \\nabla f(\\mathbf{x}+\\mathbf{p})=\\nabla f(\\mathbf{x})+\\nabla^2 f(\\mathbf{x}) \\mathbf{p}+\\int_0^1\\left[\\nabla^2 f(\\mathbf{x}+t \\mathbf{p})-\\nabla^2 f(\\mathbf{x})\\right] \\mathbf{p} d t $$\nBecause $\\nabla f(\\cdot)$ is continuous, the size of the final integral term is $o(\\lVert\\mathbf{p}\\rVert)$. By setting $\\mathbf{x}=\\mathbf{x}_k$ and $\\mathbf{p}=\\mathbf{x}_{k+1}-\\mathbf{x}_k$, we obtain\n$$ \\nabla f_{k+1}=\\nabla f_k+\\nabla^2 f_{k+1}\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right)+o\\left(\\lVert\\mathbf{x}_{k+1}-\\mathbf{x}_k\\rVert\\right) $$\nWhen $\\mathbf{x}_k$ and $\\mathbf{x}_{k+1}$ lie in a region near the solution $\\mathbf{x}^*$, within which $\\nabla f$ is positive definite, the final term in this expansion is eventually dominated by the $\\nabla^2 f_k\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right)$ term, and we can write\n$$ \\nabla^2 f_{k+1}\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right) \\approx \\nabla f_{k+1}-\\nabla f_k $$\nWe choose the new Hessian approximation $\\mathbf{B}_{k+1}$ so that it mimics this property of the true Hessian, that is, we require it to satisfy the following condition, known as the secant equation:\n\\begin{equation} \\mathbf{B}_{k+1} \\mathbf{s}_k=\\mathbf{y}_k \\label{eq:secant_equation} \\end{equation}\nwhere\n$$ \\mathbf{s}_k=\\mathbf{x}_{k+1}-\\mathbf{x}_k, \\quad \\mathbf{y}_k=\\nabla f_{k+1}-\\nabla f_k $$\nTypically, we impose additional requirements on $\\mathbf{B}_{k+1}$, such as symmetry (motivated by symmetry of the exact Hessian), and a restriction that the difference between successive approximation $\\mathbf{B}_k$ to $\\mathbf{B}_{k+1}$ have low rank. The initial approximation $\\mathbf{B}_0$ must be chosen by the user.\nTwo of the most popular formulae for updating the Hessian approximation $\\mathbf{B}_k$ are the symmetric-rank-one (SR1) formula, defined by\n\\begin{equation} \\mathbf{B}_{k+1}=\\mathbf{B}_k+\\frac{\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}}}{\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:sr1_formula} \\end{equation}\nand the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno, which is defined by\n\\begin{equation} \\mathbf{B}_{k+1}=\\mathbf{B}_k-\\frac{\\mathbf{B}_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k}+\\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:bfgs_formula} \\end{equation}\nNote that the difference between the matrices $\\mathbf{B}_k$ and $\\mathbf{B}_{k+1}$ is a rank-one matrix in the case of \\eqref{eq:sr1_formula}, and a rank-two matrix in the case of \\eqref{eq:bfgs_formula}. Both updates satisfy the secant equation and both maintain symmetry. One can show that BFGS update \\eqref{eq:bfgs_formula} generates positive definite approximations whenever the initial approximation $\\mathbf{B}_0$ is positive definite and $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k\u0026gt;0$.\nThe quasi-Newton search direction is given by using $\\mathbf{B}_k$ in place of the exact Hessian in the formula \\eqref{eq:newton_direction}, that is,\n\\begin{equation} \\mathbf{p}_k=-\\mathbf{B}_k^{-1} \\nabla f_k \\label{eq:quasi_newton_direction} \\end{equation}\nSome practical implementations of quasi-Newton methods avoid the need to factorize $\\mathbf{B}_k$ at each iteration by updating the inverse of $\\mathbf{B}_k$, instead of $\\mathbf{B}_k$ itself. In fact, the equivalent formula for \\eqref{eq:sr1_formula} and \\eqref{eq:bfgs_formula}, applied to the inverse approximation $\\mathbf{H}_k \\stackrel{\\text { def }}{=} \\mathbf{B}_k^{-1}$, is\n\\begin{equation} \\mathbf{H}_{k+1}=\\left(\\mathbf{I}-\\rho_k \\mathbf{s}_k \\mathbf{y}_k^{\\mathrm{T}}\\right) \\mathbf{H}_k\\left(\\mathbf{I}-\\rho_k \\mathbf{y}_k \\mathbf{s}_k^{\\mathrm{T}}\\right)+\\rho_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}}, \\quad \\rho_k=\\frac{1}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:inverse_bfgs} \\end{equation}\nCalculation of $\\mathbf{p}_k$ can then be performed by using the formula $\\mathbf{p}_k=-\\mathbf{H}_k \\nabla f_k$. This can be implemented as a matrix-vector multiplication, which is typically simpler than the factorization/back-substitution procedure that is needed to implement the formula \\eqref{eq:quasi_newton_direction}.\nStep-size selection # Contrarily to the steepest descent, Newton methods have a \u0026ldquo;natural\u0026rdquo; step size of 1 associated with the Newton direction. This is because the Newton direction is derived from the second-order Taylor series approximation, which is designed to minimize the quadratic model of the function. However, in practice, it is often necessary to adjust this step size to ensure sufficient decrease in the function value.\nWhen using a line search method, we can set $\\alpha_k=1$ and check if this step size leads to a sufficient decrease in the function value. If it does not, we can use a backtracking line search to find a suitable step size that satisfies the Armijo condition. The Armijo condition ensures that the step size leads to a sufficient decrease in the function value, which is crucial for convergence of the method.\nConvergence of Newton methods # As in first-order methods, we make use of Zoutendijk\u0026rsquo;s condition, that still apllies.\nConsider now the Newton-like method with $\\mathbf{p}_k = -\\mathbf{B}_k^{-1} \\nabla f_k$ and assume that the matrices $\\mathbf{B}_k$ are positive definite with a uniformly bounded condition number. That is, there is a constant $M$ such that\n\\begin{equation} \\|\\mathbf{B}_k\\|\\|\\mathbf{B}_k^{-1}\\| \\leq M, \\quad \\text { for all } k . \\label{eq:condition_bound} \\end{equation}\nIt is easy to show from the definition that\n$$ \\cos \\theta_k \\geq 1 / M $$\nBy combining this bound with (4.16) we find that\n$$ \\lim _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 $$\nTherefore, we have shown that Newton and quasi-Newton methods are globally convergent if the matrices $\\mathbf{B}_k$ have a bounded condition number and are positive definite (which is needed to ensure that $\\mathbf{p}_k$ is a descent direction), and if the step lengths satisfy the Wolfe conditions.\nFor some algorithms, such as conjugate gradient methods, we will not be able to prove the limit (4.18), but only the weaker result\n\\begin{equation} \\liminf _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 \\label{eq:weak_convergence} \\end{equation}\nIn other words, just a subsequence of the gradient norms $\\|\\nabla f_{k_j}\\|$ converges to zero, rather than the whole sequence. This result, too, can be proved by using Zoutendijk\u0026rsquo;s condition (4.16), but instead of a constructive proof, we outline a proof by contradiction. Suppose that \\eqref{eq:weak_convergence} does not hold, so that the gradients remain bounded away from zero, that is, there exists $\\gamma\u0026gt;0$ such that\n$$ \\|\\nabla f_k\\| \\geq \\gamma, \\quad \\text { for all } k \\text { sufficiently large. } $$\nThen from (4.16) we conclude that\n$$ \\cos \\theta_k \\rightarrow 0 $$\nthat is, the entire sequence $\\{\\cos \\theta_k\\}$ converges to 0. To establish \\eqref{eq:weak_convergence}, therefore, it is enough to show that a subsequence $\\{\\cos \\theta_{k_j}\\}$ is bounded away from zero.\nRate of convergence # We refer the reader to the textbook:\n\u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, 2nd edition, Springer, 2006, pages 47-51, Peculiarly, see pages 51-53.\n"},{"id":3,"href":"/numerical_optimization/docs/lectures/reminders/differentiation/","title":"Differentiation","section":"Reminders","content":" Fundamentals of Multivariate Differentiation # Soon to be added.\n"},{"id":4,"href":"/numerical_optimization/docs/lectures/","title":"Lectures","section":"Docs","content":" Lectures # This section regroups the theory : main results, proofs and exercices behind optimization algorithms.\nContent Introduction I - Fundamentals II - Advanced problems III - Machine Learning problems Reminders "},{"id":5,"href":"/numerical_optimization/docs/lectures/reminders/linear_algebra/","title":"Linear Algebra","section":"Reminders","content":" Fundamentals of Linear Algebra # 1 - Introduction # Linear algebra is one of the foundational branches of mathematics, with applications spanning from engineering and computer science to economics and physics. This document is extracted from the textbook Matrix Differential Calculus with Applications in Statistics and Econometrics from Jan R. Magnus; Heinz Neudecker, adapting the notations to the ones used in the lectures.\nIn this chapter, we summarize some of the well-known definitions and theorems of matrix algebra. Most of the theorems will be proved.\n2 - Sets # Definition 0.1 (Set)\nA set is a collection of objects, called the elements (or members) of the set. We write $x \\in S$ to mean \u0026lsquo;$x$ is an element of $S$\u0026rsquo; or \u0026lsquo;$x$ belongs to $S$\u0026rsquo;. If $x$ does not belong to $S$, we write $x \\notin S$. The set that contains no elements is called the empty set, denoted by $\\emptyset$. Sometimes a set can be defined by displaying the elements in braces. For example, $A={0,1}$ or\n$$ \\mathbb{N}={1,2,3, \\ldots} $$\nNotice that $A$ is a finite set (contains a finite number of elements), whereas $\\mathbb{N}$ is an infinite set. If $P$ is a property that any element of $S$ has or does not have, then\n$$ {x: x \\in S, x \\text { satisfies } P} $$\ndenotes the set of all the elements of $S$ that have property $P$.\nDefinition 0.2 (Subset)\nA set $A$ is called a subset of $B$, written $A \\subset B$, whenever every element of $A$ also belongs to $B$. The notation $A \\subset B$ does not rule out the possibility that $A=B$. If $A \\subset B$ and $A \\neq B$, then we say that $A$ is a proper subset of $B$. If $A$ and $B$ are two subsets of $S$, we define\n$$ A \\cup B, $$\nthe union of $A$ and $B$, as the set of elements of $S$ that belong to $A$ or to $B$ or to both, and\n$$ A \\cap B, $$\nthe intersection of $A$ and $B$, as the set of elements of $S$ that belong to both $A$ and $B$. We say that $A$ and $B$ are (mutually) disjoint if they have no common elements, that is, if\n$$ A \\cap B=\\emptyset . $$\nThe complement of $A$ relative to $B$, denoted by $B-A$, is the set ${x: x \\in B$, but $x \\notin A}$. The complement of $A$ (relative to $S$) is sometimes denoted by $A^{c}$.\nDefinition 0.3 (Cartesian Product)\nThe Cartesian product of two sets $A$ and $B$, written $A \\times B$, is the set of all ordered pairs $(a, b)$ such that $a \\in A$ and $b \\in B$. More generally, the Cartesian product of $n$ sets $A_{1}, A_{2}, \\ldots, A_{n}$, written\n$$ \\prod_{i=1}^{n} A_{i} $$\nis the set of all ordered $n$-tuples $\\left(a_{1}, a_{2}, \\ldots, a_{n}\\right)$ such that $a_{i} \\in A_{i}(i=1, \\ldots, n)$.\nThe set of (finite) real numbers (the one-dimensional Euclidean space) is denoted by $\\mathbb{R}$. The $n$-dimensional Euclidean space $\\mathbb{R}^{n}$ is the Cartesian product of $n$ sets equal to $\\mathbb{R}$:\n$$ \\mathbb{R}^{n}=\\mathbb{R} \\times \\mathbb{R} \\times \\cdots \\times \\mathbb{R} \\quad (n \\text { times }) $$\nThe elements of $\\mathbb{R}^{n}$ are thus the ordered $n$-tuples $\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$ of real numbers $x_{1}, x_{2}, \\ldots, x_{n}$.\nDefinition 0.4 (Bounded Set)\nA set $S$ of real numbers is said to be bounded if there exists a number $M$ such that $|x| \\leq M$ for all $x \\in S$. 3 - Matrices: Addition and Multiplication # Definition 0.5 (Real Matrix)\nA real $m \\times n$ matrix $\\mathbf{A}$ is a rectangular array of real numbers\n$$ \\mathbf{A}=\\left(\\begin{array}{cccc} a_{11} \u0026amp; a_{12} \u0026amp; \\ldots \u0026amp; a_{1 n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\ldots \u0026amp; a_{2 n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ a_{m 1} \u0026amp; a_{m 2} \u0026amp; \\ldots \u0026amp; a_{m n} \\end{array}\\right) $$\nWe sometimes write $\\mathbf{A}=\\left(a_{i j}\\right)$.\nIf one or more of the elements of $\\mathbf{A}$ is complex, we say that $\\mathbf{A}$ is a complex matrix. Almost all matrices in this book are real and the word \u0026lsquo;matrix\u0026rsquo; is assumed to be a real matrix, unless explicitly stated otherwise.\nAn $m \\times n$ matrix can be regarded as a point in $\\mathbb{R}^{m \\times n}$. The real numbers $a_{i j}$ are called the elements of $\\mathbf{A}$. An $m \\times 1$ matrix is a point in $\\mathbb{R}^{m \\times 1}$ (that is, in $\\mathbb{R}^{m}$) and is called a (column) vector of order $m \\times 1$. A $1 \\times n$ matrix is called a row vector (of order $1 \\times n$). The elements of a vector are usually called its components. Matrices are always denoted by capital letters and vectors by lower-case letters.\nDefinition 0.6 (Matrix Addition)\nThe sum of two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of the same order is defined as\n$$ \\mathbf{A}+\\mathbf{B}=\\left(a_{i j}\\right)+\\left(b_{i j}\\right)=\\left(a_{i j}+b_{i j}\\right) $$\nDefinition 0.7 (Scalar Multiplication)\nThe product of a matrix by a scalar $\\lambda$ is\n$$ \\lambda \\mathbf{A}=\\mathbf{A} \\lambda=\\left(\\lambda a_{i j}\\right) $$\nThe following properties are now easily proved for matrices $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$ of the same order and scalars $\\lambda$ and $\\mu$:\n\\begin{equation} \\begin{aligned} \\mathbf{A}+\\mathbf{B} \u0026amp; =\\mathbf{B}+\\mathbf{A}, \\\\ (\\mathbf{A}+\\mathbf{B})+\\mathbf{C} \u0026amp; =\\mathbf{A}+(\\mathbf{B}+\\mathbf{C}), \\\\ (\\lambda+\\mu) \\mathbf{A} \u0026amp; =\\lambda \\mathbf{A}+\\mu \\mathbf{A}, \\\\ \\lambda(\\mathbf{A}+\\mathbf{B}) \u0026amp; =\\lambda \\mathbf{A}+\\lambda \\mathbf{B}, \\\\ \\lambda(\\mu \\mathbf{A}) \u0026amp; =(\\lambda \\mu) \\mathbf{A} . \\end{aligned} \\end{equation}\nA matrix whose elements are all zero is called a null matrix and denoted by $\\mathbf{0}$. We have, of course,\n$$ \\mathbf{A}+(-1) \\mathbf{A}=\\mathbf{0} $$\nDefinition 0.8 (Matrix Multiplication)\nIf $\\mathbf{A}$ is an $m \\times n$ matrix and $\\mathbf{B}$ an $n \\times p$ matrix (so that $\\mathbf{A}$ has the same number of columns as $\\mathbf{B}$ has rows), then we define the product of $\\mathbf{A}$ and $\\mathbf{B}$ as\n$$ \\mathbf{A} \\mathbf{B}=\\left(\\sum_{j=1}^{n} a_{i j} b_{j k}\\right) $$\nThus, $\\mathbf{A} \\mathbf{B}$ is an $m \\times p$ matrix and its $ik$th element is $\\sum_{j=1}^{n} a_{i j} b_{j k}$.\nThe following properties of the matrix product can be established:\n\\begin{equation} \\begin{aligned} (\\mathbf{A} \\mathbf{B}) \\mathbf{C} \u0026amp; =\\mathbf{A}(\\mathbf{B} \\mathbf{C}) \\\\ \\mathbf{A}(\\mathbf{B}+\\mathbf{C}) \u0026amp; =\\mathbf{A} \\mathbf{B}+\\mathbf{A} \\mathbf{C} \\\\ (\\mathbf{A}+\\mathbf{B}) \\mathbf{C} \u0026amp; =\\mathbf{A} \\mathbf{C}+\\mathbf{B} \\mathbf{C} \\end{aligned} \\end{equation}\nThese relations hold provided the matrix products exist.\nWe note that the existence of $\\mathbf{A} \\mathbf{B}$ does not imply the existence of $\\mathbf{B} \\mathbf{A}$, and even when both products exist, they are not generally equal. (Two matrices $\\mathbf{A}$ and $\\mathbf{B}$ for which\n$$ \\mathbf{A} \\mathbf{B}=\\mathbf{B} \\mathbf{A} $$\nare said to commute.) We therefore distinguish between premultiplication and postmultiplication: a given $m \\times n$ matrix $\\mathbf{A}$ can be premultiplied by a $p \\times m$ matrix $\\mathbf{B}$ to form the product $\\mathbf{B} \\mathbf{A}$; it can also be postmultiplied by an $n \\times q$ matrix $\\mathbf{C}$ to form $\\mathbf{A} \\mathbf{C}$.\n4 - The transpose of a matrix # Definition 0.9 (Transpose)\nThe transpose of an $m \\times n$ matrix $\\mathbf{A}=\\left(a_{i j}\\right)$ is the $n \\times m$ matrix, denoted by $\\mathbf{A}^{\\mathrm{T}}$, whose $ij$th element is $a_{j i}$. We have\n\\begin{equation} \\begin{aligned} \\left(\\mathbf{A}^{\\mathrm{T}}\\right)^{\\mathrm{T}} \u0026amp; =\\mathbf{A} \\\\ (\\mathbf{A}+\\mathbf{B})^{\\mathrm{T}} \u0026amp; =\\mathbf{A}^{\\mathrm{T}}+\\mathbf{B}^{\\mathrm{T}} \\\\ (\\mathbf{A} \\mathbf{B})^{\\mathrm{T}} \u0026amp; =\\mathbf{B}^{\\mathrm{T}} \\mathbf{A}^{\\mathrm{T}} \\end{aligned} \\end{equation}\nIf $\\mathbf{x}$ is an $n \\times 1$ vector, then $\\mathbf{x}^{\\mathrm{T}}$ is a $1 \\times n$ row vector and\n$$ \\mathbf{x}^{\\mathrm{T}} \\mathbf{x}=\\sum_{i=1}^{n} x_{i}^{2} $$\nDefinition 0.10 (Euclidean Norm)\nThe (Euclidean) norm of $\\mathbf{x}$ is defined as\n$$ |\\mathbf{x}|=\\left(\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}\\right)^{1 / 2} $$\n5 - Square matrices # Definition 0.11 (Square Matrix)\nA matrix is said to be square if it has as many rows as it has columns. A square matrix $\\mathbf{A}=\\left(a_{i j}\\right)$, real or complex, is said to be\ntype if lower triangular if $a_{i j}=0 \\quad(i\u0026lt;j)$, strictly lower triangular if $a_{i j}=0 \\quad(i \\leq j)$, unit lower triangular if $a_{i j}=0 \\quad(i\u0026lt;j)$ and $a_{i i}=1$ (all $i$), upper triangular if $a_{i j}=0 \\quad(i\u0026gt;j)$, strictly upper triangular if $a_{i j}=0 \\quad(i \\geq j)$, unit upper triangular if $a_{i j}=0 \\quad(i\u0026gt;j)$ and $a_{i i}=1$ (all $i$), idempotent if $\\mathbf{A}^{2}=\\mathbf{A}$. A square matrix $\\mathbf{A}$ is triangular if it is either lower triangular or upper triangular (or both).\nA real square matrix $\\mathbf{A}=\\left(a_{i j}\\right)$ is said to be\ntype if symmetric if $\\mathbf{A}^{\\mathrm{T}} = \\mathbf{A}$, skew-symmetric if $\\mathbf{A}^{\\mathrm{T}} = -\\mathbf{A}$. For any square $n \\times n$ matrix $\\mathbf{A}=\\left(a_{i j}\\right)$, we define $\\operatorname{dg} \\mathbf{A}$ or $\\operatorname{dg}(\\mathbf{A})$ as\n$$ \\operatorname{dg} \\mathbf{A}=\\left(\\begin{array}{cccc} a_{11} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; a_{22} \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; a_{n n} \\end{array}\\right) $$\nor, alternatively,\n$$ \\operatorname{dg} \\mathbf{A}=\\operatorname{diag}\\left(a_{11}, a_{22}, \\ldots, a_{n n}\\right) $$\nDefinition 0.12 (Diagonal Matrix)\nIf $\\mathbf{A}=\\operatorname{dg} \\mathbf{A}$, we say that $\\mathbf{A}$ is diagonal. Definition 0.13 (Identity Matrix)\nA particular diagonal matrix is the identity matrix $\\mathbf{I}_n$ (of order $n \\times n$),\n$$ \\left (\\begin{array}{cccc} 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 1 \\end{array}\\right )=\\left(\\delta_{i j}\\right) $$\nwhere $\\delta_{i j}=1$ if $i=j$ and $\\delta_{i j}=0$ if $i \\neq j$ ($\\delta_{i j}$ is called the Kronecker delta).\nWe sometimes write $\\mathbf{I}$ instead of $\\mathbf{I}_{n}$ when the order is obvious or irrelevant. We have\n$$ \\mathbf{I} \\mathbf{A}=\\mathbf{A} \\mathbf{I}=\\mathbf{A}, $$\nif $\\mathbf{A}$ and $\\mathbf{I}$ have the same order.\nDefinition 0.14 (Orthogonal Matrix)\nA real square matrix $\\mathbf{A}$ is said to be orthogonal if\n$$ \\mathbf{A} \\mathbf{A}^{\\mathrm{T}}=\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I} $$\nand its columns are said to be orthonormal.\nA rectangular (not square) matrix can still have the property that $\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}=\\mathbf{I}$ or $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I}$, but not both. Such a matrix is called semi-orthogonal.\nNote carefully that the concepts of symmetry, skew-symmetry, and orthogonality are defined only for real square matrices. Hence, a complex matrix $\\mathbf{Z}$ satisfying $\\mathbf{Z}^{\\mathrm{T}}=\\mathbf{Z}$ is not called symmetric (in spite of what some textbooks do). This is important because complex matrices can be Hermitian, skew-Hermitian, or unitary, and there are many important results about these classes of matrices. These results should specialize to matrices that are symmetric, skew-symmetric, or orthogonal in the special case that the matrices are real. Thus, a symmetric matrix is just a real Hermitian matrix, a skew-symmetric matrix is a real skew-Hermitian matrix, and an orthogonal matrix is a real unitary matrix; see also Section 1.12.\n6 - Linear forms and quadratic forms # Let $\\mathbf{a}$ be an $n \\times 1$ vector, $\\mathbf{A}$ an $n \\times n$ matrix, and $\\mathbf{B}$ an $n \\times m$ matrix. The expression $\\mathbf{a}^{\\mathrm{T}} \\mathbf{x}$ is called a linear form in $\\mathbf{x}$, the expression $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}$ is a quadratic form in $\\mathbf{x}$, and the expression $\\mathbf{x}^{\\mathrm{T}} \\mathbf{B} \\mathbf{y}$ a bilinear form in $\\mathbf{x}$ and $\\mathbf{y}$. In quadratic forms we may, without loss of generality, assume that $\\mathbf{A}$ is symmetric, because if not then we can replace $\\mathbf{A}$ by $\\left(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}\\right) / 2$, since\n$$ \\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{x}^{\\mathrm{T}}\\left(\\frac{\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}}{2}\\right) \\mathbf{x} . $$\nThus, let $\\mathbf{A}$ be a symmetric matrix. We say that $\\mathbf{A}$ is\npositive definite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026gt;0$ for all $\\mathbf{x} \\neq \\mathbf{0}$, positive semidefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x} \\geq 0$ for all $\\mathbf{x}$, negative definite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026lt;0$ for all $\\mathbf{x} \\neq \\mathbf{0}$, negative semidefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x} \\leq 0$ for all $\\mathbf{x}$, indefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026gt;0$ for some $\\mathbf{x}$ and $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026lt;0$ for some $\\mathbf{x}$. It is clear that the matrices $\\mathbf{B} \\mathbf{B}^{\\mathrm{T}}$ and $\\mathbf{B}^{\\mathrm{T}} \\mathbf{B}$ are positive semidefinite, and that $\\mathbf{A}$ is negative (semi)definite if and only if $-\\mathbf{A}$ is positive (semi)definite. A square null matrix is both positive and negative semidefinite.\nDefinition 0.15 (Square Root Matrix)\nIf $\\mathbf{A}$ is positive semidefinite, then there are many matrices $\\mathbf{B}$ satisfying\n$$ \\mathbf{B}^{2}=\\mathbf{A} . $$\nBut there is only one positive semidefinite matrix $\\mathbf{B}$ satisfying $\\mathbf{B}^{2}=\\mathbf{A}$. This matrix is called the square root of $\\mathbf{A}$, denoted by $\\mathbf{A}^{1 / 2}$.\nThe following two theorems are often useful.\nTheorem 0.1 (Matrix Equality Conditions)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix, $\\mathbf{B}$ and $\\mathbf{C}$ $n \\times p$ matrices, and let $\\mathbf{x}$ be an $n \\times 1$ vector. Then, (a) $\\mathbf{A} \\mathbf{x}=\\mathbf{0} \\Longleftrightarrow \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$, (b) $\\mathbf{A} \\mathbf{B}=\\mathbf{0} \\Longleftrightarrow \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{B}=\\mathbf{0}$, (c) $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{B}=\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{C} \\Longleftrightarrow \\mathbf{A} \\mathbf{B}=\\mathbf{A} \\mathbf{C}$. Proof\n(a) Clearly $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ implies $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$. Conversely, if $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$, then $(\\mathbf{A} \\mathbf{x})^{\\mathrm{T}}(\\mathbf{A} \\mathbf{x})=\\mathbf{x}^{\\mathrm{T}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=0$ and hence $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$. (b) follows from (a), and (c) follows from (b) by substituting $\\mathbf{B}-\\mathbf{C}$ for $\\mathbf{B}$ in (b). ■ Theorem 0.2 (Zero Matrix Conditions)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix, $\\mathbf{B}$ and $\\mathbf{C}$ $n \\times n$ matrices, $\\mathbf{B}$ symmetric. Then, (a) $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{A}=\\mathbf{0}$, (b) $\\mathbf{x}^{\\mathrm{T}} \\mathbf{B} \\mathbf{x}=0$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{B}=\\mathbf{0}$, (c) $\\mathbf{x}^{\\mathrm{T}} \\mathbf{C} \\mathbf{x}=0$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{C}^{\\mathrm{T}}=-\\mathbf{C}$. Proof\nThe proof is easy and is left to the reader. ■ 7 - The rank of a matrix # Definition 0.16 (Linear Independence)\nA set of vectors $\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}$ is said to be linearly independent if $\\sum_{i} \\alpha_{i} \\mathbf{x}_{i}=\\mathbf{0}$ implies that all $\\alpha_{i}=0$. If $\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}$ are not linearly independent, they are said to be linearly dependent. Definition 0.17 (Matrix Rank)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix. The column rank of $\\mathbf{A}$ is the maximum number of linearly independent columns it contains. The row rank of $\\mathbf{A}$ is the maximum number of linearly independent rows it contains. It may be shown that the column rank of $\\mathbf{A}$ is equal to its row rank. Hence, the concept of rank is unambiguous. We denote the rank of $\\mathbf{A}$ by\n$$ r(\\mathbf{A}) . $$\nIt is clear that\n$$ r(\\mathbf{A}) \\leq \\min (m, n) $$\nIf $r(\\mathbf{A})=m$, we say that $\\mathbf{A}$ has full row rank. If $r(\\mathbf{A})=n$, we say that $\\mathbf{A}$ has full column rank. If $r(\\mathbf{A})=0$, then $\\mathbf{A}$ is the null matrix, and conversely, if $\\mathbf{A}$ is the null matrix, then $r(\\mathbf{A})=0$.\nWe have the following important results concerning ranks:\n\\begin{equation} \\begin{gathered} r(\\mathbf{A})=r\\left(\\mathbf{A}^{\\mathrm{T}}\\right)=r\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)=r\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right) \\\\ r(\\mathbf{A} \\mathbf{B}) \\leq \\min (r(\\mathbf{A}), r(\\mathbf{B})) \\\\ r(\\mathbf{A} \\mathbf{B})=r(\\mathbf{A}) \\quad \\text { if } \\mathbf{B} \\text { is square and of full rank, } \\\\ r(\\mathbf{A}+\\mathbf{B}) \\leq r(\\mathbf{A})+r(\\mathbf{B}) \\end{gathered} \\end{equation}\nand finally, if $\\mathbf{A}$ is an $m \\times n$ matrix and $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ for some $\\mathbf{x} \\neq \\mathbf{0}$, then\n$$ r(\\mathbf{A}) \\leq n-1 $$\nDefinition 0.18 (Column Space)\nThe column space of $\\mathbf{A}(m \\times n)$, denoted by $\\mathcal{M}(\\mathbf{A})$, is the set of vectors\n$$ \\mathcal{M}(\\mathbf{A})=\\left\\{\\mathbf{y}: \\mathbf{y}=\\mathbf{A} \\mathbf{x} \\text { for some } \\mathbf{x} \\text { in } \\mathbb{R}^{n}\\right\\} $$\nThus, $\\mathcal{M}(\\mathbf{A})$ is the vector space generated by the columns of $\\mathbf{A}$.\nThe dimension of this vector space is $r(\\mathbf{A})$. We have\n$ \\mathcal{M}(\\mathbf{A})=\\mathcal{M}\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right) $\nfor any matrix $\\mathbf{A}$.\nExercises:\nIf $\\mathbf{A}$ has full column rank and $\\mathbf{C}$ has full row rank, then $r(\\mathbf{A} \\mathbf{B} \\mathbf{C})=r(\\mathbf{B})$. Let $\\mathbf{A}$ be partitioned as $\\mathbf{A}=\\left(\\mathbf{A}_{1}: \\mathbf{A}_{2}\\right)$. Then $r(\\mathbf{A})=r\\left(\\mathbf{A}_{1}\\right)$ if and only if $\\mathcal{M}\\left(\\mathbf{A}_{2}\\right) \\subset \\mathcal{M}\\left(\\mathbf{A}_{1}\\right)$. 8 - The Inverse # Definition 0.19 (Nonsingular Matrix)\nLet $\\mathbf{A}$ be a square matrix of order $n \\times n$. We say that $\\mathbf{A}$ is nonsingular if $r(\\mathbf{A})=n$, and that $\\mathbf{A}$ is singular if $r(\\mathbf{A})\u0026lt;n$. Definition 0.20 (Matrix Inverse)\nIf $\\mathbf{A}$ is nonsingular, then there exists a nonsingular matrix $\\mathbf{B}$ such that\n$ \\mathbf{A} \\mathbf{B}=\\mathbf{B} \\mathbf{A}=\\mathbf{I}_{n} . $\nThe matrix $\\mathbf{B}$, denoted by $\\mathbf{A}^{-1}$, is unique and is called the inverse of $\\mathbf{A}$.\nWe have\n\\begin{equation} \\begin{aligned} \\left(\\mathbf{A}^{-1}\\right)^{\\mathrm{T}} \u0026amp; =\\left(\\mathbf{A}^{\\mathrm{T}}\\right)^{-1}, \\\\ (\\mathbf{A} \\mathbf{B})^{-1} \u0026amp; =\\mathbf{B}^{-1} \\mathbf{A}^{-1}, \\end{aligned} \\end{equation}\nif the inverses exist.\nDefinition 0.21 (Permutation Matrix)\nA square matrix $\\mathbf{P}$ is said to be a permutation matrix if each row and each column of $\\mathbf{P}$ contain a single element one, and the remaining elements are zero. An $n \\times n$ permutation matrix thus contains $n$ ones and $n(n-1)$ zeros. It can be proved that any permutation matrix is nonsingular. In fact, it is even true that $\\mathbf{P}$ is orthogonal, that is,\n$ \\mathbf{P}^{-1}=\\mathbf{P}^{\\mathrm{T}} $\nfor any permutation matrix $\\mathbf{P}$.\n9 - The Determinant # Definition 0.22 (Determinant)\nAssociated with any $n \\times n$ matrix $\\mathbf{A}$ is the determinant $|\\mathbf{A}|$ defined by\n$ |\\mathbf{A}|=\\sum(-1)^{\\phi\\left(j_{1}, \\ldots, j_{n}\\right)} \\prod_{i=1}^{n} a_{i j_{i}} $\nwhere the summation is taken over all permutations $\\left(j_{1}, \\ldots, j_{n}\\right)$ of the set of integers $(1, \\ldots, n)$, and $\\phi\\left(j_{1}, \\ldots, j_{n}\\right)$ is the number of transpositions required to change $(1, \\ldots, n)$ into $\\left(j_{1}, \\ldots, j_{n}\\right)$.\nWe have\n\\begin{equation} \\begin{aligned} |\\mathbf{A} \\mathbf{B}| \u0026amp; =|\\mathbf{A}||\\mathbf{B}| \\\\ \\left|\\mathbf{A}^{\\mathrm{T}}\\right| \u0026amp; =|\\mathbf{A}| \\\\ |\\alpha \\mathbf{A}| \u0026amp; =\\alpha^{n}|\\mathbf{A}| \\quad \\text { for any scalar } \\alpha \\\\ \\left|\\mathbf{A}^{-1}\\right| \u0026amp; =|\\mathbf{A}|^{-1} \\quad \\text { if } \\mathbf{A} \\text { is nonsingular, } \\\\ \\left|\\mathbf{I}_{n}\\right| \u0026amp; =1 \\end{aligned} \\end{equation}\nDefinition 0.23 (Minor and Cofactor)\nA submatrix of $\\mathbf{A}$ is the rectangular array obtained from $\\mathbf{A}$ by deleting some of its rows and/or some of its columns. A minor is the determinant of a square submatrix of $\\mathbf{A}$. The minor of an element $a_{i j}$ is the determinant of the submatrix of $\\mathbf{A}$ obtained by deleting the $i$th row and $j$th column. The cofactor of $a_{i j}$, say $c_{i j}$, is $(-1)^{i+j}$ times the minor of $a_{i j}$. The matrix $\\mathbf{C}=\\left(c_{i j}\\right)$ is called the cofactor matrix of $\\mathbf{A}$. The transpose of $\\mathbf{C}$ is called the adjoint of $\\mathbf{A}$ and will be denoted by $\\mathbf{A}^{\\#}$.\nWe have\n\\begin{equation} \\begin{aligned} |\\mathbf{A}|=\\sum_{j=1}^{n} a_{i j} c_{i j} \u0026amp; =\\sum_{j=1}^{n} a_{j k} c_{j k} \\quad(i, k=1, \\ldots, n), \\\\ \\mathbf{A} \\mathbf{A}^{\\#} \u0026amp; =\\mathbf{A}^{\\#} \\mathbf{A}=|\\mathbf{A}| \\mathbf{I}, \\\\ (\\mathbf{A} \\mathbf{B})^{\\#} \u0026amp; =\\mathbf{B}^{\\#} \\mathbf{A}^{\\#} . \\end{aligned} \\end{equation}\nDefinition 0.24 (Principal Minor)\nFor any square matrix $\\mathbf{A}$, a principal submatrix of $\\mathbf{A}$ is obtained by deleting corresponding rows and columns. The determinant of a principal submatrix is called a principal minor. Exercises:\nIf $\\mathbf{A}$ is nonsingular, show that $\\mathbf{A}^{\\#}=|\\mathbf{A}| \\mathbf{A}^{-1}$. Prove that the determinant of a triangular matrix is the product of its diagonal elements. 10 - The trace # Definition 0.25 (Trace)\nThe trace of a square $n \\times n$ matrix $\\mathbf{A}$, denoted by $\\operatorname{tr} \\mathbf{A}$ or $\\operatorname{tr}(\\mathbf{A})$, is the sum of its diagonal elements:\n$ \\operatorname{tr} \\mathbf{A}=\\sum_{i=1}^{n} a_{i i} . $\nWe have\n\\begin{equation} \\begin{aligned} \\operatorname{tr}(\\mathbf{A}+\\mathbf{B}) \u0026amp; =\\operatorname{tr} \\mathbf{A}+\\operatorname{tr} \\mathbf{B} \\\\ \\operatorname{tr}(\\lambda \\mathbf{A}) \u0026amp; =\\lambda \\operatorname{tr} \\mathbf{A} \\quad \\text { if } \\lambda \\text { is a scalar } \\\\ \\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \u0026amp; =\\operatorname{tr} \\mathbf{A} \\\\ \\operatorname{tr} \\mathbf{A} \\mathbf{B} \u0026amp; =\\operatorname{tr} \\mathbf{B} \\mathbf{A} \\end{aligned} \\end{equation}\nWe note in (25) that $\\mathbf{A} \\mathbf{B}$ and $\\mathbf{B} \\mathbf{A}$, though both square, need not be of the same order.\nCorresponding to the vector (Euclidean) norm\n$ |\\mathbf{x}|=\\left(\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}\\right)^{1 / 2}, $\ngiven in (4), we now define the matrix (Euclidean) norm as\nDefinition 0.26 (Matrix Norm)\n$ |\\mathbf{A}|=\\left(\\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{1 / 2} $ We have\n$ \\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\geq 0 $\nwith equality if and only if $\\mathbf{A}=\\mathbf{0}$.\n11 - Partitioned matrices # Definition 0.27 (Partitioned Matrix)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix. We can partition $\\mathbf{A}$ as\n$$ \\mathbf{A}=\\left(\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right), $$\nwhere $\\mathbf{A}_{11}$ is $m_1 \\times n_1$, $\\mathbf{A}_{12}$ is $m_1 \\times n_2$, $\\mathbf{A}_{21}$ is $m_2 \\times n_1$, $\\mathbf{A}_{22}$ is $m_2 \\times n_2$, and $m_1+m_2=m$ and $n_1+n_2=n$.\nLet $\\mathbf{B}(m \\times n)$ be similarly partitioned into submatrices $\\mathbf{B}_{ij}(i, j=1,2)$. Then,\n$$ \\mathbf{A}+\\mathbf{B}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}+\\mathbf{B}_{11} \u0026amp; \\mathbf{A}_{12}+\\mathbf{B}_{12} \\\\ \\mathbf{A}_{21}+\\mathbf{B}_{21} \u0026amp; \\mathbf{A}_{22}+\\mathbf{B}_{22} \\end{array}\\right) $$\nNow let $\\mathbf{C}(n \\times p)$ be partitioned into submatrices $\\mathbf{C}_{ij}(i, j=1,2)$ such that $\\mathbf{C}_{11}$ has $n_1$ rows (and hence $\\mathbf{C}_{12}$ also has $n_1$ rows and $\\mathbf{C}_{21}$ and $\\mathbf{C}_{22}$ have $n_2$ rows). Then we may postmultiply $\\mathbf{A}$ by $\\mathbf{C}$ yielding\n$$ \\mathbf{A} \\mathbf{C}=\\left(\\begin{array}{cc} \\mathbf{A}_{11} \\mathbf{C}_{11}+\\mathbf{A}_{12} \\mathbf{C}_{21} \u0026amp; \\mathbf{A}_{11} \\mathbf{C}_{12}+\\mathbf{A}_{12} \\mathbf{C}_{22} \\\\ \\mathbf{A}_{21} \\mathbf{C}_{11}+\\mathbf{A}_{22} \\mathbf{C}_{21} \u0026amp; \\mathbf{A}_{21} \\mathbf{C}_{12}+\\mathbf{A}_{22} \\mathbf{C}_{22} \\end{array}\\right) $$\nThe transpose of the matrix $\\mathbf{A}$ given in (28) is\n$$ \\mathbf{A}^{\\mathrm{T}}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{\\mathrm{T}} \u0026amp; \\mathbf{A}_{21}^{\\mathrm{T}} \\\\ \\mathbf{A}_{12}^{\\mathrm{T}} \u0026amp; \\mathbf{A}_{22}^{\\mathrm{T}} \\end{array}\\right) $$\nIf the off-diagonal blocks $\\mathbf{A}_{12}$ and $\\mathbf{A}_{21}$ are both zero, and $\\mathbf{A}_{11}$ and $\\mathbf{A}_{22}$ are square and nonsingular, then $\\mathbf{A}$ is also nonsingular and its inverse is\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{-1} \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; \\mathbf{A}_{22}^{-1} \\end{array}\\right) $$\nMore generally, if $\\mathbf{A}$ as given in (28) is nonsingular and $\\mathbf{D}=\\mathbf{A}_{22}-\\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}$ is also nonsingular, then\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{-1}+\\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{D}^{-1} \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \u0026amp; -\\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1} \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \u0026amp; \\mathbf{D}^{-1} \\end{array}\\right) $$\nAlternatively, if $\\mathbf{A}$ is nonsingular and $\\mathbf{E}=\\mathbf{A}_{11}-\\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21}$ is also nonsingular, then\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{E}^{-1} \u0026amp; -\\mathbf{E}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\\\ -\\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\mathbf{E}^{-1} \u0026amp; \\mathbf{A}_{22}^{-1}+\\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\mathbf{E}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\end{array}\\right) . $$\nOf course, if both $\\mathbf{D}$ and $\\mathbf{E}$ are nonsingular, blocks in (29) and (30) can be interchanged. The results (29) and (30) can be easily extended to a $3 \\times 3$ matrix partition. We only consider the following symmetric case where two of the off-diagonal blocks are null matrices.\nTheorem 0.3 (3x3 Symmetric Partitioned Matrix Inverse)\nIf the matrix\n$$ \\left(\\begin{array}{lll} \\mathbf{A} \u0026amp; \\mathbf{B} \u0026amp; \\mathbf{C} \\\\ \\mathbf{B}^{\\mathrm{T}} \u0026amp; \\mathbf{D} \u0026amp; \\mathbf{0} \\\\ \\mathbf{C}^{\\mathrm{T}} \u0026amp; \\mathbf{0} \u0026amp; \\mathbf{E} \\end{array}\\right) $$\nis symmetric and nonsingular, its inverse is given by\n$$ \\left(\\begin{array}{ccc} \\mathbf{Q}^{-1} \u0026amp; -\\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; -\\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\\\ -\\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \u0026amp; \\mathbf{D}^{-1}+\\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; \\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\\\ -\\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \u0026amp; \\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; \\mathbf{E}^{-1}+\\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\end{array}\\right) $$\nwhere\n$$ \\mathbf{Q}=\\mathbf{A}-\\mathbf{B} \\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}}-\\mathbf{C} \\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} . $$\nProof\nThe proof is left to the reader. ■ As to the determinants of partitioned matrices, we note that\n$$ \\left|\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{0} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right|=\\left|\\mathbf{A}_{11}\\right|\\left|\\mathbf{A}_{22}\\right|=\\left|\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{0} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right| $$\nif both $\\mathbf{A}_{11}$ and $\\mathbf{A}_{22}$ are square matrices.\nExercises:\nFind the determinant and inverse (if it exists) of $$ \\mathbf{B}=\\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{0} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; 1 \\end{array}\\right) . $$\nIf $|\\mathbf{A}| \\neq 0$, prove that $$ \\left|\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right|=\\left(\\alpha-\\mathbf{a}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\\right)|\\mathbf{A}| . $$\nIf $\\alpha \\neq 0$, prove that $$ \\left|\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right|=\\alpha\\left|\\mathbf{A}-(1 / \\alpha) \\mathbf{b} \\mathbf{a}^{\\mathrm{T}}\\right| . $$\n12 - Complex Matrices # If $\\mathbf{A}$ and $\\mathbf{B}$ are real matrices of the same order, then a complex matrix $\\mathbf{Z}$ can be defined as\n$$ \\mathbf{Z}=\\mathbf{A}+i \\mathbf{B} $$\nwhere $i$ denotes the imaginary unit with the property $i^2=-1$. The complex conjugate of $\\mathbf{Z}$, denoted by $\\mathbf{Z}^mathrm{H}$, is defined as\n$$ \\mathbf{Z}^\\mathrm{H}=\\mathbf{A}^{\\mathrm{T}}-i \\mathbf{B}^{\\mathrm{T}} $$\nIf $\\mathbf{Z}$ is real, then $\\mathbf{Z}^\\mathrm{H}=\\mathbf{Z}^{\\mathrm{T}}$. If $\\mathbf{Z}$ is a scalar, say $\\zeta$, we usually write $\\bar{\\zeta}$ instead of $\\zeta^mathrm{H}$.\nA square complex matrix $\\mathbf{Z}$ is said to be Hermitian if $\\mathbf{Z}^{\\mathrm{H}}=\\mathbf{Z}$ (the complex equivalent to a symmetric matrix), skew-Hermitian if $\\mathbf{Z}^{\\mathrm{H}}=-\\mathbf{Z}$ (the complex equivalent to a skew-symmetric matrix), and unitary if $\\mathbf{Z}^{\\mathrm{H}} \\mathbf{Z}=\\mathbf{I}$ (the complex equivalent to an orthogonal matrix).\nWe shall see in this theorem that the eigenvalues of a symmetric matrix are real. In general, however, eigenvalues (and hence eigenvectors) are complex. In this book, complex numbers appear only in connection with eigenvalues and eigenvectors of matrices that are not symmetric (Chapter 8). A detailed treatment is therefore omitted. Matrices and vectors are assumed to be real, unless it is explicitly specified that they are complex.\n13 - Eigenvalues and Eigenvectors # Definition 0.28 (Eigenvalues and Eigenvectors)\nLet $\\mathbf{A}$ be a square matrix, say $n \\times n$. The eigenvalues of $\\mathbf{A}$ are defined as the roots of the characteristic equation\n$$ \\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right|=0 $$\nThe characteristic equation (31) has $n$ roots, in general complex. Let $\\lambda$ be an eigenvalue of $\\mathbf{A}$. Then there exist vectors $\\mathbf{x}$ and $\\mathbf{y}(\\mathbf{x} \\neq \\mathbf{0}, \\mathbf{y} \\neq \\mathbf{0})$ such that\n$$ (\\lambda \\mathbf{I}-\\mathbf{A}) \\mathbf{x}=\\mathbf{0}, \\quad \\mathbf{y}^{\\mathrm{T}}(\\lambda \\mathbf{I}-\\mathbf{A})=\\mathbf{0} $$\nThat is,\n$$ \\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}, \\quad \\mathbf{y}^{\\mathrm{T}} \\mathbf{A}=\\lambda \\mathbf{y}^{\\mathrm{T}} $$\nThe vectors $\\mathbf{x}$ and $\\mathbf{y}$ are called a (column) eigenvector and row eigenvector of $\\mathbf{A}$ associated with the eigenvalue $\\lambda$.\nEigenvectors are usually normalized in some way to make them unique, for example, by $\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}=\\mathbf{y}^{\\mathrm{T}} \\mathbf{y}=1$ (when $\\mathbf{x}$ and $\\mathbf{y}$ are real).\nNot all roots of the characteristic equation need to be different. Each root is counted a number of times equal to its multiplicity. When a root (eigenvalue) appears more than once it is called a multiple eigenvalue; if it appears only once it is called a simple eigenvalue.\nAlthough eigenvalues are in general complex, the eigenvalues of a symmetric matrix are always real.\nTheorem 0.4 (Symmetric Matrix Eigenvalues)\nA symmetric matrix has only real eigenvalues. Proof\nLet $\\lambda$ be an eigenvalue of a symmetric matrix $\\mathbf{A}$ and let $\\mathbf{x}=\\mathbf{u}+i \\mathbf{v}$ be an associated eigenvector. Then,\n$$ \\mathbf{A}(\\mathbf{u}+i \\mathbf{v})=\\lambda(\\mathbf{u}+i \\mathbf{v}) $$\nand hence\n$$ (\\mathbf{u}-i \\mathbf{v})^{\\mathrm{T}} \\mathbf{A}(\\mathbf{u}+i \\mathbf{v})=\\lambda(\\mathbf{u}-i \\mathbf{v})^{\\mathrm{T}}(\\mathbf{u}+i \\mathbf{v}) $$\nwhich leads to\n$$ \\mathbf{u}^{\\mathrm{T}} \\mathbf{A} \\mathbf{u}+\\mathbf{v}^{\\mathrm{T}} \\mathbf{A} \\mathbf{v}=\\lambda\\left(\\mathbf{u}^{\\mathrm{T}} \\mathbf{u}+\\mathbf{v}^{\\mathrm{T}} \\mathbf{v}\\right) $$\nbecause of the symmetry of $\\mathbf{A}$. This implies that $\\lambda$ is real.\n■ Let us prove the following three results, which will be useful to us later.\nTheorem 0.5 (Similar Matrices Eigenvalues)\nIf $\\mathbf{A}$ is an $n \\times n$ matrix and $\\mathbf{G}$ is a nonsingular $n \\times n$ matrix, then $\\mathbf{A}$ and $\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}$ have the same set of eigenvalues (with the same multiplicities). Proof\nFrom\n$$ \\lambda \\mathbf{I}_n-\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}=\\mathbf{G}^{-1}\\left(\\lambda \\mathbf{I}_n-\\mathbf{A}\\right) \\mathbf{G} $$\nwe obtain\n$$ \\left|\\lambda \\mathbf{I}_n-\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}\\right|=\\left|\\mathbf{G}^{-1}\\right|\\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right||\\mathbf{G}|=\\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right| $$\nand the result follows.\n■ Theorem 0.6 (Singular Matrix Zero Eigenvalue)\nA singular matrix has at least one zero eigenvalue. Proof\nIf $\\mathbf{A}$ is singular, then $|\\mathbf{A}|=0$ and hence $|\\lambda \\mathbf{I}-\\mathbf{A}|=0$ for $\\lambda=0$. ■ Theorem 0.7 (Special Matrix Eigenvalues)\nAn idempotent matrix has only eigenvalues 0 or 1. All eigenvalues of a unitary matrix have unit modulus. Proof\nLet $\\mathbf{A}$ be idempotent. Then $\\mathbf{A}^2=\\mathbf{A}$. Thus, if $\\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}$, then\n$$ \\lambda \\mathbf{x}=\\mathbf{A} \\mathbf{x}=\\mathbf{A}(\\mathbf{A} \\mathbf{x})=\\mathbf{A}(\\lambda \\mathbf{x})=\\lambda(\\mathbf{A} \\mathbf{x})=\\lambda^2 \\mathbf{x} $$\nand hence $\\lambda=\\lambda^2$, which implies $\\lambda=0$ or $\\lambda=1$.\nIf $\\mathbf{A}$ is unitary, then $\\mathbf{A}^{\\mathrm{H}} \\mathbf{A}=\\mathbf{I}$. Thus, if $\\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}$, then\n$$ \\mathbf{x}^{\\mathrm{H}} \\mathbf{A}^{\\mathrm{H}}=\\bar{\\lambda} \\mathbf{x}^{\\mathrm{H}} $$\nusing the notation of Section 1.12. Hence,\n$$ \\mathbf{x}^{\\mathrm{H}} \\mathbf{x}=\\mathbf{x}^{\\mathrm{H}} \\mathbf{A}^{\\mathrm{H}} \\mathbf{A} \\mathbf{x}=\\bar{\\lambda} \\lambda \\mathbf{x}^{\\mathrm{H}} \\mathbf{x} $$\nSince $\\mathbf{x}^{\\mathrm{H}} \\mathbf{x} \\neq 0$, we obtain $\\bar{\\lambda} \\lambda=1$ and hence $|\\lambda|=1$.\n■ An important theorem regarding positive definite matrices is stated below.\nTheorem 0.8 (Positive Definite Eigenvalues)\nA symmetric matrix is positive definite if and only if all its eigenvalues are positive. Proof\nIf $\\mathbf{A}$ is positive definite and $\\mathbf{A}\\mathbf{x}=\\lambda \\mathbf{x}$, then $\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}^\\mathrm{T} \\mathbf{x}$. Now, $\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}\u0026gt;0$ and $\\mathbf{x}^\\mathrm{T} \\mathbf{x}\u0026gt;0$ imply $\\lambda\u0026gt;0$. The converse will not be proved here. (It follows from this theorem.) ■ Next, let us prove this theorem.\nTheorem 0.9 (Eigenvalue Identity)\nLet $\\mathbf{A}$ be $m \\times n$ and let $\\mathbf{B}$ be $n \\times m(n \\geq m)$. Then the nonzero eigenvalues of $\\mathbf{B}\\mathbf{A}$ and $\\mathbf{A}\\mathbf{B}$ are identical, and $\\left|I_{m}-\\mathbf{A}\\mathbf{B}\\right|=\\left|I_{n}-\\mathbf{B}\\mathbf{A}\\right|$. Proof\nTaking determinants on both sides of the equality\n\\begin{equation} \\left(\\begin{array}{cc} I_{m}-\\mathbf{A}\\mathbf{B} \u0026amp; \\mathbf{A} \\\\ 0 \u0026amp; I_{n} \\end{array}\\right)\\left(\\begin{array}{cc} I_{m} \u0026amp; 0 \\\\ \\mathbf{B} \u0026amp; I_{n} \\end{array}\\right)=\\left(\\begin{array}{cc} I_{m} \u0026amp; 0 \\\\ \\mathbf{B} \u0026amp; I_{n} \\end{array}\\right)\\left(\\begin{array}{cc} I_{m} \u0026amp; \\mathbf{A} \\\\ 0 \u0026amp; I_{n}-\\mathbf{B}\\mathbf{A} \\end{array}\\right), \\end{equation}\nwe obtain\n\\begin{equation} \\left|I_{m}-\\mathbf{A}\\mathbf{B}\\right|=\\left|I_{n}-\\mathbf{B}\\mathbf{A}\\right| . \\end{equation}\nNow let $\\lambda \\neq 0$. Then,\n\\begin{equation} \\begin{aligned} \\left|\\lambda I_{n}-\\mathbf{B}\\mathbf{A}\\right| \u0026amp; =\\lambda^{n}\\left|I_{n}-\\mathbf{B}\\left(\\lambda^{-1} \\mathbf{A}\\right)\\right| \\\\ \u0026amp; =\\lambda^{n}\\left|I_{m}-\\left(\\lambda^{-1} \\mathbf{A}\\right) \\mathbf{B}\\right|=\\lambda^{n-m}\\left|\\lambda I_{m}-\\mathbf{A}\\mathbf{B}\\right| . \\end{aligned} \\end{equation}\nHence, the nonzero eigenvalues of $\\mathbf{B}\\mathbf{A}$ are the same as the nonzero eigenvalues of $\\mathbf{A}\\mathbf{B}$, and this is equivalent to the statement in the theorem.\n■ Without proof we state the following famous result.\nTheorem 0.10 (Cayley-Hamilton)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$. Then,\n\\begin{equation} \\prod_{i=1}^{n}\\left(\\lambda_{i} I_{n}-\\mathbf{A}\\right)=0 . \\end{equation}\nFinally, we present the following result on eigenvectors.\nTheorem 0.11 (Linear Independence of Eigenvectors)\nEigenvectors associated with distinct eigenvalues are linearly independent. Proof\nLet $\\mathbf{A}\\mathbf{x}_{1}=\\lambda_{1} \\mathbf{x}_{1}$, $\\mathbf{A}\\mathbf{x}_{2}=\\lambda_{2} \\mathbf{x}_{2}$, and $\\lambda_{1} \\neq \\lambda_{2}$. Assume that $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ are linearly dependent. Then there is an $\\alpha \\neq 0$ such that $\\mathbf{x}_{2}=\\alpha \\mathbf{x}_{1}$, and hence\n\\begin{equation} \\alpha \\lambda_{1} \\mathbf{x}_{1}=\\alpha \\mathbf{A} \\mathbf{x}_{1}=\\mathbf{A} \\mathbf{x}_{2}=\\lambda_{2} \\mathbf{x}_{2}=\\alpha \\lambda_{2} \\mathbf{x}_{1} \\end{equation}\nthat is\n\\begin{equation} \\alpha\\left(\\lambda_{1}-\\lambda_{2}\\right) \\mathbf{x}_{1}=0 \\end{equation}\nSince $\\alpha \\neq 0$ and $\\lambda_{1} \\neq \\lambda_{2}$, this implies that $\\mathbf{x}_{1}=0$, a contradiction.\n■ Exercices\nShow that \\begin{equation} \\left|\\begin{array}{ll} 0 \u0026amp; I_{m} \\\\ I_{m} \u0026amp; 0 \\end{array}\\right|=(-1)^{m} \\end{equation}\nShow that, for $n=2$, \\begin{equation} |I+\\epsilon \\mathbf{A}|=1+\\epsilon \\operatorname{tr} \\mathbf{A}+\\epsilon^{2}|\\mathbf{A}| \\end{equation}\nShow that, for $n=3$, \\begin{equation} |I+\\epsilon \\mathbf{A}|=1+\\epsilon \\operatorname{tr} \\mathbf{A}+\\frac{\\epsilon^{2}}{2}\\left((\\operatorname{tr} \\mathbf{A})^{2}-\\operatorname{tr} \\mathbf{A}^{2}\\right)+\\epsilon^{3}|\\mathbf{A}| \\end{equation}\n14 - Schur\u0026rsquo;s decomposition theorem # In the next few sections, we present three decomposition theorems: Schur\u0026rsquo;s theorem, Jordan\u0026rsquo;s theorem, and the singular-value decomposition. Each of these theorems will prove useful later in this book. We first state Schur\u0026rsquo;s theorem.\nTheorem 0.12 (Schur decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix, possibly complex. Then there exist a unitary $n \\times n$ matrix $\\mathbf{S}$ (that is, $\\mathbf{S}^\\mathrm{H} \\mathbf{S}=I_{n}$ ) and an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M} \\end{equation}\nThe most important special case of Schur\u0026rsquo;s decomposition theorem is the case where $\\mathbf{A}$ is symmetric.\nTheorem 0.13 (Symmetric Matrix Decomposition)\nLet $\\mathbf{A}$ be a symmetric $n \\times n$ matrix. Then there exist an orthogonal $n \\times n$ matrix $\\mathbf{S}$ (that is, $\\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{n}$ ) whose columns are eigenvectors of $\\mathbf{A}$ and a diagonal matrix $\\boldsymbol{\\Lambda}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{S}^\\mathrm{T} \\mathbf{A} \\mathbf{S}=\\boldsymbol{\\Lambda} \\end{equation}\nProof\nUsing Theorem 0.12, there exists a unitary matrix $\\mathbf{S}=\\mathbf{R}+i \\mathbf{T}$ with real $\\mathbf{R}$ and $\\mathbf{T}$ and an upper triangular matrix $\\mathbf{M}$ such that $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. Then,\n\\begin{equation} \\begin{aligned} \\mathbf{M} \u0026amp; =\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=(\\mathbf{R}-i \\mathbf{T})^\\mathrm{T} \\mathbf{A}(\\mathbf{R}+i \\mathbf{T}) \\\\ \u0026amp; =\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T}\\right)+i\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{T}-\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{R}\\right) \\end{aligned} \\end{equation}\nand hence, using the symmetry of $\\mathbf{A}$,\n\\begin{equation} \\mathbf{M}+\\mathbf{M}^\\mathrm{T}=2\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T}\\right) . \\end{equation}\nIt follows that $\\mathbf{M}+\\mathbf{M}^\\mathrm{T}$ is a real matrix and hence, since $\\mathbf{M}$ is triangular, that $\\mathbf{M}$ is a real matrix. We thus obtain, from (32),\n\\begin{equation} \\mathbf{M}=\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T} . \\end{equation}\nSince $\\mathbf{A}$ is symmetric, $\\mathbf{M}$ is symmetric. But, since $\\mathbf{M}$ is also triangular, $\\mathbf{M}$ must be diagonal. The columns of $\\mathbf{S}$ are then eigenvectors of $\\mathbf{A}$, and since the diagonal elements of $\\mathbf{M}$ are real, $\\mathbf{S}$ can be chosen to be real as well.\n■ Exercices\nLet $\\mathbf{A}$ be a symmetric $n \\times n$ matrix with eigenvalues $\\lambda_{1} \\leq \\lambda_{2} \\leq \\cdots \\leq \\lambda_{n}$. Use Theorem 0.13 to prove that \\begin{equation} \\lambda_{1} \\leq \\frac{\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}}{\\mathbf{x}^\\mathrm{T} \\mathbf{x}} \\leq \\lambda_{n} . \\end{equation}\nHence show that, for any $m \\times n$ matrix $\\mathbf{A}$, \\begin{equation} |\\mathbf{A} \\mathbf{x}| \\leq \\mu|\\mathbf{x}|, \\end{equation}\nwhere $\\mu^{2}$ denotes the largest eigenvalue of $\\mathbf{A}^\\mathrm{T} \\mathbf{A}$.\nLet $\\mathbf{A}$ be an $m \\times n$ matrix of rank $r$. Show that there exists an $n \\times(n-r)$ matrix $\\mathbf{S}$ such that \\begin{equation} \\mathbf{A} \\mathbf{S}=0, \\quad \\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{n-r} \\end{equation}\nLet $\\mathbf{A}$ be an $m \\times n$ matrix of rank $r$. Let $\\mathbf{S}$ be a matrix such that $\\mathbf{A} \\mathbf{S}=0$. Show that $r(\\mathbf{S}) \\leq n-r$. 15 - The Jordan decomposition # Schur\u0026rsquo;s theorem tells us that there exists, for every square matrix $\\mathbf{A}$, a unitary (possibly orthogonal) matrix $\\mathbf{S}$ which \u0026rsquo;transforms\u0026rsquo; $\\mathbf{A}$ into an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$.\nJordan\u0026rsquo;s theorem similarly states that there exists a nonsingular matrix, say $\\mathbf{T}$, which transforms $\\mathbf{A}$ into an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$. The difference between the two decomposition theorems is that in Jordan\u0026rsquo;s theorem less structure is put on the matrix $\\mathbf{T}$ (nonsingular, but not necessarily unitary) and more structure on the matrix $\\mathbf{M}$.\nTheorem 0.14 (Jordan decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix and denote by $\\mathbf{J}_{k}(\\lambda)$ a $k \\times k$ matrix of the form\n\\begin{equation} \\mathbf{J}_{k}(\\lambda)=\\left(\\begin{array}{cccccc} \\lambda \u0026amp; 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\lambda \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; \\lambda \\end{array}\\right) \\end{equation}\n(a so-called Jordan block), where $\\mathbf{J}_{1}(\\lambda)=\\lambda$. Then there exists a nonsingular $n \\times n$ matrix $\\mathbf{T}$ such that\n\\begin{equation} \\mathbf{T}^{-1} \\mathbf{A} \\mathbf{T}=\\left(\\begin{array}{cccc} \\mathbf{J}_{k_{1}}\\left(\\lambda_{1}\\right) \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; \\mathbf{J}_{k_{2}}\\left(\\lambda_{2}\\right) \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\mathbf{J}_{k_{r}}\\left(\\lambda_{r}\\right) \\end{array}\\right) \\end{equation}\nwith $k_{1}+k_{2}+\\cdots+k_{r}=n$. The $\\lambda_{i}$ are the eigenvalues of $\\mathbf{A}$, not necessarily distinct.\nThe most important special case of Theorem 0.14 is this theorem.\nTheorem 0.15 (Distinct Eigenvalues Decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix with distinct eigenvalues. Then there exist a nonsingular $n \\times n$ matrix $\\mathbf{T}$ and a diagonal $n \\times n$ matrix $\\boldsymbol{\\Lambda}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{T}^{-1} \\mathbf{A} \\mathbf{T}=\\boldsymbol{\\Lambda} \\end{equation}\nProof\nImmediate from Theorem 0.14 (or Theorem 0.11). ■ Exercices\nShow that $\\left(\\lambda I_{k}-\\mathbf{J}_{k}(\\lambda)\\right)^{k}=0$ and use this fact to prove Theorem 0.10. Show that Theorem 0.15 remains valid when $\\mathbf{A}$ is complex. 16 - The singular value decomposition # The third important decomposition theorem is the singular-value decomposition.\nTheorem 0.16 (Singular-value decomposition)\nLet $\\mathbf{A}$ be a real $m \\times n$ matrix with $r(\\mathbf{A})=r\u0026gt;0$. Then there exist an $m \\times r$ matrix $\\mathbf{S}$ such that $\\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{r}$, an $n \\times r$ matrix $\\mathbf{T}$ such that $\\mathbf{T}^\\mathrm{T} \\mathbf{T}=I_{r}$ and an $r \\times r$ diagonal matrix $\\boldsymbol{\\Lambda}$ with positive diagonal elements, such that\n\\begin{equation} \\mathbf{A}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{T}^\\mathrm{T} \\end{equation}\nProof\nSince $\\mathbf{A} \\mathbf{A}^\\mathrm{T}$ is an $m \\times m$ positive semidefinite matrix of rank $r$ (by (6)), its nonzero eigenvalues are all positive (this theorem). From Theorem 0.13 we know that there exists an orthogonal $m \\times m$ matrix $( \\mathbf{S}: \\mathbf{S}_{2})$ such that\n\\begin{equation} \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}, \\quad \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}_{2}=0, \\quad \\mathbf{S} \\mathbf{S}^\\mathrm{T}+\\mathbf{S}_{2} \\mathbf{S}_{2}^\\mathrm{T}=I_{m} \\end{equation}\nwhere $\\boldsymbol{\\Lambda}$ is an $r \\times r$ diagonal matrix having these $r$ positive eigenvalues as its diagonal elements. Define $\\mathbf{T}=\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}$. Then we see that\n\\begin{equation} \\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda}, \\quad \\mathbf{T}^\\mathrm{T} \\mathbf{T}=I_{r} \\label{eq:33} \\end{equation}\nThus, since $\\mathbf{A}^\\mathrm{T} \\mathbf{S}_{2}=0$, we have\n\\begin{equation} \\mathbf{A}=\\left(\\mathbf{S} \\mathbf{S}^\\mathrm{T}+\\mathbf{S}_{2} \\mathbf{S}_{2}^\\mathrm{T}\\right) \\mathbf{A}=\\mathbf{S} \\mathbf{S}^\\mathrm{T} \\mathbf{A}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2}\\left(\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}\\right)^\\mathrm{T}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{T}^\\mathrm{T} \\label{eq:34} \\end{equation}\nwhich concludes the proof.\n■ We see from \\eqref{eq:33} and \\eqref{eq:34} that the semi-orthogonal matrices $\\mathbf{S}$ and $\\mathbf{T}$ satisfy\n\\begin{equation} \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}, \\quad \\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda} \\end{equation}\nHence, $\\boldsymbol{\\Lambda}$ contains the $r$ nonzero eigenvalues of $\\mathbf{A} \\mathbf{A}^\\mathrm{T}$ (and of $\\mathbf{A}^\\mathrm{T} \\mathbf{A}$ ) and $\\mathbf{S}$ (by construction) and $\\mathbf{T}$ contain corresponding eigenvectors. A common mistake in applying the singular-value decomposition is to find $\\mathbf{S}$, $\\mathbf{T}$, and $\\boldsymbol{\\Lambda}$ from (35). This is incorrect because, given $\\mathbf{S}$, $\\mathbf{T}$ is not unique. The correct procedure is to find $\\mathbf{S}$ and $\\boldsymbol{\\Lambda}$ from $\\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}$ and then define $\\mathbf{T}=\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}$. Alternatively, we can find $\\mathbf{T}$ and $\\boldsymbol{\\Lambda}$ from $\\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda}$ and define $\\mathbf{S}=\\mathbf{A} \\mathbf{T} \\boldsymbol{\\Lambda}^{-1 / 2}$.\n17 - Further results concerning eigenvalues # Let us now prove the following five theorems, all of which concern eigenvalues. this theorem deals with the sum and the product of the eigenvalues. this theorem and this theorem discuss the relationship between the rank and the number of nonzero eigenvalues, and this theorem concerns idempotent matrices.\nTheorem 0.17 (Trace and Determinant)\nLet $\\mathbf{A}$ be a square, possibly complex, $n \\times n$ matrix with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$. Then,\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\sum_{i=1}^{n} \\lambda_{i}, \\quad |\\mathbf{A}|=\\prod_{i=1}^{n} \\lambda_{i} \\end{equation}\nProof\nWe write, using Theorem 0.12, $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. Then,\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\operatorname{tr} \\mathbf{S} \\mathbf{M} \\mathbf{S}^\\mathrm{H}=\\operatorname{tr} \\mathbf{M} \\mathbf{S}^\\mathrm{H} \\mathbf{S}=\\operatorname{tr} \\mathbf{M}=\\sum_{i} \\lambda_{i} \\end{equation}\nand\n\\begin{equation} |\\mathbf{A}|=\\left|\\mathbf{S} \\mathbf{M} \\mathbf{S}^\\mathrm{H}\\right|=|\\mathbf{S}||\\mathbf{M}|\\left|\\mathbf{S}^\\mathrm{H}\\right|=|\\mathbf{M}|=\\prod_{i} \\lambda_{i} \\end{equation}\nand the result follows.\n■ Theorem 0.18 (Rank and Nonzero Eigenvalues)\nIf $\\mathbf{A}$ has $r$ nonzero eigenvalues, then $r(\\mathbf{A}) \\geq r$. Proof\nWe write again, using Theorem 0.12, $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. We partition\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) \\end{equation}\nwhere $\\mathbf{M}_{1}$ is a nonsingular upper triangular $r \\times r$ matrix and $\\mathbf{M}_{3}$ is strictly upper triangular. Since $r(\\mathbf{A})=r(\\mathbf{M}) \\geq r\\left(\\mathbf{M}_{1}\\right)=r$, the result follows.\n■ The following example shows that it is indeed possible that $r(\\mathbf{A})\u0026gt;r$. Let\n\\begin{equation} \\mathbf{A}=\\left(\\begin{array}{ll} 1 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \\end{array}\\right) \\end{equation}\nThen $r(\\mathbf{A})=1$ and both eigenvalues of $\\mathbf{A}$ are zero.\nTheorem 0.19 (Simple Eigenvalue Rank)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix. If $\\lambda$ is a simple eigenvalue of $\\mathbf{A}$, then $r(\\lambda I-\\mathbf{A})=n-1$. Conversely, if $r(\\lambda I-\\mathbf{A})=n-1$, then $\\lambda$ is an eigenvalue of $\\mathbf{A}$, but not necessarily a simple eigenvalue. Proof\nLet $\\lambda_{1}, \\ldots, \\lambda_{n}$ be the eigenvalues of $\\mathbf{A}$. Then $\\mathbf{B}=\\lambda I-\\mathbf{A}$ has eigenvalues $\\lambda-\\lambda_{i}(i=1, \\ldots, n)$, and since $\\lambda$ is a simple eigenvalue of $\\mathbf{A}$, $\\mathbf{B}$ has a simple eigenvalue zero. Hence, $r(\\mathbf{B}) \\leq n-1$. Also, since $\\mathbf{B}$ has $n-1$ nonzero eigenvalues, $r(\\mathbf{B}) \\geq n-1$ (Theorem 0.18). Hence $r(\\mathbf{B})=n-1$. Conversely, if $r(\\mathbf{B})=n-1$, then $\\mathbf{B}$ has at least one zero eigenvalue and hence $\\lambda=\\lambda_{i}$ for at least one $i$. ■ Definition 0.29 (Simple Zero Eigenvalue Corollary)\nAn $n \\times n$ matrix with a simple zero eigenvalue has rank $n-1$. Theorem 0.20 (Symmetric Matrix Rank and Eigenvalues)\nIf $\\mathbf{A}$ is a symmetric matrix with $r$ nonzero eigenvalues, then $r(\\mathbf{A})=r$. Proof\nUsing Theorem 0.13, we have $\\mathbf{S}^\\mathrm{T} \\mathbf{A} \\mathbf{S}=\\boldsymbol{\\Lambda}$ and hence\n\\begin{equation} r(\\mathbf{A})=r\\left(\\mathbf{S} \\boldsymbol{\\Lambda} \\mathbf{S}^\\mathrm{T}\\right)=r(\\boldsymbol{\\Lambda})=r, \\end{equation}\nand the result follows.\n■ Theorem 0.21 (Idempotent Matrix Properties)\nIf $\\mathbf{A}$ is an idempotent matrix, possibly complex, with $r$ eigenvalues equal to one, then $r(\\mathbf{A})=\\operatorname{tr} \\mathbf{A}=r$. Proof\nBy this theorem, $\\mathbf{S}^{*} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$ (upper triangular), where\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) \\end{equation}\nwith $\\mathbf{M}_{1}$ a unit upper triangular $r \\times r$ matrix and $\\mathbf{M}_{3}$ a strictly upper triangular matrix. Since $\\mathbf{A}$ is idempotent, so is $\\mathbf{M}$ and hence\n\\begin{equation} \\left(\\begin{array}{cc} \\mathbf{M}_{1}^{2} \u0026amp; \\mathbf{M}_{1} \\mathbf{M}_{2}+\\mathbf{M}_{2} \\mathbf{M}_{3} \\\\ 0 \u0026amp; \\mathbf{M}_{3}^{2} \\end{array}\\right)=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) . \\end{equation}\nThis implies that $\\mathbf{M}_{1}$ is idempotent; it is nonsingular, hence $\\mathbf{M}_{1}=\\mathbf{I}_{r}$ (see Exercise 1 below). Also, $\\mathbf{M}_{3}$ is idempotent and all its eigenvalues are zero, hence $\\mathbf{M}_{3}=0$ (see Exercise 2 below), so that\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{I}_{r} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; 0 \\end{array}\\right) \\end{equation}\nHence,\n\\begin{equation} r(\\mathbf{A})=r(\\mathbf{M})=r \\end{equation}\nAlso, by:\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\text { sum of eigenvalues of } \\mathbf{A}=r, \\end{equation}\nthus completing the proof.\n■ We note that in, the matrix $\\mathbf{A}$ is not required to be symmetric. If $\\mathbf{A}$ is idempotent and symmetric, then it is positive semidefinite. Since its eigenvalues are only 0 and 1 and its rank equals $r$, it that $\\mathbf{A}$ can be written as\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{P}^{\\mathrm{T}} \\mathbf{P}=\\mathbf{I}_{r} \\end{equation}\nExercises\nThe only nonsingular idempotent matrix is the identity matrix. The only idempotent matrix whose eigenvalues are all zero is the null matrix. If $\\mathbf{A}$ is a positive semidefinite $n \\times n$ matrix with $r(\\mathbf{A})=r$, then there exists an $n \\times r$ matrix $\\mathbf{P}$ such that \\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{P}^{\\mathrm{T}} \\mathbf{P}=\\mathbf{\\Lambda} \\end{equation}\nwhere $\\mathbf{\\Lambda}$ is an $r \\times r$ diagonal matrix containing the positive eigenvalues of $\\mathbf{A}$.\nPositive (semi)definite matrices # Positive (semi)definite matrices were introduced in Section 1.6. We have already seen that $\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}$ and $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}$ are both positive semidefinite and that the eigenvalues of a positive (semi)definite matrix are all positive (nonnegative). We now present some more properties of positive (semi)definite matrices.\nTheorem 0.22 (Determinant inequality for positive definite matrices)\nLet $\\mathbf{A}$ be positive definite and $\\mathbf{B}$ positive semidefinite. Then,\n\\begin{equation} |\\mathbf{A}+\\mathbf{B}| \\geq|\\mathbf{A}| \\end{equation}\nwith equality if and only if $\\mathbf{B}=0$.\nProof\nLet $\\mathbf{\\Lambda}$ be a positive definite diagonal matrix such that\n\\begin{equation} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A} \\mathbf{S}=\\mathbf{\\Lambda}, \\quad \\mathbf{S}^{\\mathrm{T}} \\mathbf{S}=\\mathbf{I} . \\end{equation}\nThen, $\\mathbf{S} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{I}$ and\n\\begin{equation} \\mathbf{A}+\\mathbf{B}=\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2}\\left(\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right) \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}} . \\end{equation}\nHence, using determinant results,\n\\begin{equation} \\begin{aligned} |\\mathbf{A}+\\mathbf{B}| \u0026amp; =\\left|\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2}\\right|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right|\\left|\\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right| \\\\ \u0026amp; =\\left|\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right| \\\\ \u0026amp; =|\\mathbf{A}|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right| \\end{aligned} \\end{equation}\nIf $\\mathbf{B}=0$ then $|\\mathbf{A}+\\mathbf{B}|=|\\mathbf{A}|$. If $\\mathbf{B} \\neq 0$, then the matrix $\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}$ will be positive semidefinite with at least one positive eigenvalue. Hence we have $\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right|\u0026gt;1$ and $|\\mathbf{A}+\\mathbf{B}|\u0026gt;|\\mathbf{A}|$.\n■ Theorem 0.23 (Simultaneous diagonalization)\nLet $\\mathbf{A}$ be positive definite and $\\mathbf{B}$ symmetric of the same order. Then there exist a nonsingular matrix $\\mathbf{P}$ and a diagonal matrix $\\mathbf{\\Lambda}$ such that\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}=\\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}} \\end{equation}\nIf $\\mathbf{B}$ is positive semidefinite, then so is $\\mathbf{\\Lambda}$.\nProof\nLet $\\mathbf{C}=\\mathbf{A}^{-1 / 2} \\mathbf{B} \\mathbf{A}^{-1 / 2}$. Since $\\mathbf{C}$ is symmetric, there exist an orthogonal matrix $\\mathbf{S}$ and a diagonal matrix $\\mathbf{\\Lambda}$ such that\n\\begin{equation} \\mathbf{S}^{\\mathrm{T}} \\mathbf{C} \\mathbf{S}=\\mathbf{\\Lambda}, \\quad \\mathbf{S}^{\\mathrm{T}} \\mathbf{S}=\\mathbf{I} \\end{equation}\nNow define $\\mathbf{P}=\\mathbf{A}^{1 / 2} \\mathbf{S}$. Then,\n\\begin{equation} \\mathbf{P} \\mathbf{P}^{\\mathrm{T}}=\\mathbf{A}^{1 / 2} \\mathbf{S} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{A} \\end{equation}\nand\n\\begin{equation} \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}}=\\mathbf{A}^{1 / 2} \\mathbf{S} \\mathbf{\\Lambda} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{C} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{A}^{-1 / 2} \\mathbf{B} \\mathbf{A}^{-1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{B} . \\end{equation}\nIf $\\mathbf{B}$ is positive semidefinite, then so is $\\mathbf{C}$ and so is $\\mathbf{\\Lambda}$.\n■ For two symmetric matrices $\\mathbf{A}$ and $\\mathbf{B}$, we shall write $\\mathbf{A} \\geq \\mathbf{B}$ (or $\\mathbf{B} \\leq \\mathbf{A}$ ) if $\\mathbf{A}-\\mathbf{B}$ is positive semidefinite, and $\\mathbf{A}\u0026gt;\\mathbf{B}$ (or $\\mathbf{B}\u0026lt;\\mathbf{A}$ ) if $\\mathbf{A}-\\mathbf{B}$ is positive definite.\nTheorem 0.24 (Inverse order for positive definite matrices)\nLet $\\mathbf{A}$ and $\\mathbf{B}$ be positive definite $n \\times n$ matrices. Then $\\mathbf{A}\u0026gt;\\mathbf{B}$ if and only if $\\mathbf{B}^{-1}\u0026gt;\\mathbf{A}^{-1}$. Proof\nBy Theorem 0.23, there exist a nonsingular matrix $\\mathbf{P}$ and a positive definite diagonal matrix $\\mathbf{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n}\\right)$ such that\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}=\\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}} \\end{equation}\nThen,\n\\begin{equation} \\mathbf{A}-\\mathbf{B}=\\mathbf{P}(\\mathbf{I}-\\mathbf{\\Lambda}) \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}^{-1}-\\mathbf{A}^{-1}=\\mathbf{P}^{\\mathrm{T}-1}\\left(\\mathbf{\\Lambda}^{-1}-\\mathbf{I}\\right) \\mathbf{P}^{-1} . \\end{equation}\nIf $\\mathbf{A}-\\mathbf{B}$ is positive definite, then $\\mathbf{I}-\\mathbf{\\Lambda}$ is positive definite and hence $0\u0026lt;\\lambda_{i}\u0026lt;$ $1(i=1, \\ldots, n)$. This implies that $\\mathbf{\\Lambda}^{-1}-\\mathbf{I}$ is positive definite and hence that $\\mathbf{B}^{-1}-\\mathbf{A}^{-1}$ is positive definite.\n■ Theorem 0.25 (Determinant monotonicity)\nLet $\\mathbf{A}$ and $\\mathbf{B}$ be positive definite matrices such that $\\mathbf{A}-\\mathbf{B}$ is positive semidefinite. Then, $|\\mathbf{A}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{A}=\\mathbf{B}$. Proof\nLet $\\mathbf{C}=\\mathbf{A}-\\mathbf{B}$. Then $\\mathbf{A}=\\mathbf{B}+\\mathbf{C}$, where $\\mathbf{B}$ is positive definite and $\\mathbf{C}$ is positive semidefinite. Thus, by Theorem 0.22, $|\\mathbf{B}+\\mathbf{C}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{C}=0$, that is, $|\\mathbf{A}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{A}=\\mathbf{B}$. ■ A useful special case of Theorem 0.25 is this theorem.\nTheorem 0.26 (Identity characterization)\nLet $\\mathbf{A}$ be positive definite with $|\\mathbf{A}|=1$. If $\\mathbf{I}-\\mathbf{A}$ is also positive semidefinite, then $\\mathbf{A}=\\mathbf{I}$. Proof\nThis follows immediately from Theorem 0.25. ■ Three further results for positive definite matrices # Let us now prove this theorem.\nTheorem 0.27 (Block matrix determinant and positive definiteness)\nLet $\\mathbf{A}$ be a positive definite $n \\times n$ matrix, and let $\\mathbf{B}$ be the $(n+1) \\times(n+1)$ matrix\n\\begin{equation} \\mathbf{B}=\\left(\\begin{array}{ll} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{b}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right) \\end{equation}\nThen, (i) $|\\mathbf{B}| \\leq \\alpha|\\mathbf{A}|$ with equality if and only if $\\mathbf{b}=0$; and (ii) $\\mathbf{B}$ is positive definite if and only if $|\\mathbf{B}|\u0026gt;0$.\nProof\nDefine the $(n+1) \\times(n+1)$ matrix\n\\begin{equation} \\mathbf{P}=\\left(\\begin{array}{cc} \\mathbf{I}_{n} \u0026amp; -\\mathbf{A}^{-1} \\mathbf{b} \\\\ \\mathbf{0}^{\\mathrm{T}} \u0026amp; 1 \\end{array}\\right) \\end{equation}\nThen,\n\\begin{equation} \\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}=\\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; 0 \\\\ \\mathbf{0}^{\\mathrm{T}} \u0026amp; \\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{array}\\right) \\end{equation}\nso that\n\\begin{equation} |\\mathbf{B}|=\\left|\\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}\\right|=|\\mathbf{A}|\\left(\\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\\right) . \\label{eq:det_block} \\end{equation}\n(Compare Exercise 2 in Section 1.11.) Then (i) is an immediate consequence of \\eqref{eq:det_block}. To prove (ii) we note that $|\\mathbf{B}|\u0026gt;0$ if and only if $\\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\u0026gt;0$ (from \\eqref{eq:det_block}), which is the case if and only if $\\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}$ is positive definite (from the previous equation). This in turn is true if and only if $\\mathbf{B}$ is positive definite.\n■ An immediate consequence of Theorem 0.27, proved by induction, is the following.\nTheorem 0.28 (Hadamard\u0026#39;s inequality)\nIf $\\mathbf{A}=\\left(a_{ij}\\right)$ is a positive definite $n \\times n$ matrix, then\n\\begin{equation} |\\mathbf{A}| \\leq \\prod_{i=1}^{n} a_{ii} \\end{equation}\nwith equality if and only if $\\mathbf{A}$ is diagonal.\nAnother consequence of Theorem 0.27 is this theorem.\nTheorem 0.29 (Principal minor test)\nA symmetric $n \\times n$ matrix $\\mathbf{A}$ is positive definite if and only if all principal minors $\\left|\\mathbf{A}_{k}\\right|(k=1, \\ldots, n)$ are positive. Note. The $k \\times k$ matrix $\\mathbf{A}_{k}$ is obtained from $\\mathbf{A}$ by deleting the last $n-k$ rows and columns of $\\mathbf{A}$. Notice that $\\mathbf{A}_{n}=\\mathbf{A}$.\nProof\nLet $\\mathbf{E}_{k}=\\left(\\mathbf{I}_{k}: 0\\right)$ be a $k \\times n$ matrix, so that $\\mathbf{A}_{k}=\\mathbf{E}_{k} \\mathbf{A} \\mathbf{E}_{k}^{\\mathrm{T}}$. Let $\\mathbf{y}$ be an arbitrary $k \\times 1$ vector, $\\mathbf{y} \\neq 0$. Then,\n\\begin{equation} \\mathbf{y}^{\\mathrm{T}} \\mathbf{A}_{k} \\mathbf{y}=\\left(\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y}\\right)^{\\mathrm{T}} \\mathbf{A}\\left(\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y}\\right)\u0026gt;0 \\end{equation}\nsince $\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y} \\neq 0$ and $\\mathbf{A}$ is positive definite. Hence, $\\mathbf{A}_{k}$ is positive definite and, in particular, $\\left|\\mathbf{A}_{k}\\right|\u0026gt;0$. The converse follows by repeated application of Theorem 0.27(ii).\n■ Exercises\nIf $\\mathbf{A}$ is positive definite show that the matrix \\begin{equation} \\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{b}^{\\mathrm{T}} \u0026amp; \\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{array}\\right) \\end{equation}\nis positive semidefinite and singular, and find the eigenvector associated with the zero eigenvalue.\nHence show that, for positive definite $\\mathbf{A}$, \\begin{equation} \\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}-2 \\mathbf{b}^{\\mathrm{T}} \\mathbf{x} \\geq-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{equation}\nfor every $\\mathbf{x}$, with equality if and only if $\\mathbf{x}=\\mathbf{A}^{-1} \\mathbf{b}$.\nA useful result # If $\\mathbf{A}$ is a positive definite $n \\times n$ matrix, then, in accordance with Theorem 0.28,\n\\begin{equation} |\\mathbf{A}|=\\prod_{i=1}^{n} a_{ii} \\label{eq:diagonal_det} \\end{equation}\nif and only if $\\mathbf{A}$ is diagonal. If $\\mathbf{A}$ is merely symmetric, then \\eqref{eq:diagonal_det}, while obviously necessary, is no longer sufficient for the diagonality of $\\mathbf{A}$. For example, the matrix\n\\begin{equation} \\mathbf{A}=\\left(\\begin{array}{lll} 2 \u0026amp; 3 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 3 \u0026amp; 2 \\end{array}\\right) \\end{equation}\nhas determinant $|\\mathbf{A}|=8$ (its eigenvalues are $-1,-1$, and 8 ), thus satisfying \\eqref{eq:diagonal_det}, but $\\mathbf{A}$ is not diagonal.\nthis theorem gives a necessary and sufficient condition for the diagonality of a symmetric matrix.\nTheorem 0.30 (Diagonal matrix characterization)\nA symmetric matrix is diagonal if and only if its eigenvalues and its diagonal elements coincide. Proof\nLet $\\mathbf{A}=\\left(a_{ij}\\right)$ be a symmetric $n \\times n$ matrix. The \u0026lsquo;only if\u0026rsquo; part of the theorem is trivial. To prove the \u0026lsquo;if\u0026rsquo; part, assume that $\\lambda_{i}(\\mathbf{A})=a_{ii}, i=1, \\ldots, n$, and consider the matrix\n\\begin{equation} \\mathbf{B}=\\mathbf{A}+k \\mathbf{I}, \\end{equation}\nwhere $k\u0026gt;0$ is such that $\\mathbf{B}$ is positive definite. Then,\n\\begin{equation} \\lambda_{i}(\\mathbf{B})=\\lambda_{i}(\\mathbf{A})+k=a_{ii}+k=b_{ii} \\quad(i=1, \\ldots, n), \\end{equation}\nand hence\n\\begin{equation} |\\mathbf{B}|=\\prod_{1}^{n} \\lambda_{i}(\\mathbf{B})=\\prod_{i=1}^{n} b_{ii} . \\end{equation}\nIt then follows from Theorem 0.28 that $\\mathbf{B}$ is diagonal, and hence that $\\mathbf{A}$ is diagonal.\n■ Symmetric matrix functions # Let $\\mathbf{A}$ be a square matrix of order $n \\times n$. The $\\operatorname{trace} \\operatorname{tr} \\mathbf{A}$ and the determinant $|\\mathbf{A}|$ are examples of scalar functions of $\\mathbf{A}$. We can also consider matrix functions, for example, the inverse $\\mathbf{A}^{-1}$. The general definition of a matrix function is somewhat complicated, but for symmetric matrices it is easy. So, let us assume that $\\mathbf{A}$ is symmetric.\nWe known from that any symmetric $n \\times n$ matrix $\\mathbf{A}$ can be diagonalized, which means that there exists an orthogonal matrix $\\mathbf{S}$ and a diagonal matrix $\\mathbf{\\Lambda}$ (containing the eigenvalues of $\\mathbf{A}$ ) such that $\\mathbf{S}^{\\mathrm{T}} \\mathbf{A} \\mathbf{S}=\\mathbf{\\Lambda}$. Let $\\lambda_{i}$ denote the $i$ th diagonal element of $\\mathbf{\\Lambda}$ and let $\\phi$ be a function so that $\\phi(\\lambda)$ is defined, for example, $\\phi(\\lambda)=\\sqrt{\\lambda}$ or $1 / \\lambda$ or $\\log \\lambda$ or $e^{\\lambda}$.\nWe now define the matrix function $F$ as\n\\begin{equation} F(\\mathbf{\\Lambda})=\\left(\\begin{array}{cccc} \\phi\\left(\\lambda_{1}\\right) \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; \\phi\\left(\\lambda_{2}\\right) \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\phi\\left(\\lambda_{n}\\right) \\end{array}\\right), \\end{equation}\nand then\n\\begin{equation} F(\\mathbf{A})=\\mathbf{S} F(\\mathbf{\\Lambda}) \\mathbf{S}^{\\mathrm{T}} \\end{equation}\nFor example, if $\\mathbf{A}$ is nonsingular then all $\\lambda_{i}$ are nonzero, and letting $\\phi(\\lambda)=$ $1 / \\lambda$, we have\n\\begin{equation} F(\\mathbf{\\Lambda})=\\mathbf{\\Lambda}^{-1}=\\left(\\begin{array}{cccc} 1 / \\lambda_{1} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; 1 / \\lambda_{2} \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 1 / \\lambda_{n} \\end{array}\\right) \\end{equation}\nand hence $\\mathbf{A}^{-1}=\\mathbf{S} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}$. To check, we have\n\\begin{equation} \\mathbf{A} \\mathbf{A}^{-1}=\\mathbf{S} \\mathbf{\\Lambda} \\mathbf{S}^{\\mathrm{T}} \\mathbf{S} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\mathbf{\\Lambda} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{I}_{n} \\end{equation}\nas we should. Similarly, if $\\mathbf{A}$ is positive semidefinite, then $\\mathbf{A}^{1 / 2}=\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}$ and\n$$ \\mathbf{A}^{1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\boldsymbol{\\Lambda} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{A}, $$\nagain as it should be. Also, when $\\mathbf{A}$ is positive definite (hence nonsingular), then $\\mathbf{A}^{-1 / 2}=\\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}}$ and\n$$ \\begin{aligned} \\left(\\mathbf{A}^{1 / 2}\\right)^{-1} \u0026amp; =\\left(\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right)^{-1}=\\mathbf{S}\\left(\\boldsymbol{\\Lambda}^{1 / 2}\\right)^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S}\\left(\\boldsymbol{\\Lambda}^{-1}\\right)^{1 / 2} \\mathbf{S}^{\\mathrm{T}} \\\\ \u0026amp; =\\left(\\mathbf{S} \\boldsymbol{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}\\right)^{1 / 2}=\\left(\\mathbf{A}^{-1}\\right)^{1 / 2} \\end{aligned} $$\nso that this expression is unambiguously defined. Symmetric matrix functions are thus always defined through their eigenvalues. For example, the logarithm or exponential of $\\mathbf{A}$ is not the matrix with elements $\\log a_{i j}$ or $e^{a_{i j}}$, but rather a matrix whose eigenvalues are $\\log \\lambda_{i}$ or $e^{\\lambda_{i}}$ and whose eigenvectors are the same as the eigenvectors of $\\mathbf{A}$. This is similar to the definition of a positive definite matrix, which is not a matrix all whose elements are positive, but rather a matrix all whose eigenvalues are positive.\nMiscellaneous exercises # Exercises\nIf $\\mathbf{A}$ and $\\mathbf{B}$ are square matrices such that $\\mathbf{A}\\mathbf{B}=0, \\mathbf{A} \\neq 0, \\mathbf{B} \\neq 0$, then prove that $|\\mathbf{A}|=|\\mathbf{B}|=0$. If $\\mathbf{x}$ and $\\mathbf{y}$ are vectors of the same order, prove that $\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}=\\operatorname{tr} \\mathbf{y} \\mathbf{x}^{\\mathrm{T}}$. Let $$ \\mathbf{A}=\\left(\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right) $$\nShow that\n$$ |\\mathbf{A}|=\\left|\\mathbf{A}_{11}\\right|\\left|\\mathbf{A}_{22}-\\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}\\right| $$\nif $\\mathbf{A}_{11}$ is nonsingular, and\n$$ |\\mathbf{A}|=\\left|\\mathbf{A}_{22}\\right|\\left|\\mathbf{A}_{11}-\\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21}\\right| $$\nif $\\mathbf{A}_{22}$ is nonsingular. 4. Show that $(\\mathbf{I}-\\mathbf{A}\\mathbf{B})^{-1}=\\mathbf{I}+\\mathbf{A}(\\mathbf{I}-\\mathbf{B}\\mathbf{A})^{-1}\\mathbf{B}$, if the inverses exist. 5. Show that\n$$ (\\alpha \\mathbf{I}-\\mathbf{A})^{-1}-(\\beta \\mathbf{I}-\\mathbf{A})^{-1}=(\\beta-\\alpha)(\\beta \\mathbf{I}-\\mathbf{A})^{-1}(\\alpha \\mathbf{I}-\\mathbf{A})^{-1} . $$\nIf $\\mathbf{A}$ is positive definite, show that $\\mathbf{A}+\\mathbf{A}^{-1}-2 \\mathbf{I}$ is positive semidefinite. For any symmetric matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that $\\mathbf{A}\\mathbf{B}-\\mathbf{B}\\mathbf{A}$ is skewsymmetric. Let $\\mathbf{A}$ and $\\mathbf{B}$ be two $m \\times n$ matrices of rank $r$. If $\\mathbf{A}\\mathbf{A}^{\\mathrm{T}}=\\mathbf{B}\\mathbf{B}^{\\mathrm{T}}$ then $\\mathbf{A}=\\mathbf{B}\\mathbf{Q}$, where $\\mathbf{Q}\\mathbf{Q}^{\\mathrm{T}}$ (and hence $\\mathbf{Q}^{\\mathrm{T}} \\mathbf{Q}$ ) is idempotent of rank $k \\geq r$ (Neudecker and van de Velden 2000). Let $\\mathbf{A}$ be an $m \\times n$ matrix partitioned as $\\mathbf{A}=\\left(\\mathbf{A}_{1}: \\mathbf{A}_{2}\\right)$ and satisfying $\\mathbf{A}_{1}^{\\mathrm{T}} \\mathbf{A}_{2}=0$ and $r\\left(\\mathbf{A}_{1}\\right)+r\\left(\\mathbf{A}_{2}\\right)=m$. Then, for any positive semidefinite matrix $\\mathbf{V}$, we have $$ r(\\mathbf{V})=r\\left(\\mathbf{A}_{1}\\right)+r\\left(\\mathbf{A}_{2}^{\\mathrm{T}} \\mathbf{V} \\mathbf{A}_{2}\\right) \\Longleftrightarrow r(\\mathbf{V})=r\\left(\\mathbf{V}: \\mathbf{A}_{1}\\right) $$\nProve that the eigenvalues $\\lambda_{i}$ of $(\\mathbf{A}+\\mathbf{B})^{-1} \\mathbf{A}$, where $\\mathbf{A}$ is positive semidefinite and $\\mathbf{B}$ is positive definite, satisfy $0 \\leq \\lambda_{i}\u0026lt;1$. Let $\\mathbf{x}$ and $\\mathbf{y}$ be $n \\times 1$ vectors. Prove that $\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}$ has $n-1$ zero eigenvalues and one eigenvalue $\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. Show that $\\left|\\mathbf{I}+\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}\\right|=1+\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. Let $\\mu=1+\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. If $\\mu \\neq 0$, show that $\\left(\\mathbf{I}+\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}\\right)^{-1}=\\mathbf{I}-(1 / \\mu) \\mathbf{x} \\mathbf{y}^{\\mathrm{T}}$. Show that $\\left(\\mathbf{I}+\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right)^{-1} \\mathbf{A}=\\mathbf{A}\\left(\\mathbf{I}+\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{-1}$. Show that $\\mathbf{A}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{1 / 2}=\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right)^{1 / 2} \\mathbf{A}$. (Monotonicity of the entropic complexity.) Let $\\mathbf{A}_{n}$ be a positive definite $n \\times n$ matrix and define $$ \\phi(n)=\\frac{n}{2} \\log \\operatorname{tr}\\left(\\mathbf{A}_{n} / n\\right)-\\frac{1}{2} \\log \\left|\\mathbf{A}_{n}\\right| . $$\nLet $\\mathbf{A}_{n+1}$ be a positive definite $(n+1) \\times(n+1)$ matrix such that\n$$ \\mathbf{A}_{n+1}=\\left(\\begin{array}{cc} \\mathbf{A}_{n} \u0026amp; \\mathbf{a}_{n} \\\\ \\mathbf{a}_{n}^{\\mathrm{T}} \u0026amp; \\alpha_{n} \\end{array}\\right) $$\nThen,\n$$ \\phi(n+1) \\geq \\phi(n) $$\nwith equality if and only if\n$$ \\mathbf{a}_{n}=0, \\quad \\alpha_{n}=\\operatorname{tr} \\mathbf{A}_{n} / n $$\n"},{"id":6,"href":"/numerical_optimization/docs/lectures/advanced/proximal_methods/","title":"2. Proximal methods","section":"II - Advanced problems","content":" Proximal methods # Soon to be added.\n"},{"id":7,"href":"/numerical_optimization/docs/lectures/machine_learning/svm/","title":"2. Support Vector Machine","section":"III - Machine Learning problems","content":" Support Vector Machine # Soon to be added.\n"},{"id":8,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/","title":"2. Unconstrained optimization : basics","section":"I - Fundamentals","content":" Unconstrained optimization - basics # We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve: $$ \\underset{\\mathbf{x}\\in\\mathbb{R}^d}{\\operatorname{argmin}} f(\\mathbf{x}). $$\nLet us try to characterizes the nature of the solutions under this setup.\nWhat is a solution ? # Figure 2.1: Local and global minimum can coexist.\nGenerally, we would be happiest if we found a global minimizer of $f$ , a point where the function attains its least value. A formal definition is :\nDefinition 2.1 (Global minimizer)\nA point $\\mathbf{x}^\\star$ is a global minimizer if $f(\\mathbf{x}^\\star)\\leq f(\\mathbf{x})$, where $\\mathbf{x}$ ranges over all of $\\mathbb{R}^d$ (or at least over the domain of interest to the modeler). The global minimizer can be difficult to find, because our knowledge of $f$ is usually only local. Since our algorithm does not visit many points (we hope!), we usually do not have a good picture of the overall shape of $f$ , and we can never be sure that the function does not take a sharp dip in some region that has not been sampled by the algorithm. Most algorithms are able to find only a local minimizer, which is a point that achieves the smallest value of f in its neighborhood. Formally, we say:\nDefinition 2.2 (Local minimizer)\nA point $\\mathbf{x}^\\star$ is a local minimizer if $\\exists r\u0026gt;0,\\, f(\\mathbf{x}^\\star)\\leq f(\\mathbf{x})$, $\\forall \\mathbf{x}\\in\\mathcal{B}(\\mathbf{x}^\\star, r)$. A point that satisfies this definition is sometimes called a weak local minimizer. Alternatively, when $f(\\mathbf{x}^\\star)\u0026lt;f(\\mathbf{x})$, we say that the minimum is a strict local minimizer.\nTaylor\u0026rsquo;s theorem # From the definitions given above, it might seem that the only way to find out whether a point $\\mathbf{x}^\\star$ is a local minimum is to examine all the points in its immediate vicinity, to make sure that none of them has a smaller function value. When the function $f$ is smooth, however, there are much more efficient and practical ways to identify local minima. In particular, if $f$ is twice continuously differentiable, we may be able to tell that $\\mathbf{x}^\\star$ is a local minimizer (and possibly a strict local minimizer) by examining just the gradient $\\nabla f (\\mathbf{x}^\\star)$ and the Hessian $\\nabla^2 f (\\mathbf{x}^\\star)$. The mathematical tool used to study minimizers of smooth functions is Taylor’s the- orem. Because this theorem is central to our analysis we state it now.\nTheorem 2.1 (Taylor\u0026#39;s theorem)\nSuppose that $f:\\mathbb{R}^d\\mapsto\\mathbb{R}$ is continuously differentiable and that we have $\\mathbf{p}\\in\\mathbb{R}^d $. The we have : \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x}+t\\mathbf{p})^\\mathrm{T}\\mathbf{p}, \\end{equation} for some $t\\in [0,1]$.\nMoreover, if $f$ is twice continuously differentiable, we have : \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\mathrm{T}\\nabla^2 f(\\mathbf{x}+t\\mathbf{p})\\mathbf{p}), \\end{equation} for some $t\\in [0,1]$.\nProof\nSee any calculus book ■ Note that in this formulation, the definition is exact and the $t$ scalar is usually unknown. The interest lies in skeching proofs. In practical matters, we rather use the following approximation: Theorem 2.2 (Taylor\u0026#39;s approximation)\nFirst order approximation: \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + o(\\lVert\\mathbf{p}\\rVert), \\end{equation}\nSecond-order approximation:\n\\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\mathrm{T}\\nabla^2 f(\\mathbf{x})\\mathbf{p} + o(\\lVert\\mathbf{p}\\rVert^2), \\end{equation}\nwhere $o(\\lVert\\mathbf{p}\\rVert)$ and $o(\\lVert\\mathbf{p}\\rVert^2)$ represent terms that grow slower than $\\lVert\\mathbf{p}\\rVert$ and $\\lVert\\mathbf{p}\\rVert^2$ respectively as $\\lVert\\mathbf{p}\\rVert \\to 0$.\nSufficient and necessary conditions for local minima # Let us consider a local minimum and see how they can be characterized to later design appropriate solution finding methods. The first well-known result is as follows: Theorem 2.3 (First-order necessary conditions)\nif $\\mathbf{x}^\\star$ is a local minimize, and $f$ is continuously differentiable in a neighborhood of $\\mathbf{x}^\\star$, then $\\nabla f(\\mathbf{x}^\\star) = \\mathbf{0}$. Proof\nSuppose for contradiction that $\\nabla f(\\mathbf{x}^\\star) \\neq 0$, and define vector $\\mathbf{p}=-\\nabla f(\\mathbf{x}^\\star)$ such that by construction $\\mathbf{p}^\\mathrm{T}\\nabla f(\\mathbf{x}^\\star) = - \\lVert f(\\mathbf{x}^\\star) \\rVert^2 \u0026lt; 0$.\nSince $f$ is a continuous function, we can define a scalar $T\u0026gt;0$ such that $\\forall t\\in [0,T[$, we still have: $$ \\mathbf{p}^\\mathrm{T}f(\\mathbf{x}+t\\mathbf{p}) \u0026lt; 0. $$\nUsing Theorem 2.1 first-order result, we can write: $$ f(\\mathbf{x}^\\star+t\\mathbf{p}) = f(\\mathbf{x}^\\star) + t\\mathbf{p}^\\mathrm{T}\\nabla f(\\mathbf{x}^\\star+\\overline{t}\\mathbf{p}), $$ for some $\\overline{t}\\in[0,T[$ and any $t\\in[0,T[$. Given previous inequality, we obtain: $$ f(\\mathbf{x}^\\star+t\\mathbf{p}) \u0026lt; f(\\mathbf{x}^\\star), $$ which contradicts the fact that $\\mathbf{x}^\\star$ is a local minimizer.\n■ Henceforth, we will call stationary point, any $\\mathbf{x}$ such that $\\nabla f(\\mathbf{x}) = 0$.\nFor the next result we recall that a matrix $\\mathbf{B}$ is positive definite if $\\mathbf{p}^\\mathrm{T} \\mathbf{B} \\mathbf{p}\u0026gt;0$ for all $\\mathbf{p} \\neq \\mathbf{0}$, and positive semidefinite if $\\mathbf{p}^\\mathrm{T} \\mathbf{B} \\mathbf{p} \\geq 0$ for all $\\mathbf{p}$.\nTheorem 2.4 (Second-order necessary conditions)\nIf $\\mathbf{x}^\\star$ is a local minimizer of $f$ and $\\nabla^2 f$ is continuous in an open neighborhood of $\\mathbf{x}^\\star$, then $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$ and $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is positive semidefinite. Proof\nProof. We know from Theorem 2.3 that $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$. For contradiction, assume that $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is not positive semidefinite. Then we can choose a vector $\\mathbf{p}$ such that $\\mathbf{p}^T \\nabla^2 f\\left(\\mathbf{x}^\\star\\right) \\mathbf{p}\u0026lt;0$, and because $\\nabla^2 f$ is continuous near $\\mathbf{x}^\\star$, there is a scalar $T\u0026gt;0$ such that $\\mathbf{p}^\\mathrm{T} \\nabla^2 f\\left(\\mathbf{x}^*+\\overline{t} \\mathbf{p}\\right) \\mathbf{p}\u0026lt;0$ for all $t \\in[0, T[$.\nBy doing a Taylor series expansion around $\\mathbf{x}^\\star$, we have for all $\\bar{t} \\in[0, T[$ and some $t \\in[0, \\bar{t}]$ that\n$$ f\\left(\\mathbf{x}^\\star+\\bar{t} \\mathbf{p}\\right) = f\\left(\\mathbf{x}^\\star\\right)+\\bar{t} \\mathbf{p}^\\mathrm{T} \\nabla f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\bar{t}^2 \\mathbf{p}^\\mathrm{T} \\nabla^2 f\\left(\\mathbf{x}^\\star+t \\mathbf{p}\\right) \\mathbf{p}\u0026lt;f\\left(\\mathbf{x}^\\star\\right) . $$\nAs in Theorem 2.3, we have found a direction from $\\mathbf{x}^\\star$ along which $f$ is decreasing, and so again, $\\mathbf{x}^\\star$ is not a local minimizer.\n■ We now describe sufficient conditions, which are conditions on the derivatives of $f$ at the point $\\mathbf{z}^\\star$ that guarantee that $\\mathbf{x}^\\star$ is a local minimizer.\nTheorem 2.5 (Second-Order Sufficient Conditions)\nSuppose that $\\nabla^2 f$ is continuous in an open neighborhood of $\\mathbf{x}^\\star$ and that $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$ and $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is positive definite. Then $\\mathbf{x}^\\star$ is a strict local minimizer of $f$. Proof\nBecause the Hessian is continuous and positive definite at $\\mathbf{x}^\\star$, we can choose a radius $r\u0026gt;0$ so that $\\nabla^2 f(x)$ remains positive definite for all $x$ in the open ball $\\mathcal{D}=\\left\\{\\mathbf{z} \\mid\\left\\lVert\\mathbf{z}-\\mathbf{x}^\\star\\right\\rVert\u0026lt;\\right.$ $r\\}$. Taking any nonzero vector $p$ with $\\lVert\\mathbf{p}\\rVert\u0026lt;r$, we have $\\mathbf{x}^\\star+\\mathbf{p} \\in \\mathcal{D}$ and so\n$$ \\begin{aligned} f\\left(\\mathbf{x}^\\star+p\\right) \u0026amp; =f\\left(\\mathbf{x}^\\star\\right)+\\mathbf{p}^\\mathrm{T} \\nabla f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\mathbf{p}^\\mathrm{T} \\nabla^2 f(\\mathbf{z}) \\mathbf{p} \\ \u0026amp; =f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\mathbf{p}^\\mathrm{T} \\nabla^2 f(\\mathbf{z}) \\mathbf{p} \\end{aligned} $$\nwhere $\\mathbf{z}=\\mathbf{x}^\\star+t \\mathbf{p}$ for some $t \\in(0,1)$. Since $\\mathbf{z} \\in \\mathcal{D}$, we have $\\mathbf{p}^{\\mathrm{T}} \\nabla^2 f(\\mathbf{z}) \\mathbf{p}\u0026gt;0$, and therefore $f\\left(\\mathbf{x}^\\star+\\mathbf{p}\\right)\u0026gt;f\\left(\\mathbf{x}^\\star\\right)$, giving the result.\n■ Note that the second-order sufficient conditions of Theorem 2.5 guarantee something stronger than the necessary conditions discussed earlier; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: A point $\\mathbf{x}^\\star$ may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function $f(x)=x^4$, for which the point $x^\\star=0$ is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite). These results, which are based on elementary calculus, provide the foundations for unconstrained optimization algorithms. In one way or another, all algorithms seek a point where $\\nabla f(\\cdot)$ vanishes.\nThe need for algorithms # On question we might ask here is why do we need algorithms to find local minima? After all, we have just shown that if $\\nabla f(\\mathbf{x}^\\star)=0$, then $\\mathbf{x}^\\star$ is a local minimizer. The answer is that in practice, we do not always have the luxury to know the exact solution to $\\nabla f(\\mathbf{x})=0$. Moreover, we can\u0026rsquo;t always compute the Hessian matrix to check the second-order conditions.\nThus, to circumvent the need to solve analytically the equations $\\nabla f(\\mathbf{x})=0$, we will design algorithms that iteratively update a point $\\mathbf{x}$ until it converges to a local minimizer. The algorithms will be based on the properties of the gradient and Hessian, and will use the information they provide to guide the search for a local minimum. When hessian is not computable or too expensive to compute, we will use the gradient only, and the algorithms will be called gradient-based methods. When the Hessian is available, we will use it to accelerate convergence, and the algorithms will be called Newton methods. As a between between these methods lie quasi-Newton methods, which use an approximation of the Hessian to guide the search for a local minimum.\nBut before we dive in more complicated algorithms, let us consider the most obvious approaches and try to understand their limitations.\nSteepest-descent approach # The first algorithm we consider is the so-called Steepest-descent algorithm which involves choosing an initial point $\\mathbf{x}_0$ and compute a series of subsequent points with the following formula:\n\\begin{equation} \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k), \\label{eq: steepest descent} \\end{equation}\nwhere $\\alpha_k$ are a series of scalar values called step-size (or learning rate in machine learning context).\nThe intuition behind this approach is beautifully simple yet profound. Imagine yourself standing on a mountainside in thick fog, trying to find the bottom of the valley. Since you can\u0026rsquo;t see the overall landscape, the most sensible strategy is to feel the slope beneath your feet and take a step in the direction that descends most steeply. This is precisely what the steepest-descent algorithm does mathematically.\nThe negative gradient $-\\nabla f(\\mathbf{x}_k)$ points in the direction of steepest decrease of the function at point $\\mathbf{x}_k$. This isn\u0026rsquo;t just a convenient mathematical fact—it\u0026rsquo;s the fundamental geometric property that makes gradient-based optimization possible. By moving in this direction, we ensure that we\u0026rsquo;re making the most aggressive local progress toward reducing the function value.\nFigure 2.2: Optimization with steepest descent\nUnderstanding the algorithm step by step # Let\u0026rsquo;s walk through what happens in each iteration of steepest descent. Starting from point $\\mathbf{x}_k$, we compute the gradient $\\nabla f(\\mathbf{x}_k)$. This vector tells us which direction the function increases most rapidly. Since we want to minimize, we go in the opposite direction: $-\\nabla f(\\mathbf{x}_k)$.\nThe step size $\\alpha_k$ determines how far we travel in this direction. Think of it as the length of your stride as you walk down the mountain. The choice of step size involves a fundamental trade-off: too small and you make painfully slow progress; too large and you might overshoot the valley bottom or even start climbing uphill again.\nWhy steepest descent can struggle # Figure 2.3: Problem of stepsize\nHere\u0026rsquo;s where the method reveals its first major limitation. Consider a function that looks like a long, narrow valley—mathematically, this corresponds to a function with a large condition number. Steepest descent exhibits what we call \u0026ldquo;zigzag behavior\u0026rdquo; in such cases as illustrated in Figure 2.3 .\nPicture this scenario: you\u0026rsquo;re in a narrow canyon, and the steepest direction points toward one wall rather than down the canyon. You take steps toward that wall, then the gradient changes direction and points toward the other wall. Instead of walking efficiently down the canyon, you find yourself bouncing back and forth between the walls, making very slow progress toward your destination.\nThis zigzag pattern occurs because steepest descent is fundamentally myopic. At each step, it only considers the immediate local slope and ignores the broader geometric structure of the function. The algorithm doesn\u0026rsquo;t \u0026ldquo;remember\u0026rdquo; where it came from or \u0026ldquo;anticipate\u0026rdquo; where the function is heading.\nConvergence properties # Despite these limitations, steepest descent does have reliable convergence properties. Under reasonable mathematical conditions—essentially requiring that the function is well-behaved and doesn\u0026rsquo;t have any pathological features—the algorithm will eventually reach a stationary point where the gradient vanishes.\nThe convergence is what we call \u0026ldquo;linear,\u0026rdquo; meaning that the error decreases by a constant factor at each iteration. While this sounds reasonable, it can be frustratingly slow in practice, especially for poorly conditioned problems where that constant factor is very close to one.\nNewton method # If steepest descent is like navigating with only your immediate sense of slope, Newton\u0026rsquo;s method is like having a detailed topographic map of your local surroundings. This method incorporates not just information about which way is downhill, but also how the slope itself is changing—what we call the curvature of the function.\nThe mathematical foundation # Newton\u0026rsquo;s method emerges from a clever idea: instead of trying to minimize the original function directly, let\u0026rsquo;s create a simpler approximation and minimize that instead. We use the second-order Taylor approximation around our current point $\\mathbf{x}_k$:\n$$f(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^T\\nabla^2 f(\\mathbf{x}_k)\\mathbf{p}$$\nThis quadratic approximation captures both the slope (first-order term) and the curvature (second-order term) at our current location. The brilliant insight is that quadratic functions are easy to minimize—we simply set the gradient of the approximation equal to zero and solve for the optimal step $\\mathbf{p}$.\nTaking the gradient of our quadratic model and setting it to zero gives us: $$\\nabla f(\\mathbf{x}_k) + \\nabla^2 f(\\mathbf{x}_k)\\mathbf{p} = \\mathbf{0}$$\nSolving for the Newton step: $$\\mathbf{p}_k = -[\\nabla^2 f(\\mathbf{x}_k)]^{-1}\\nabla f(\\mathbf{x}_k)$$\nThe complete Newton iteration becomes: $$\\mathbf{x}_{k+1} = \\mathbf{x}_k - [\\nabla^2 f(\\mathbf{x}_k)]^{-1}\\nabla f(\\mathbf{x}_k)$$\nFigure 2.4: Newton optimization step\nThe geometric insight # What makes Newton\u0026rsquo;s method so powerful becomes clear when we think geometrically. The Hessian matrix $\\nabla^2 f(\\mathbf{x}_k)$ encodes information about how the gradient changes in different directions. If the function curves sharply in one direction and gently in another, the Hessian \u0026ldquo;knows\u0026rdquo; this and adjusts the step accordingly.\nConsider our narrow valley example again. While steepest descent keeps pointing toward the valley walls, Newton\u0026rsquo;s method recognizes the elongated shape of the valley and naturally takes larger steps along the valley floor and smaller steps perpendicular to it. This geometric awareness eliminates the zigzag behavior that plagues steepest descent.\nFor quadratic functions, this geometric understanding leads to a remarkable property: Newton\u0026rsquo;s method finds the exact minimum in a single step, regardless of how poorly conditioned the function might be. This happens because our second-order approximation is exact for quadratic functions.\nThe power of quadratic convergence # Near a solution that satisfies our second-order sufficient conditions, Newton\u0026rsquo;s method exhibits quadratic convergence. This technical term describes an almost magical property: the number of correct digits in your answer roughly doubles with each iteration.\nTo appreciate this, consider what linear convergence means: if you have one correct digit, you need about three more iterations to get two correct digits. But with quadratic convergence, if you have one correct digit, the next iteration gives you two, then four, then eight. The improvement accelerates dramatically as you approach the solution.\nThis rapid convergence makes Newton\u0026rsquo;s method incredibly efficient for high-precision optimization, which is why it forms the backbone of many sophisticated algorithms.\nThe computational cost # Newton\u0026rsquo;s method\u0026rsquo;s power comes with a price. At each iteration, we must compute the Hessian matrix, which requires evaluating all second partial derivatives of our function. For a function of $d$ variables, this means computing and storing $d(d+1)/2$ distinct second derivatives.\nEven more expensive is solving the linear system $\\nabla^2 f(\\mathbf{x}_k)\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ at each iteration. For general matrices, this requires roughly $d^3/3$ arithmetic operations, which becomes prohibitive as the dimension grows.\nWhen Newton\u0026rsquo;s method can fail # Pure Newton\u0026rsquo;s method isn\u0026rsquo;t foolproof. The Hessian matrix might not be positive definite away from the minimum, which means our quadratic model might not have a minimum—it could have a maximum or a saddle point instead. In such cases, the Newton step might point in completely the wrong direction.\nAdditionally, if we start too far from a minimum, the quadratic approximation might be a poor representation of the true function, leading to steps that actually increase the function value.\nLooking ahead: the bridge between methods # The contrasting strengths and weaknesses of steepest descent and Newton\u0026rsquo;s method naturally lead to interesting questions. Can we capture some of Newton\u0026rsquo;s geometric insight without the full computational burden? Can we ensure the global reliability of gradient methods while achieving faster local convergence?\nThese questions motivate more sophisticated approaches. Quasi-Newton methods, which we\u0026rsquo;ll explore in later chapters, build approximations to the Hessian using only gradient information. Methods like BFGS achieve superlinear convergence—faster than linear but not quite quadratic—while requiring much less computation than full Newton steps.\nSimilarly, trust region methods and linesearch strategies, which we\u0026rsquo;ll study later, provide systematic ways to ensure that our algorithms make reliable progress even when our local approximations aren\u0026rsquo;t perfect.\n"},{"id":9,"href":"/numerical_optimization/docs/lectures/1_introduction/","title":"Introduction","section":"Lectures","content":" Introduction # Notations # Let us start by defining the notation used troughout all the lectures and practical labs.\nBasic Notation # Scalars are represented by italic letters (e.g., $x$, $y$, $\\lambda$). Vectors are denoted by bold lowercase letters (e.g., $\\mathbf{v}$, $\\mathbf{x}$), while matrices are represented by bold uppercase letters (e.g., $\\mathbf{A}$, $\\mathbf{B}$). The dimensionality of a vector $\\mathbf{v} \\in \\mathbb{R}^n$ indicates it contains $n$ elements, and similarly, a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ has $m$ rows and $n$ columns.\nMatrix Operations # The transpose of a matrix $\\mathbf{A}$ is denoted as $\\mathbf{A}^\\mathrm{T}$, which reflects the matrix across its diagonal. The trace of a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, written as $\\mathrm{tr}(\\mathbf{A})$, is the sum of its diagonal elements, i.e., $\\mathrm{tr}(\\mathbf{A}) = \\sum_{i=1}^{n} a_{ii}$. The determinant of $\\mathbf{A}$ is represented as $\\det(\\mathbf{A})$ or $|\\mathbf{A}|$. A matrix $\\mathbf{A}$ is invertible if and only if $\\det(\\mathbf{A}) \\neq 0$, and its inverse is denoted as $\\mathbf{A}^{-1}$, satisfying $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix.\nVector Operations # The dot product between two vectors $\\mathbf{a}$ and $\\mathbf{b}$ of the same dimension is written as $\\mathbf{a} \\cdot \\mathbf{b}$ or $\\mathbf{a}^\\mathrm{T}\\mathbf{b}$, resulting in a scalar value.\nThe p-norm of a vector $\\mathbf{v}$ is denoted as $\\lVert\\mathbf{v}\\rVert_p$ and defined as $\\lVert\\mathbf{v}\\rVert_p = \\left(\\sum_{i=1}^{n} |v_i|^p\\right)^{1/p}$ for $p \\geq 1$, with common choices being $p=1$ (Manhattan norm), $p=2$ (Euclidean norm), and $p=\\infty$ (maximum norm, defined as $\\lVert\\mathbf{v}\\rVert_{\\infty} = \\max_i |v_i|$); when the subscript $p$ is omitted, as in $\\lVert\\mathbf{v}\\rVert$, it is conventionally understood to refer to the Euclidean (L2) norm. The Euclidean norm (or length) of a vector $\\mathbf{v}$ is represented as $\\lVert\\mathbf{v}\\rVert$ or $\\lVert\\mathbf{v}\\rVert_2$, defined as $\\lVert\\mathbf{v}\\rVert = \\sqrt{\\mathbf{v}^\\mathrm{T}\\mathbf{v}} = \\sqrt{\\sum_{i=1}^{n} v_i^2}$. A unit vector in the direction of $\\mathbf{v}$ is given by $\\hat{\\mathbf{v}} = \\mathbf{v}/\\lVert\\mathbf{v}\\rVert$, having a norm of 1.\nEigenvalues and Eigenvectors # For a square matrix $\\mathbf{A}$, a scalar $\\lambda$ is an eigenvalue if there exists a non-zero vector $\\mathbf{v}$ such that $\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$. The vector $\\mathbf{v}$ is called an eigenvector corresponding to the eigenvalue $\\lambda$. The characteristic polynomial of $\\mathbf{A}$ is defined as $p(\\lambda) = \\det(\\lambda\\mathbf{I} - \\mathbf{A})$, and its roots are the eigenvalues of $\\mathbf{A}$. The spectrum of $\\mathbf{A}$, denoted by $\\sigma(\\mathbf{A})$, is the set of all eigenvalues of $\\mathbf{A}$.\nMatrix Decompositions # The singular value decomposition (SVD) of a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is expressed as $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\mathrm{T}$, where $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ and $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values of $\\mathbf{A}$. The eigendecomposition of a diagonalizable matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is given by $\\mathbf{A} = \\mathbf{P}\\mathbf{\\Lambda}\\mathbf{P}^{-1}$, where $\\mathbf{P}$ is a matrix whose columns are the eigenvectors of $\\mathbf{A}$, and $\\mathbf{\\Lambda}$ is a diagonal matrix containing the corresponding eigenvalues.\nMultivariate Calculus # The gradient of a scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is denoted as $\\nabla f$ or $\\mathrm{grad}(f)$, resulting in a vector of partial derivatives $\\nabla f = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}]^\\mathrm{T}$. The Jacobian matrix of a vector-valued function $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is represented as $\\mathbf{J}_\\mathbf{f}$ or $\\nabla \\mathbf{f}^\\mathrm{T}$, where $ (\\mathbf{J}_\\mathbf{f})_{ij} = \\frac{\\partial f_i}{\\partial x_j} $.\nThe Hessian matrix of a twice-differentiable scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is denoted as $\\mathbf{H}_f$ or $\\nabla^2 f$, where $(\\mathbf{H}_f)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$.\nSpecial Matrices and Properties # A symmetric matrix satisfies $\\mathbf{A} = \\mathbf{A}^\\mathrm{T}$, while a skew-symmetric matrix has $\\mathbf{A} = -\\mathbf{A}^\\mathrm{T}$. An orthogonal matrix $\\mathbf{Q}$ satisfies $\\mathbf{Q}^\\mathrm{T}\\mathbf{Q} = \\mathbf{Q}\\mathbf{Q}^\\mathrm{T} = \\mathbf{I}$, meaning its inverse equals its transpose: $\\mathbf{Q}^{-1} = \\mathbf{Q}^\\mathrm{T}$. A matrix $\\mathbf{A}$ is positive definite if $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} \u0026gt; 0$ for all non-zero vectors $\\mathbf{x}$, and positive semidefinite if $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} \\geq 0$.\nDerivatives of Matrix Expressions # The derivative of a scalar function with respect to a vector $\\mathbf{x}$ is denoted as $\\frac{\\partial f}{\\partial \\mathbf{x}}$, resulting in a vector of the same dimension as $\\mathbf{x}$. For matrix functions, the derivative with respect to a matrix $\\mathbf{X}$ is written as $\\frac{\\partial f}{\\partial \\mathbf{X}}$, producing a matrix of the same dimensions as $\\mathbf{X}$. Common matrix derivatives include $\\frac{\\partial}{\\partial \\mathbf{X}}\\mathrm{tr}(\\mathbf{AX}) = \\mathbf{A}^\\mathrm{T}$ and $\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}) = \\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathrm{T}\\mathbf{x}$ (with $\\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathrm{T}\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}$ when $\\mathbf{A}$ is symmetric).\n"},{"id":10,"href":"/numerical_optimization/docs/lectures/fundamentals/convexity/","title":"3. Convexity theory","section":"I - Fundamentals","content":" Convexity theory # Convexity is a powerful property of functions and sets that simplifies the analysis of optimization problems and the characterization of global minimizers. In this chapter, we will explore the concepts of convex sets, convex functions, and their implications for unconstrained optimization.\nConvex sets # Let us first start by defining the convexity of a given set $\\mathcal{S}\\subset\\mathbb{R}^d$:\nDefinition 3.1 (Convex set)\nLet $\\mathcal{S}\\subset\\mathbb{R}^d$ be a set. The set $\\mathcal{S}$ is convex if, for any two points $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}$, the line segment that connects them is also contained in $\\mathcal{S}$, that is, \\begin{equation} \\mathbf{x}, \\mathbf{y} \\in \\mathcal{S} \\implies \\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y} \\in \\mathcal{S}, \\quad \\forall \\lambda \\in [0, 1]. \\label{eq:convex_set} \\end{equation} Figure 3.1: Convex set\nThis is illustrated by Figure 3.1 , where the set $\\mathcal{S}$ is convex, as the line segment between any two points $\\mathbf{x}$ and $\\mathbf{y}$ lies entirely within $\\mathcal{S}$. If this property does not hold, then the set is called non-convex.\nWhile we will not do deeper now, this property is desirable for the constraints of an optimization problem, .because it means that for a given algorithm, any subsequent step is feasible by staying true to the given constraints for the problem.\nConvex functions # A function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is convex if its domain is a convex set and it satisfies the following property:\nDefinition 3.2 (Convex function)\nA function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is convex if, for any two points $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ and for all $\\lambda \\in [0, 1]$, the following inequality holds: \\begin{equation} f(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}). \\label{eq:convex_function} \\end{equation} Figure 3.2: Convex function\nThis means that the line segment connecting the points $(\\mathbf{x}, f(\\mathbf{x}))$ and $(\\mathbf{y}, f(\\mathbf{y}))$ lies above the graph of the function $f$. In other words, the function is \u0026ldquo;bowl-shaped\u0026rdquo; or \u0026ldquo;curves upwards\u0026rdquo;. Such an illustration is given for a 1-dimensional function in Figure 3.2 , where the function $f$ is convex, as the line segment between any two points $(\\mathbf{x}, f(\\mathbf{x}))$ and $(\\mathbf{y}, f(\\mathbf{y}))$ lies above the graph of $f$.\nIn practice to show that a function is convex, we can make use of the following properties, given convex functions $f$ and $g$:\nlet $\\alpha, \\beta\u0026gt;0$, then $\\alpha f + \\beta g$ is convex $f \\circ g$ is convex Convexity and unconstrained optimization # When the objective function is convex, local and global minimizers are simple to characterize.\nTheorem 3.1 When $f$ is convex, any local minimizer $\\mathbf{x}^\\star$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $\\mathbf{x}^\\star$ is a global minimizer of $f$. Proof\nSuppose that $\\mathbf{x}^\\star$ is a local but not a global minimizer. Then we can find a point $\\mathbf{z} \\in \\mathbb{R}^n$ with $f(\\mathbf{z})\u0026lt;f\\left(\\mathbf{x}^\\star\\right)$. Consider the line segment that joins $\\mathbf{x}^\\star$ to $\\mathbf{z}$, that is, \\begin{equation} \\mathbf{x}=\\lambda \\mathbf{z}+(1-\\lambda) \\mathbf{x}^\\star, \\quad \\text { for some } \\lambda \\in(0,1] \\label{eq:line_segment} \\end{equation} By the convexity property for $f$, we have \\begin{equation} f(\\mathbf{x}) \\leq \\lambda f(\\mathbf{z})+(1-\\lambda) f\\left(\\mathbf{x}^\\star\\right)\u0026lt;f\\left(\\mathbf{x}^\\star\\right) \\label{eq:convexity} \\end{equation}\nAny neighborhood $\\mathcal{N}$ of $\\mathbf{x}^\\star$ contains a piece of the line segment \\eqref{eq:line_segment}, so there will always be points $\\mathbf{x} \\in \\mathcal{N}$ at which \\eqref{eq:convexity} is satisfied. Hence, $\\mathbf{x}^\\star$ is not a local minimizer. For the second part of the theorem, suppose that $\\mathbf{x}^\\star$ is not a global minimizer and choose $\\mathbf{z}$ as above. Then, from convexity, we have\n\\begin{equation} \\begin{aligned} \\nabla f\\left(\\mathbf{x}^\\star\\right)^{\\mathrm{T}}\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right) \u0026amp; =\\left.\\frac{d}{d \\lambda} f\\left(\\mathbf{x}^\\star+\\lambda\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right)\\right)\\right|_{\\lambda=0} \\\\ \u0026amp; =\\lim _{\\lambda \\downarrow 0} \\frac{f\\left(\\mathbf{x}^\\star+\\lambda\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right)\\right)-f\\left(\\mathbf{x}^\\star\\right)}{\\lambda} \\\\ \u0026amp; \\leq \\lim _{\\lambda \\downarrow 0} \\frac{\\lambda f(\\mathbf{z})+(1-\\lambda) f\\left(\\mathbf{x}^\\star\\right)-f\\left(\\mathbf{x}^\\star\\right)}{\\lambda} \\\\ \u0026amp; =f(\\mathbf{z})-f\\left(\\mathbf{x}^\\star\\right)\u0026lt;0 \\end{aligned} \\end{equation}\nTherefore, $\\nabla f\\left(\\mathbf{x}^\\star\\right) \\neq 0$, and so $\\mathbf{x}^\\star$ is not a stationary point.\n■ This result is fundamental in optimization, as it guarantees that if we find a local minimizer of a convex function, we can be sure that it is also the global minimizer. This property greatly simplifies the search for optimal solutions. As such, finding that the function we minimize is convex often means that the problem is easier to solve, as we can use algorithms that are guaranteed to converge to the global minimum.\nConversely, in the design stage, we might prefer to design a convex function, or try to find a convex approximation of a non-convex function, to ensure that the optimization problem is well-behaved and that we can find the global minimum efficiently.\n"},{"id":11,"href":"/numerical_optimization/docs/lectures/machine_learning/neural_networks/","title":"3. Neural Networks","section":"III - Machine Learning problems","content":" Neural Networks # Soon to be added.\n"},{"id":12,"href":"/numerical_optimization/docs/lectures/machine_learning/modern/","title":"4. Modern trends","section":"III - Machine Learning problems","content":" Modern trends # Soon to be added.\n"},{"id":13,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/","title":"4. Unconstrained optimization : linesearch","section":"I - Fundamentals","content":" Unconstrained optimization - Linesearch methods # All algorithms for unconstrained minimization require the user to supply a starting point, which we usually denote by $\\mathbf{x}_0$. The user with knowledge about the application and the data set may be in a good position to choose $\\mathbf{x}_0$ to be a reasonable estimate of the solution. Otherwise, the starting point must be chosen in some arbitrary manner.\nBeginning at $\\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\\left\\{\\mathbf{x}_k\\right\\}_{k=0}^{\\infty}$ that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy. In deciding how to move from one iterate $\\mathbf{x}_k$ to the next, the algorithms use information about the function $f$ at $\\mathbf{x}_k$, and possibly also information from earlier iterates $\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{k-1}$. They use this information to find a new iterate $\\mathbf{x}_{k+1}$ with a lower function value than $\\mathbf{x}_k$. (There exist nonmonotone algorithms that do not insist on a decrease in $f$ at every step, but even these algorithms require $f$ to be decreased after some prescribed number $m$ of iterations. That is, they enforce $f\\left(\\mathbf{x}_k\\right)\u0026lt;f\\left(\\mathbf{x}_{k-m}\\right)$.)\nThere are two fundamental strategies for moving from the current point $\\mathbf{x}_k$ to a new iterate $\\mathbf{x}_{k+1}$. Most of the algorithms described in this book follow one of these approaches.\nTwo strategies: line search and trust region # In the line search strategy, the algorithm chooses a direction $\\mathbf{p}_k$ and searches along this direction from the current iterate $\\mathbf{x}_k$ for a new iterate with a lower function value. The distance to move along $\\mathbf{p}_k$ can be found by approximately solving the following one-dimensional minimization problem to find a step length $\\alpha$ :\n\\begin{equation} \\min _{\\alpha\u0026gt;0} f\\left(\\mathbf{x}_k+\\alpha \\mathbf{p}_k\\right) \\label{eq:line_search_min} \\end{equation}\nBy solving \\eqref{eq:line_search_min} exactly, we would derive the maximum benefit from the direction $\\mathbf{p}_k$, but an exact minimization is expensive and unnecessary. Instead, the line search algorithm generates a limited number of trial step lengths until it finds one that loosely approximates the minimum of \\eqref{eq:line_search_min}. At the new point a new search direction and step length are computed, and the process is repeated.\nIn the second algorithmic strategy, known as trust region, the information gathered about $f$ is used to construct a model function $m_k$ whose behavior near the current point $\\mathbf{x}_k$ is similar to that of the actual objective function $f$. Because the model $m_k$ may not be a good approximation of $f$ when $\\mathbf{x}$ is far from $\\mathbf{x}_k$, we restrict the search for a minimizer of $m_k$ to some region around $\\mathbf{x}_k$. In other words, we find the candidate step $\\mathbf{p}$ by approximately solving the following subproblem:\n\\begin{equation} \\min _{\\mathbf{p}} m_k\\left(\\mathbf{x}_k+\\mathbf{p}\\right), \\quad \\text { where } \\mathbf{x}_k+\\mathbf{p} \\text { lies inside the trust region. } \\label{eq:trust_region_subproblem} \\end{equation}\nIf the candidate solution does not produce a sufficient decrease in $f$, we conclude that the trust region is too large, and we shrink it and re-solve \\eqref{eq:trust_region_subproblem}. Usually, the trust region is a ball defined by $\\lVert\\mathbf{p}\\rVert_2 \\leq \\Delta$, where the scalar $\\Delta\u0026gt;0$ is called the trust-region radius. Elliptical and box-shaped trust regions may also be used.\nThe model $m_k$ in \\eqref{eq:trust_region_subproblem} is usually defined to be a quadratic function of the form\n\\begin{equation} m_k\\left(\\mathbf{x}_k+\\mathbf{p}\\right)=f_k+\\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\mathbf{p}^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{p} \\label{eq:quadratic_model} \\end{equation}\nwhere $f_k$, $\\nabla f_k$, and $\\mathbf{B}_k$ are a scalar, vector, and matrix, respectively. As the notation indicates, $f_k$ and $\\nabla f_k$ are chosen to be the function and gradient values at the point $\\mathbf{x}_k$, so that $m_k$ and $f$ are in agreement to first order at the current iterate $\\mathbf{x}_k$. The matrix $\\mathbf{B}_k$ is either the Hessian $\\nabla^2 f_k$ or some approximation to it.\nSuppose that the objective function is given by $f(\\mathbf{x})=10\\left(x_2-x_1^2\\right)^2+\\left(1-x_1\\right)^2$. At the point $\\mathbf{x}_k=(0,1)$ its gradient and Hessian are\n$$ \\nabla f_k=\\left[\\begin{array}{c} -2 \\\\ 20 \\end{array}\\right], \\quad \\nabla^2 f_k=\\left[\\begin{array}{cc} -38 \u0026amp; 0 \\\\ 0 \u0026amp; 20 \\end{array}\\right] $$\nNote that each time we decrease the size of the trust region after failure of a candidate iterate, the step from $\\mathbf{x}_k$ to the new candidate will be shorter, and it usually points in a different direction from the previous candidate. The trust-region strategy differs in this respect from line search, which stays with a single search direction.\nIn a sense, the line search and trust-region approaches differ in the order in which they choose the direction and distance of the move to the next iterate. Line search starts by fixing the direction $\\mathbf{p}_k$ and then identifying an appropriate distance, namely the step length $\\alpha_k$. In trust region, we first choose a maximum distance-the trust-region radius $\\Delta_k$-and then seek a direction and step that attain the best improvement possible subject to this distance constraint. If this step proves to be unsatisfactory, we reduce the distance measure $\\Delta_k$ and try again.\nThe line search approach is discussed in more detail in this lecture while the trust-region strategy, is left to the reader to study.\nSearch directions for line search methods # The steepest-descent direction $-\\nabla f_k$ is the most obvious choice for search direction for a line search method. It is intuitive; among all the directions we could move from $\\mathbf{x}_k$, it is the one along which $f$ decreases most rapidly. To verify this claim, we appeal again to Taylor\u0026rsquo;s theorem, which tells us that for any search direction $\\mathbf{p}$ and step-length parameter $\\alpha$, we have\n\\begin{equation} f\\left(\\mathbf{x}_k+\\alpha \\mathbf{p}\\right)=f\\left(\\mathbf{x}_k\\right)+\\alpha \\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\alpha^2 \\mathbf{p}^{\\mathrm{T}} \\nabla^2 f\\left(\\mathbf{x}_k+t \\mathbf{p}\\right) \\mathbf{p}, \\quad \\text { for some } t \\in(0, \\alpha) \\label{eq:taylor_expansion} \\end{equation}\nThe rate of change in $f$ along the direction $\\mathbf{p}$ at $\\mathbf{x}_k$ is simply the coefficient of $\\alpha$, namely, $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k$. Hence, the unit direction $\\mathbf{p}$ of most rapid decrease is the solution to the problem\n\\begin{equation} \\min _{\\mathbf{p}} \\mathbf{p}^{\\mathrm{T}} \\nabla f_k, \\quad \\text { subject to }\\lVert\\mathbf{p}\\rVert=1 \\label{eq:steepest_descent_problem} \\end{equation}\nSince $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k=\\lVert\\mathbf{p}\\rVert\\lVert\\nabla f_k\\rVert \\cos \\theta$, where $\\theta$ is the angle between $\\mathbf{p}$ and $\\nabla f_k$, we have from $\\lVert\\mathbf{p}\\rVert=1$ that $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k=\\lVert\\nabla f_k\\rVert \\cos \\theta$, so the objective in \\eqref{eq:steepest_descent_problem} is minimized when $\\cos \\theta$ takes on its minimum value of -1 at $\\theta=\\pi$ radians. In other words, the solution to \\eqref{eq:steepest_descent_problem} is\n$$ \\mathbf{p}=-\\nabla f_k /\\lVert\\nabla f_k\\rVert $$\nas claimed. This direction is orthogonal to the contours of the function.\nThe steepest descent method is a line search method that moves along $\\mathbf{p}_k=-\\nabla f_k$ at every step. It can choose the step length $\\alpha_k$ in a variety of ways, as we will see in next chapter. One advantage of the steepest descent direction is that it requires calculation of the gradient $\\nabla f_k$ but not of second derivatives. However, it can be excruciatingly slow on difficult problems.\nLine search methods may use search directions other than the steepest descent direction. In general, any descent direction-one that makes an angle of strictly less than $\\pi / 2$ radians with $-\\nabla f_k$-is guaranteed to produce a decrease in $f$, provided that the step length is sufficiently small. We can verify this claim by using Taylor\u0026rsquo;s theorem. From \\eqref{eq:taylor_expansion}, we have that\n$$ f\\left(\\mathbf{x}_k+\\epsilon \\mathbf{p}_k\\right)=f\\left(\\mathbf{x}_k\\right)+\\epsilon \\mathbf{p}_k^{\\mathrm{T}} \\nabla f_k+O\\left(\\epsilon^2\\right) $$\nWhen $\\mathbf{p}_k$ is a downhill direction, the angle $\\theta_k$ between $\\mathbf{p}_k$ and $\\nabla f_k$ has $\\cos \\theta_k\u0026lt;0$, so that\n$$ \\mathbf{p}_k^{\\mathrm{T}} \\nabla f_k=\\lVert\\mathbf{p}_k\\rVert\\lVert\\nabla f_k\\rVert \\cos \\theta_k\u0026lt;0 $$\nIt follows that $f\\left(\\mathbf{x}_k+\\epsilon \\mathbf{p}_k\\right)\u0026lt;f\\left(\\mathbf{x}_k\\right)$ for all positive but sufficiently small values of $\\epsilon$.\nAll of the search directions discussed so far can be used directly in a line search framework. They give rise to the steepest descent, Newton, quasi-Newton, and conjugate gradient line search methods. For Newton and quasi-Newton methods, see the next chapter.\nStep-length conditions # In computing the step length $\\alpha_{k}$, we face a tradeoff. We would like to choose $\\alpha_{k}$ to give a substantial reduction of $f$, but at the same time, we do not want to spend too much time making the choice. The ideal choice would be the global minimizer of the univariate function $\\phi(\\cdot)$ defined by\n\\begin{equation} \\phi(\\alpha)=f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right), \\quad \\alpha\u0026gt;0 \\label{eq:phi_def} \\end{equation}\nbut in general, it is too expensive to identify this value. To find even a local minimizer of $\\phi$ to moderate precision generally requires too many evaluations of the objective function $f$ and possibly the gradient $\\nabla f$. More practical strategies perform an inexact line search to identify a step length that achieves adequate reductions in $f$ at minimal cost.\nTypical line search algorithms try out a sequence of candidate values for $\\alpha$, stopping to accept one of these values when certain conditions are satisfied. The line search is done in two stages: A bracketing phase finds an interval containing desirable step lengths, and a bisection or interpolation phase computes a good step length within this interval. Sophisticated line search algorithms can be quite complicated, so we defer a full description until the end of this chapter. We now discuss various termination conditions for the line search algorithm and show that effective step lengths need not lie near minimizers of the univariate function $\\phi(\\alpha)$ defined in \\eqref{eq:phi_def}.\nA simple condition we could impose on $\\alpha_{k}$ is that it provide a reduction in $f$, i.e., $f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)\u0026lt;f\\left(\\mathbf{x}_{k}\\right)$. The difficulty is that we do not have sufficient reduction in $f$, a concept we discuss next.\nThe Wolfe conditions # A popular inexact line search condition stipulates that $\\alpha_{k}$ should first of all give sufficient decrease in the objective function $f$, as measured by the following inequality:\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:armijo} \\end{equation}\nfor some constant $c_{1} \\in(0,1)$. In other words, the reduction in $f$ should be proportional to both the step length $\\alpha_{k}$ and the directional derivative $\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$. Inequality \\eqref{eq:armijo} is sometimes called the Armijo condition.\nThe right-hand-side of \\eqref{eq:armijo}, which is a linear function, can be denoted by $l(\\alpha)$. The function $l(\\cdot)$ has negative slope $c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$, but because $c_{1} \\in(0,1)$, it lies above the graph of $\\phi$ for small positive values of $\\alpha$. The sufficient decrease condition states that $\\alpha$ is acceptable only if $\\phi(\\alpha) \\leq l(\\alpha)$. In practice, $c_{1}$ is chosen to be quite small, say $c_{1}=10^{-4}$.\nThe sufficient decrease condition is not enough by itself to ensure that the algorithm makes reasonable progress, because it is satisfied for all sufficiently small values of $\\alpha$. To rule out unacceptably short steps we introduce a second requirement, called the curvature condition, which requires $\\alpha_{k}$ to satisfy\n\\begin{equation} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \\geq c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:curvature} \\end{equation}\nfor some constant $c_{2} \\in\\left(c_{1}, 1\\right)$, where $c_{1}$ is the constant from \\eqref{eq:armijo}. Note that the left-handside is simply the derivative $\\phi^{\\prime}\\left(\\alpha_{k}\\right)$, so the curvature condition ensures that the slope of $\\phi\\left(\\alpha_{k}\\right)$ is greater than $c_{2}$ times the gradient $\\phi^{\\prime}(0)$. This makes sense because if the slope $\\phi^{\\prime}(\\alpha)$ is strongly negative, we have an indication that we can reduce $f$ significantly by moving further along the chosen direction. On the other hand, if the slope is only slightly negative or even positive, it is a sign that we cannot expect much more decrease in $f$ in this direction, so it might make sense to terminate the line search. Typical values of $c_{2}$ are 0.9 when the search direction $\\mathbf{p}_{k}$ is chosen by a Newton or quasi-Newton method, and 0.1 when $\\mathbf{p}_{k}$ is obtained from a nonlinear conjugate gradient method.\nThe sufficient decrease and curvature conditions are known collectively as the Wolfe conditions. We restate them here for future reference:\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \u0026amp; \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\\\ \\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \u0026amp; \\geq c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\end{aligned} \\label{eq:wolfe} \\end{equation}\nwith $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$.\nA step length may satisfy the Wolfe conditions without being particularly close to a minimizer of $\\phi$. We can, however, modify the curvature condition to force $\\alpha_{k}$ to lie in at least a broad neighborhood of a local minimizer or stationary point of $\\phi$. The strong Wolfe conditions require $\\alpha_{k}$ to satisfy\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \u0026amp; \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\\\ \\left|\\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k}\\right| \u0026amp; \\leq c_{2}\\left|\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\\right|, \\end{aligned} \\label{eq:strong_wolfe} \\end{equation}\nwith $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$. The only difference with the Wolfe conditions is that we no longer allow the derivative $\\phi^{\\prime}\\left(\\alpha_{k}\\right)$ to be too positive. Hence, we exclude points that are far from stationary points of $\\phi$.\nIt is not difficult to prove that there exist step lengths that satisfy the Wolfe conditions for every function $f$ that is smooth and bounded below.\nLemma 4.1 (Existence of step lengths)\nSuppose that $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ is continuously differentiable. Let $\\mathbf{p}_{k}$ be a descent direction at $\\mathbf{x}_{k}$, and assume that $f$ is bounded below along the ray $\\left\\{\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k} \\mid \\alpha\u0026gt;0\\right\\}$. Then if $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$, there exist intervals of step lengths satisfying the Wolfe conditions \\eqref{eq:wolfe} and the strong Wolfe conditions \\eqref{eq:strong_wolfe}. Proof\nSince $\\phi(\\alpha)=f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right)$ is bounded below for all $\\alpha\u0026gt;0$ and since $0\u0026lt;c_{1}\u0026lt;1$, the line $l(\\alpha)=f\\left(\\mathbf{x}_{k}\\right)+\\alpha c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$ must intersect the graph of $\\phi$ at least once. Let $\\alpha^{\\prime}\u0026gt;0$ be the smallest intersecting value of $\\alpha$, that is,\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime} \\mathbf{p}_{k}\\right)=f\\left(\\mathbf{x}_{k}\\right)+\\alpha^{\\prime} c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:intersection} \\end{equation}\nThe sufficient decrease condition \\eqref{eq:armijo} clearly holds for all step lengths less than $\\alpha^{\\prime}$.\nBy the mean value theorem, there exists $\\alpha^{\\prime \\prime} \\in\\left(0, \\alpha^{\\prime}\\right)$ such that\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime} \\mathbf{p}_{k}\\right)-f\\left(\\mathbf{x}_{k}\\right)=\\alpha^{\\prime} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime \\prime} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:mean_value} \\end{equation}\nBy combining \\eqref{eq:intersection} and \\eqref{eq:mean_value}, we obtain\n\\begin{equation} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime \\prime} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k}=c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\u0026gt;c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:inequality_proof} \\end{equation}\nsince $c_{1}\u0026lt;c_{2}$ and $\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\u0026lt;0$. Therefore, $\\alpha^{\\prime \\prime}$ satisfies the Wolfe conditions \\eqref{eq:wolfe}, and the inequalities hold strictly in both conditions. Hence, by our smoothness assumption on $f$, there is an interval around $\\alpha^{\\prime \\prime}$ for which the Wolfe conditions hold. Moreover, since the term in the left-hand side of \\eqref{eq:inequality_proof} is negative, the strong Wolfe conditions \\eqref{eq:strong_wolfe} hold in the same interval.\n■ The Wolfe conditions are scale-invariant in a broad sense: Multiplying the objective function by a constant or making an affine change of variables does not alter them. They can be used in most line search methods, and are particularly important in the implementation of quasi-Newton methods.\nTo summarize, see the following interactive visualisation of the Wolfe conditions, which illustrates the sufficient decrease and curvature conditions in action:\nThe Goldstein conditions # Like the Wolfe conditions, the Goldstein conditions also ensure that the step length $\\alpha$ achieves sufficient decrease while preventing $\\alpha$ from being too small. The Goldstein conditions can also be stated as a pair of inequalities, in the following way:\n\\begin{equation} f\\left(\\mathbf{x}_{k}\\right)+(1-c) \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\leq f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:goldstein} \\end{equation}\nwith $0\u0026lt;c\u0026lt;\\frac{1}{2}$. The second inequality is the sufficient decrease condition \\eqref{eq:armijo}, whereas the first inequality is introduced to control the step length from below.\nA disadvantage of the Goldstein conditions vis-à-vis the Wolfe conditions is that the first inequality in \\eqref{eq:goldstein} may exclude all minimizers of $\\phi$. However, the Goldstein and Wolfe conditions have much in common, and their convergence theories are quite similar. The Goldstein conditions are often used in Newton-type methods but are not well suited for quasi-Newton methods that maintain a positive definite Hessian approximation.\nAn illustration of the Goldstein conditions is shown in the following interactive visualisation:\nSufficient decrease and backtracking # We have mentioned that the sufficient decrease condition \\eqref{eq:armijo} alone is not sufficient to ensure that the algorithm makes reasonable progress along the given search direction. However, if the line search algorithm chooses its candidate step lengths appropriately, by using a so-called backtracking approach, we can dispense with the extra condition \\eqref{eq:curvature} and use just the sufficient decrease condition to terminate the line search procedure. In its most basic form, backtracking proceeds as follows.\nProcedure (Backtracking Line Search).\nChoose $\\bar{\\alpha}\u0026gt;0, \\rho, c \\in(0,1)$;\nset $\\alpha \\leftarrow \\bar{\\alpha}$;\nrepeat until $f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c \\alpha \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$\n$\\alpha \\leftarrow\\rho\\alpha$;\nend (repeat)\nterminate with $\\alpha_{k}=\\alpha$.\nIn this procedure, the initial step length $\\bar{\\alpha}$ is chosen to be 1 in Newton and quasi-Newton methods, but can have different values in other algorithms such as steepest descent or conjugate gradient. An acceptable step length will be found after a finite number of trials because $\\alpha_{k}$ will eventually become small enough that the sufficient decrease condition holds. In practice, the contraction factor $\\rho$ is often allowed to vary at each iteration of the line search. For example, it can be chosen by safeguarded interpolation, as we describe later. We need ensure only that at each iteration we have $\\rho \\in\\left[\\rho_{\\mathrm{lo}}, \\rho_{\\mathrm{hi}}\\right]$, for some fixed constants $0\u0026lt;\\rho_{\\text {lo }}\u0026lt;\\rho_{\\text {hi }}\u0026lt;1$.\nThe backtracking approach ensures either that the selected step length $\\alpha_{k}$ is some fixed value (the initial choice $\\bar{\\alpha}$ ), or else that it is short enough to satisfy the sufficient decrease condition but not too short. The latter claim holds because the accepted value $\\alpha_{k}$ is within striking distance of the previous trial value, $\\alpha_{k} / \\rho$, which was rejected for violating the sufficient decrease condition, that is, for being too long.\nConvergence of line search methods # To obtain global convergence, we must not only have well-chosen step lengths but also well-chosen search directions $\\mathbf{p}_k$. We discuss requirements on the search direction in this section, focusing on one key property: the angle $\\theta_k$ between $\\mathbf{p}_k$ and the steepest descent direction $-\\nabla f_k$, defined by\n$$ \\cos \\theta_k=\\frac{-\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k}{\\|\\nabla f_k\\|\\|\\mathbf{p}_k\\|} $$\nThe following theorem, due to Zoutendijk, has far-reaching consequences. It shows, for example, that the steepest descent method is globally convergent. For other algorithms it describes how far $\\mathbf{p}_k$ can deviate from the steepest descent direction and still give rise to a globally convergent iteration. Various line search termination conditions can be used to establish this result, but for concreteness we will consider only the Wolfe conditions. Though Zoutendijk\u0026rsquo;s result appears, at first, to be technical and obscure, its power will soon become evident.\nTheorem 4.1 (Zoutendijk\u0026#39;s theorem)\nConsider any iteration of the form $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, where $\\mathbf{p}_k$ is a descent direction and $\\alpha_k$ satisfies the Wolfe conditions. Suppose that $f$ is bounded below in $\\mathbb{R}^n$ and that $f$ is continuously differentiable in an open set $\\mathcal{N}$ containing the level set $\\mathcal{L} = \\{\\mathbf{x}: f(\\mathbf{x}) \\leq f(\\mathbf{x}_0)\\}$, where $\\mathbf{x}_0$ is the starting point of the iteration. Assume also that the gradient $\\nabla f$ is Lipschitz continuous on $\\mathcal{N}$, that is, there exists a constant $L\u0026gt;0$ such that\n\\begin{equation} \\|\\nabla f(\\mathbf{x})-\\nabla f(\\tilde{\\mathbf{x}})\\| \\leq L\\|\\mathbf{x}-\\tilde{\\mathbf{x}}\\|, \\quad \\text { for all } \\mathbf{x}, \\tilde{\\mathbf{x}} \\in \\mathcal{N} . \\label{eq:lipschitz} \\end{equation}\nThen\n\\begin{equation} \\sum_{k \\geq 0} \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2}\u0026lt;\\infty \\label{eq:zoutendijk_condition} \\end{equation}\nProof\nFrom the second Wolfe condition and the iteration formula we have that\n$$ (\\nabla f_{k+1}-\\nabla f_k)^{\\mathrm{T}} \\mathbf{p}_k \\geq(c_2-1) \\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k $$\nwhile the Lipschitz condition \\eqref{eq:lipschitz} implies that\n$$ (\\nabla f_{k+1}-\\nabla f_k)^{\\mathrm{T}} \\mathbf{p}_k \\leq \\alpha_k L\\|\\mathbf{p}_k\\|^{2} $$\nBy combining these two relations, we obtain\n$$ \\alpha_k \\geq \\frac{c_2-1}{L} \\frac{\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k}{\\|\\mathbf{p}_k\\|^{2}} $$\nBy substituting this inequality into the first Wolfe condition, we obtain\n$$ f_{k+1} \\leq f_k-c_1 \\frac{1-c_2}{L} \\frac{(\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k)^{2}}{\\|\\mathbf{p}_k\\|^{2}} $$\nFrom the definition of $\\cos \\theta_k$, we can write this relation as\n$$ f_{k+1} \\leq f_k-c \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2} $$\nwhere $c=c_1(1-c_2) / L$. By summing this expression over all indices less than or equal to $k$, we obtain\n$$ f_{k+1} \\leq f_0-c \\sum_{j=0}^{k} \\cos ^{2} \\theta_j\\|\\nabla f_j\\|^{2} $$\nSince $f$ is bounded below, we have that $f_0-f_{k+1}$ is less than some positive constant, for all $k$. Hence by taking limits, we obtain\n$$ \\sum_{k=0}^{\\infty} \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2}\u0026lt;\\infty $$\nwhich concludes the proof.\n■ Similar results to this theorem hold when the Goldstein conditions or strong Wolfe conditions are used in place of the Wolfe conditions.\nNote that the assumptions of Theorem 4.1 are not too restrictive. If the function $f$ were not bounded below, the optimization problem would not be well-defined. The smoothness assumption—Lipschitz continuity of the gradient—is implied by many of the smoothness conditions that are used in local convergence theorems and are often satisfied in practice.\nInequality \\eqref{eq:zoutendijk_condition}, which we call the Zoutendijk condition, implies that\n$$ \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2} \\rightarrow 0 $$\nThis limit can be used in turn to derive global convergence results for line search algorithms. If our method for choosing the search direction $\\mathbf{p}_k$ in the iteration ensures that the angle $\\theta_k$ is bounded away from $90^{\\circ}$, there is a positive constant $\\delta$ such that\n\\begin{equation} \\cos \\theta_k \\geq \\delta\u0026gt;0, \\quad \\text { for all } k \\label{eq:angle_bound} \\end{equation}\nIt follows immediately from \\eqref{eq:zoutendijk_condition} that\n\\begin{equation} \\lim _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 \\label{eq:global_convergence} \\end{equation}\nIn other words, we can be sure that the gradient norms $\\|\\nabla f_k\\|$ converge to zero, provided that the search directions are never too close to orthogonality with the gradient. In particular, the method of steepest descent (for which the search direction $\\mathbf{p}_k$ makes an angle of zero degrees with the negative gradient) produces a gradient sequence that converges to zero, provided that it uses a line search satisfying the Wolfe or Goldstein conditions.\nWe use the term globally convergent to refer to algorithms for which the property \\eqref{eq:global_convergence} is satisfied, but note that this term is sometimes used in other contexts to mean different things. For line search methods of the general form $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, the limit \\eqref{eq:global_convergence} is the strongest global convergence result that can be obtained: We cannot guarantee that the method converges to a minimizer, but only that it is attracted by stationary points. Only by making additional requirements on the search direction $\\mathbf{p}_k$—by introducing negative curvature information from the Hessian $\\nabla^{2} f(\\mathbf{x}_k)$, for example—can we strengthen these results to include convergence to a local minimum.\nNote that throughout this section we have used only the fact that Zoutendijk\u0026rsquo;s condition implies the limit \\eqref{eq:zoutendijk_condition}.\nRate of convergence # We refer the reader to the textbook\n\u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, 2nd edition, Springer, 2006, pages 47-51,\nfor a detailed discussion of the rate of convergence of line search methods.\nPeculiarly, see pages 47-51. In general, the rate of convergence depends on the choice of search direction and the step length conditions used.\n"},{"id":14,"href":"/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/","title":"6. Constrained optimization","section":"I - Fundamentals","content":" Constrained optimization methods # Lorem\n$$ \\mathbf{X} = \\operatorname{argmin} || \\mathbf{Y} - \\mathbf{A}\\mathbf{X} \\|_2 + \\mathcal{R}(\\mathbf{X}) $$\n"},{"id":15,"href":"/numerical_optimization/docs/lectures/fundamentals/","title":"I - Fundamentals","section":"Lectures","content":" Fundamentals # Content 1. Optimization problems 2. Unconstrained optimization : basics 3. Convexity theory 4. Unconstrained optimization : linesearch 6. Constrained optimization "},{"id":16,"href":"/numerical_optimization/docs/practical_labs/linear_regression/","title":"I - Linear Regression models","section":"Practical labs","content":" Linear Regression models # Introduction # In this lab session, we will explore the fundamental concepts of numerical optimization through the lens of linear regression. We\u0026rsquo;ll begin with the simplest case and gradually build up to more complex scenarios, comparing analytical solutions with numerical methods at each step.\nLinear regression is perhaps the most fundamental problem in machine learning and statistics. While it has a closed-form solution, implementing numerical optimization methods for this problem provides excellent intuition for more complex optimization scenarios where analytical solutions don\u0026rsquo;t exist.\nLearning objectives # By the end of this session, you should be able to:\nDerive the analytical solution for simple linear regression Implement gradient descent with various step size strategies Understand the connection between the one-dimensional and multi-dimensional cases Apply line search techniques to improve convergence I - One dimensional case # Let us first start with the form that most people are familiar with, the linear regression model in one dimension. The setup is as follows:\nWe have a set of data points $\\{(x_i, y_i)\\}_{i=1}^n$. Here the $x_i$ are the input features and the $y_i$ are the target values. Assuming there is a linear relationship between and target because of some underlying phenomenon, we model the observations as: \\begin{equation} y_i = \\alpha x_i + \\beta + \\epsilon \\label{eq:linear_model_1d} \\end{equation} where $\\epsilon$ is a random noise term that we assume to be normally distributed with mean 0 and variance $\\sigma^2$. Our goal is then to find the parameters $\\alpha$ and $\\beta$ that \u0026ldquo;best match\u0026rdquo; the data points. Such a program can is illustrated with following interactive plot.\n1. Modeling and solving the problem # Propose a loss function that quantifies the difference between the observed $y_i$ and the predicted values $\\hat{y}_i = \\alpha x_i + \\beta$. Hint The most common loss function for regression problems is the mean squared error (MSE): $$ L(\\alpha, \\beta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - (\\alpha x_i + \\beta))^2 $$ Show that the loss function is convex in the parameters $\\alpha$ and $\\beta$. Hint To show convexity, we need to demonstrate that the Hessian matrix of second derivatives is positive semi-definite. Or that the function is a positive linear combination of convex functions. The loss function is a quadratic function in $\\alpha$ and $\\beta$, which is convex. The Hessian matrix will have positive eigenvalues, confirming convexity. Derive the analytical solution for the parameters $\\alpha$ and $\\beta$ by setting the gradients of the loss function with respect to these parameters to zero. Hint It is often useful to express the gradients in terms of the means and variances of the data points:\nmeans : $$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$ variance: $$s_{xx} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2$$ covariance: $$s_{xy} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$$ Implement the analytical solution in Python and compute the optimal parameters for a given dataset. To generate dataset, you can use following code snippet: import numpy as np import matplotlib.pyplot as plt # Set random seed for reproducibility rng = np.random.default_rng(42) # Generate synthetic data n_samples = 50 x = np.linspace(0, 10, n_samples) # True parameters alpha = 2.5 beta = 1.0 # Add Gaussian noise noise = rng.normal(0, 1, n_samples) y = alpha * x + beta + noise # Visualize the data plt.figure(figsize=(8, 6)) plt.scatter(x, y, alpha=0.7, label=\u0026#39;Data points\u0026#39;) plt.xlabel(\u0026#39;x\u0026#39;) plt.ylabel(\u0026#39;y\u0026#39;) plt.title(\u0026#39;Synthetic linear data with noise\u0026#39;) plt.grid(True, alpha=0.3) plt.legend() plt.show() (Bonus) Show that doing a Maximum Likelihood Estimation (MLE) for the parameters $\\alpha$ and $\\beta$ leads to the same solution as minimizing the loss function derived above. Hint The MLE for the parameters in a linear regression model with Gaussian noise leads to minimizing the negative log-likelihood, which is equivalent to minimizing the mean squared error loss function. The derivation involves taking the logarithm of the Gaussian probability density function and simplifying it, leading to the same equations for $\\alpha$ and $\\beta$ as derived from the loss function. 2. Gradient descent for the one-dimensional case # Now that we have a good understanding of the problem and have implemented the analytical solution, let\u0026rsquo;s explore how we can solve this problem using numerical optimization techniques, specifically steepest gradient descent, i.e always taking the gradient as the direction.\nRecalling the update rule for steepest gradient descent of a point $\\theta_k$ at iteration $k$: $$ \\theta_{k+1} = \\theta_k - \\alpha_k \\nabla L(\\theta_k) $$ where $\\alpha_k$ is the step size at iteration $k$, give the update rule for the parameters $\\alpha$ and $\\beta$ in the context of our linear regression problem.\nHint The update rules for the parameters $\\alpha$ and $\\beta$ can be derived from the gradients of the loss function: $$ \\begin{align*} \\alpha_{k+1} \u0026amp;= \\alpha_k - \\alpha_k \\frac{\\partial L}{\\partial \\alpha}(\\alpha_k, \\beta_k) \\\\ \\beta_{k+1} \u0026amp;= \\beta_k - \\alpha_k \\frac{\\partial L}{\\partial \\beta}(\\alpha_k, \\beta_k) \\end{align*} $$ where the gradients are computed as follows: $$ \\begin{align*} \\frac{\\partial L}{\\partial \\alpha} \u0026amp;= -\\sum_{i=1}^n (y_i - (\\alpha_k x_i + \\beta_k)) x_i \\\\ \\frac{\\partial L}{\\partial \\beta} \u0026amp;= -\\sum_{i=1}^n (y_i - (\\alpha_k x_i + \\beta_k)) \\end{align*} $$ Implement gradient descent with a constant step size $\\alpha_k = \\alpha$ for all iterations. Your function should:\nTake initial parameters $(\\alpha_0, \\beta_0)$, step size $\\alpha$, and number of iterations as inputs. Return the trajectory of parameters and loss values. Include a stopping criterion based on gradient magnitude. Experiment with different step sizes: $\\alpha \\in \\{0.0001, 0.001, 0.01, 0.1\\}$. Plot the loss function over iterations for each case. What do you observe?\nHint The loss function should decrease over iterations, but the rate of decrease will depend on the step size. A very small step size will lead to slow convergence, while a very large step size may cause divergence or oscillations. For a fixed number of iterations (say 1000), plot the final error as a function of step size on a logarithmic scale. What is the optimal range for $\\alpha$? Hint The optimal range for $\\alpha$ is typically small enough to ensure convergence but large enough to allow for reasonable speed of convergence. You may find that values around $0.001$ to $0.01$ work well, but this can depend on the specific dataset and problem. Let\u0026rsquo;s try a first experiment with a decreasing step size. Implement a linear decay strategy: $$ \\alpha_k = \\alpha_0 \\cdot \\frac{T - k}{T}$$ where $T$ is the total number of iterations. Compare the convergence behavior with constant step size. Plot the loss function and parameter trajectories over iterations.\nWhy might decreasing step sizes be beneficial? What are the trade-offs between aggressive and conservative decay rates? Hint Decreasing step sizes can help avoid overshooting the minimum and allow for finer adjustments as the algorithm converges.\nIn nonconvex problmes, aggressive decay rates may lead to faster convergence initially but can cause the algorithm to get stuck in local minima, while conservative rates may lead to slower convergence but better exploration of the parameter space.\nTry implementing an exponential decay strategy: $$ \\alpha_k = \\alpha_0 \\cdot \\gamma^k$$ where $\\gamma \\in (0, 1)$ is the decay rate. Experiment with different values of $\\gamma$ (e.g., $0.9$, $0.95$, $0.99$) and compare the convergence behavior with constant and linear decay strategies. Hint Exponential decay is more aggressive, thus it may also cause the step size to become too small too quickly, leading to slow convergence in later iterations. The choice of $\\gamma$ can significantly affect the convergence behavior. II - Multiple variables case # Now that we have a good understanding of the one-dimensional case, let\u0026rsquo;s generalize our approach to multiple dimensions. The setup is similar, but now we have multiple features and parameters:\nWe have a set of data points $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ are the input features and $y_i \\in \\mathbb{R}$ are the target values. We model the observations as: \\begin{equation} y_i = \\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_i + \\beta + \\epsilon \\label{eq:linear_model_d} \\end{equation} where $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector, $\\beta \\in \\mathbb{R}$ is the bias term, and $\\epsilon$ is a random noise term that we assume to be normally distributed with mean 0 and variance $\\sigma^2$. Figure 0.1: 2D linear regression\nAn example of this model is illustrated in Figure 0.1 , where the data points are represented in a two-dimensional space, and the linear regression model fits a plane to the data.\nThis model is more general than the one-dimensional case, as it allows for multiple features to influence the target variable. Notably, we can also augment the feature vectors with a constant term to simplify the notation so that we don\u0026rsquo;t have to deal with the bias term separately. We define: \\begin{equation} \\tilde{\\mathbf{x}}_i = [1, \\mathbf{x}_i^{\\mathrm{T}}]^{\\mathrm{T}} \\in \\mathbb{R}^{d+1} \\end{equation} and \\begin{equation} \\tilde{\\mathbf{w}} = [\\beta, \\mathbf{w}^{\\mathrm{T}}]^{\\mathrm{T}} \\in \\mathbb{R}^{d+1} \\end{equation} so that we can rewrite the model as: \\begin{equation} y_i = \\tilde{\\mathbf{w}}^{\\mathrm{T}} \\tilde{\\mathbf{x}}_i + \\epsilon \\label{eq:linear_model_d_augmented} \\end{equation}\nOur goal is then to find the parameters $\\mathbf{w}$ and $\\beta$ that \u0026ldquo;best match\u0026rdquo; the data points, or in augmented notation, to find $\\tilde{\\mathbf{w}}$ that minimizes the loss function.\n1. Modeling and solving the problem # While the augmented formulation is nice, we can also express the model in matrix form for the observed data. We define the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times (d+1)}$ as: \\begin{equation} \\mathbf{X} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; x_{12} \u0026amp; \\ldots \u0026amp; x_{1d} \\\\ 1 \u0026amp; x_{21} \u0026amp; x_{22} \u0026amp; \\ldots \u0026amp; x_{2d} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n1} \u0026amp; x_{n2} \u0026amp; \\ldots \u0026amp; x_{nd} \\end{bmatrix} \\end{equation} and the target vector $\\mathbf{y} \\in \\mathbb{R}^n$ as: \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\end{equation}\nThen we can express the model as: \\begin{equation} \\mathbf{y} = \\mathbf{X} \\tilde{\\mathbf{w}} + \\boldsymbol{\\epsilon}, \\label{eq:linear_model_matrix} \\end{equation}\nwhere $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n$ is the noise vector. This more compact formulation is interesting for several reasons:\nIt already encapsulates the observed data in the model and we consider all the $y_i$ as a vector, which allows us to work with the entire dataset at once. the matrix form allow us to obtain solutions that will be expressed as matrix operations, which is more efficient for larger datasets it allows us to use linear algebra techniques to derive the solution Propose a loss function that quantifies the difference between the observed $y_i$ and the predicted values $\\hat{y}_i = \\mathbf{X} \\tilde{\\mathbf{w}}$. Hint The most common loss function for regression problems is the mean squared error (MSE): $$ L(\\tilde{\\mathbf{w}}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}\\|^2_2 $$ Show that the loss function is convex in the parameters $\\tilde{\\mathbf{w}}$. Hint As in the one-dimensional case, the loss function is a quadratic function in $\\tilde{\\mathbf{w}}$, which is convex. The Hessian matrix of second derivatives will be positive semi-definite, confirming convexity. Derive the gradient of the loss function with respect to $\\tilde{\\mathbf{w}}$ in matrix form. To help yourselves, you can use the properties of matrix derivatives from matrix cookbook available here and identity of vector norms: $$ \\lVert \\mathbf{u} - \\mathbf{v} \\rVert^2_2 = \\mathbf{u}^{\\mathrm{T}} \\mathbf{u} - 2 \\mathbf{u}^{\\mathrm{T}} \\mathbf{v} + \\mathbf{v}^{\\mathrm{T}} \\mathbf{v}. $$ and show that optimal solution $\\tilde{\\mathbf{w}}$ satisfies the normal equations: $$ \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} \\tilde{\\mathbf{w}} = \\mathbf{X}^{\\mathrm{T}} \\mathbf{y}. $$\nHint Make use of\n$\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^{\\mathrm{T}} \\mathbf{a} = \\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{a}^{\\mathrm{T}} \\mathbf{x} = \\mathbf{a}$ $\\frac{\\partial}{\\partial \\mathbf{x}} \\|\\mathbf{x}\\|^2_2 = \\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^{\\mathrm{T}} \\mathbf{x} = 2 \\mathbf{x}$ $\\frac{\\partial}{\\partial \\mathbf{x}} \\lVert \\mathbf{A} \\mathbf{x} \\rVert^2_2 = 2 \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}$ Thus, to obtain optimal parameters $\\tilde{\\mathbf{w}}$, we can solve the normal equations: $$ \\tilde{\\mathbf{w}} = (\\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{-1} \\mathbf{X}^{\\mathrm{T}} \\mathbf{y}. $$\nNote: The matrix $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ is known as the Gram matrix, and it is positive semi-definite. If $\\mathbf{X}$ has full column rank, then $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ is invertible, and we can compute the unique solution for $\\tilde{\\mathbf{w}}$. Otherwise, the solution is not unique, and we may need to use regularization techniques (e.g., ridge regression) to obtain a stable solution, but we will not cover this in this lab.\nImplement the analytical solution using NumPy\u0026rsquo;s linear algebra functions. Compare your result with np.linalg.lstsq. To generate dataset, you can use following code snippet: import numpy as np # Generate multi-dimensional data d = 5 # number of features n_samples = 100 # Generate random features X = np.random.randn(n_samples, d) # Add intercept term X_augmented = np.column_stack([np.ones(n_samples), X]) # True parameters w_true = np.random.randn(d + 1) # including bias # Generate targets with noise y = X_augmented @ w_true + 0.5 * np.random.randn(n_samples) print(f\u0026#34;Data shape: {X.shape}\u0026#34;) print(f\u0026#34;Augmented data shape: {X_augmented.shape}\u0026#34;) print(f\u0026#34;True parameters: {w_true}\u0026#34;) 2. Gradient descent for the multiple variables case # Rather than inverting the matrix $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$, which can be computationally expensive for large datasets, we can use gradient descent to find the optimal parameters $\\tilde{\\mathbf{w}}$.\nGive the update rule for the parameters $\\tilde{\\mathbf{w}}$ in the context of our linear regression problem using steepest gradient descent. Hint The update rule for the parameters $\\tilde{\\mathbf{w}}$ can be derived from the gradient of the loss function: $$ \\tilde{\\mathbf{w}}_{k+1} = \\tilde{\\mathbf{w}}_k - \\alpha_k \\nabla L(\\tilde{\\mathbf{w}}_k) $$ where the gradient is given by: $$ \\nabla L(\\tilde{\\mathbf{w}}) = -\\mathbf{X}^{\\mathrm{T}} (\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}) $$ Implement gradient descent with a constant step size $\\alpha_k = \\alpha$ for all iterations. Your function should: Take initial parameters $\\tilde{\\mathbf{w}}_0$, step size $\\alpha$, and number of iterations as inputs. Return the trajectory of parameters and loss values. Include a stopping criterion based on gradient magnitude. 3. Experimenting with backtracking line search # From implementing gradient descent, we have seen that the choice of step size $\\alpha$ can significantly affect the convergence behavior. A fixed step size may not be optimal for all iterations, leading to slow convergence or oscillations. On the other hand, a decreasing step size is not the best choice as it may lead to very small step sizes in later iterations, causing slow convergence. Let us put in practice the theory we have set in place around line search techniques to adaptively choose the step size at each iteration.\nImplement a backtracking line search algorithm to adaptively choose the step size $\\alpha_k$ at each iteration. For a reminder, check the memo here.\nUse the backtracking line search to find the optimal step size for each iteration of gradient descent. Compare the convergence behavior with constant and decreasing step sizes.\nExperiment with different parameters for the backtracking line search, such as the initial step size $\\alpha_0$, reduction factor $\\rho$, and Armijo parameter $c_1$. How do these parameters affect the convergence behavior?\nHint The backtracking line search will adaptively adjust the step size based on the Armijo condition, allowing for more efficient convergence. The choice of $\\alpha_0$, $\\rho$, and $c_1$ can significantly affect the speed of convergence and stability of the algorithm. 4. Using more complex linesearch techniques using toolboxes # In practice, we often use more sophisticated line search techniques that are not so easy to implement from scratch. One such technique is the line_search function from SciPy\u0026rsquo;s optimization module, which implements interpolation techniques to find an optimal step size.\nUse the line_search function from SciPy\u0026rsquo;s optimization module to find the optimal step size for each iteration of gradient descent. Compare the convergence behavior with the backtracking line search. Documentation is available here. III - (Bonus) The general case # Consider the general case where the target is also a vector, i.e., we have a multi-output linear regression problem. The model can be expressed as: \\begin{equation} \\mathbf{Y} = \\mathbf{X} \\tilde{\\mathbf{W}} + \\boldsymbol{\\epsilon}, \\label{eq:linear_model_multi_output} \\end{equation} where $\\mathbf{Y} \\in \\mathbb{R}^{n \\times m}$ is the target matrix with $m$ outputs, and $\\tilde{\\mathbf{W}} \\in \\mathbb{R}^{(d+1) \\times m}$ is the weight matrix.\nDerive all the necessary tools to solve this problem using the same techniques as in the previous sections.\nIV - (Bonus) Regularization # In practice, we often encounter situations where the model is overfitting the data, especially in high-dimensional settings. Regularization techniques are used to prevent overfitting by adding a penalty term to the loss function.\nImplement L2 regularization (ridge regression) by adding a penalty term to the loss function: \\begin{equation} L(\\tilde{\\mathbf{w}}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}\\|^2_2 + \\frac{\\lambda}{2} \\|\\tilde{\\mathbf{w}}\\|^2_2, \\end{equation} where $\\lambda \u0026gt; 0$ is the regularization parameter.\nV - (Bonus) Nonlinear regression # It\u0026rsquo;s actually possible to extend the linear regression model to nonlinear regression by setting up the design matrix $\\mathbf{X}$ to include nonlinear features of the input data. For example, we can include polynomial features suc as: \\begin{equation} \\mathbf{X} = \\begin{bmatrix} 1 \u0026amp; x_1 \u0026amp; x_1^2 \u0026amp; \\ldots \u0026amp; x_1^d \\\\ 1 \u0026amp; x_2 \u0026amp; x_2^2 \u0026amp; \\ldots \u0026amp; x_2^d \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_n \u0026amp; x_n^2 \u0026amp; \\ldots \u0026amp; x_n^d \\end{bmatrix} \\end{equation}\nFrom this information and your own research , implement a nonlinear regression model using polynomial features. You can use the PolynomialFeatures class from sklearn.preprocessing to generate polynomial features.\n"},{"id":17,"href":"/numerical_optimization/docs/lectures/advanced/","title":"II - Advanced problems","section":"Lectures","content":" Advanced problems # Content 1. Unconstrained optimization : Second-order 2. Proximal methods "},{"id":18,"href":"/numerical_optimization/docs/practical_labs/remote_sensing/","title":"II - Remote Sensing project","section":"Practical labs","content":" Remote sensing project # Soon to be added.\n"},{"id":19,"href":"/numerical_optimization/docs/practical_labs/mnist/","title":"III - Digit recognition","section":"Practical labs","content":" Digit recognition with multi-layer perceptron # Soon to be added.\n"},{"id":20,"href":"/numerical_optimization/docs/lectures/machine_learning/","title":"III - Machine Learning problems","section":"Lectures","content":" Machine Learning problems # Content 1. From Linear regression to perceptron 2. Support Vector Machine 3. Neural Networks 4. Modern trends "},{"id":21,"href":"/numerical_optimization/docs/lectures/reminders/","title":"Reminders","section":"Lectures","content":" Reminders # Content Differentiation Linear Algebra "},{"id":22,"href":"/numerical_optimization/docs/practical_labs/environment/","title":"Lab environment","section":"Practical labs","content":" Lab Environment Setup # Welcome to the numerical optimization course! This page will guide you through setting up a modern, efficient Python environment using uv.\nPrerequisites # Good news! uv doesn\u0026rsquo;t require Python to be pre-installed - it can manage Python installations for you. However, having Python already installed won\u0026rsquo;t hurt.\nInstalling uv # 🐧 Linux \u0026amp; 🍎 macOS # The fastest way to install uv is using the official installer:\ncurl -LsSf https://astral.sh/uv/install.sh | sh This will:\nDownload and install the latest version of uv Add uv to your PATH automatically Work on both Linux and macOS Alternative installation methods:\nUsing pipx (if you have it): pipx install uv Using pip: pip install uv Using Homebrew (macOS): brew install uv 🪟 Windows # Option 1: PowerShell Installer (Recommended) Open PowerShell and run:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Option 2: Using pipx or pip If you have Python already installed:\npipx install uv # or pip install uv Option 3: Download from GitHub You can also download the installer or binaries directly from the GitHub releases page.\nVerify Installation # After installation, restart your terminal and verify uv is working:\nuv --version You should see output like uv 0.7.8 or similar (version numbers may vary).\nSetting Up the Lab Project # Now let\u0026rsquo;s create a dedicated project for all your numerical optimization lab sessions.\nStep 1: Create the Project # Navigate to where you want to store your course materials and run:\nuv init --bare numerical-optimization-labs cd numerical-optimization-labs The --bare flag creates a minimal project structure with only essential files: pyproject.toml, .python-version, and README.md (no default main.py file).\nStep 2: Install Required Packages # Now we\u0026rsquo;ll install all the packages you\u0026rsquo;ll need for the course. The uv add command will automatically create a virtual environment, install packages, and update both pyproject.toml and the lock file:\n# Core scientific computing packages uv add numpy scipy scikit-learn # Visualization libraries uv add matplotlib plotly # Machine learning framework uv add torch # Development and interactive tools uv add rich jupyter ipython # Optional: Add some useful development tools uv add pytest black ruff What\u0026rsquo;s happening behind the scenes:\nuv creates a virtual environment at .venv/ in your project directory All packages are installed into this isolated environment A lockfile (uv.lock) is generated containing exact versions of all dependencies for reproducible installations Your pyproject.toml is updated with the new dependencies Step 3: Verify Installation # Check that everything installed correctly:\nuv run python -c \u0026#34;import numpy, scipy, sklearn, matplotlib, plotly, torch, rich, jupyter, IPython; print(\u0026#39;✅ All packages imported successfully!\u0026#39;)\u0026#34; Using Your Lab Environment # Running Python Scripts # To run any Python script in your lab environment:\nuv run python your_script.py The uv run command ensures your script runs in the project\u0026rsquo;s virtual environment with all dependencies available.\nStarting Jupyter Lab/Notebook # To start Jupyter for interactive development:\n# For Jupyter Lab (recommended) uv run jupyter lab # For classic Jupyter Notebook uv run jupyter notebook Interactive Python (IPython) # For an enhanced interactive Python experience:\nuv run ipython Adding More Packages Later # If you need additional packages during the course:\nuv add package-name To remove packages you no longer need:\nuv remove package-name Project Structure # Your lab project will look like this:\nnumerical-optimization-labs/ ├── .venv/ # Virtual environment (auto-created) ├── .python-version # Pinned Python version ├── pyproject.toml # Project configuration and dependencies ├── uv.lock # Exact dependency versions (for reproducibility) ├── README.md # Project documentation └── your_lab_files.py # Your lab work goes here Sharing and Collaboration # Setting up the Environment on Another Machine # If you clone this project or share it with others, they can recreate the exact environment by running:\ncd numerical-optimization-labs uv sync This command reads the lockfile and installs the exact same versions of all dependencies.\nVersion Control # Make sure to commit these files to Git:\n✅ pyproject.toml ✅ uv.lock ✅ .python-version ❌ .venv/ (add this to .gitignore) Troubleshooting # Permission Issues # If you encounter permission errors, use sudo on macOS/Linux or run your command prompt as administrator on Windows.\nPython Version Issues # If you need a specific Python version:\n# Install a specific Python version uv python install 3.11 # Pin it to your project uv python pin 3.11 Environment Issues # If something goes wrong with your environment:\n# Sync environment with lockfile uv sync # Force recreate environment rm -rf .venv uv sync Package Conflicts # uv\u0026rsquo;s dependency resolver is much more robust than pip and should handle conflicts automatically. If you encounter issues, check the error message and try updating conflicting packages.\nQuick Reference Commands # Task Command Create new project uv init project-name Add packages uv add package1 package2 Remove packages uv remove package-name Run Python script uv run python script.py Start Jupyter uv run jupyter lab Install from lockfile uv sync List installed packages uv tree Update a package uv add package-name --upgrade Getting Help # uv Documentation: docs.astral.sh/uv Command Help: uv help or uv command --help Course Forum: [link to your course forum/discussion board] 🎉 You\u0026rsquo;re all set! Your lab environment is ready for numerical optimization adventures. If you encounter any issues during setup, don\u0026rsquo;t hesitate to ask for help during class or on the course forum.\n"},{"id":23,"href":"/numerical_optimization/docs/practical_labs/backtracking/","title":"Backtracking memo","section":"Practical labs","content":" Backtracking procedure for step size selection # Introduction # The backtracking line search is a fundamental technique in optimization algorithms for determining an appropriate step size that ensures sufficient decrease in the objective function. This procedure is particularly useful in gradient-based methods where choosing an optimal step size analytically is difficult or computationally expensive.\nMathematical setup # Consider the optimization problem: $\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x})$\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a continuously differentiable function. At iteration $k$, we have:\nCurrent point: $\\mathbf{x}_k$ Search direction: $\\mathbf{p}_k$ (typically $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ for steepest descent) Step size: $\\alpha_k \u0026gt; 0$ The Armijo condition # The backtracking procedure is based on the Armijo condition, which requires: $f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\leq f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k$\nwhere $c_1 \\in (0, 1)$ is a constant, typically $c_1 = 10^{-4}$.\nBacktracking algorithm steps # Step 1: Initialize parameters # Choose initial step size $\\alpha_0 \u0026gt; 0$ (e.g., $\\alpha_0 = 1$) Set reduction factor $\\rho \\in (0, 1)$ (typically $\\rho = 0.5$) Set Armijo parameter $c_1 \\in (0, 1)$ (typically $c_1 = 10^{-4}$) Set $\\alpha = \\alpha_0$ Step 2: Check Armijo condition # Evaluate the condition: $f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\leq f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k$\nStep 3: Backtrack if necessary # If the Armijo condition is satisfied:\nAccept $\\alpha_k = \\alpha$ Go to Step 4 Else:\nUpdate $\\alpha \\leftarrow \\rho \\alpha$ Return to Step 2 Step 4: Update iteration # Compute the new iterate: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$\nAlgorithmic description # Algorithm: Backtracking Line Search Input: x_k, p_k, α₀, ρ, c₁ Output: α_k 1. Set α = α₀ 2. While f(x_k + α·p_k) \u0026gt; f(x_k) + c₁·α·∇f(x_k)ᵀ·p_k do 3. α ← ρ·α 4. End while 5. Return α_k = α Theoretical properties # Convergence guarantee # Under mild conditions on $f$ and $\\mathbf{p}_k$, the backtracking procedure terminates in finite steps. Specifically, if:\n$f$ is continuously differentiable $\\mathbf{p}_k$ is a descent direction: $\\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k \u0026lt; 0$ Then there exists a step size $\\alpha \u0026gt; 0$ satisfying the Armijo condition.\nSufficient decrease property # The accepted step size $\\alpha_k$ ensures: $f(\\mathbf{x}_{k+1}) - f(\\mathbf{x}_k) \\leq c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k \u0026lt; 0$\nThis guarantees that each iteration decreases the objective function value.\nImplementation considerations # Choice of parameters # Initial step size $\\alpha_0$: Common choices are $\\alpha_0 = 1$ for Newton-type methods, or $\\alpha_0 = 1/|\\nabla f(\\mathbf{x}_k)|$ for gradient methods Reduction factor $\\rho$: Typically $\\rho = 0.5$ or $\\rho = 0.8$ Armijo parameter $c_1$: Usually $c_1 = 10^{-4}$ or $c_1 = 10^{-3}$ Computational complexity # Each backtracking iteration requires:\nOne function evaluation: $f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$ One gradient evaluation: $\\nabla f(\\mathbf{x}_k)$ (if not already computed) One vector operation: $\\mathbf{x}_k + \\alpha \\mathbf{p}_k$ Practical modifications # Maximum iterations: Limit the number of backtracking steps to prevent infinite loops:\nmax_backtracks = 50 iter = 0 while (Armijo condition not satisfied) and (iter \u0026lt; max_backtracks): α ← ρ·α iter ← iter + 1 Minimum step size: Set a lower bound $\\alpha_{min}$ to avoid numerical issues:\nif α \u0026lt; α_min: α = α_min break Applications # The backtracking procedure is widely used in:\nGradient descent: $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ Newton\u0026rsquo;s method: $\\mathbf{p}_k = -(\\mathbf{H}_k)^{-1} \\nabla f(\\mathbf{x}_k)$ where $\\mathbf{H}_k$ is the Hessian Quasi-Newton methods: $\\mathbf{p}_k = -\\mathbf{B}_k^{-1} \\nabla f(\\mathbf{x}_k)$ where $\\mathbf{B}_k$ approximates the Hessian Conjugate gradient methods Example implementation # def backtracking_line_search(f, grad_f, x_k, p_k, alpha_0=1.0, rho=0.5, c1=1e-4): \u0026#34;\u0026#34;\u0026#34; Backtracking line search for step size selection Parameters: - f: objective function - grad_f: gradient function - x_k: current point - p_k: search direction - alpha_0: initial step size - rho: reduction factor - c1: Armijo parameter Returns: - alpha_k: accepted step size \u0026#34;\u0026#34;\u0026#34; alpha = alpha_0 f_k = f(x_k) grad_k = grad_f(x_k) # Armijo condition right-hand side armijo_rhs = f_k + c1 * alpha * np.dot(grad_k, p_k) while f(x_k + alpha * p_k) \u0026gt; armijo_rhs: alpha *= rho armijo_rhs = f_k + c1 * alpha * np.dot(grad_k, p_k) return alpha Exercises\nImplement the backtracking line search for the quadratic function $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\mathrm{T}} \\mathbf{Q} \\mathbf{x} - \\mathbf{b}^{\\mathrm{T}} \\mathbf{x}$, where $\\mathbf{Q}$ is positive definite.\nCompare the performance of different values of $\\rho$ and $c_1$ on a test optimization problem.\nAnalyze the number of backtracking steps required as a function of the condition number of the Hessian matrix.\n"},{"id":24,"href":"/numerical_optimization/docs/practical_labs/","title":"Practical labs","section":"Docs","content":" Practical labs # Content I - Linear Regression models II - Remote Sensing project III - Digit recognition Lab environment Backtracking memo "}]