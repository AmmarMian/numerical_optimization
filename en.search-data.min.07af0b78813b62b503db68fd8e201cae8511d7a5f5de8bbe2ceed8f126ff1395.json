[{"id":0,"href":"/numerical_optimization/docs/lectures/machine_learning/perceptron/","title":"1. From Linear regression to perceptron","section":"III - Machine Learning problems","content":" From Linear regression to perceptron # Soon to be added.\n"},{"id":1,"href":"/numerical_optimization/docs/lectures/fundamentals/optimization_problems/","title":"1. Optimization problems","section":"I - Fundamentals","content":" Optimization problems # Unconstrained vs constrained # What we are interested in these lectures is to solve problems of the form :\n\\begin{equation} \\underset{\\mathbf{x}\\in\\mathbb{R}^d}{\\operatorname{(arg)min}} f(\\mathbf{x}), \\label{eq: optim general unconstrained} \\end{equation} where $\\mathbf{x}\\in\\mathbb{R}^d$ and $f:\\mathcal{D}_f \\mapsto \\mathbb{R} $ is a scalar-valued function with domain $\\mathcal{D}_f$. Under this formulation, the problem is said to be an unconstrained optimization problem.\nIf additionally, we add a set of equalities constraints functions: $$ \\{h_i : \\mathbb{R}^d \\mapsto \\mathbb{R} \\, /\\, 1 \\leq i \\leq N \\} $$ and inequalities constraints functions: $$ \\{g_j : \\mathbb{R}^d \\mapsto \\mathbb{R} \\, /\\, 1 \\leq j \\leq M \\} $$ and define the set $\\mathcal{S} = \\{\\mathbf{x} \\in \\mathbb{R}^d \\,/\\, \\forall\\,(i, j),\\, h_i(\\mathbf{x})=0,\\, g_j(\\mathbf{x})\\leq 0\\}$ and want to solve: \\begin{equation} \\underset{\\mathbf{x}\\in\\mathcal{S}}{\\operatorname{(arg)min}} f(\\mathbf{x}), \\label{eq: optim general constrained} \\end{equation} then the problem is said to be a constrained optimization problem.\nNote that here, the constraints and the function domain are not the same sets. Constraints usually stem from modelling of the problem whilst the function domain only characterizes for which values of $\\mathbf{x}$ it is possible to compute a value of the function.\nGlobal optimization vs local optimization # Figure 1.1: An example of multiple local minima\nIn the context of optimization, we can distinguish between global optimization and local optimization:\nGlobal optimization refers to the process of finding the best solution (minimum or maximum) across the entire search space. This means identifying the point where the function achieves its absolute minimum or maximum value, regardless of how many local minima or maxima exist. Local optimization, on the other hand, focuses on finding a solution that is optimal within a limited neighborhood of the search space. This means identifying a point where the function achieves a minimum or maximum value relative to nearby points, but not necessarily the absolute best solution across the entire space. Often, global optimization is not feasible unless the function is convex, or the search space is small enough. In practice, we often use local optimization methods to find a good enough solution, which may not be the global optimum. This is peculiarly true in machine learning, where the loss function is often non-convex and may have many local minima.\n"},{"id":2,"href":"/numerical_optimization/docs/lectures/advanced/unconstrained_newton/","title":"1. Unconstrained optimization : Second-order ","section":"II - Advanced problems","content":" Unconstrained optimization - Second-order methods # Note : This is in part the content of the book \u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, with some modifications to the notations used in this lecture.\nWe have seen in the previous chapter that first-order methods, such as steepest descent, are often used to find a local minimum of a function $f(\\mathbf{x})$. However, these methods can be slow to converge, especially when the function has ill-conditioned Hessian or when the initial guess is far from the solution. Second-order methods, which use information about the curvature of the function, can provide faster convergence rates.\nSearch directions # Another important search direction-perhaps the most important one of all-is the Newton direction. This direction is derived from the second-order Taylor series approximation to $f\\left(\\mathbf{x}_k+\\mathbf{p}\\right)$, which is $$ f\\left(\\mathbf{x}_k+\\mathbf{p}\\right) \\approx f_k+\\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\mathbf{p}^{\\mathrm{T}} \\nabla^2 f_k \\mathbf{p} \\stackrel{\\text { def }}{=} m_k(\\mathbf{p}) $$\nAssuming for the moment that $\\nabla^2 f_k$ is positive definite, we obtain the Newton direction by finding the vector $\\mathbf{p}$ that minimizes $m_k(\\mathbf{p})$. By simply setting the derivative of $m_k(\\mathbf{p})$ to zero, we obtain the following explicit formula:\n\\begin{equation} \\mathbf{p}_k^{\\mathrm{N}}=-\\nabla^2 f_k^{-1} \\nabla f_k \\label{eq:newton_direction} \\end{equation}\nThe Newton direction is reliable when the difference between the true function $f\\left(\\mathbf{x}_k+ \\mathbf{p}\\right)$ and its quadratic model $m_k(\\mathbf{p})$ is not too large. By comparing \\eqref{eq:newton_direction} with traditional Taylor expansion, we see that the only difference between these functions is that the matrix $\\nabla^2 f\\left(\\mathbf{x}_k+t \\mathbf{p}\\right)$ in the third term of the expansion has been replaced by $\\nabla^2 f_k=\\nabla^2 f\\left(\\mathbf{x}_k\\right)$. If $\\nabla^2 f(\\cdot)$ is sufficiently smooth, this difference introduces a perturbation of only $O\\left(\\lVert\\mathbf{p}\\rVert^3\\right)$ into the expansion, so that when $\\lVert\\mathbf{p}\\rVert$ is small, the approximation $f\\left(\\mathbf{x}_k+\\mathbf{p}\\right) \\approx m_k(\\mathbf{p})$ is very accurate indeed.\nThe Newton direction can be used in a line search method when $\\nabla^2 f_k$ is positive definite, for in this case we have\n$$ \\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}=-\\mathbf{p}_k^{\\mathrm{N} \\mathrm{T}} \\nabla^2 f_k \\mathbf{p}_k^{\\mathrm{N}} \\leq-\\sigma_k\\lVert\\mathbf{p}_k^{\\mathrm{N}}\\rVert^2 $$\nfor some $\\sigma_k\u0026gt;0$. Unless the gradient $\\nabla f_k$ (and therefore the step $\\mathbf{p}_k^N$) is zero, we have that $\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}\u0026lt;0$, so the Newton direction is a descent direction. Unlike the steepest descent direction, there is a \u0026ldquo;natural\u0026rdquo; step length of 1 associated with the Newton direction. Most line search implementations of Newton\u0026rsquo;s method use the unit step $\\alpha=1$ where possible and adjust this step length only when it does not produce a satisfactory reduction in the value of $f$.\nWhen $\\nabla^2 f_k$ is not positive definite, the Newton direction may not even be defined, since $\\nabla^2 f_k^{-1}$ may not exist. Even when it is defined, it may not satisfy the descent property $\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}\u0026lt;0$, in which case it is unsuitable as a search direction. In these situations, line search methods modify the definition of $\\mathbf{p}_k$ to make it satisfy the downhill condition while retaining the benefit of the second-order information contained in $\\nabla^2 f_k$.\nMethods that use the Newton direction have a fast rate of local convergence, typically quadratic. When a neighborhood of the solution is reached, convergence to high accuracy often occurs in just a few iterations. The main drawback of the Newton direction is the need for the Hessian $\\nabla^2 f(\\mathbf{x})$. Explicit computation of this matrix of second derivatives is sometimes, though not always, a cumbersome, error-prone, and expensive process.\nQuasi-Newton search directions provide an attractive alternative in that they do not require computation of the Hessian and yet still attain a superlinear rate of convergence. In place of the true Hessian $\\nabla^2 f_k$, they use an approximation $\\mathbf{B}_k$, which is updated after each step to take account of the additional knowledge gained during the step. The updates make use of the fact that changes in the gradient $\\mathbf{g}$ provide information about the second derivative of $f$ along the search direction. By using the expression from our statement of Taylor\u0026rsquo;s theorem, we have by adding and subtracting the term $\\nabla^2 f(\\mathbf{x}) \\mathbf{p}$ that\n$$ \\nabla f(\\mathbf{x}+\\mathbf{p})=\\nabla f(\\mathbf{x})+\\nabla^2 f(\\mathbf{x}) \\mathbf{p}+\\int_0^1\\left[\\nabla^2 f(\\mathbf{x}+t \\mathbf{p})-\\nabla^2 f(\\mathbf{x})\\right] \\mathbf{p} d t $$\nBecause $\\nabla f(\\cdot)$ is continuous, the size of the final integral term is $o(\\lVert\\mathbf{p}\\rVert)$. By setting $\\mathbf{x}=\\mathbf{x}_k$ and $\\mathbf{p}=\\mathbf{x}_{k+1}-\\mathbf{x}_k$, we obtain\n$$ \\nabla f_{k+1}=\\nabla f_k+\\nabla^2 f_{k+1}\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right)+o\\left(\\lVert\\mathbf{x}_{k+1}-\\mathbf{x}_k\\rVert\\right) $$\nWhen $\\mathbf{x}_k$ and $\\mathbf{x}_{k+1}$ lie in a region near the solution $\\mathbf{x}^*$, within which $\\nabla f$ is positive definite, the final term in this expansion is eventually dominated by the $\\nabla^2 f_k\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right)$ term, and we can write\n$$ \\nabla^2 f_{k+1}\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right) \\approx \\nabla f_{k+1}-\\nabla f_k $$\nWe choose the new Hessian approximation $\\mathbf{B}_{k+1}$ so that it mimics this property of the true Hessian, that is, we require it to satisfy the following condition, known as the secant equation:\n\\begin{equation} \\mathbf{B}_{k+1} \\mathbf{s}_k=\\mathbf{y}_k \\label{eq:secant_equation} \\end{equation}\nwhere\n$$ \\mathbf{s}_k=\\mathbf{x}_{k+1}-\\mathbf{x}_k, \\quad \\mathbf{y}_k=\\nabla f_{k+1}-\\nabla f_k $$\nTypically, we impose additional requirements on $\\mathbf{B}_{k+1}$, such as symmetry (motivated by symmetry of the exact Hessian), and a restriction that the difference between successive approximation $\\mathbf{B}_k$ to $\\mathbf{B}_{k+1}$ have low rank. The initial approximation $\\mathbf{B}_0$ must be chosen by the user.\nTwo of the most popular formulae for updating the Hessian approximation $\\mathbf{B}_k$ are the symmetric-rank-one (SR1) formula, defined by\n\\begin{equation} \\mathbf{B}_{k+1}=\\mathbf{B}_k+\\frac{\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}}}{\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:sr1_formula} \\end{equation}\nand the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno, which is defined by\n\\begin{equation} \\mathbf{B}_{k+1}=\\mathbf{B}_k-\\frac{\\mathbf{B}_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k}+\\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:bfgs_formula} \\end{equation}\nNote that the difference between the matrices $\\mathbf{B}_k$ and $\\mathbf{B}_{k+1}$ is a rank-one matrix in the case of \\eqref{eq:sr1_formula}, and a rank-two matrix in the case of \\eqref{eq:bfgs_formula}. Both updates satisfy the secant equation and both maintain symmetry. One can show that BFGS update \\eqref{eq:bfgs_formula} generates positive definite approximations whenever the initial approximation $\\mathbf{B}_0$ is positive definite and $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k\u0026gt;0$.\nThe quasi-Newton search direction is given by using $\\mathbf{B}_k$ in place of the exact Hessian in the formula \\eqref{eq:newton_direction}, that is,\n\\begin{equation} \\mathbf{p}_k=-\\mathbf{B}_k^{-1} \\nabla f_k \\label{eq:quasi_newton_direction} \\end{equation}\nSome practical implementations of quasi-Newton methods avoid the need to factorize $\\mathbf{B}_k$ at each iteration by updating the inverse of $\\mathbf{B}_k$, instead of $\\mathbf{B}_k$ itself. In fact, the equivalent formula for \\eqref{eq:sr1_formula} and \\eqref{eq:bfgs_formula}, applied to the inverse approximation $\\mathbf{H}_k \\stackrel{\\text { def }}{=} \\mathbf{B}_k^{-1}$, is\n\\begin{equation} \\mathbf{H}_{k+1}=\\left(\\mathbf{I}-\\rho_k \\mathbf{s}_k \\mathbf{y}_k^{\\mathrm{T}}\\right) \\mathbf{H}_k\\left(\\mathbf{I}-\\rho_k \\mathbf{y}_k \\mathbf{s}_k^{\\mathrm{T}}\\right)+\\rho_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}}, \\quad \\rho_k=\\frac{1}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:inverse_bfgs} \\end{equation}\nCalculation of $\\mathbf{p}_k$ can then be performed by using the formula $\\mathbf{p}_k=-\\mathbf{H}_k \\nabla f_k$. This can be implemented as a matrix-vector multiplication, which is typically simpler than the factorization/back-substitution procedure that is needed to implement the formula \\eqref{eq:quasi_newton_direction}.\nStep-size selection # Contrarily to the steepest descent, Newton methods have a \u0026ldquo;natural\u0026rdquo; step size of 1 associated with the Newton direction. This is because the Newton direction is derived from the second-order Taylor series approximation, which is designed to minimize the quadratic model of the function. However, in practice, it is often necessary to adjust this step size to ensure sufficient decrease in the function value.\nWhen using a line search method, we can set $\\alpha_k=1$ and check if this step size leads to a sufficient decrease in the function value. If it does not, we can use a backtracking line search to find a suitable step size that satisfies the Armijo condition. The Armijo condition ensures that the step size leads to a sufficient decrease in the function value, which is crucial for convergence of the method.\nConvergence of Newton methods # As in first-order methods, we make use of Zoutendijk\u0026rsquo;s condition, that still apllies.\nConsider now the Newton-like method with $\\mathbf{p}_k = -\\mathbf{B}_k^{-1} \\nabla f_k$ and assume that the matrices $\\mathbf{B}_k$ are positive definite with a uniformly bounded condition number. That is, there is a constant $M$ such that\n\\begin{equation} \\|\\mathbf{B}_k\\|\\|\\mathbf{B}_k^{-1}\\| \\leq M, \\quad \\text { for all } k . \\label{eq:condition_bound} \\end{equation}\nIt is easy to show from the definition that\n$$ \\cos \\theta_k \\geq 1 / M $$\nBy combining this bound with (4.16) we find that\n$$ \\lim _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 $$\nTherefore, we have shown that Newton and quasi-Newton methods are globally convergent if the matrices $\\mathbf{B}_k$ have a bounded condition number and are positive definite (which is needed to ensure that $\\mathbf{p}_k$ is a descent direction), and if the step lengths satisfy the Wolfe conditions.\nFor some algorithms, such as conjugate gradient methods, we will not be able to prove the limit (4.18), but only the weaker result\n\\begin{equation} \\liminf _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 \\label{eq:weak_convergence} \\end{equation}\nIn other words, just a subsequence of the gradient norms $\\|\\nabla f_{k_j}\\|$ converges to zero, rather than the whole sequence. This result, too, can be proved by using Zoutendijk\u0026rsquo;s condition (4.16), but instead of a constructive proof, we outline a proof by contradiction. Suppose that \\eqref{eq:weak_convergence} does not hold, so that the gradients remain bounded away from zero, that is, there exists $\\gamma\u0026gt;0$ such that\n$$ \\|\\nabla f_k\\| \\geq \\gamma, \\quad \\text { for all } k \\text { sufficiently large. } $$\nThen from (4.16) we conclude that\n$$ \\cos \\theta_k \\rightarrow 0 $$\nthat is, the entire sequence $\\{\\cos \\theta_k\\}$ converges to 0. To establish \\eqref{eq:weak_convergence}, therefore, it is enough to show that a subsequence $\\{\\cos \\theta_{k_j}\\}$ is bounded away from zero.\nRate of convergence # We refer the reader to the textbook:\n\u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, 2nd edition, Springer, 2006, pages 47-51, Peculiarly, see pages 51-53.\n"},{"id":3,"href":"/numerical_optimization/docs/lectures/","title":"Lectures","section":"Docs","content":" Lectures # This section regroups the theory : main results, proofs and exercices behind optimization algorithms.\nContent Introduction I - Fundamentals II - Advanced problems III - Machine Learning problems Reminders "},{"id":4,"href":"/numerical_optimization/docs/lectures/reminders/linear_algebra/","title":"Linear Algebra","section":"Reminders","content":" Fundamentals of Linear Algebra # 1 - Introduction # Linear algebra is one of the foundational branches of mathematics, with applications spanning from engineering and computer science to economics and physics. This document is extracted from the textbook Matrix Differential Calculus with Applications in Statistics and Econometrics from Jan R. Magnus; Heinz Neudecker, adapting the notations to the ones used in the lectures.\nIn this chapter, we summarize some of the well-known definitions and theorems of matrix algebra. Most of the theorems will be proved.\n2 - Sets # Definition 0.1 (Set)\nA set is a collection of objects, called the elements (or members) of the set. We write $x \\in S$ to mean \u0026lsquo;$x$ is an element of $S$\u0026rsquo; or \u0026lsquo;$x$ belongs to $S$\u0026rsquo;. If $x$ does not belong to $S$, we write $x \\notin S$. The set that contains no elements is called the empty set, denoted by $\\emptyset$. Sometimes a set can be defined by displaying the elements in braces. For example, $A={0,1}$ or\n$$ \\mathbb{N}={1,2,3, \\ldots} $$\nNotice that $A$ is a finite set (contains a finite number of elements), whereas $\\mathbb{N}$ is an infinite set. If $P$ is a property that any element of $S$ has or does not have, then\n$$ {x: x \\in S, x \\text { satisfies } P} $$\ndenotes the set of all the elements of $S$ that have property $P$.\nDefinition 0.2 (Subset)\nA set $A$ is called a subset of $B$, written $A \\subset B$, whenever every element of $A$ also belongs to $B$. The notation $A \\subset B$ does not rule out the possibility that $A=B$. If $A \\subset B$ and $A \\neq B$, then we say that $A$ is a proper subset of $B$. If $A$ and $B$ are two subsets of $S$, we define\n$$ A \\cup B, $$\nthe union of $A$ and $B$, as the set of elements of $S$ that belong to $A$ or to $B$ or to both, and\n$$ A \\cap B, $$\nthe intersection of $A$ and $B$, as the set of elements of $S$ that belong to both $A$ and $B$. We say that $A$ and $B$ are (mutually) disjoint if they have no common elements, that is, if\n$$ A \\cap B=\\emptyset . $$\nThe complement of $A$ relative to $B$, denoted by $B-A$, is the set ${x: x \\in B$, but $x \\notin A}$. The complement of $A$ (relative to $S$) is sometimes denoted by $A^{c}$.\nDefinition 0.3 (Cartesian Product)\nThe Cartesian product of two sets $A$ and $B$, written $A \\times B$, is the set of all ordered pairs $(a, b)$ such that $a \\in A$ and $b \\in B$. More generally, the Cartesian product of $n$ sets $A_{1}, A_{2}, \\ldots, A_{n}$, written\n$$ \\prod_{i=1}^{n} A_{i} $$\nis the set of all ordered $n$-tuples $\\left(a_{1}, a_{2}, \\ldots, a_{n}\\right)$ such that $a_{i} \\in A_{i}(i=1, \\ldots, n)$.\nThe set of (finite) real numbers (the one-dimensional Euclidean space) is denoted by $\\mathbb{R}$. The $n$-dimensional Euclidean space $\\mathbb{R}^{n}$ is the Cartesian product of $n$ sets equal to $\\mathbb{R}$:\n$$ \\mathbb{R}^{n}=\\mathbb{R} \\times \\mathbb{R} \\times \\cdots \\times \\mathbb{R} \\quad (n \\text { times }) $$\nThe elements of $\\mathbb{R}^{n}$ are thus the ordered $n$-tuples $\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$ of real numbers $x_{1}, x_{2}, \\ldots, x_{n}$.\nDefinition 0.4 (Bounded Set)\nA set $S$ of real numbers is said to be bounded if there exists a number $M$ such that $|x| \\leq M$ for all $x \\in S$. 3 - Matrices: Addition and Multiplication # Definition 0.5 (Real Matrix)\nA real $m \\times n$ matrix $\\mathbf{A}$ is a rectangular array of real numbers\n$$ \\mathbf{A}=\\left(\\begin{array}{cccc} a_{11} \u0026amp; a_{12} \u0026amp; \\ldots \u0026amp; a_{1 n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\ldots \u0026amp; a_{2 n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ a_{m 1} \u0026amp; a_{m 2} \u0026amp; \\ldots \u0026amp; a_{m n} \\end{array}\\right) $$\nWe sometimes write $\\mathbf{A}=\\left(a_{i j}\\right)$.\nIf one or more of the elements of $\\mathbf{A}$ is complex, we say that $\\mathbf{A}$ is a complex matrix. Almost all matrices in this book are real and the word \u0026lsquo;matrix\u0026rsquo; is assumed to be a real matrix, unless explicitly stated otherwise.\nAn $m \\times n$ matrix can be regarded as a point in $\\mathbb{R}^{m \\times n}$. The real numbers $a_{i j}$ are called the elements of $\\mathbf{A}$. An $m \\times 1$ matrix is a point in $\\mathbb{R}^{m \\times 1}$ (that is, in $\\mathbb{R}^{m}$) and is called a (column) vector of order $m \\times 1$. A $1 \\times n$ matrix is called a row vector (of order $1 \\times n$). The elements of a vector are usually called its components. Matrices are always denoted by capital letters and vectors by lower-case letters.\nDefinition 0.6 (Matrix Addition)\nThe sum of two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of the same order is defined as\n$$ \\mathbf{A}+\\mathbf{B}=\\left(a_{i j}\\right)+\\left(b_{i j}\\right)=\\left(a_{i j}+b_{i j}\\right) $$\nDefinition 0.7 (Scalar Multiplication)\nThe product of a matrix by a scalar $\\lambda$ is\n$$ \\lambda \\mathbf{A}=\\mathbf{A} \\lambda=\\left(\\lambda a_{i j}\\right) $$\nThe following properties are now easily proved for matrices $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$ of the same order and scalars $\\lambda$ and $\\mu$:\n\\begin{equation} \\begin{aligned} \\mathbf{A}+\\mathbf{B} \u0026amp; =\\mathbf{B}+\\mathbf{A}, \\\\ (\\mathbf{A}+\\mathbf{B})+\\mathbf{C} \u0026amp; =\\mathbf{A}+(\\mathbf{B}+\\mathbf{C}), \\\\ (\\lambda+\\mu) \\mathbf{A} \u0026amp; =\\lambda \\mathbf{A}+\\mu \\mathbf{A}, \\\\ \\lambda(\\mathbf{A}+\\mathbf{B}) \u0026amp; =\\lambda \\mathbf{A}+\\lambda \\mathbf{B}, \\\\ \\lambda(\\mu \\mathbf{A}) \u0026amp; =(\\lambda \\mu) \\mathbf{A} . \\end{aligned} \\end{equation}\nA matrix whose elements are all zero is called a null matrix and denoted by $\\mathbf{0}$. We have, of course,\n$$ \\mathbf{A}+(-1) \\mathbf{A}=\\mathbf{0} $$\nDefinition 0.8 (Matrix Multiplication)\nIf $\\mathbf{A}$ is an $m \\times n$ matrix and $\\mathbf{B}$ an $n \\times p$ matrix (so that $\\mathbf{A}$ has the same number of columns as $\\mathbf{B}$ has rows), then we define the product of $\\mathbf{A}$ and $\\mathbf{B}$ as\n$$ \\mathbf{A} \\mathbf{B}=\\left(\\sum_{j=1}^{n} a_{i j} b_{j k}\\right) $$\nThus, $\\mathbf{A} \\mathbf{B}$ is an $m \\times p$ matrix and its $ik$th element is $\\sum_{j=1}^{n} a_{i j} b_{j k}$.\nThe following properties of the matrix product can be established:\n\\begin{equation} \\begin{aligned} (\\mathbf{A} \\mathbf{B}) \\mathbf{C} \u0026amp; =\\mathbf{A}(\\mathbf{B} \\mathbf{C}) \\\\ \\mathbf{A}(\\mathbf{B}+\\mathbf{C}) \u0026amp; =\\mathbf{A} \\mathbf{B}+\\mathbf{A} \\mathbf{C} \\\\ (\\mathbf{A}+\\mathbf{B}) \\mathbf{C} \u0026amp; =\\mathbf{A} \\mathbf{C}+\\mathbf{B} \\mathbf{C} \\end{aligned} \\end{equation}\nThese relations hold provided the matrix products exist.\nWe note that the existence of $\\mathbf{A} \\mathbf{B}$ does not imply the existence of $\\mathbf{B} \\mathbf{A}$, and even when both products exist, they are not generally equal. (Two matrices $\\mathbf{A}$ and $\\mathbf{B}$ for which\n$$ \\mathbf{A} \\mathbf{B}=\\mathbf{B} \\mathbf{A} $$\nare said to commute.) We therefore distinguish between premultiplication and postmultiplication: a given $m \\times n$ matrix $\\mathbf{A}$ can be premultiplied by a $p \\times m$ matrix $\\mathbf{B}$ to form the product $\\mathbf{B} \\mathbf{A}$; it can also be postmultiplied by an $n \\times q$ matrix $\\mathbf{C}$ to form $\\mathbf{A} \\mathbf{C}$.\n4 - The transpose of a matrix # Definition 0.9 (Transpose)\nThe transpose of an $m \\times n$ matrix $\\mathbf{A}=\\left(a_{i j}\\right)$ is the $n \\times m$ matrix, denoted by $\\mathbf{A}^{\\mathrm{T}}$, whose $ij$th element is $a_{j i}$. We have\n\\begin{equation} \\begin{aligned} \\left(\\mathbf{A}^{\\mathrm{T}}\\right)^{\\mathrm{T}} \u0026amp; =\\mathbf{A} \\\\ (\\mathbf{A}+\\mathbf{B})^{\\mathrm{T}} \u0026amp; =\\mathbf{A}^{\\mathrm{T}}+\\mathbf{B}^{\\mathrm{T}} \\\\ (\\mathbf{A} \\mathbf{B})^{\\mathrm{T}} \u0026amp; =\\mathbf{B}^{\\mathrm{T}} \\mathbf{A}^{\\mathrm{T}} \\end{aligned} \\end{equation}\nIf $\\mathbf{x}$ is an $n \\times 1$ vector, then $\\mathbf{x}^{\\mathrm{T}}$ is a $1 \\times n$ row vector and\n$$ \\mathbf{x}^{\\mathrm{T}} \\mathbf{x}=\\sum_{i=1}^{n} x_{i}^{2} $$\nDefinition 0.10 (Euclidean Norm)\nThe (Euclidean) norm of $\\mathbf{x}$ is defined as\n$$ |\\mathbf{x}|=\\left(\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}\\right)^{1 / 2} $$\n5 - Square matrices # Definition 0.11 (Square Matrix)\nA matrix is said to be square if it has as many rows as it has columns. A square matrix $\\mathbf{A}=\\left(a_{i j}\\right)$, real or complex, is said to be\ntype if lower triangular if $a_{i j}=0 \\quad(i\u0026lt;j)$, strictly lower triangular if $a_{i j}=0 \\quad(i \\leq j)$, unit lower triangular if $a_{i j}=0 \\quad(i\u0026lt;j)$ and $a_{i i}=1$ (all $i$), upper triangular if $a_{i j}=0 \\quad(i\u0026gt;j)$, strictly upper triangular if $a_{i j}=0 \\quad(i \\geq j)$, unit upper triangular if $a_{i j}=0 \\quad(i\u0026gt;j)$ and $a_{i i}=1$ (all $i$), idempotent if $\\mathbf{A}^{2}=\\mathbf{A}$. A square matrix $\\mathbf{A}$ is triangular if it is either lower triangular or upper triangular (or both).\nA real square matrix $\\mathbf{A}=\\left(a_{i j}\\right)$ is said to be\ntype if symmetric if $\\mathbf{A}^{\\mathrm{T}} = \\mathbf{A}$, skew-symmetric if $\\mathbf{A}^{\\mathrm{T}} = -\\mathbf{A}$. For any square $n \\times n$ matrix $\\mathbf{A}=\\left(a_{i j}\\right)$, we define $\\operatorname{dg} \\mathbf{A}$ or $\\operatorname{dg}(\\mathbf{A})$ as\n$$ \\operatorname{dg} \\mathbf{A}=\\left(\\begin{array}{cccc} a_{11} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; a_{22} \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; a_{n n} \\end{array}\\right) $$\nor, alternatively,\n$$ \\operatorname{dg} \\mathbf{A}=\\operatorname{diag}\\left(a_{11}, a_{22}, \\ldots, a_{n n}\\right) $$\nDefinition 0.12 (Diagonal Matrix)\nIf $\\mathbf{A}=\\operatorname{dg} \\mathbf{A}$, we say that $\\mathbf{A}$ is diagonal. Definition 0.13 (Identity Matrix)\nA particular diagonal matrix is the identity matrix $\\mathbf{I}_n$ (of order $n \\times n$),\n$$ \\left (\\begin{array}{cccc} 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 1 \\end{array}\\right )=\\left(\\delta_{i j}\\right) $$\nwhere $\\delta_{i j}=1$ if $i=j$ and $\\delta_{i j}=0$ if $i \\neq j$ ($\\delta_{i j}$ is called the Kronecker delta).\nWe sometimes write $\\mathbf{I}$ instead of $\\mathbf{I}_{n}$ when the order is obvious or irrelevant. We have\n$$ \\mathbf{I} \\mathbf{A}=\\mathbf{A} \\mathbf{I}=\\mathbf{A}, $$\nif $\\mathbf{A}$ and $\\mathbf{I}$ have the same order.\nDefinition 0.14 (Orthogonal Matrix)\nA real square matrix $\\mathbf{A}$ is said to be orthogonal if\n$$ \\mathbf{A} \\mathbf{A}^{\\mathrm{T}}=\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I} $$\nand its columns are said to be orthonormal.\nA rectangular (not square) matrix can still have the property that $\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}=\\mathbf{I}$ or $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I}$, but not both. Such a matrix is called semi-orthogonal.\nNote carefully that the concepts of symmetry, skew-symmetry, and orthogonality are defined only for real square matrices. Hence, a complex matrix $\\mathbf{Z}$ satisfying $\\mathbf{Z}^{\\mathrm{T}}=\\mathbf{Z}$ is not called symmetric (in spite of what some textbooks do). This is important because complex matrices can be Hermitian, skew-Hermitian, or unitary, and there are many important results about these classes of matrices. These results should specialize to matrices that are symmetric, skew-symmetric, or orthogonal in the special case that the matrices are real. Thus, a symmetric matrix is just a real Hermitian matrix, a skew-symmetric matrix is a real skew-Hermitian matrix, and an orthogonal matrix is a real unitary matrix; see also Section 1.12.\n6 - Linear forms and quadratic forms # Let $\\mathbf{a}$ be an $n \\times 1$ vector, $\\mathbf{A}$ an $n \\times n$ matrix, and $\\mathbf{B}$ an $n \\times m$ matrix. The expression $\\mathbf{a}^{\\mathrm{T}} \\mathbf{x}$ is called a linear form in $\\mathbf{x}$, the expression $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}$ is a quadratic form in $\\mathbf{x}$, and the expression $\\mathbf{x}^{\\mathrm{T}} \\mathbf{B} \\mathbf{y}$ a bilinear form in $\\mathbf{x}$ and $\\mathbf{y}$. In quadratic forms we may, without loss of generality, assume that $\\mathbf{A}$ is symmetric, because if not then we can replace $\\mathbf{A}$ by $\\left(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}\\right) / 2$, since\n$$ \\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{x}^{\\mathrm{T}}\\left(\\frac{\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}}{2}\\right) \\mathbf{x} . $$\nThus, let $\\mathbf{A}$ be a symmetric matrix. We say that $\\mathbf{A}$ is\npositive definite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026gt;0$ for all $\\mathbf{x} \\neq \\mathbf{0}$, positive semidefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x} \\geq 0$ for all $\\mathbf{x}$, negative definite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026lt;0$ for all $\\mathbf{x} \\neq \\mathbf{0}$, negative semidefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x} \\leq 0$ for all $\\mathbf{x}$, indefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026gt;0$ for some $\\mathbf{x}$ and $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026lt;0$ for some $\\mathbf{x}$. It is clear that the matrices $\\mathbf{B} \\mathbf{B}^{\\mathrm{T}}$ and $\\mathbf{B}^{\\mathrm{T}} \\mathbf{B}$ are positive semidefinite, and that $\\mathbf{A}$ is negative (semi)definite if and only if $-\\mathbf{A}$ is positive (semi)definite. A square null matrix is both positive and negative semidefinite.\nDefinition 0.15 (Square Root Matrix)\nIf $\\mathbf{A}$ is positive semidefinite, then there are many matrices $\\mathbf{B}$ satisfying\n$$ \\mathbf{B}^{2}=\\mathbf{A} . $$\nBut there is only one positive semidefinite matrix $\\mathbf{B}$ satisfying $\\mathbf{B}^{2}=\\mathbf{A}$. This matrix is called the square root of $\\mathbf{A}$, denoted by $\\mathbf{A}^{1 / 2}$.\nThe following two theorems are often useful.\nTheorem 0.1 (Matrix Equality Conditions)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix, $\\mathbf{B}$ and $\\mathbf{C}$ $n \\times p$ matrices, and let $\\mathbf{x}$ be an $n \\times 1$ vector. Then, (a) $\\mathbf{A} \\mathbf{x}=\\mathbf{0} \\Longleftrightarrow \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$, (b) $\\mathbf{A} \\mathbf{B}=\\mathbf{0} \\Longleftrightarrow \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{B}=\\mathbf{0}$, (c) $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{B}=\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{C} \\Longleftrightarrow \\mathbf{A} \\mathbf{B}=\\mathbf{A} \\mathbf{C}$. Proof\n(a) Clearly $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ implies $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$. Conversely, if $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$, then $(\\mathbf{A} \\mathbf{x})^{\\mathrm{T}}(\\mathbf{A} \\mathbf{x})=\\mathbf{x}^{\\mathrm{T}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=0$ and hence $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$. (b) follows from (a), and (c) follows from (b) by substituting $\\mathbf{B}-\\mathbf{C}$ for $\\mathbf{B}$ in (b). ■ Theorem 0.2 (Zero Matrix Conditions)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix, $\\mathbf{B}$ and $\\mathbf{C}$ $n \\times n$ matrices, $\\mathbf{B}$ symmetric. Then, (a) $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{A}=\\mathbf{0}$, (b) $\\mathbf{x}^{\\mathrm{T}} \\mathbf{B} \\mathbf{x}=0$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{B}=\\mathbf{0}$, (c) $\\mathbf{x}^{\\mathrm{T}} \\mathbf{C} \\mathbf{x}=0$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{C}^{\\mathrm{T}}=-\\mathbf{C}$. Proof\nThe proof is easy and is left to the reader. ■ 7 - The rank of a matrix # Definition 0.16 (Linear Independence)\nA set of vectors $\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}$ is said to be linearly independent if $\\sum_{i} \\alpha_{i} \\mathbf{x}_{i}=\\mathbf{0}$ implies that all $\\alpha_{i}=0$. If $\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}$ are not linearly independent, they are said to be linearly dependent. Definition 0.17 (Matrix Rank)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix. The column rank of $\\mathbf{A}$ is the maximum number of linearly independent columns it contains. The row rank of $\\mathbf{A}$ is the maximum number of linearly independent rows it contains. It may be shown that the column rank of $\\mathbf{A}$ is equal to its row rank. Hence, the concept of rank is unambiguous. We denote the rank of $\\mathbf{A}$ by\n$$ r(\\mathbf{A}) . $$\nIt is clear that\n$$ r(\\mathbf{A}) \\leq \\min (m, n) $$\nIf $r(\\mathbf{A})=m$, we say that $\\mathbf{A}$ has full row rank. If $r(\\mathbf{A})=n$, we say that $\\mathbf{A}$ has full column rank. If $r(\\mathbf{A})=0$, then $\\mathbf{A}$ is the null matrix, and conversely, if $\\mathbf{A}$ is the null matrix, then $r(\\mathbf{A})=0$.\nWe have the following important results concerning ranks:\n\\begin{equation} \\begin{gathered} r(\\mathbf{A})=r\\left(\\mathbf{A}^{\\mathrm{T}}\\right)=r\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)=r\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right) \\\\ r(\\mathbf{A} \\mathbf{B}) \\leq \\min (r(\\mathbf{A}), r(\\mathbf{B})) \\\\ r(\\mathbf{A} \\mathbf{B})=r(\\mathbf{A}) \\quad \\text { if } \\mathbf{B} \\text { is square and of full rank, } \\\\ r(\\mathbf{A}+\\mathbf{B}) \\leq r(\\mathbf{A})+r(\\mathbf{B}) \\end{gathered} \\end{equation}\nand finally, if $\\mathbf{A}$ is an $m \\times n$ matrix and $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ for some $\\mathbf{x} \\neq \\mathbf{0}$, then\n$$ r(\\mathbf{A}) \\leq n-1 $$\nDefinition 0.18 (Column Space)\nThe column space of $\\mathbf{A}(m \\times n)$, denoted by $\\mathcal{M}(\\mathbf{A})$, is the set of vectors\n$$ \\mathcal{M}(\\mathbf{A})=\\left\\{\\mathbf{y}: \\mathbf{y}=\\mathbf{A} \\mathbf{x} \\text { for some } \\mathbf{x} \\text { in } \\mathbb{R}^{n}\\right\\} $$\nThus, $\\mathcal{M}(\\mathbf{A})$ is the vector space generated by the columns of $\\mathbf{A}$.\nThe dimension of this vector space is $r(\\mathbf{A})$. We have\n$ \\mathcal{M}(\\mathbf{A})=\\mathcal{M}\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right) $\nfor any matrix $\\mathbf{A}$.\nExercises:\nIf $\\mathbf{A}$ has full column rank and $\\mathbf{C}$ has full row rank, then $r(\\mathbf{A} \\mathbf{B} \\mathbf{C})=r(\\mathbf{B})$. Let $\\mathbf{A}$ be partitioned as $\\mathbf{A}=\\left(\\mathbf{A}_{1}: \\mathbf{A}_{2}\\right)$. Then $r(\\mathbf{A})=r\\left(\\mathbf{A}_{1}\\right)$ if and only if $\\mathcal{M}\\left(\\mathbf{A}_{2}\\right) \\subset \\mathcal{M}\\left(\\mathbf{A}_{1}\\right)$. 8 - The Inverse # Definition 0.19 (Nonsingular Matrix)\nLet $\\mathbf{A}$ be a square matrix of order $n \\times n$. We say that $\\mathbf{A}$ is nonsingular if $r(\\mathbf{A})=n$, and that $\\mathbf{A}$ is singular if $r(\\mathbf{A})\u0026lt;n$. Definition 0.20 (Matrix Inverse)\nIf $\\mathbf{A}$ is nonsingular, then there exists a nonsingular matrix $\\mathbf{B}$ such that\n$ \\mathbf{A} \\mathbf{B}=\\mathbf{B} \\mathbf{A}=\\mathbf{I}_{n} . $\nThe matrix $\\mathbf{B}$, denoted by $\\mathbf{A}^{-1}$, is unique and is called the inverse of $\\mathbf{A}$.\nWe have\n\\begin{equation} \\begin{aligned} \\left(\\mathbf{A}^{-1}\\right)^{\\mathrm{T}} \u0026amp; =\\left(\\mathbf{A}^{\\mathrm{T}}\\right)^{-1}, \\\\ (\\mathbf{A} \\mathbf{B})^{-1} \u0026amp; =\\mathbf{B}^{-1} \\mathbf{A}^{-1}, \\end{aligned} \\end{equation}\nif the inverses exist.\nDefinition 0.21 (Permutation Matrix)\nA square matrix $\\mathbf{P}$ is said to be a permutation matrix if each row and each column of $\\mathbf{P}$ contain a single element one, and the remaining elements are zero. An $n \\times n$ permutation matrix thus contains $n$ ones and $n(n-1)$ zeros. It can be proved that any permutation matrix is nonsingular. In fact, it is even true that $\\mathbf{P}$ is orthogonal, that is,\n$ \\mathbf{P}^{-1}=\\mathbf{P}^{\\mathrm{T}} $\nfor any permutation matrix $\\mathbf{P}$.\n9 - The Determinant # Definition 0.22 (Determinant)\nAssociated with any $n \\times n$ matrix $\\mathbf{A}$ is the determinant $|\\mathbf{A}|$ defined by\n$ |\\mathbf{A}|=\\sum(-1)^{\\phi\\left(j_{1}, \\ldots, j_{n}\\right)} \\prod_{i=1}^{n} a_{i j_{i}} $\nwhere the summation is taken over all permutations $\\left(j_{1}, \\ldots, j_{n}\\right)$ of the set of integers $(1, \\ldots, n)$, and $\\phi\\left(j_{1}, \\ldots, j_{n}\\right)$ is the number of transpositions required to change $(1, \\ldots, n)$ into $\\left(j_{1}, \\ldots, j_{n}\\right)$.\nWe have\n\\begin{equation} \\begin{aligned} |\\mathbf{A} \\mathbf{B}| \u0026amp; =|\\mathbf{A}||\\mathbf{B}| \\\\ \\left|\\mathbf{A}^{\\mathrm{T}}\\right| \u0026amp; =|\\mathbf{A}| \\\\ |\\alpha \\mathbf{A}| \u0026amp; =\\alpha^{n}|\\mathbf{A}| \\quad \\text { for any scalar } \\alpha \\\\ \\left|\\mathbf{A}^{-1}\\right| \u0026amp; =|\\mathbf{A}|^{-1} \\quad \\text { if } \\mathbf{A} \\text { is nonsingular, } \\\\ \\left|\\mathbf{I}_{n}\\right| \u0026amp; =1 \\end{aligned} \\end{equation}\nDefinition 0.23 (Minor and Cofactor)\nA submatrix of $\\mathbf{A}$ is the rectangular array obtained from $\\mathbf{A}$ by deleting some of its rows and/or some of its columns. A minor is the determinant of a square submatrix of $\\mathbf{A}$. The minor of an element $a_{i j}$ is the determinant of the submatrix of $\\mathbf{A}$ obtained by deleting the $i$th row and $j$th column. The cofactor of $a_{i j}$, say $c_{i j}$, is $(-1)^{i+j}$ times the minor of $a_{i j}$. The matrix $\\mathbf{C}=\\left(c_{i j}\\right)$ is called the cofactor matrix of $\\mathbf{A}$. The transpose of $\\mathbf{C}$ is called the adjoint of $\\mathbf{A}$ and will be denoted by $\\mathbf{A}^{\\#}$.\nWe have\n\\begin{equation} \\begin{aligned} |\\mathbf{A}|=\\sum_{j=1}^{n} a_{i j} c_{i j} \u0026amp; =\\sum_{j=1}^{n} a_{j k} c_{j k} \\quad(i, k=1, \\ldots, n), \\\\ \\mathbf{A} \\mathbf{A}^{\\#} \u0026amp; =\\mathbf{A}^{\\#} \\mathbf{A}=|\\mathbf{A}| \\mathbf{I}, \\\\ (\\mathbf{A} \\mathbf{B})^{\\#} \u0026amp; =\\mathbf{B}^{\\#} \\mathbf{A}^{\\#} . \\end{aligned} \\end{equation}\nDefinition 0.24 (Principal Minor)\nFor any square matrix $\\mathbf{A}$, a principal submatrix of $\\mathbf{A}$ is obtained by deleting corresponding rows and columns. The determinant of a principal submatrix is called a principal minor. Exercises:\nIf $\\mathbf{A}$ is nonsingular, show that $\\mathbf{A}^{\\#}=|\\mathbf{A}| \\mathbf{A}^{-1}$. Prove that the determinant of a triangular matrix is the product of its diagonal elements. 10 - The trace # Definition 0.25 (Trace)\nThe trace of a square $n \\times n$ matrix $\\mathbf{A}$, denoted by $\\operatorname{tr} \\mathbf{A}$ or $\\operatorname{tr}(\\mathbf{A})$, is the sum of its diagonal elements:\n$ \\operatorname{tr} \\mathbf{A}=\\sum_{i=1}^{n} a_{i i} . $\nWe have\n\\begin{equation} \\begin{aligned} \\operatorname{tr}(\\mathbf{A}+\\mathbf{B}) \u0026amp; =\\operatorname{tr} \\mathbf{A}+\\operatorname{tr} \\mathbf{B} \\\\ \\operatorname{tr}(\\lambda \\mathbf{A}) \u0026amp; =\\lambda \\operatorname{tr} \\mathbf{A} \\quad \\text { if } \\lambda \\text { is a scalar } \\\\ \\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \u0026amp; =\\operatorname{tr} \\mathbf{A} \\\\ \\operatorname{tr} \\mathbf{A} \\mathbf{B} \u0026amp; =\\operatorname{tr} \\mathbf{B} \\mathbf{A} \\end{aligned} \\end{equation}\nWe note in (25) that $\\mathbf{A} \\mathbf{B}$ and $\\mathbf{B} \\mathbf{A}$, though both square, need not be of the same order.\nCorresponding to the vector (Euclidean) norm\n$ |\\mathbf{x}|=\\left(\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}\\right)^{1 / 2}, $\ngiven in (4), we now define the matrix (Euclidean) norm as\nDefinition 0.26 (Matrix Norm)\n$ |\\mathbf{A}|=\\left(\\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{1 / 2} $ We have\n$ \\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\geq 0 $\nwith equality if and only if $\\mathbf{A}=\\mathbf{0}$.\n11 - Partitioned matrices # Definition 0.27 (Partitioned Matrix)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix. We can partition $\\mathbf{A}$ as\n$$ \\mathbf{A}=\\left(\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right), $$\nwhere $\\mathbf{A}_{11}$ is $m_1 \\times n_1$, $\\mathbf{A}_{12}$ is $m_1 \\times n_2$, $\\mathbf{A}_{21}$ is $m_2 \\times n_1$, $\\mathbf{A}_{22}$ is $m_2 \\times n_2$, and $m_1+m_2=m$ and $n_1+n_2=n$.\nLet $\\mathbf{B}(m \\times n)$ be similarly partitioned into submatrices $\\mathbf{B}_{ij}(i, j=1,2)$. Then,\n$$ \\mathbf{A}+\\mathbf{B}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}+\\mathbf{B}_{11} \u0026amp; \\mathbf{A}_{12}+\\mathbf{B}_{12} \\\\ \\mathbf{A}_{21}+\\mathbf{B}_{21} \u0026amp; \\mathbf{A}_{22}+\\mathbf{B}_{22} \\end{array}\\right) $$\nNow let $\\mathbf{C}(n \\times p)$ be partitioned into submatrices $\\mathbf{C}_{ij}(i, j=1,2)$ such that $\\mathbf{C}_{11}$ has $n_1$ rows (and hence $\\mathbf{C}_{12}$ also has $n_1$ rows and $\\mathbf{C}_{21}$ and $\\mathbf{C}_{22}$ have $n_2$ rows). Then we may postmultiply $\\mathbf{A}$ by $\\mathbf{C}$ yielding\n$$ \\mathbf{A} \\mathbf{C}=\\left(\\begin{array}{cc} \\mathbf{A}_{11} \\mathbf{C}_{11}+\\mathbf{A}_{12} \\mathbf{C}_{21} \u0026amp; \\mathbf{A}_{11} \\mathbf{C}_{12}+\\mathbf{A}_{12} \\mathbf{C}_{22} \\\\ \\mathbf{A}_{21} \\mathbf{C}_{11}+\\mathbf{A}_{22} \\mathbf{C}_{21} \u0026amp; \\mathbf{A}_{21} \\mathbf{C}_{12}+\\mathbf{A}_{22} \\mathbf{C}_{22} \\end{array}\\right) $$\nThe transpose of the matrix $\\mathbf{A}$ given in (28) is\n$$ \\mathbf{A}^{\\mathrm{T}}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{\\mathrm{T}} \u0026amp; \\mathbf{A}_{21}^{\\mathrm{T}} \\\\ \\mathbf{A}_{12}^{\\mathrm{T}} \u0026amp; \\mathbf{A}_{22}^{\\mathrm{T}} \\end{array}\\right) $$\nIf the off-diagonal blocks $\\mathbf{A}_{12}$ and $\\mathbf{A}_{21}$ are both zero, and $\\mathbf{A}_{11}$ and $\\mathbf{A}_{22}$ are square and nonsingular, then $\\mathbf{A}$ is also nonsingular and its inverse is\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{-1} \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; \\mathbf{A}_{22}^{-1} \\end{array}\\right) $$\nMore generally, if $\\mathbf{A}$ as given in (28) is nonsingular and $\\mathbf{D}=\\mathbf{A}_{22}-\\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}$ is also nonsingular, then\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{-1}+\\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{D}^{-1} \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \u0026amp; -\\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1} \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \u0026amp; \\mathbf{D}^{-1} \\end{array}\\right) $$\nAlternatively, if $\\mathbf{A}$ is nonsingular and $\\mathbf{E}=\\mathbf{A}_{11}-\\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21}$ is also nonsingular, then\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{E}^{-1} \u0026amp; -\\mathbf{E}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\\\ -\\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\mathbf{E}^{-1} \u0026amp; \\mathbf{A}_{22}^{-1}+\\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\mathbf{E}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\end{array}\\right) . $$\nOf course, if both $\\mathbf{D}$ and $\\mathbf{E}$ are nonsingular, blocks in (29) and (30) can be interchanged. The results (29) and (30) can be easily extended to a $3 \\times 3$ matrix partition. We only consider the following symmetric case where two of the off-diagonal blocks are null matrices.\nTheorem 0.3 (3x3 Symmetric Partitioned Matrix Inverse)\nIf the matrix\n$$ \\left(\\begin{array}{lll} \\mathbf{A} \u0026amp; \\mathbf{B} \u0026amp; \\mathbf{C} \\\\ \\mathbf{B}^{\\mathrm{T}} \u0026amp; \\mathbf{D} \u0026amp; \\mathbf{0} \\\\ \\mathbf{C}^{\\mathrm{T}} \u0026amp; \\mathbf{0} \u0026amp; \\mathbf{E} \\end{array}\\right) $$\nis symmetric and nonsingular, its inverse is given by\n$$ \\left(\\begin{array}{ccc} \\mathbf{Q}^{-1} \u0026amp; -\\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; -\\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\\\ -\\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \u0026amp; \\mathbf{D}^{-1}+\\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; \\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\\\ -\\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \u0026amp; \\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; \\mathbf{E}^{-1}+\\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\end{array}\\right) $$\nwhere\n$$ \\mathbf{Q}=\\mathbf{A}-\\mathbf{B} \\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}}-\\mathbf{C} \\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} . $$\nProof\nThe proof is left to the reader. ■ As to the determinants of partitioned matrices, we note that\n$$ \\left|\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{0} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right|=\\left|\\mathbf{A}_{11}\\right|\\left|\\mathbf{A}_{22}\\right|=\\left|\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{0} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right| $$\nif both $\\mathbf{A}_{11}$ and $\\mathbf{A}_{22}$ are square matrices.\nExercises:\nFind the determinant and inverse (if it exists) of $$ \\mathbf{B}=\\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{0} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; 1 \\end{array}\\right) . $$\nIf $|\\mathbf{A}| \\neq 0$, prove that $$ \\left|\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right|=\\left(\\alpha-\\mathbf{a}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\\right)|\\mathbf{A}| . $$\nIf $\\alpha \\neq 0$, prove that $$ \\left|\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right|=\\alpha\\left|\\mathbf{A}-(1 / \\alpha) \\mathbf{b} \\mathbf{a}^{\\mathrm{T}}\\right| . $$\n12 - Complex Matrices # If $\\mathbf{A}$ and $\\mathbf{B}$ are real matrices of the same order, then a complex matrix $\\mathbf{Z}$ can be defined as\n$$ \\mathbf{Z}=\\mathbf{A}+i \\mathbf{B} $$\nwhere $i$ denotes the imaginary unit with the property $i^2=-1$. The complex conjugate of $\\mathbf{Z}$, denoted by $\\mathbf{Z}^mathrm{H}$, is defined as\n$$ \\mathbf{Z}^\\mathrm{H}=\\mathbf{A}^{\\mathrm{T}}-i \\mathbf{B}^{\\mathrm{T}} $$\nIf $\\mathbf{Z}$ is real, then $\\mathbf{Z}^\\mathrm{H}=\\mathbf{Z}^{\\mathrm{T}}$. If $\\mathbf{Z}$ is a scalar, say $\\zeta$, we usually write $\\bar{\\zeta}$ instead of $\\zeta^mathrm{H}$.\nA square complex matrix $\\mathbf{Z}$ is said to be Hermitian if $\\mathbf{Z}^{\\mathrm{H}}=\\mathbf{Z}$ (the complex equivalent to a symmetric matrix), skew-Hermitian if $\\mathbf{Z}^{\\mathrm{H}}=-\\mathbf{Z}$ (the complex equivalent to a skew-symmetric matrix), and unitary if $\\mathbf{Z}^{\\mathrm{H}} \\mathbf{Z}=\\mathbf{I}$ (the complex equivalent to an orthogonal matrix).\nWe shall see in this theorem that the eigenvalues of a symmetric matrix are real. In general, however, eigenvalues (and hence eigenvectors) are complex. In this book, complex numbers appear only in connection with eigenvalues and eigenvectors of matrices that are not symmetric (Chapter 8). A detailed treatment is therefore omitted. Matrices and vectors are assumed to be real, unless it is explicitly specified that they are complex.\n13 - Eigenvalues and Eigenvectors # Definition 0.28 (Eigenvalues and Eigenvectors)\nLet $\\mathbf{A}$ be a square matrix, say $n \\times n$. The eigenvalues of $\\mathbf{A}$ are defined as the roots of the characteristic equation\n$$ \\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right|=0 $$\nThe characteristic equation (31) has $n$ roots, in general complex. Let $\\lambda$ be an eigenvalue of $\\mathbf{A}$. Then there exist vectors $\\mathbf{x}$ and $\\mathbf{y}(\\mathbf{x} \\neq \\mathbf{0}, \\mathbf{y} \\neq \\mathbf{0})$ such that\n$$ (\\lambda \\mathbf{I}-\\mathbf{A}) \\mathbf{x}=\\mathbf{0}, \\quad \\mathbf{y}^{\\mathrm{T}}(\\lambda \\mathbf{I}-\\mathbf{A})=\\mathbf{0} $$\nThat is,\n$$ \\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}, \\quad \\mathbf{y}^{\\mathrm{T}} \\mathbf{A}=\\lambda \\mathbf{y}^{\\mathrm{T}} $$\nThe vectors $\\mathbf{x}$ and $\\mathbf{y}$ are called a (column) eigenvector and row eigenvector of $\\mathbf{A}$ associated with the eigenvalue $\\lambda$.\nEigenvectors are usually normalized in some way to make them unique, for example, by $\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}=\\mathbf{y}^{\\mathrm{T}} \\mathbf{y}=1$ (when $\\mathbf{x}$ and $\\mathbf{y}$ are real).\nNot all roots of the characteristic equation need to be different. Each root is counted a number of times equal to its multiplicity. When a root (eigenvalue) appears more than once it is called a multiple eigenvalue; if it appears only once it is called a simple eigenvalue.\nAlthough eigenvalues are in general complex, the eigenvalues of a symmetric matrix are always real.\nTheorem 0.4 (Symmetric Matrix Eigenvalues)\nA symmetric matrix has only real eigenvalues. Proof\nLet $\\lambda$ be an eigenvalue of a symmetric matrix $\\mathbf{A}$ and let $\\mathbf{x}=\\mathbf{u}+i \\mathbf{v}$ be an associated eigenvector. Then,\n$$ \\mathbf{A}(\\mathbf{u}+i \\mathbf{v})=\\lambda(\\mathbf{u}+i \\mathbf{v}) $$\nand hence\n$$ (\\mathbf{u}-i \\mathbf{v})^{\\mathrm{T}} \\mathbf{A}(\\mathbf{u}+i \\mathbf{v})=\\lambda(\\mathbf{u}-i \\mathbf{v})^{\\mathrm{T}}(\\mathbf{u}+i \\mathbf{v}) $$\nwhich leads to\n$$ \\mathbf{u}^{\\mathrm{T}} \\mathbf{A} \\mathbf{u}+\\mathbf{v}^{\\mathrm{T}} \\mathbf{A} \\mathbf{v}=\\lambda\\left(\\mathbf{u}^{\\mathrm{T}} \\mathbf{u}+\\mathbf{v}^{\\mathrm{T}} \\mathbf{v}\\right) $$\nbecause of the symmetry of $\\mathbf{A}$. This implies that $\\lambda$ is real.\n■ Let us prove the following three results, which will be useful to us later.\nTheorem 0.5 (Similar Matrices Eigenvalues)\nIf $\\mathbf{A}$ is an $n \\times n$ matrix and $\\mathbf{G}$ is a nonsingular $n \\times n$ matrix, then $\\mathbf{A}$ and $\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}$ have the same set of eigenvalues (with the same multiplicities). Proof\nFrom\n$$ \\lambda \\mathbf{I}_n-\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}=\\mathbf{G}^{-1}\\left(\\lambda \\mathbf{I}_n-\\mathbf{A}\\right) \\mathbf{G} $$\nwe obtain\n$$ \\left|\\lambda \\mathbf{I}_n-\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}\\right|=\\left|\\mathbf{G}^{-1}\\right|\\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right||\\mathbf{G}|=\\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right| $$\nand the result follows.\n■ Theorem 0.6 (Singular Matrix Zero Eigenvalue)\nA singular matrix has at least one zero eigenvalue. Proof\nIf $\\mathbf{A}$ is singular, then $|\\mathbf{A}|=0$ and hence $|\\lambda \\mathbf{I}-\\mathbf{A}|=0$ for $\\lambda=0$. ■ Theorem 0.7 (Special Matrix Eigenvalues)\nAn idempotent matrix has only eigenvalues 0 or 1. All eigenvalues of a unitary matrix have unit modulus. Proof\nLet $\\mathbf{A}$ be idempotent. Then $\\mathbf{A}^2=\\mathbf{A}$. Thus, if $\\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}$, then\n$$ \\lambda \\mathbf{x}=\\mathbf{A} \\mathbf{x}=\\mathbf{A}(\\mathbf{A} \\mathbf{x})=\\mathbf{A}(\\lambda \\mathbf{x})=\\lambda(\\mathbf{A} \\mathbf{x})=\\lambda^2 \\mathbf{x} $$\nand hence $\\lambda=\\lambda^2$, which implies $\\lambda=0$ or $\\lambda=1$.\nIf $\\mathbf{A}$ is unitary, then $\\mathbf{A}^{\\mathrm{H}} \\mathbf{A}=\\mathbf{I}$. Thus, if $\\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}$, then\n$$ \\mathbf{x}^{\\mathrm{H}} \\mathbf{A}^{\\mathrm{H}}=\\bar{\\lambda} \\mathbf{x}^{\\mathrm{H}} $$\nusing the notation of Section 1.12. Hence,\n$$ \\mathbf{x}^{\\mathrm{H}} \\mathbf{x}=\\mathbf{x}^{\\mathrm{H}} \\mathbf{A}^{\\mathrm{H}} \\mathbf{A} \\mathbf{x}=\\bar{\\lambda} \\lambda \\mathbf{x}^{\\mathrm{H}} \\mathbf{x} $$\nSince $\\mathbf{x}^{\\mathrm{H}} \\mathbf{x} \\neq 0$, we obtain $\\bar{\\lambda} \\lambda=1$ and hence $|\\lambda|=1$.\n■ An important theorem regarding positive definite matrices is stated below.\nTheorem 0.8 (Positive Definite Eigenvalues)\nA symmetric matrix is positive definite if and only if all its eigenvalues are positive. Proof\nIf $\\mathbf{A}$ is positive definite and $\\mathbf{A}\\mathbf{x}=\\lambda \\mathbf{x}$, then $\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}^\\mathrm{T} \\mathbf{x}$. Now, $\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}\u0026gt;0$ and $\\mathbf{x}^\\mathrm{T} \\mathbf{x}\u0026gt;0$ imply $\\lambda\u0026gt;0$. The converse will not be proved here. (It follows from this theorem.) ■ Next, let us prove this theorem.\nTheorem 0.9 (Eigenvalue Identity)\nLet $\\mathbf{A}$ be $m \\times n$ and let $\\mathbf{B}$ be $n \\times m(n \\geq m)$. Then the nonzero eigenvalues of $\\mathbf{B}\\mathbf{A}$ and $\\mathbf{A}\\mathbf{B}$ are identical, and $\\left|I_{m}-\\mathbf{A}\\mathbf{B}\\right|=\\left|I_{n}-\\mathbf{B}\\mathbf{A}\\right|$. Proof\nTaking determinants on both sides of the equality\n\\begin{equation} \\left(\\begin{array}{cc} I_{m}-\\mathbf{A}\\mathbf{B} \u0026amp; \\mathbf{A} \\\\ 0 \u0026amp; I_{n} \\end{array}\\right)\\left(\\begin{array}{cc} I_{m} \u0026amp; 0 \\\\ \\mathbf{B} \u0026amp; I_{n} \\end{array}\\right)=\\left(\\begin{array}{cc} I_{m} \u0026amp; 0 \\\\ \\mathbf{B} \u0026amp; I_{n} \\end{array}\\right)\\left(\\begin{array}{cc} I_{m} \u0026amp; \\mathbf{A} \\\\ 0 \u0026amp; I_{n}-\\mathbf{B}\\mathbf{A} \\end{array}\\right), \\end{equation}\nwe obtain\n\\begin{equation} \\left|I_{m}-\\mathbf{A}\\mathbf{B}\\right|=\\left|I_{n}-\\mathbf{B}\\mathbf{A}\\right| . \\end{equation}\nNow let $\\lambda \\neq 0$. Then,\n\\begin{equation} \\begin{aligned} \\left|\\lambda I_{n}-\\mathbf{B}\\mathbf{A}\\right| \u0026amp; =\\lambda^{n}\\left|I_{n}-\\mathbf{B}\\left(\\lambda^{-1} \\mathbf{A}\\right)\\right| \\\\ \u0026amp; =\\lambda^{n}\\left|I_{m}-\\left(\\lambda^{-1} \\mathbf{A}\\right) \\mathbf{B}\\right|=\\lambda^{n-m}\\left|\\lambda I_{m}-\\mathbf{A}\\mathbf{B}\\right| . \\end{aligned} \\end{equation}\nHence, the nonzero eigenvalues of $\\mathbf{B}\\mathbf{A}$ are the same as the nonzero eigenvalues of $\\mathbf{A}\\mathbf{B}$, and this is equivalent to the statement in the theorem.\n■ Without proof we state the following famous result.\nTheorem 0.10 (Cayley-Hamilton)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$. Then,\n\\begin{equation} \\prod_{i=1}^{n}\\left(\\lambda_{i} I_{n}-\\mathbf{A}\\right)=0 . \\end{equation}\nFinally, we present the following result on eigenvectors.\nTheorem 0.11 (Linear Independence of Eigenvectors)\nEigenvectors associated with distinct eigenvalues are linearly independent. Proof\nLet $\\mathbf{A}\\mathbf{x}_{1}=\\lambda_{1} \\mathbf{x}_{1}$, $\\mathbf{A}\\mathbf{x}_{2}=\\lambda_{2} \\mathbf{x}_{2}$, and $\\lambda_{1} \\neq \\lambda_{2}$. Assume that $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ are linearly dependent. Then there is an $\\alpha \\neq 0$ such that $\\mathbf{x}_{2}=\\alpha \\mathbf{x}_{1}$, and hence\n\\begin{equation} \\alpha \\lambda_{1} \\mathbf{x}_{1}=\\alpha \\mathbf{A} \\mathbf{x}_{1}=\\mathbf{A} \\mathbf{x}_{2}=\\lambda_{2} \\mathbf{x}_{2}=\\alpha \\lambda_{2} \\mathbf{x}_{1} \\end{equation}\nthat is\n\\begin{equation} \\alpha\\left(\\lambda_{1}-\\lambda_{2}\\right) \\mathbf{x}_{1}=0 \\end{equation}\nSince $\\alpha \\neq 0$ and $\\lambda_{1} \\neq \\lambda_{2}$, this implies that $\\mathbf{x}_{1}=0$, a contradiction.\n■ Exercices\nShow that \\begin{equation} \\left|\\begin{array}{ll} 0 \u0026amp; I_{m} \\\\ I_{m} \u0026amp; 0 \\end{array}\\right|=(-1)^{m} \\end{equation}\nShow that, for $n=2$, \\begin{equation} |I+\\epsilon \\mathbf{A}|=1+\\epsilon \\operatorname{tr} \\mathbf{A}+\\epsilon^{2}|\\mathbf{A}| \\end{equation}\nShow that, for $n=3$, \\begin{equation} |I+\\epsilon \\mathbf{A}|=1+\\epsilon \\operatorname{tr} \\mathbf{A}+\\frac{\\epsilon^{2}}{2}\\left((\\operatorname{tr} \\mathbf{A})^{2}-\\operatorname{tr} \\mathbf{A}^{2}\\right)+\\epsilon^{3}|\\mathbf{A}| \\end{equation}\n14 - Schur\u0026rsquo;s decomposition theorem # In the next few sections, we present three decomposition theorems: Schur\u0026rsquo;s theorem, Jordan\u0026rsquo;s theorem, and the singular-value decomposition. Each of these theorems will prove useful later in this book. We first state Schur\u0026rsquo;s theorem.\nTheorem 0.12 (Schur decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix, possibly complex. Then there exist a unitary $n \\times n$ matrix $\\mathbf{S}$ (that is, $\\mathbf{S}^\\mathrm{H} \\mathbf{S}=I_{n}$ ) and an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M} \\end{equation}\nThe most important special case of Schur\u0026rsquo;s decomposition theorem is the case where $\\mathbf{A}$ is symmetric.\nTheorem 0.13 (Symmetric Matrix Decomposition)\nLet $\\mathbf{A}$ be a symmetric $n \\times n$ matrix. Then there exist an orthogonal $n \\times n$ matrix $\\mathbf{S}$ (that is, $\\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{n}$ ) whose columns are eigenvectors of $\\mathbf{A}$ and a diagonal matrix $\\boldsymbol{\\Lambda}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{S}^\\mathrm{T} \\mathbf{A} \\mathbf{S}=\\boldsymbol{\\Lambda} \\end{equation}\nProof\nUsing Theorem 0.12, there exists a unitary matrix $\\mathbf{S}=\\mathbf{R}+i \\mathbf{T}$ with real $\\mathbf{R}$ and $\\mathbf{T}$ and an upper triangular matrix $\\mathbf{M}$ such that $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. Then,\n\\begin{equation} \\begin{aligned} \\mathbf{M} \u0026amp; =\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=(\\mathbf{R}-i \\mathbf{T})^\\mathrm{T} \\mathbf{A}(\\mathbf{R}+i \\mathbf{T}) \\\\ \u0026amp; =\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T}\\right)+i\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{T}-\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{R}\\right) \\end{aligned} \\end{equation}\nand hence, using the symmetry of $\\mathbf{A}$,\n\\begin{equation} \\mathbf{M}+\\mathbf{M}^\\mathrm{T}=2\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T}\\right) . \\end{equation}\nIt follows that $\\mathbf{M}+\\mathbf{M}^\\mathrm{T}$ is a real matrix and hence, since $\\mathbf{M}$ is triangular, that $\\mathbf{M}$ is a real matrix. We thus obtain, from (32),\n\\begin{equation} \\mathbf{M}=\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T} . \\end{equation}\nSince $\\mathbf{A}$ is symmetric, $\\mathbf{M}$ is symmetric. But, since $\\mathbf{M}$ is also triangular, $\\mathbf{M}$ must be diagonal. The columns of $\\mathbf{S}$ are then eigenvectors of $\\mathbf{A}$, and since the diagonal elements of $\\mathbf{M}$ are real, $\\mathbf{S}$ can be chosen to be real as well.\n■ Exercices\nLet $\\mathbf{A}$ be a symmetric $n \\times n$ matrix with eigenvalues $\\lambda_{1} \\leq \\lambda_{2} \\leq \\cdots \\leq \\lambda_{n}$. Use Theorem 0.13 to prove that \\begin{equation} \\lambda_{1} \\leq \\frac{\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}}{\\mathbf{x}^\\mathrm{T} \\mathbf{x}} \\leq \\lambda_{n} . \\end{equation}\nHence show that, for any $m \\times n$ matrix $\\mathbf{A}$, \\begin{equation} |\\mathbf{A} \\mathbf{x}| \\leq \\mu|\\mathbf{x}|, \\end{equation}\nwhere $\\mu^{2}$ denotes the largest eigenvalue of $\\mathbf{A}^\\mathrm{T} \\mathbf{A}$.\nLet $\\mathbf{A}$ be an $m \\times n$ matrix of rank $r$. Show that there exists an $n \\times(n-r)$ matrix $\\mathbf{S}$ such that \\begin{equation} \\mathbf{A} \\mathbf{S}=0, \\quad \\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{n-r} \\end{equation}\nLet $\\mathbf{A}$ be an $m \\times n$ matrix of rank $r$. Let $\\mathbf{S}$ be a matrix such that $\\mathbf{A} \\mathbf{S}=0$. Show that $r(\\mathbf{S}) \\leq n-r$. 15 - The Jordan decomposition # Schur\u0026rsquo;s theorem tells us that there exists, for every square matrix $\\mathbf{A}$, a unitary (possibly orthogonal) matrix $\\mathbf{S}$ which \u0026rsquo;transforms\u0026rsquo; $\\mathbf{A}$ into an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$.\nJordan\u0026rsquo;s theorem similarly states that there exists a nonsingular matrix, say $\\mathbf{T}$, which transforms $\\mathbf{A}$ into an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$. The difference between the two decomposition theorems is that in Jordan\u0026rsquo;s theorem less structure is put on the matrix $\\mathbf{T}$ (nonsingular, but not necessarily unitary) and more structure on the matrix $\\mathbf{M}$.\nTheorem 0.14 (Jordan decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix and denote by $\\mathbf{J}_{k}(\\lambda)$ a $k \\times k$ matrix of the form\n\\begin{equation} \\mathbf{J}_{k}(\\lambda)=\\left(\\begin{array}{cccccc} \\lambda \u0026amp; 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\lambda \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; \\lambda \\end{array}\\right) \\end{equation}\n(a so-called Jordan block), where $\\mathbf{J}_{1}(\\lambda)=\\lambda$. Then there exists a nonsingular $n \\times n$ matrix $\\mathbf{T}$ such that\n\\begin{equation} \\mathbf{T}^{-1} \\mathbf{A} \\mathbf{T}=\\left(\\begin{array}{cccc} \\mathbf{J}_{k_{1}}\\left(\\lambda_{1}\\right) \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; \\mathbf{J}_{k_{2}}\\left(\\lambda_{2}\\right) \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\mathbf{J}_{k_{r}}\\left(\\lambda_{r}\\right) \\end{array}\\right) \\end{equation}\nwith $k_{1}+k_{2}+\\cdots+k_{r}=n$. The $\\lambda_{i}$ are the eigenvalues of $\\mathbf{A}$, not necessarily distinct.\nThe most important special case of Theorem 0.14 is this theorem.\nTheorem 0.15 (Distinct Eigenvalues Decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix with distinct eigenvalues. Then there exist a nonsingular $n \\times n$ matrix $\\mathbf{T}$ and a diagonal $n \\times n$ matrix $\\boldsymbol{\\Lambda}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{T}^{-1} \\mathbf{A} \\mathbf{T}=\\boldsymbol{\\Lambda} \\end{equation}\nProof\nImmediate from Theorem 0.14 (or Theorem 0.11). ■ Exercices\nShow that $\\left(\\lambda I_{k}-\\mathbf{J}_{k}(\\lambda)\\right)^{k}=0$ and use this fact to prove Theorem 0.10. Show that Theorem 0.15 remains valid when $\\mathbf{A}$ is complex. 16 - The singular value decomposition # The third important decomposition theorem is the singular-value decomposition.\nTheorem 0.16 (Singular-value decomposition)\nLet $\\mathbf{A}$ be a real $m \\times n$ matrix with $r(\\mathbf{A})=r\u0026gt;0$. Then there exist an $m \\times r$ matrix $\\mathbf{S}$ such that $\\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{r}$, an $n \\times r$ matrix $\\mathbf{T}$ such that $\\mathbf{T}^\\mathrm{T} \\mathbf{T}=I_{r}$ and an $r \\times r$ diagonal matrix $\\boldsymbol{\\Lambda}$ with positive diagonal elements, such that\n\\begin{equation} \\mathbf{A}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{T}^\\mathrm{T} \\end{equation}\nProof\nSince $\\mathbf{A} \\mathbf{A}^\\mathrm{T}$ is an $m \\times m$ positive semidefinite matrix of rank $r$ (by (6)), its nonzero eigenvalues are all positive (this theorem). From Theorem 0.13 we know that there exists an orthogonal $m \\times m$ matrix $( \\mathbf{S}: \\mathbf{S}_{2})$ such that\n\\begin{equation} \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}, \\quad \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}_{2}=0, \\quad \\mathbf{S} \\mathbf{S}^\\mathrm{T}+\\mathbf{S}_{2} \\mathbf{S}_{2}^\\mathrm{T}=I_{m} \\end{equation}\nwhere $\\boldsymbol{\\Lambda}$ is an $r \\times r$ diagonal matrix having these $r$ positive eigenvalues as its diagonal elements. Define $\\mathbf{T}=\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}$. Then we see that\n\\begin{equation} \\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda}, \\quad \\mathbf{T}^\\mathrm{T} \\mathbf{T}=I_{r} \\label{eq:33} \\end{equation}\nThus, since $\\mathbf{A}^\\mathrm{T} \\mathbf{S}_{2}=0$, we have\n\\begin{equation} \\mathbf{A}=\\left(\\mathbf{S} \\mathbf{S}^\\mathrm{T}+\\mathbf{S}_{2} \\mathbf{S}_{2}^\\mathrm{T}\\right) \\mathbf{A}=\\mathbf{S} \\mathbf{S}^\\mathrm{T} \\mathbf{A}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2}\\left(\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}\\right)^\\mathrm{T}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{T}^\\mathrm{T} \\label{eq:34} \\end{equation}\nwhich concludes the proof.\n■ We see from \\eqref{eq:33} and \\eqref{eq:34} that the semi-orthogonal matrices $\\mathbf{S}$ and $\\mathbf{T}$ satisfy\n\\begin{equation} \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}, \\quad \\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda} \\end{equation}\nHence, $\\boldsymbol{\\Lambda}$ contains the $r$ nonzero eigenvalues of $\\mathbf{A} \\mathbf{A}^\\mathrm{T}$ (and of $\\mathbf{A}^\\mathrm{T} \\mathbf{A}$ ) and $\\mathbf{S}$ (by construction) and $\\mathbf{T}$ contain corresponding eigenvectors. A common mistake in applying the singular-value decomposition is to find $\\mathbf{S}$, $\\mathbf{T}$, and $\\boldsymbol{\\Lambda}$ from (35). This is incorrect because, given $\\mathbf{S}$, $\\mathbf{T}$ is not unique. The correct procedure is to find $\\mathbf{S}$ and $\\boldsymbol{\\Lambda}$ from $\\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}$ and then define $\\mathbf{T}=\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}$. Alternatively, we can find $\\mathbf{T}$ and $\\boldsymbol{\\Lambda}$ from $\\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda}$ and define $\\mathbf{S}=\\mathbf{A} \\mathbf{T} \\boldsymbol{\\Lambda}^{-1 / 2}$.\n17 - Further results concerning eigenvalues # Let us now prove the following five theorems, all of which concern eigenvalues. this theorem deals with the sum and the product of the eigenvalues. this theorem and this theorem discuss the relationship between the rank and the number of nonzero eigenvalues, and this theorem concerns idempotent matrices.\nTheorem 0.17 (Trace and Determinant)\nLet $\\mathbf{A}$ be a square, possibly complex, $n \\times n$ matrix with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$. Then,\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\sum_{i=1}^{n} \\lambda_{i}, \\quad |\\mathbf{A}|=\\prod_{i=1}^{n} \\lambda_{i} \\end{equation}\nProof\nWe write, using Theorem 0.12, $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. Then,\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\operatorname{tr} \\mathbf{S} \\mathbf{M} \\mathbf{S}^\\mathrm{H}=\\operatorname{tr} \\mathbf{M} \\mathbf{S}^\\mathrm{H} \\mathbf{S}=\\operatorname{tr} \\mathbf{M}=\\sum_{i} \\lambda_{i} \\end{equation}\nand\n\\begin{equation} |\\mathbf{A}|=\\left|\\mathbf{S} \\mathbf{M} \\mathbf{S}^\\mathrm{H}\\right|=|\\mathbf{S}||\\mathbf{M}|\\left|\\mathbf{S}^\\mathrm{H}\\right|=|\\mathbf{M}|=\\prod_{i} \\lambda_{i} \\end{equation}\nand the result follows.\n■ Theorem 0.18 (Rank and Nonzero Eigenvalues)\nIf $\\mathbf{A}$ has $r$ nonzero eigenvalues, then $r(\\mathbf{A}) \\geq r$. Proof\nWe write again, using Theorem 0.12, $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. We partition\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) \\end{equation}\nwhere $\\mathbf{M}_{1}$ is a nonsingular upper triangular $r \\times r$ matrix and $\\mathbf{M}_{3}$ is strictly upper triangular. Since $r(\\mathbf{A})=r(\\mathbf{M}) \\geq r\\left(\\mathbf{M}_{1}\\right)=r$, the result follows.\n■ The following example shows that it is indeed possible that $r(\\mathbf{A})\u0026gt;r$. Let\n\\begin{equation} \\mathbf{A}=\\left(\\begin{array}{ll} 1 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \\end{array}\\right) \\end{equation}\nThen $r(\\mathbf{A})=1$ and both eigenvalues of $\\mathbf{A}$ are zero.\nTheorem 0.19 (Simple Eigenvalue Rank)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix. If $\\lambda$ is a simple eigenvalue of $\\mathbf{A}$, then $r(\\lambda I-\\mathbf{A})=n-1$. Conversely, if $r(\\lambda I-\\mathbf{A})=n-1$, then $\\lambda$ is an eigenvalue of $\\mathbf{A}$, but not necessarily a simple eigenvalue. Proof\nLet $\\lambda_{1}, \\ldots, \\lambda_{n}$ be the eigenvalues of $\\mathbf{A}$. Then $\\mathbf{B}=\\lambda I-\\mathbf{A}$ has eigenvalues $\\lambda-\\lambda_{i}(i=1, \\ldots, n)$, and since $\\lambda$ is a simple eigenvalue of $\\mathbf{A}$, $\\mathbf{B}$ has a simple eigenvalue zero. Hence, $r(\\mathbf{B}) \\leq n-1$. Also, since $\\mathbf{B}$ has $n-1$ nonzero eigenvalues, $r(\\mathbf{B}) \\geq n-1$ (Theorem 0.18). Hence $r(\\mathbf{B})=n-1$. Conversely, if $r(\\mathbf{B})=n-1$, then $\\mathbf{B}$ has at least one zero eigenvalue and hence $\\lambda=\\lambda_{i}$ for at least one $i$. ■ Definition 0.29 (Simple Zero Eigenvalue Corollary)\nAn $n \\times n$ matrix with a simple zero eigenvalue has rank $n-1$. Theorem 0.20 (Symmetric Matrix Rank and Eigenvalues)\nIf $\\mathbf{A}$ is a symmetric matrix with $r$ nonzero eigenvalues, then $r(\\mathbf{A})=r$. Proof\nUsing Theorem 0.13, we have $\\mathbf{S}^\\mathrm{T} \\mathbf{A} \\mathbf{S}=\\boldsymbol{\\Lambda}$ and hence\n\\begin{equation} r(\\mathbf{A})=r\\left(\\mathbf{S} \\boldsymbol{\\Lambda} \\mathbf{S}^\\mathrm{T}\\right)=r(\\boldsymbol{\\Lambda})=r, \\end{equation}\nand the result follows.\n■ Theorem 0.21 (Idempotent Matrix Properties)\nIf $\\mathbf{A}$ is an idempotent matrix, possibly complex, with $r$ eigenvalues equal to one, then $r(\\mathbf{A})=\\operatorname{tr} \\mathbf{A}=r$. Proof\nBy this theorem, $\\mathbf{S}^{*} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$ (upper triangular), where\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) \\end{equation}\nwith $\\mathbf{M}_{1}$ a unit upper triangular $r \\times r$ matrix and $\\mathbf{M}_{3}$ a strictly upper triangular matrix. Since $\\mathbf{A}$ is idempotent, so is $\\mathbf{M}$ and hence\n\\begin{equation} \\left(\\begin{array}{cc} \\mathbf{M}_{1}^{2} \u0026amp; \\mathbf{M}_{1} \\mathbf{M}_{2}+\\mathbf{M}_{2} \\mathbf{M}_{3} \\\\ 0 \u0026amp; \\mathbf{M}_{3}^{2} \\end{array}\\right)=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) . \\end{equation}\nThis implies that $\\mathbf{M}_{1}$ is idempotent; it is nonsingular, hence $\\mathbf{M}_{1}=\\mathbf{I}_{r}$ (see Exercise 1 below). Also, $\\mathbf{M}_{3}$ is idempotent and all its eigenvalues are zero, hence $\\mathbf{M}_{3}=0$ (see Exercise 2 below), so that\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{I}_{r} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; 0 \\end{array}\\right) \\end{equation}\nHence,\n\\begin{equation} r(\\mathbf{A})=r(\\mathbf{M})=r \\end{equation}\nAlso, by:\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\text { sum of eigenvalues of } \\mathbf{A}=r, \\end{equation}\nthus completing the proof.\n■ We note that in, the matrix $\\mathbf{A}$ is not required to be symmetric. If $\\mathbf{A}$ is idempotent and symmetric, then it is positive semidefinite. Since its eigenvalues are only 0 and 1 and its rank equals $r$, it that $\\mathbf{A}$ can be written as\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{P}^{\\mathrm{T}} \\mathbf{P}=\\mathbf{I}_{r} \\end{equation}\nExercises\nThe only nonsingular idempotent matrix is the identity matrix. The only idempotent matrix whose eigenvalues are all zero is the null matrix. If $\\mathbf{A}$ is a positive semidefinite $n \\times n$ matrix with $r(\\mathbf{A})=r$, then there exists an $n \\times r$ matrix $\\mathbf{P}$ such that \\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{P}^{\\mathrm{T}} \\mathbf{P}=\\mathbf{\\Lambda} \\end{equation}\nwhere $\\mathbf{\\Lambda}$ is an $r \\times r$ diagonal matrix containing the positive eigenvalues of $\\mathbf{A}$.\nPositive (semi)definite matrices # Positive (semi)definite matrices were introduced in Section 1.6. We have already seen that $\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}$ and $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}$ are both positive semidefinite and that the eigenvalues of a positive (semi)definite matrix are all positive (nonnegative). We now present some more properties of positive (semi)definite matrices.\nTheorem 0.22 (Determinant inequality for positive definite matrices)\nLet $\\mathbf{A}$ be positive definite and $\\mathbf{B}$ positive semidefinite. Then,\n\\begin{equation} |\\mathbf{A}+\\mathbf{B}| \\geq|\\mathbf{A}| \\end{equation}\nwith equality if and only if $\\mathbf{B}=0$.\nProof\nLet $\\mathbf{\\Lambda}$ be a positive definite diagonal matrix such that\n\\begin{equation} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A} \\mathbf{S}=\\mathbf{\\Lambda}, \\quad \\mathbf{S}^{\\mathrm{T}} \\mathbf{S}=\\mathbf{I} . \\end{equation}\nThen, $\\mathbf{S} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{I}$ and\n\\begin{equation} \\mathbf{A}+\\mathbf{B}=\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2}\\left(\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right) \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}} . \\end{equation}\nHence, using determinant results,\n\\begin{equation} \\begin{aligned} |\\mathbf{A}+\\mathbf{B}| \u0026amp; =\\left|\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2}\\right|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right|\\left|\\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right| \\\\ \u0026amp; =\\left|\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right| \\\\ \u0026amp; =|\\mathbf{A}|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right| \\end{aligned} \\end{equation}\nIf $\\mathbf{B}=0$ then $|\\mathbf{A}+\\mathbf{B}|=|\\mathbf{A}|$. If $\\mathbf{B} \\neq 0$, then the matrix $\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}$ will be positive semidefinite with at least one positive eigenvalue. Hence we have $\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right|\u0026gt;1$ and $|\\mathbf{A}+\\mathbf{B}|\u0026gt;|\\mathbf{A}|$.\n■ Theorem 0.23 (Simultaneous diagonalization)\nLet $\\mathbf{A}$ be positive definite and $\\mathbf{B}$ symmetric of the same order. Then there exist a nonsingular matrix $\\mathbf{P}$ and a diagonal matrix $\\mathbf{\\Lambda}$ such that\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}=\\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}} \\end{equation}\nIf $\\mathbf{B}$ is positive semidefinite, then so is $\\mathbf{\\Lambda}$.\nProof\nLet $\\mathbf{C}=\\mathbf{A}^{-1 / 2} \\mathbf{B} \\mathbf{A}^{-1 / 2}$. Since $\\mathbf{C}$ is symmetric, there exist an orthogonal matrix $\\mathbf{S}$ and a diagonal matrix $\\mathbf{\\Lambda}$ such that\n\\begin{equation} \\mathbf{S}^{\\mathrm{T}} \\mathbf{C} \\mathbf{S}=\\mathbf{\\Lambda}, \\quad \\mathbf{S}^{\\mathrm{T}} \\mathbf{S}=\\mathbf{I} \\end{equation}\nNow define $\\mathbf{P}=\\mathbf{A}^{1 / 2} \\mathbf{S}$. Then,\n\\begin{equation} \\mathbf{P} \\mathbf{P}^{\\mathrm{T}}=\\mathbf{A}^{1 / 2} \\mathbf{S} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{A} \\end{equation}\nand\n\\begin{equation} \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}}=\\mathbf{A}^{1 / 2} \\mathbf{S} \\mathbf{\\Lambda} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{C} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{A}^{-1 / 2} \\mathbf{B} \\mathbf{A}^{-1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{B} . \\end{equation}\nIf $\\mathbf{B}$ is positive semidefinite, then so is $\\mathbf{C}$ and so is $\\mathbf{\\Lambda}$.\n■ For two symmetric matrices $\\mathbf{A}$ and $\\mathbf{B}$, we shall write $\\mathbf{A} \\geq \\mathbf{B}$ (or $\\mathbf{B} \\leq \\mathbf{A}$ ) if $\\mathbf{A}-\\mathbf{B}$ is positive semidefinite, and $\\mathbf{A}\u0026gt;\\mathbf{B}$ (or $\\mathbf{B}\u0026lt;\\mathbf{A}$ ) if $\\mathbf{A}-\\mathbf{B}$ is positive definite.\nTheorem 0.24 (Inverse order for positive definite matrices)\nLet $\\mathbf{A}$ and $\\mathbf{B}$ be positive definite $n \\times n$ matrices. Then $\\mathbf{A}\u0026gt;\\mathbf{B}$ if and only if $\\mathbf{B}^{-1}\u0026gt;\\mathbf{A}^{-1}$. Proof\nBy Theorem 0.23, there exist a nonsingular matrix $\\mathbf{P}$ and a positive definite diagonal matrix $\\mathbf{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n}\\right)$ such that\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}=\\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}} \\end{equation}\nThen,\n\\begin{equation} \\mathbf{A}-\\mathbf{B}=\\mathbf{P}(\\mathbf{I}-\\mathbf{\\Lambda}) \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}^{-1}-\\mathbf{A}^{-1}=\\mathbf{P}^{\\mathrm{T}-1}\\left(\\mathbf{\\Lambda}^{-1}-\\mathbf{I}\\right) \\mathbf{P}^{-1} . \\end{equation}\nIf $\\mathbf{A}-\\mathbf{B}$ is positive definite, then $\\mathbf{I}-\\mathbf{\\Lambda}$ is positive definite and hence $0\u0026lt;\\lambda_{i}\u0026lt;$ $1(i=1, \\ldots, n)$. This implies that $\\mathbf{\\Lambda}^{-1}-\\mathbf{I}$ is positive definite and hence that $\\mathbf{B}^{-1}-\\mathbf{A}^{-1}$ is positive definite.\n■ Theorem 0.25 (Determinant monotonicity)\nLet $\\mathbf{A}$ and $\\mathbf{B}$ be positive definite matrices such that $\\mathbf{A}-\\mathbf{B}$ is positive semidefinite. Then, $|\\mathbf{A}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{A}=\\mathbf{B}$. Proof\nLet $\\mathbf{C}=\\mathbf{A}-\\mathbf{B}$. Then $\\mathbf{A}=\\mathbf{B}+\\mathbf{C}$, where $\\mathbf{B}$ is positive definite and $\\mathbf{C}$ is positive semidefinite. Thus, by Theorem 0.22, $|\\mathbf{B}+\\mathbf{C}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{C}=0$, that is, $|\\mathbf{A}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{A}=\\mathbf{B}$. ■ A useful special case of Theorem 0.25 is this theorem.\nTheorem 0.26 (Identity characterization)\nLet $\\mathbf{A}$ be positive definite with $|\\mathbf{A}|=1$. If $\\mathbf{I}-\\mathbf{A}$ is also positive semidefinite, then $\\mathbf{A}=\\mathbf{I}$. Proof\nThis follows immediately from Theorem 0.25. ■ Three further results for positive definite matrices # Let us now prove this theorem.\nTheorem 0.27 (Block matrix determinant and positive definiteness)\nLet $\\mathbf{A}$ be a positive definite $n \\times n$ matrix, and let $\\mathbf{B}$ be the $(n+1) \\times(n+1)$ matrix\n\\begin{equation} \\mathbf{B}=\\left(\\begin{array}{ll} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{b}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right) \\end{equation}\nThen, (i) $|\\mathbf{B}| \\leq \\alpha|\\mathbf{A}|$ with equality if and only if $\\mathbf{b}=0$; and (ii) $\\mathbf{B}$ is positive definite if and only if $|\\mathbf{B}|\u0026gt;0$.\nProof\nDefine the $(n+1) \\times(n+1)$ matrix\n\\begin{equation} \\mathbf{P}=\\left(\\begin{array}{cc} \\mathbf{I}_{n} \u0026amp; -\\mathbf{A}^{-1} \\mathbf{b} \\\\ \\mathbf{0}^{\\mathrm{T}} \u0026amp; 1 \\end{array}\\right) \\end{equation}\nThen,\n\\begin{equation} \\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}=\\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; 0 \\\\ \\mathbf{0}^{\\mathrm{T}} \u0026amp; \\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{array}\\right) \\end{equation}\nso that\n\\begin{equation} |\\mathbf{B}|=\\left|\\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}\\right|=|\\mathbf{A}|\\left(\\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\\right) . \\label{eq:det_block} \\end{equation}\n(Compare Exercise 2 in Section 1.11.) Then (i) is an immediate consequence of \\eqref{eq:det_block}. To prove (ii) we note that $|\\mathbf{B}|\u0026gt;0$ if and only if $\\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\u0026gt;0$ (from \\eqref{eq:det_block}), which is the case if and only if $\\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}$ is positive definite (from the previous equation). This in turn is true if and only if $\\mathbf{B}$ is positive definite.\n■ An immediate consequence of Theorem 0.27, proved by induction, is the following.\nTheorem 0.28 (Hadamard\u0026#39;s inequality)\nIf $\\mathbf{A}=\\left(a_{ij}\\right)$ is a positive definite $n \\times n$ matrix, then\n\\begin{equation} |\\mathbf{A}| \\leq \\prod_{i=1}^{n} a_{ii} \\end{equation}\nwith equality if and only if $\\mathbf{A}$ is diagonal.\nAnother consequence of Theorem 0.27 is this theorem.\nTheorem 0.29 (Principal minor test)\nA symmetric $n \\times n$ matrix $\\mathbf{A}$ is positive definite if and only if all principal minors $\\left|\\mathbf{A}_{k}\\right|(k=1, \\ldots, n)$ are positive. Note. The $k \\times k$ matrix $\\mathbf{A}_{k}$ is obtained from $\\mathbf{A}$ by deleting the last $n-k$ rows and columns of $\\mathbf{A}$. Notice that $\\mathbf{A}_{n}=\\mathbf{A}$.\nProof\nLet $\\mathbf{E}_{k}=\\left(\\mathbf{I}_{k}: 0\\right)$ be a $k \\times n$ matrix, so that $\\mathbf{A}_{k}=\\mathbf{E}_{k} \\mathbf{A} \\mathbf{E}_{k}^{\\mathrm{T}}$. Let $\\mathbf{y}$ be an arbitrary $k \\times 1$ vector, $\\mathbf{y} \\neq 0$. Then,\n\\begin{equation} \\mathbf{y}^{\\mathrm{T}} \\mathbf{A}_{k} \\mathbf{y}=\\left(\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y}\\right)^{\\mathrm{T}} \\mathbf{A}\\left(\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y}\\right)\u0026gt;0 \\end{equation}\nsince $\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y} \\neq 0$ and $\\mathbf{A}$ is positive definite. Hence, $\\mathbf{A}_{k}$ is positive definite and, in particular, $\\left|\\mathbf{A}_{k}\\right|\u0026gt;0$. The converse follows by repeated application of Theorem 0.27(ii).\n■ Exercises\nIf $\\mathbf{A}$ is positive definite show that the matrix \\begin{equation} \\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{b}^{\\mathrm{T}} \u0026amp; \\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{array}\\right) \\end{equation}\nis positive semidefinite and singular, and find the eigenvector associated with the zero eigenvalue.\nHence show that, for positive definite $\\mathbf{A}$, \\begin{equation} \\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}-2 \\mathbf{b}^{\\mathrm{T}} \\mathbf{x} \\geq-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{equation}\nfor every $\\mathbf{x}$, with equality if and only if $\\mathbf{x}=\\mathbf{A}^{-1} \\mathbf{b}$.\nA useful result # If $\\mathbf{A}$ is a positive definite $n \\times n$ matrix, then, in accordance with Theorem 0.28,\n\\begin{equation} |\\mathbf{A}|=\\prod_{i=1}^{n} a_{ii} \\label{eq:diagonal_det} \\end{equation}\nif and only if $\\mathbf{A}$ is diagonal. If $\\mathbf{A}$ is merely symmetric, then \\eqref{eq:diagonal_det}, while obviously necessary, is no longer sufficient for the diagonality of $\\mathbf{A}$. For example, the matrix\n\\begin{equation} \\mathbf{A}=\\left(\\begin{array}{lll} 2 \u0026amp; 3 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 3 \u0026amp; 2 \\end{array}\\right) \\end{equation}\nhas determinant $|\\mathbf{A}|=8$ (its eigenvalues are $-1,-1$, and 8 ), thus satisfying \\eqref{eq:diagonal_det}, but $\\mathbf{A}$ is not diagonal.\nthis theorem gives a necessary and sufficient condition for the diagonality of a symmetric matrix.\nTheorem 0.30 (Diagonal matrix characterization)\nA symmetric matrix is diagonal if and only if its eigenvalues and its diagonal elements coincide. Proof\nLet $\\mathbf{A}=\\left(a_{ij}\\right)$ be a symmetric $n \\times n$ matrix. The \u0026lsquo;only if\u0026rsquo; part of the theorem is trivial. To prove the \u0026lsquo;if\u0026rsquo; part, assume that $\\lambda_{i}(\\mathbf{A})=a_{ii}, i=1, \\ldots, n$, and consider the matrix\n\\begin{equation} \\mathbf{B}=\\mathbf{A}+k \\mathbf{I}, \\end{equation}\nwhere $k\u0026gt;0$ is such that $\\mathbf{B}$ is positive definite. Then,\n\\begin{equation} \\lambda_{i}(\\mathbf{B})=\\lambda_{i}(\\mathbf{A})+k=a_{ii}+k=b_{ii} \\quad(i=1, \\ldots, n), \\end{equation}\nand hence\n\\begin{equation} |\\mathbf{B}|=\\prod_{1}^{n} \\lambda_{i}(\\mathbf{B})=\\prod_{i=1}^{n} b_{ii} . \\end{equation}\nIt then follows from Theorem 0.28 that $\\mathbf{B}$ is diagonal, and hence that $\\mathbf{A}$ is diagonal.\n■ Symmetric matrix functions # Let $\\mathbf{A}$ be a square matrix of order $n \\times n$. The $\\operatorname{trace} \\operatorname{tr} \\mathbf{A}$ and the determinant $|\\mathbf{A}|$ are examples of scalar functions of $\\mathbf{A}$. We can also consider matrix functions, for example, the inverse $\\mathbf{A}^{-1}$. The general definition of a matrix function is somewhat complicated, but for symmetric matrices it is easy. So, let us assume that $\\mathbf{A}$ is symmetric.\nWe known from that any symmetric $n \\times n$ matrix $\\mathbf{A}$ can be diagonalized, which means that there exists an orthogonal matrix $\\mathbf{S}$ and a diagonal matrix $\\mathbf{\\Lambda}$ (containing the eigenvalues of $\\mathbf{A}$ ) such that $\\mathbf{S}^{\\mathrm{T}} \\mathbf{A} \\mathbf{S}=\\mathbf{\\Lambda}$. Let $\\lambda_{i}$ denote the $i$ th diagonal element of $\\mathbf{\\Lambda}$ and let $\\phi$ be a function so that $\\phi(\\lambda)$ is defined, for example, $\\phi(\\lambda)=\\sqrt{\\lambda}$ or $1 / \\lambda$ or $\\log \\lambda$ or $e^{\\lambda}$.\nWe now define the matrix function $F$ as\n\\begin{equation} F(\\mathbf{\\Lambda})=\\left(\\begin{array}{cccc} \\phi\\left(\\lambda_{1}\\right) \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; \\phi\\left(\\lambda_{2}\\right) \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\phi\\left(\\lambda_{n}\\right) \\end{array}\\right), \\end{equation}\nand then\n\\begin{equation} F(\\mathbf{A})=\\mathbf{S} F(\\mathbf{\\Lambda}) \\mathbf{S}^{\\mathrm{T}} \\end{equation}\nFor example, if $\\mathbf{A}$ is nonsingular then all $\\lambda_{i}$ are nonzero, and letting $\\phi(\\lambda)=$ $1 / \\lambda$, we have\n\\begin{equation} F(\\mathbf{\\Lambda})=\\mathbf{\\Lambda}^{-1}=\\left(\\begin{array}{cccc} 1 / \\lambda_{1} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; 1 / \\lambda_{2} \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 1 / \\lambda_{n} \\end{array}\\right) \\end{equation}\nand hence $\\mathbf{A}^{-1}=\\mathbf{S} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}$. To check, we have\n\\begin{equation} \\mathbf{A} \\mathbf{A}^{-1}=\\mathbf{S} \\mathbf{\\Lambda} \\mathbf{S}^{\\mathrm{T}} \\mathbf{S} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\mathbf{\\Lambda} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{I}_{n} \\end{equation}\nas we should. Similarly, if $\\mathbf{A}$ is positive semidefinite, then $\\mathbf{A}^{1 / 2}=\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}$ and\n$$ \\mathbf{A}^{1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\boldsymbol{\\Lambda} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{A}, $$\nagain as it should be. Also, when $\\mathbf{A}$ is positive definite (hence nonsingular), then $\\mathbf{A}^{-1 / 2}=\\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}}$ and\n$$ \\begin{aligned} \\left(\\mathbf{A}^{1 / 2}\\right)^{-1} \u0026amp; =\\left(\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right)^{-1}=\\mathbf{S}\\left(\\boldsymbol{\\Lambda}^{1 / 2}\\right)^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S}\\left(\\boldsymbol{\\Lambda}^{-1}\\right)^{1 / 2} \\mathbf{S}^{\\mathrm{T}} \\\\ \u0026amp; =\\left(\\mathbf{S} \\boldsymbol{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}\\right)^{1 / 2}=\\left(\\mathbf{A}^{-1}\\right)^{1 / 2} \\end{aligned} $$\nso that this expression is unambiguously defined. Symmetric matrix functions are thus always defined through their eigenvalues. For example, the logarithm or exponential of $\\mathbf{A}$ is not the matrix with elements $\\log a_{i j}$ or $e^{a_{i j}}$, but rather a matrix whose eigenvalues are $\\log \\lambda_{i}$ or $e^{\\lambda_{i}}$ and whose eigenvectors are the same as the eigenvectors of $\\mathbf{A}$. This is similar to the definition of a positive definite matrix, which is not a matrix all whose elements are positive, but rather a matrix all whose eigenvalues are positive.\nMiscellaneous exercises # Exercises\nIf $\\mathbf{A}$ and $\\mathbf{B}$ are square matrices such that $\\mathbf{A}\\mathbf{B}=0, \\mathbf{A} \\neq 0, \\mathbf{B} \\neq 0$, then prove that $|\\mathbf{A}|=|\\mathbf{B}|=0$. If $\\mathbf{x}$ and $\\mathbf{y}$ are vectors of the same order, prove that $\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}=\\operatorname{tr} \\mathbf{y} \\mathbf{x}^{\\mathrm{T}}$. Let $$ \\mathbf{A}=\\left(\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right) $$\nShow that\n$$ |\\mathbf{A}|=\\left|\\mathbf{A}_{11}\\right|\\left|\\mathbf{A}_{22}-\\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}\\right| $$\nif $\\mathbf{A}_{11}$ is nonsingular, and\n$$ |\\mathbf{A}|=\\left|\\mathbf{A}_{22}\\right|\\left|\\mathbf{A}_{11}-\\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21}\\right| $$\nif $\\mathbf{A}_{22}$ is nonsingular. 4. Show that $(\\mathbf{I}-\\mathbf{A}\\mathbf{B})^{-1}=\\mathbf{I}+\\mathbf{A}(\\mathbf{I}-\\mathbf{B}\\mathbf{A})^{-1}\\mathbf{B}$, if the inverses exist. 5. Show that\n$$ (\\alpha \\mathbf{I}-\\mathbf{A})^{-1}-(\\beta \\mathbf{I}-\\mathbf{A})^{-1}=(\\beta-\\alpha)(\\beta \\mathbf{I}-\\mathbf{A})^{-1}(\\alpha \\mathbf{I}-\\mathbf{A})^{-1} . $$\nIf $\\mathbf{A}$ is positive definite, show that $\\mathbf{A}+\\mathbf{A}^{-1}-2 \\mathbf{I}$ is positive semidefinite. For any symmetric matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that $\\mathbf{A}\\mathbf{B}-\\mathbf{B}\\mathbf{A}$ is skewsymmetric. Let $\\mathbf{A}$ and $\\mathbf{B}$ be two $m \\times n$ matrices of rank $r$. If $\\mathbf{A}\\mathbf{A}^{\\mathrm{T}}=\\mathbf{B}\\mathbf{B}^{\\mathrm{T}}$ then $\\mathbf{A}=\\mathbf{B}\\mathbf{Q}$, where $\\mathbf{Q}\\mathbf{Q}^{\\mathrm{T}}$ (and hence $\\mathbf{Q}^{\\mathrm{T}} \\mathbf{Q}$ ) is idempotent of rank $k \\geq r$ (Neudecker and van de Velden 2000). Let $\\mathbf{A}$ be an $m \\times n$ matrix partitioned as $\\mathbf{A}=\\left(\\mathbf{A}_{1}: \\mathbf{A}_{2}\\right)$ and satisfying $\\mathbf{A}_{1}^{\\mathrm{T}} \\mathbf{A}_{2}=0$ and $r\\left(\\mathbf{A}_{1}\\right)+r\\left(\\mathbf{A}_{2}\\right)=m$. Then, for any positive semidefinite matrix $\\mathbf{V}$, we have $$ r(\\mathbf{V})=r\\left(\\mathbf{A}_{1}\\right)+r\\left(\\mathbf{A}_{2}^{\\mathrm{T}} \\mathbf{V} \\mathbf{A}_{2}\\right) \\Longleftrightarrow r(\\mathbf{V})=r\\left(\\mathbf{V}: \\mathbf{A}_{1}\\right) $$\nProve that the eigenvalues $\\lambda_{i}$ of $(\\mathbf{A}+\\mathbf{B})^{-1} \\mathbf{A}$, where $\\mathbf{A}$ is positive semidefinite and $\\mathbf{B}$ is positive definite, satisfy $0 \\leq \\lambda_{i}\u0026lt;1$. Let $\\mathbf{x}$ and $\\mathbf{y}$ be $n \\times 1$ vectors. Prove that $\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}$ has $n-1$ zero eigenvalues and one eigenvalue $\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. Show that $\\left|\\mathbf{I}+\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}\\right|=1+\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. Let $\\mu=1+\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. If $\\mu \\neq 0$, show that $\\left(\\mathbf{I}+\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}\\right)^{-1}=\\mathbf{I}-(1 / \\mu) \\mathbf{x} \\mathbf{y}^{\\mathrm{T}}$. Show that $\\left(\\mathbf{I}+\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right)^{-1} \\mathbf{A}=\\mathbf{A}\\left(\\mathbf{I}+\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{-1}$. Show that $\\mathbf{A}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{1 / 2}=\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right)^{1 / 2} \\mathbf{A}$. (Monotonicity of the entropic complexity.) Let $\\mathbf{A}_{n}$ be a positive definite $n \\times n$ matrix and define $$ \\phi(n)=\\frac{n}{2} \\log \\operatorname{tr}\\left(\\mathbf{A}_{n} / n\\right)-\\frac{1}{2} \\log \\left|\\mathbf{A}_{n}\\right| . $$\nLet $\\mathbf{A}_{n+1}$ be a positive definite $(n+1) \\times(n+1)$ matrix such that\n$$ \\mathbf{A}_{n+1}=\\left(\\begin{array}{cc} \\mathbf{A}_{n} \u0026amp; \\mathbf{a}_{n} \\\\ \\mathbf{a}_{n}^{\\mathrm{T}} \u0026amp; \\alpha_{n} \\end{array}\\right) $$\nThen,\n$$ \\phi(n+1) \\geq \\phi(n) $$\nwith equality if and only if\n$$ \\mathbf{a}_{n}=0, \\quad \\alpha_{n}=\\operatorname{tr} \\mathbf{A}_{n} / n $$\n"},{"id":5,"href":"/numerical_optimization/docs/lectures/advanced/proximal_methods/","title":"2. Proximal methods","section":"II - Advanced problems","content":" Proximal methods # Soon to be added.\n"},{"id":6,"href":"/numerical_optimization/docs/lectures/machine_learning/svm/","title":"2. Support Vector Machine","section":"III - Machine Learning problems","content":" Support Vector Machine # Soon to be added.\n"},{"id":7,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/","title":"2. Unconstrained optimization : basics","section":"I - Fundamentals","content":" Unconstrained optimization - basics # We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve: $$ \\underset{\\mathbf{x}\\in\\mathbb{R}^d}{\\operatorname{argmin}} f(\\mathbf{x}). $$\nLet us try to characterizes the nature of the solutions under this setup.\nWhat is a solution ? # Figure 2.1: Local and global minimum can coexist.\nGenerally, we would be happiest if we found a global minimizer of $f$ , a point where the function attains its least value. A formal definition is :\nDefinition 2.1 (Global minimizer)\nA point $\\mathbf{x}^\\star$ is a global minimizer if $f(\\mathbf{x}^\\star)\\leq f(\\mathbf{x})$, where $\\mathbf{x}$ ranges over all of $\\mathbb{R}^d$ (or at least over the domain of interest to the modeler). The global minimizer can be difficult to find, because our knowledge of $f$ is usually only local. Since our algorithm does not visit many points (we hope!), we usually do not have a good picture of the overall shape of $f$ , and we can never be sure that the function does not take a sharp dip in some region that has not been sampled by the algorithm. Most algorithms are able to find only a local minimizer, which is a point that achieves the smallest value of f in its neighborhood. Formally, we say:\nDefinition 2.2 (Local minimizer)\nA point $\\mathbf{x}^\\star$ is a local minimizer if $\\exists r\u0026gt;0,\\, f(\\mathbf{x}^\\star)\\leq f(\\mathbf{x})$, $\\forall \\mathbf{x}\\in\\mathcal{B}(\\mathbf{x}^\\star, r)$. A point that satisfies this definition is sometimes called a weak local minimizer. Alternatively, when $f(\\mathbf{x}^\\star)\u0026lt;f(\\mathbf{x})$, we say that the minimum is a strict local minimizer.\nTaylor\u0026rsquo;s theorem # From the definitions given above, it might seem that the only way to find out whether a point $\\mathbf{x}^\\star$ is a local minimum is to examine all the points in its immediate vicinity, to make sure that none of them has a smaller function value. When the function $f$ is smooth, however, there are much more efficient and practical ways to identify local minima. In particular, if $f$ is twice continuously differentiable, we may be able to tell that $\\mathbf{x}^\\star$ is a local minimizer (and possibly a strict local minimizer) by examining just the gradient $\\nabla f (\\mathbf{x}^\\star)$ and the Hessian $\\nabla^2 f (\\mathbf{x}^\\star)$. The mathematical tool used to study minimizers of smooth functions is Taylor’s the- orem. Because this theorem is central to our analysis we state it now.\nTheorem 2.1 (Taylor\u0026#39;s theorem)\nSuppose that $f:\\mathbb{R}^d\\mapsto\\mathbb{R}$ is continuously differentiable and that we have $\\mathbf{p}\\in\\mathbb{R}^d $. The we have : \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x}+t\\mathbf{p})^\\mathrm{T}\\mathbf{p}, \\end{equation} for some $t\\in [0,1]$.\nMoreover, if $f$ is twice continuously differentiable, we have : \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\mathrm{T}\\nabla^2 f(\\mathbf{x}+t\\mathbf{p})\\mathbf{p}), \\end{equation} for some $t\\in [0,1]$.\nProof\nSee any calculus book ■ Note that in this formulation, the definition is exact and the $t$ scalar is usually unknown. The interest lies in skeching proofs. In practical matters, we rather use the following approximation: Theorem 2.2 (Taylor\u0026#39;s approximation)\nFirst order approximation: \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + o(\\lVert\\mathbf{p}\\rVert), \\end{equation}\nSecond-order approximation:\n\\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\mathrm{T}\\nabla^2 f(\\mathbf{x})\\mathbf{p} + o(\\lVert\\mathbf{p}\\rVert^2), \\end{equation}\nwhere $o(\\lVert\\mathbf{p}\\rVert)$ and $o(\\lVert\\mathbf{p}\\rVert^2)$ represent terms that grow slower than $\\lVert\\mathbf{p}\\rVert$ and $\\lVert\\mathbf{p}\\rVert^2$ respectively as $\\lVert\\mathbf{p}\\rVert \\to 0$.\nSufficient and necessary conditions for local minima # Let us consider a local minimum and see how they can be characterized to later design appropriate solution finding methods. The first well-known result is as follows: Theorem 2.3 (First-order necessary conditions)\nif $\\mathbf{x}^\\star$ is a local minimize, and $f$ is continuously differentiable in a neighborhood of $\\mathbf{x}^\\star$, then $\\nabla f(\\mathbf{x}^\\star) = \\mathbf{0}$. Proof\nSuppose for contradiction that $\\nabla f(\\mathbf{x}^\\star) \\neq 0$, and define vector $\\mathbf{p}=-\\nabla f(\\mathbf{x}^\\star)$ such that by construction $\\mathbf{p}^\\mathrm{T}\\nabla f(\\mathbf{x}^\\star) = - \\lVert f(\\mathbf{x}^\\star) \\rVert^2 \u0026lt; 0$.\nSince $f$ is a continuous function, we can define a scalar $T\u0026gt;0$ such that $\\forall t\\in [0,T[$, we still have: $$ \\mathbf{p}^\\mathrm{T}f(\\mathbf{x}+t\\mathbf{p}) \u0026lt; 0. $$\nUsing Theorem 2.1 first-order result, we can write: $$ f(\\mathbf{x}^\\star+t\\mathbf{p}) = f(\\mathbf{x}^\\star) + t\\mathbf{p}^\\mathrm{T}\\nabla f(\\mathbf{x}^\\star+\\overline{t}\\mathbf{p}), $$ for some $\\overline{t}\\in[0,T[$ and any $t\\in[0,T[$. Given previous inequality, we obtain: $$ f(\\mathbf{x}^\\star+t\\mathbf{p}) \u0026lt; f(\\mathbf{x}^\\star), $$ which contradicts the fact that $\\mathbf{x}^\\star$ is a local minimizer.\n■ Henceforth, we will call stationary point, any $\\mathbf{x}$ such that $\\nabla f(\\mathbf{x}) = 0$.\nFor the next result we recall that a matrix $\\mathbf{B}$ is positive definite if $\\mathbf{p}^\\mathrm{T} \\mathbf{B} \\mathbf{p}\u0026gt;0$ for all $\\mathbf{p} \\neq \\mathbf{0}$, and positive semidefinite if $\\mathbf{p}^\\mathrm{T} \\mathbf{B} \\mathbf{p} \\geq 0$ for all $\\mathbf{p}$.\nTheorem 2.4 (Second-order necessary conditions)\nIf $\\mathbf{x}^\\star$ is a local minimizer of $f$ and $\\nabla^2 f$ is continuous in an open neighborhood of $\\mathbf{x}^\\star$, then $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$ and $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is positive semidefinite. Proof\nProof. We know from Theorem 2.3 that $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$. For contradiction, assume that $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is not positive semidefinite. Then we can choose a vector $\\mathbf{p}$ such that $\\mathbf{p}^T \\nabla^2 f\\left(\\mathbf{x}^\\star\\right) \\mathbf{p}\u0026lt;0$, and because $\\nabla^2 f$ is continuous near $\\mathbf{x}^\\star$, there is a scalar $T\u0026gt;0$ such that $\\mathbf{p}^\\mathrm{T} \\nabla^2 f\\left(\\mathbf{x}^*+\\overline{t} \\mathbf{p}\\right) \\mathbf{p}\u0026lt;0$ for all $t \\in[0, T[$.\nBy doing a Taylor series expansion around $\\mathbf{x}^\\star$, we have for all $\\bar{t} \\in[0, T[$ and some $t \\in[0, \\bar{t}]$ that\n$$ f\\left(\\mathbf{x}^\\star+\\bar{t} \\mathbf{p}\\right) = f\\left(\\mathbf{x}^\\star\\right)+\\bar{t} \\mathbf{p}^\\mathrm{T} \\nabla f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\bar{t}^2 \\mathbf{p}^\\mathrm{T} \\nabla^2 f\\left(\\mathbf{x}^\\star+t \\mathbf{p}\\right) \\mathbf{p}\u0026lt;f\\left(\\mathbf{x}^\\star\\right) . $$\nAs in Theorem 2.3, we have found a direction from $\\mathbf{x}^\\star$ along which $f$ is decreasing, and so again, $\\mathbf{x}^\\star$ is not a local minimizer.\n■ We now describe sufficient conditions, which are conditions on the derivatives of $f$ at the point $\\mathbf{z}^\\star$ that guarantee that $\\mathbf{x}^\\star$ is a local minimizer.\nTheorem 2.5 (Second-Order Sufficient Conditions)\nSuppose that $\\nabla^2 f$ is continuous in an open neighborhood of $\\mathbf{x}^\\star$ and that $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$ and $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is positive definite. Then $\\mathbf{x}^\\star$ is a strict local minimizer of $f$. Proof\nBecause the Hessian is continuous and positive definite at $\\mathbf{x}^\\star$, we can choose a radius $r\u0026gt;0$ so that $\\nabla^2 f(x)$ remains positive definite for all $x$ in the open ball $\\mathcal{D}=\\left\\{\\mathbf{z} \\mid\\left\\lVert\\mathbf{z}-\\mathbf{x}^\\star\\right\\rVert\u0026lt;\\right.$ $r\\}$. Taking any nonzero vector $p$ with $\\lVert\\mathbf{p}\\rVert\u0026lt;r$, we have $\\mathbf{x}^\\star+\\mathbf{p} \\in \\mathcal{D}$ and so\n$$ \\begin{aligned} f\\left(\\mathbf{x}^\\star+p\\right) \u0026amp; =f\\left(\\mathbf{x}^\\star\\right)+\\mathbf{p}^\\mathrm{T} \\nabla f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\mathbf{p}^\\mathrm{T} \\nabla^2 f(\\mathbf{z}) \\mathbf{p} \\ \u0026amp; =f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\mathbf{p}^\\mathrm{T} \\nabla^2 f(\\mathbf{z}) \\mathbf{p} \\end{aligned} $$\nwhere $\\mathbf{z}=\\mathbf{x}^\\star+t \\mathbf{p}$ for some $t \\in(0,1)$. Since $\\mathbf{z} \\in \\mathcal{D}$, we have $\\mathbf{p}^{\\mathrm{T}} \\nabla^2 f(\\mathbf{z}) \\mathbf{p}\u0026gt;0$, and therefore $f\\left(\\mathbf{x}^\\star+\\mathbf{p}\\right)\u0026gt;f\\left(\\mathbf{x}^\\star\\right)$, giving the result.\n■ Note that the second-order sufficient conditions of Theorem 2.5 guarantee something stronger than the necessary conditions discussed earlier; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: A point $\\mathbf{x}^\\star$ may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function $f(x)=x^4$, for which the point $x^\\star=0$ is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite). These results, which are based on elementary calculus, provide the foundations for unconstrained optimization algorithms. In one way or another, all algorithms seek a point where $\\nabla f(\\cdot)$ vanishes.\nThe need for algorithms # On question we might ask here is why do we need algorithms to find local minima? After all, we have just shown that if $\\nabla f(\\mathbf{x}^\\star)=0$, then $\\mathbf{x}^\\star$ is a local minimizer. The answer is that in practice, we do not always have the luxury to know the exact solution to $\\nabla f(\\mathbf{x})=0$. Moreover, we can\u0026rsquo;t always compute the Hessian matrix to check the second-order conditions.\nThus, to circumvent the need to solve analytically the equations $\\nabla f(\\mathbf{x})=0$, we will design algorithms that iteratively update a point $\\mathbf{x}$ until it converges to a local minimizer. The algorithms will be based on the properties of the gradient and Hessian, and will use the information they provide to guide the search for a local minimum. When hessian is not computable or too expensive to compute, we will use the gradient only, and the algorithms will be called gradient-based methods. When the Hessian is available, we will use it to accelerate convergence, and the algorithms will be called Newton methods. As a between between these methods lie quasi-Newton methods, which use an approximation of the Hessian to guide the search for a local minimum.\nBut before we dive in more complicated algorithms, let us consider the most obvious approaches and try to understand their limitations.\nSteepest-descent approach # The first algorithm we consider is the so-called Steepest-descent algorithm which involves choosing an initial point $\\mathbf{x}_0$ and compute a series of subsequent points with the following formula:\n\\begin{equation} \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k), \\label{eq: steepest descent} \\end{equation}\nwhere $\\alpha_k$ are a series of scalar values called step-size (or learning rate in machine learning context).\nThe intuition behind this approach is beautifully simple yet profound. Imagine yourself standing on a mountainside in thick fog, trying to find the bottom of the valley. Since you can\u0026rsquo;t see the overall landscape, the most sensible strategy is to feel the slope beneath your feet and take a step in the direction that descends most steeply. This is precisely what the steepest-descent algorithm does mathematically.\nThe negative gradient $-\\nabla f(\\mathbf{x}_k)$ points in the direction of steepest decrease of the function at point $\\mathbf{x}_k$. This isn\u0026rsquo;t just a convenient mathematical fact—it\u0026rsquo;s the fundamental geometric property that makes gradient-based optimization possible. By moving in this direction, we ensure that we\u0026rsquo;re making the most aggressive local progress toward reducing the function value.\nFigure 2.2: Optimization with steepest descent\nUnderstanding the algorithm step by step # Let\u0026rsquo;s walk through what happens in each iteration of steepest descent. Starting from point $\\mathbf{x}_k$, we compute the gradient $\\nabla f(\\mathbf{x}_k)$. This vector tells us which direction the function increases most rapidly. Since we want to minimize, we go in the opposite direction: $-\\nabla f(\\mathbf{x}_k)$.\nThe step size $\\alpha_k$ determines how far we travel in this direction. Think of it as the length of your stride as you walk down the mountain. The choice of step size involves a fundamental trade-off: too small and you make painfully slow progress; too large and you might overshoot the valley bottom or even start climbing uphill again.\nWhy steepest descent can struggle # Figure 2.3: Problem of stepsize\nHere\u0026rsquo;s where the method reveals its first major limitation. Consider a function that looks like a long, narrow valley—mathematically, this corresponds to a function with a large condition number. Steepest descent exhibits what we call \u0026ldquo;zigzag behavior\u0026rdquo; in such cases as illustrated in Figure 2.3 .\nPicture this scenario: you\u0026rsquo;re in a narrow canyon, and the steepest direction points toward one wall rather than down the canyon. You take steps toward that wall, then the gradient changes direction and points toward the other wall. Instead of walking efficiently down the canyon, you find yourself bouncing back and forth between the walls, making very slow progress toward your destination.\nThis zigzag pattern occurs because steepest descent is fundamentally myopic. At each step, it only considers the immediate local slope and ignores the broader geometric structure of the function. The algorithm doesn\u0026rsquo;t \u0026ldquo;remember\u0026rdquo; where it came from or \u0026ldquo;anticipate\u0026rdquo; where the function is heading.\nConvergence properties # Despite these limitations, steepest descent does have reliable convergence properties. Under reasonable mathematical conditions—essentially requiring that the function is well-behaved and doesn\u0026rsquo;t have any pathological features—the algorithm will eventually reach a stationary point where the gradient vanishes.\nThe convergence is what we call \u0026ldquo;linear,\u0026rdquo; meaning that the error decreases by a constant factor at each iteration. While this sounds reasonable, it can be frustratingly slow in practice, especially for poorly conditioned problems where that constant factor is very close to one.\nNewton method # If steepest descent is like navigating with only your immediate sense of slope, Newton\u0026rsquo;s method is like having a detailed topographic map of your local surroundings. This method incorporates not just information about which way is downhill, but also how the slope itself is changing—what we call the curvature of the function.\nThe mathematical foundation # Newton\u0026rsquo;s method emerges from a clever idea: instead of trying to minimize the original function directly, let\u0026rsquo;s create a simpler approximation and minimize that instead. We use the second-order Taylor approximation around our current point $\\mathbf{x}_k$:\n$$f(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^T\\nabla^2 f(\\mathbf{x}_k)\\mathbf{p}$$\nThis quadratic approximation captures both the slope (first-order term) and the curvature (second-order term) at our current location. The brilliant insight is that quadratic functions are easy to minimize—we simply set the gradient of the approximation equal to zero and solve for the optimal step $\\mathbf{p}$.\nTaking the gradient of our quadratic model and setting it to zero gives us: $$\\nabla f(\\mathbf{x}_k) + \\nabla^2 f(\\mathbf{x}_k)\\mathbf{p} = \\mathbf{0}$$\nSolving for the Newton step: $$\\mathbf{p}_k = -[\\nabla^2 f(\\mathbf{x}_k)]^{-1}\\nabla f(\\mathbf{x}_k)$$\nThe complete Newton iteration becomes: $$\\mathbf{x}_{k+1} = \\mathbf{x}_k - [\\nabla^2 f(\\mathbf{x}_k)]^{-1}\\nabla f(\\mathbf{x}_k)$$\nFigure 2.4: Newton optimization step\nThe geometric insight # What makes Newton\u0026rsquo;s method so powerful becomes clear when we think geometrically. The Hessian matrix $\\nabla^2 f(\\mathbf{x}_k)$ encodes information about how the gradient changes in different directions. If the function curves sharply in one direction and gently in another, the Hessian \u0026ldquo;knows\u0026rdquo; this and adjusts the step accordingly.\nConsider our narrow valley example again. While steepest descent keeps pointing toward the valley walls, Newton\u0026rsquo;s method recognizes the elongated shape of the valley and naturally takes larger steps along the valley floor and smaller steps perpendicular to it. This geometric awareness eliminates the zigzag behavior that plagues steepest descent.\nFor quadratic functions, this geometric understanding leads to a remarkable property: Newton\u0026rsquo;s method finds the exact minimum in a single step, regardless of how poorly conditioned the function might be. This happens because our second-order approximation is exact for quadratic functions.\nThe power of quadratic convergence # Near a solution that satisfies our second-order sufficient conditions, Newton\u0026rsquo;s method exhibits quadratic convergence. This technical term describes an almost magical property: the number of correct digits in your answer roughly doubles with each iteration.\nTo appreciate this, consider what linear convergence means: if you have one correct digit, you need about three more iterations to get two correct digits. But with quadratic convergence, if you have one correct digit, the next iteration gives you two, then four, then eight. The improvement accelerates dramatically as you approach the solution.\nThis rapid convergence makes Newton\u0026rsquo;s method incredibly efficient for high-precision optimization, which is why it forms the backbone of many sophisticated algorithms.\nThe computational cost # Newton\u0026rsquo;s method\u0026rsquo;s power comes with a price. At each iteration, we must compute the Hessian matrix, which requires evaluating all second partial derivatives of our function. For a function of $d$ variables, this means computing and storing $d(d+1)/2$ distinct second derivatives.\nEven more expensive is solving the linear system $\\nabla^2 f(\\mathbf{x}_k)\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ at each iteration. For general matrices, this requires roughly $d^3/3$ arithmetic operations, which becomes prohibitive as the dimension grows.\nWhen Newton\u0026rsquo;s method can fail # Pure Newton\u0026rsquo;s method isn\u0026rsquo;t foolproof. The Hessian matrix might not be positive definite away from the minimum, which means our quadratic model might not have a minimum—it could have a maximum or a saddle point instead. In such cases, the Newton step might point in completely the wrong direction.\nAdditionally, if we start too far from a minimum, the quadratic approximation might be a poor representation of the true function, leading to steps that actually increase the function value.\nLooking ahead: the bridge between methods # The contrasting strengths and weaknesses of steepest descent and Newton\u0026rsquo;s method naturally lead to interesting questions. Can we capture some of Newton\u0026rsquo;s geometric insight without the full computational burden? Can we ensure the global reliability of gradient methods while achieving faster local convergence?\nThese questions motivate more sophisticated approaches. Quasi-Newton methods, which we\u0026rsquo;ll explore in later chapters, build approximations to the Hessian using only gradient information. Methods like BFGS achieve superlinear convergence—faster than linear but not quite quadratic—while requiring much less computation than full Newton steps.\nSimilarly, trust region methods and linesearch strategies, which we\u0026rsquo;ll study later, provide systematic ways to ensure that our algorithms make reliable progress even when our local approximations aren\u0026rsquo;t perfect.\n"},{"id":8,"href":"/numerical_optimization/docs/lectures/reminders/differentiation/","title":"Differentiation","section":"Reminders","content":" Differentiation in Multiple Dimensions # 1 - Introduction # Differentiation provides the mathematical framework for understanding how functions change locally. While single-variable calculus introduces derivatives, most applications require working with functions of multiple variables. This chapter extends differentiation concepts to multivariate and matrix-valued functions, building the tools needed for optimization and analysis in higher dimensions.\n2 - Monovariate Reminders # Derivative of a Function # Definition 0.1 (Derivative)\nThe derivative of a function $f:\\mathbb{R}\\to\\mathbb{R}$ at a point $x_0$ is defined as: $$f\u0026rsquo;(x_0) = \\lim_{h\\to 0} \\frac{f(x_0 + h) - f(x_0)}{h}$$ provided this limit exists. The derivative represents the instantaneous rate of change of the function at a specific point. Geometrically, $f\u0026rsquo;(x_0)$ gives the slope of the tangent line to the curve $y = f(x)$ at the point $(x_0, f(x_0))$. This tangent line provides the best linear approximation to the function near $x_0$.\nFor practical computation, we use two fundamental rules:\nProduct rule: $(uv)\u0026rsquo; = u\u0026rsquo;v + uv'$ Chain rule: $(f(g(x)))\u0026rsquo; = f\u0026rsquo;(g(x))g\u0026rsquo;(x)$ These rules allow us to differentiate complex expressions by breaking them down into simpler components.\n3 - Extension to Multivariate Setup: $f:\\mathbb{R}^d \\to \\mathbb{R}$ # Limits and Continuity # Definition 0.2 (Open Disk)\nAn open disk of radius $\\epsilon \u0026gt; 0$ centered at a point $\\mathbf{x}_0 \\in \\mathbb{R}^d$ is defined as: $$\\mathcal{B}(\\mathbf{x}_0, \\epsilon) = {\\mathbf{x} \\in \\mathbb{R}^d : |\\mathbf{x} - \\mathbf{x}_0|_2 \u0026lt; \\epsilon}$$ Definition 0.3 (Limit)\nThe limit of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ at a point $\\mathbf{x}_0$ is defined as: $$\\lim_{\\mathbf{x} \\to \\mathbf{x}_0} f(\\mathbf{x}) = L$$ if for every $\\epsilon \u0026gt; 0$, there exists $\\delta \u0026gt; 0$ such that whenever $|\\mathbf{x} - \\mathbf{x}_0|_2 \u0026lt; \\delta$, we have $|f(\\mathbf{x}) - L| \u0026lt; \\epsilon$. A function is continuous at a point $\\mathbf{x}_0$ if $\\lim_{\\mathbf{x} \\to \\mathbf{x}_0} f(\\mathbf{x}) = f(\\mathbf{x}_0)$. These definitions generalize the single-variable concepts using the Euclidean norm to measure distances in $\\mathbb{R}^d$.\nDirectional Derivative # Definition 0.4 (Directional Derivative)\nThe directional derivative of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ at a point $\\mathbf{x}_0$ in the direction of a vector $\\mathbf{v} \\in \\mathbb{R}^d$ is defined as: $$Df(\\mathbf{x}_0)[\\mathbf{v}] = \\lim_{h \\to 0} \\frac{f(\\mathbf{x}_0 + h\\mathbf{v}) - f(\\mathbf{x}_0)}{h}$$ When $|\\mathbf{v}|_2 = 1$, the directional derivative $Df(\\mathbf{x}_0)[\\mathbf{v}]$ represents the rate of change of $f$ in the direction of $\\mathbf{v}$ at the point $\\mathbf{x}_0$. This generalizes the concept of derivative to any direction in the input space.\nWe also use the notation $\\nabla_{\\mathbf{v}}f(\\mathbf{x}_0)$ for the directional derivative.\nGradient # Definition 0.5 (Gradient)\nThe gradient of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ at a point $\\mathbf{x}_0$ is defined as the vector of all directional derivatives in the standard basis directions: $$\\nabla f(\\mathbf{x}_0) = \\left( Df(\\mathbf{x}_0)[\\mathbf{e}_1], Df(\\mathbf{x}_0)[\\mathbf{e}_2], \\ldots, Df(\\mathbf{x}_0)[\\mathbf{e}_d] \\right)^\\mathrm{T}$$ where ${\\mathbf{e}_1, \\ldots, \\mathbf{e}_d}$ is the standard basis of $\\mathbb{R}^d$. The gradient points in the direction of steepest ascent of the function $f$ at the point $\\mathbf{x}_0$. It encodes all the first-order information about how the function changes locally.\nFor any vector $\\mathbf{v} \\in \\mathbb{R}^d$, the directional derivative can be expressed as: $$Df(\\mathbf{x}_0)[\\mathbf{v}] = \\nabla f(\\mathbf{x}_0)^\\mathrm{T} \\mathbf{v}$$\nThis shows that the gradient contains all the information needed to compute directional derivatives in any direction.\nGradient and Partial Derivatives # Definition 0.6 (Partial Derivative)\nThe partial derivative of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ with respect to the $i$-th variable is defined as: $$\\frac{\\partial f}{\\partial x_i}(\\mathbf{x}_0) = \\lim_{h \\to 0} \\frac{f(\\mathbf{x}_0 + h\\mathbf{e}_i) - f(\\mathbf{x}_0)}{h}$$ where $\\mathbf{e}_i$ is the $i$-th standard basis vector. The gradient can be expressed in terms of partial derivatives as: $$\\nabla f(\\mathbf{x}_0) = \\left( \\frac{\\partial f}{\\partial x_1}(\\mathbf{x}_0), \\frac{\\partial f}{\\partial x_2}(\\mathbf{x}_0), \\ldots, \\frac{\\partial f}{\\partial x_d}(\\mathbf{x}_0) \\right)^\\mathrm{T}$$\nThis representation makes it clear that the gradient is a vector containing all the partial derivatives of the function at the point $\\mathbf{x}_0$.\nGradient Properties and Practical Computation # When computing gradients in practice, we use the following rules:\nTheorem 0.1 (Product Rule for Gradients)\nLet $g:\\mathbb{R}^d \\to \\mathbb{R}$ and $h:\\mathbb{R}^d \\to \\mathbb{R}$ be two functions. Then the gradient of their product $f(\\mathbf{x}) = g(\\mathbf{x})h(\\mathbf{x})$ is: $$\\nabla f(\\mathbf{x}) = g(\\mathbf{x})\\nabla h(\\mathbf{x}) + h(\\mathbf{x})\\nabla g(\\mathbf{x})$$ Theorem 0.2 (Chain Rule for Gradients)\nFor composition of functions, we have two main cases:\nIf $f=h\\circ g$ where $h:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}^d\\to\\mathbb{R}$, then: $$\\nabla f(\\mathbf{x}) = h\u0026rsquo;(g(\\mathbf{x}))\\nabla g(\\mathbf{x})$$ where $h\u0026rsquo;$ is the derivative of $h$. If $f=h\\circ g$ where $h:\\mathbb{R}^d\\to\\mathbb{R}$ and $g:\\mathbb{R}^{d\u0026rsquo;}\\to\\mathbb{R}^d$, we need the more general chain rule discussed later. Hessian Matrix # Definition 0.7 (Hessian Matrix)\nThe Hessian matrix of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ at a point $\\mathbf{x}_0$ is defined as the square matrix of second-order partial derivatives: $$\\mathbf{H}(\\mathbf{x}_0) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}(\\mathbf{x}_0) \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}(\\mathbf{x}_0) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_d}(\\mathbf{x}_0) \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}(\\mathbf{x}_0) \u0026amp; \\frac{\\partial^2 f}{\\partial x_2^2}(\\mathbf{x}_0) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_d}(\\mathbf{x}_0) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_d \\partial x_1}(\\mathbf{x}_0) \u0026amp; \\frac{\\partial^2 f}{\\partial x_d \\partial x_2}(\\mathbf{x}_0) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_d^2}(\\mathbf{x}_0) \\end{pmatrix}$$ The Hessian matrix captures the second-order behavior of the function, providing information about its curvature at the point $\\mathbf{x}_0$.\nExercise 1: Compute the gradient and Hessian matrix of the function $f(x,y) = x^2 + 3xy + y^2$ at the point $(1,2)$.\nExercise 2: Using the chain rule, compute the gradient of $f(\\mathbf{x}) = \\left(\\sum_{i=1}^{d}x_i^2\\right)^{1/2}$.\nHessian Matrix Properties # The Hessian matrix has several important properties:\nSymmetry: If $f$ is twice continuously differentiable, then $\\mathbf{H}(\\mathbf{x}_0) = \\mathbf{H}(\\mathbf{x}_0)^\\mathrm{T}$ because mixed partial derivatives are equal: $\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}$.\nCurvature information: The eigenvalues of the Hessian determine the local curvature:\nAll eigenvalues positive: $f$ is locally convex at $\\mathbf{x}_0$ All eigenvalues negative: $f$ is locally concave at $\\mathbf{x}_0$ Mixed positive and negative eigenvalues: $f$ has a saddle point at $\\mathbf{x}_0$ Exercise 3 (Rosenbrock function): The Rosenbrock function is defined as: $$f(x,y) = (a - x)^2 + b(y - x^2)^2$$ where $a$ and $b$ are constants (commonly $a=1$ and $b=100$).\nCompute the gradient $\\nabla f(x,y)$ and find stationary points. Compute the Hessian matrix $\\mathbf{H}(x,y)$ and analyze local curvature at the stationary points. 4 - Multivariate Case: $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ # Multivariate Functions # Definition 0.8 (Vector-Valued Function)\nA vector-valued function $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ maps a vector $\\mathbf{x} \\in \\mathbb{R}^d$ to a vector $\\mathbf{y} \\in \\mathbb{R}^p$. We can write: $$f(\\mathbf{x}) = \\begin{pmatrix} f_1(\\mathbf{x}) \\\\ f_2(\\mathbf{x}) \\\\ \\vdots \\\\ f_p(\\mathbf{x}) \\end{pmatrix}$$ where each component $f_i:\\mathbb{R}^d \\to \\mathbb{R}$ is a scalar function. Gradient and Jacobian # For scalar-valued functions, we defined the gradient. For vector-valued functions, we need the Jacobian matrix.\nDefinition 0.9 (Jacobian Matrix)\nThe Jacobian matrix of a function $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ is defined as: $$\\mathbf{J}_f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\frac{\\partial f_1}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_1}{\\partial x_d} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026amp; \\frac{\\partial f_2}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_2}{\\partial x_d} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_p}{\\partial x_1} \u0026amp; \\frac{\\partial f_p}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_p}{\\partial x_d} \\end{pmatrix} \\in \\mathbb{R}^{p \\times d}$$ The Jacobian matrix generalizes the gradient to vector-valued functions. Each row is the gradient of one component function.\nJacobian and Directional Derivative # The directional derivative of a vector-valued function $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ in the direction of a vector $\\mathbf{v} \\in \\mathbb{R}^d$ is: $$Df(\\mathbf{x})[\\mathbf{v}] = \\mathbf{J}_f(\\mathbf{x})\\mathbf{v} = \\begin{pmatrix} \\nabla f_1(\\mathbf{x})^T \\mathbf{v} \\\\ \\nabla f_2(\\mathbf{x})^T \\mathbf{v} \\\\ \\vdots \\\\ \\nabla f_p(\\mathbf{x})^T \\mathbf{v} \\end{pmatrix} \\in \\mathbb{R}^p$$\nThis shows how the Jacobian matrix encodes all directional derivative information.\nChain Rule for Composition of Functions # Theorem 0.3 (General Chain Rule)\nIf $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ and $g:\\mathbb{R}^m \\to \\mathbb{R}^d$, then the composition $h = f \\circ g : \\mathbb{R}^m \\to \\mathbb{R}^p$ is defined as: $$h(\\mathbf{y}) = f(g(\\mathbf{y}))$$ The Jacobian of $h$ can be computed using the chain rule: $$\\mathbf{J}_h(\\mathbf{y}) = \\mathbf{J}_f(g(\\mathbf{y})) \\mathbf{J}_g(\\mathbf{y})$$ where $\\mathbf{J}_h(\\mathbf{y}) \\in \\mathbb{R}^{p \\times m}$, $\\mathbf{J}_f(g(\\mathbf{y})) \\in \\mathbb{R}^{p \\times d}$, and $\\mathbf{J}_g(\\mathbf{y}) \\in \\mathbb{R}^{d \\times m}$. Chain Rule: Special Cases # Case 1: If $f:\\mathbb{R}^d \\to \\mathbb{R}$ and $g:\\mathbb{R}^m \\to \\mathbb{R}^d$, then for $h = f \\circ g : \\mathbb{R}^m \\to \\mathbb{R}$: $$\\nabla h(\\mathbf{y}) = \\mathbf{J}_g(\\mathbf{y})^T \\nabla f(g(\\mathbf{y}))$$\nCase 2: If $f:\\mathbb{R} \\to \\mathbb{R}$ and $g:\\mathbb{R}^m \\to \\mathbb{R}$, then for $h = f \\circ g : \\mathbb{R}^m \\to \\mathbb{R}$: $$\\nabla h(\\mathbf{y}) = f\u0026rsquo;(g(\\mathbf{y})) \\nabla g(\\mathbf{y})$$\nWorked Examples # Example 1: Given:\n$f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{x}$ where $f: \\mathbb{R}^2 \\to \\mathbb{R}$ $g(\\mathbf{y}) = \\begin{pmatrix} y_1 + y_2 \\\\ y_1 - y_2 \\end{pmatrix}$ where $g: \\mathbb{R}^2 \\to \\mathbb{R}^2$ $h = f \\circ g$ Find $\\nabla h(\\mathbf{y})$ using the chain rule.\nSolution:\nFirst, $\\nabla f(\\mathbf{x}) = 2\\mathbf{x}$ The Jacobian is $\\mathbf{J}_g(\\mathbf{y}) = \\begin{pmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix}$ Applying the chain rule: $$\\nabla h(\\mathbf{y}) = \\mathbf{J}_g(\\mathbf{y})^T \\nabla f(g(\\mathbf{y})) = \\begin{pmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} \\cdot 2g(\\mathbf{y})$$ $$= 2\\begin{pmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix}\\begin{pmatrix} y_1 + y_2 \\ y_1 - y_2 \\end{pmatrix} = \\begin{pmatrix} 4y_1 \\ 4y_2 \\end{pmatrix}$$ Example 2 (General Quadratic Forms): Given:\n$f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{A}\\mathbf{x} + \\mathbf{b}^T\\mathbf{x}$ where $\\mathbf{A}$ is symmetric $g(\\mathbf{y}) = \\mathbf{C}\\mathbf{y}$ (linear transformation) Find $\\nabla h(\\mathbf{y})$ for $h = f \\circ g$.\nSolution:\n$\\nabla f(\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x} + \\mathbf{b}$ $\\mathbf{J}_g(\\mathbf{y}) = \\mathbf{C}$ Therefore: $$\\nabla h(\\mathbf{y}) = \\mathbf{C}^T [2\\mathbf{A}(\\mathbf{C}\\mathbf{y}) + \\mathbf{b}] = 2\\mathbf{C}^T\\mathbf{A}\\mathbf{C}\\mathbf{y} + \\mathbf{C}^T\\mathbf{b}$$ 5 - Matrix Functions: $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}$ # Fréchet Derivative # Definition 0.10 (Fréchet Differentiability)\nA function $f:\\mathbb{R}^{m \\times n}\\to\\mathbb{R}^{p \\times q}$ is Fréchet differentiable at $\\mathbf{X}$ if there exists a linear mapping $Df(\\mathbf{X}):\\mathbb{R}^{m \\times n}\\to\\mathbb{R}^{p \\times q}$ such that $$\\lim_{|\\mathbf{V}|_F\\to 0} \\frac{|f(\\mathbf{X}+\\mathbf{V}) - f(\\mathbf{X}) - Df(\\mathbf{X})[\\mathbf{V}]|_F}{|\\mathbf{V}|_F} = 0$$ The Fréchet derivative can also be characterized using the Gateaux derivative: $$Df(\\mathbf{X})[\\mathbf{V}] = \\left.\\frac{d}{dt}\\right|_{t=0} f(\\mathbf{X}+t\\mathbf{V}) = \\lim_{t\\to 0} \\frac{f(\\mathbf{X}+t\\mathbf{V}) - f(\\mathbf{X})}{t}$$\nIf this limit is not linear in $\\mathbf{V}$, then $f$ is not Fréchet differentiable.\nOften it is useful to see this derivative as a linear operator such that: $$ \\mathbf{D} f(\\mathbf{X})[\\boldsymbol{\\xi}] = f(\\mathbf{X}+\\mathbf{\\xi}) - f(\\mathbf{X}) + o(\\lVert\\boldsymbol{\\xi}\\rVert)$$\nMatrix-to-Scalar Functions # For a function $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}$, the directional derivative at $\\mathbf{X}$ in direction $\\mathbf{V}$ is: $$Df(\\mathbf{X})[\\mathbf{V}] = \\lim_{h \\to 0} \\frac{f(\\mathbf{X} + h\\mathbf{V}) - f(\\mathbf{X})}{h}$$\nDefinition 0.11 (Matrix Gradient)\nFor $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}$, the gradient $\\nabla f(\\mathbf{X}) \\in \\mathbb{R}^{m \\times n}$ satisfies: $$Df(\\mathbf{X})[\\mathbf{V}] = \\mathrm{Tr}(\\nabla f(\\mathbf{X})^\\mathrm{T} \\mathbf{V})$$ where $\\mathrm{Tr}(\\cdot)$ denotes the trace of a matrix. The gradient can be computed element-wise as: $$\\nabla f(\\mathbf{X}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial X_{11}} \u0026amp; \\frac{\\partial f}{\\partial X_{12}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial X_{1n}} \\\\ \\frac{\\partial f}{\\partial X_{21}} \u0026amp; \\frac{\\partial f}{\\partial X_{22}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial X_{2n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f}{\\partial X_{m1}} \u0026amp; \\frac{\\partial f}{\\partial X_{m2}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial X_{mn}} \\end{pmatrix}$$\nExamples of Matrix-to-Scalar Functions # Example 1: $f(\\mathbf{X}) = |\\mathbf{X}|_F^2 = \\mathrm{Tr}(\\mathbf{X}^\\mathrm{T}\\mathbf{X})$\nUsing the Gateaux derivative: $$Df(\\mathbf{X})[\\mathbf{V}] = \\left.\\frac{d}{dt}\\right|_{t=0} \\mathrm{Tr}((\\mathbf{X}+t\\mathbf{V})^\\mathrm{T}(\\mathbf{X}+t\\mathbf{V}))$$\nExpanding and differentiating: $$= \\left.\\frac{d}{dt}\\right|_{t=0} [\\mathrm{Tr}(\\mathbf{X}^\\mathrm{T}\\mathbf{X}) + 2t\\mathrm{Tr}(\\mathbf{X}^\\mathrm{T}\\mathbf{V}) + t^2\\mathrm{Tr}(\\mathbf{V}^\\mathrm{T}\\mathbf{V})]$$ $$= 2\\mathrm{Tr}(\\mathbf{X}^\\mathrm{T}\\mathbf{V})$$\nTherefore: $\\nabla f(\\mathbf{X}) = 2\\mathbf{X}$\nExample 2: $f(\\mathbf{X}) = \\log\\det(\\mathbf{X})$ (for invertible $\\mathbf{X}$)\nFor this function: $$Df(\\mathbf{X})[\\mathbf{V}] = \\left.\\frac{d}{dt}\\right|_{t=0} \\log\\det(\\mathbf{X}+t\\mathbf{V}) = \\mathrm{Tr}(\\mathbf{X}^{-1}\\mathbf{V})$$\nTherefore: $\\nabla f(\\mathbf{X}) = \\mathbf{X}^{-\\mathrm{T}}$\n6 - Matrix Functions: $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{p \\times q}$ # Matrix-to-Matrix Functions # For a function $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{p \\times q}$, the directional derivative $Df(\\mathbf{X})[\\mathbf{V}]$ is a linear mapping from $\\mathbb{R}^{m \\times n}$ to $\\mathbb{R}^{p \\times q}$.\nSince $Df(\\mathbf{X})$ is linear, there exists a matrix $\\mathbf{M}_{\\mathbf{X}} \\in \\mathbb{R}^{pq \\times mn}$ such that: $$\\mathrm{vec}(Df(\\mathbf{X})[\\mathbf{V}]) = \\mathbf{M}_{\\mathbf{X}} \\mathrm{vec}(\\mathbf{V})$$ where $\\mathrm{vec}(\\cdot)$ stacks matrix columns into a vector.\nThis representation transforms the problem of computing matrix derivatives into standard matrix-vector multiplication. The matrix $\\mathbf{M}_{\\mathbf{X}}$ is sometimes called the derivative matrix or Jacobian matrix of the vectorized function.\nThe power of this representation becomes clear when combined with the Kronecker product identity:\nTheorem 0.4 (Kronecker Product Identity)\nFor matrices $\\mathbf{A} \\in \\mathbb{R}^{p \\times m}$, $\\mathbf{B} \\in \\mathbb{R}^{n \\times q}$, and $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$: $$\\mathrm{vec}(\\mathbf{A}\\mathbf{X}\\mathbf{B}) = (\\mathbf{B}^\\mathrm{T} \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{X})$$ Example: Consider $f(\\mathbf{X}) = \\mathbf{A}\\mathbf{X}\\mathbf{B}$ where $\\mathbf{A} \\in \\mathbb{R}^{p \\times m}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times q}$ are fixed matrices.\nTo find the derivative, we compute: $$Df(\\mathbf{X})[\\mathbf{V}] = f(\\mathbf{X} + \\mathbf{V}) - f(\\mathbf{X}) = \\mathbf{A}(\\mathbf{X} + \\mathbf{V})\\mathbf{B} - \\mathbf{A}\\mathbf{X}\\mathbf{B} = \\mathbf{A}\\mathbf{V}\\mathbf{B}$$\nUsing the Kronecker product identity: $$\\mathrm{vec}(Df(\\mathbf{X})[\\mathbf{V}]) = \\mathrm{vec}(\\mathbf{A}\\mathbf{V}\\mathbf{B}) = (\\mathbf{B}^\\mathrm{T} \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{V})$$\nTherefore, $\\mathbf{M}_{\\mathbf{X}} = \\mathbf{B}^\\mathrm{T} \\otimes \\mathbf{A}$, which is independent of $\\mathbf{X}$ since $f$ is linear.\nVectorization Identities # Key identities for working with matrix derivatives:\n$\\mathrm{vec}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) = (\\mathbf{C}^\\mathrm{T} \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{B})$ $\\mathrm{Tr}(\\mathbf{A}\\mathbf{B}) = \\mathrm{vec}(\\mathbf{A})^\\mathrm{T}\\mathrm{vec}(\\mathbf{B})$ $\\mathrm{Tr}(\\mathbf{A}^\\mathrm{T}\\mathbf{B}) = \\mathrm{vec}(\\mathbf{A})^\\mathrm{T}\\mathrm{vec}(\\mathbf{B})$ where $\\otimes$ denotes the Kronecker product.\nExamples of Matrix-to-Matrix Functions # Example 1: $f(\\mathbf{X}) = \\mathbf{X}^2$\nUsing the Gateaux derivative: $$Df(\\mathbf{X})[\\mathbf{V}] = \\left.\\frac{d}{dt}\\right|_{t=0} (\\mathbf{X}+t\\mathbf{V})^2 = \\mathbf{X}\\mathbf{V} + \\mathbf{V}\\mathbf{X}$$\nExample 2: $f(\\mathbf{X}) = \\mathbf{X}^{-1}$ (for invertible $\\mathbf{X}$)\nFrom the identity $\\mathbf{X}\\mathbf{X}^{-1} = \\mathbf{I}$ and differentiating: $$Df(\\mathbf{X})[\\mathbf{V}] = -\\mathbf{X}^{-1}\\mathbf{V}\\mathbf{X}^{-1}$$\nProperties of Matrix Function Derivatives # The derivatives of matrix functions follow familiar rules:\nLinearity: For $f = \\alpha g + \\beta h$: $$Df(\\mathbf{X})[\\mathbf{V}] = \\alpha , Dg(\\mathbf{X})[\\mathbf{V}] + \\beta , Dh(\\mathbf{X})[\\mathbf{V}]$$\nProduct rule: For $f(\\mathbf{X}) = g(\\mathbf{X}) \\cdot h(\\mathbf{X})$: $$Df(\\mathbf{X})[\\mathbf{V}] = Dg(\\mathbf{X})[\\mathbf{V}] \\cdot h(\\mathbf{X}) + g(\\mathbf{X}) \\cdot Dh(\\mathbf{X})[\\mathbf{V}]$$\nChain rule: For $f(\\mathbf{X}) = g(h(\\mathbf{X}))$: $$Df(\\mathbf{X})[\\mathbf{V}] = Dg(h(\\mathbf{X}))[Dh(\\mathbf{X})[\\mathbf{V}]]$$\n"},{"id":9,"href":"/numerical_optimization/docs/lectures/1_introduction/","title":"Introduction","section":"Lectures","content":" Introduction # Notations # Let us start by defining the notation used troughout all the lectures and practical labs.\nBasic Notation # Scalars are represented by italic letters (e.g., $x$, $y$, $\\lambda$). Vectors are denoted by bold lowercase letters (e.g., $\\mathbf{v}$, $\\mathbf{x}$), while matrices are represented by bold uppercase letters (e.g., $\\mathbf{A}$, $\\mathbf{B}$). The dimensionality of a vector $\\mathbf{v} \\in \\mathbb{R}^n$ indicates it contains $n$ elements, and similarly, a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ has $m$ rows and $n$ columns.\nMatrix Operations # The transpose of a matrix $\\mathbf{A}$ is denoted as $\\mathbf{A}^\\mathrm{T}$, which reflects the matrix across its diagonal. The trace of a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, written as $\\mathrm{tr}(\\mathbf{A})$, is the sum of its diagonal elements, i.e., $\\mathrm{tr}(\\mathbf{A}) = \\sum_{i=1}^{n} a_{ii}$. The determinant of $\\mathbf{A}$ is represented as $\\det(\\mathbf{A})$ or $|\\mathbf{A}|$. A matrix $\\mathbf{A}$ is invertible if and only if $\\det(\\mathbf{A}) \\neq 0$, and its inverse is denoted as $\\mathbf{A}^{-1}$, satisfying $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix.\nVector Operations # The dot product between two vectors $\\mathbf{a}$ and $\\mathbf{b}$ of the same dimension is written as $\\mathbf{a} \\cdot \\mathbf{b}$ or $\\mathbf{a}^\\mathrm{T}\\mathbf{b}$, resulting in a scalar value.\nThe p-norm of a vector $\\mathbf{v}$ is denoted as $\\lVert\\mathbf{v}\\rVert_p$ and defined as $\\lVert\\mathbf{v}\\rVert_p = \\left(\\sum_{i=1}^{n} |v_i|^p\\right)^{1/p}$ for $p \\geq 1$, with common choices being $p=1$ (Manhattan norm), $p=2$ (Euclidean norm), and $p=\\infty$ (maximum norm, defined as $\\lVert\\mathbf{v}\\rVert_{\\infty} = \\max_i |v_i|$); when the subscript $p$ is omitted, as in $\\lVert\\mathbf{v}\\rVert$, it is conventionally understood to refer to the Euclidean (L2) norm. The Euclidean norm (or length) of a vector $\\mathbf{v}$ is represented as $\\lVert\\mathbf{v}\\rVert$ or $\\lVert\\mathbf{v}\\rVert_2$, defined as $\\lVert\\mathbf{v}\\rVert = \\sqrt{\\mathbf{v}^\\mathrm{T}\\mathbf{v}} = \\sqrt{\\sum_{i=1}^{n} v_i^2}$. A unit vector in the direction of $\\mathbf{v}$ is given by $\\hat{\\mathbf{v}} = \\mathbf{v}/\\lVert\\mathbf{v}\\rVert$, having a norm of 1.\nEigenvalues and Eigenvectors # For a square matrix $\\mathbf{A}$, a scalar $\\lambda$ is an eigenvalue if there exists a non-zero vector $\\mathbf{v}$ such that $\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$. The vector $\\mathbf{v}$ is called an eigenvector corresponding to the eigenvalue $\\lambda$. The characteristic polynomial of $\\mathbf{A}$ is defined as $p(\\lambda) = \\det(\\lambda\\mathbf{I} - \\mathbf{A})$, and its roots are the eigenvalues of $\\mathbf{A}$. The spectrum of $\\mathbf{A}$, denoted by $\\sigma(\\mathbf{A})$, is the set of all eigenvalues of $\\mathbf{A}$.\nMatrix Decompositions # The singular value decomposition (SVD) of a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is expressed as $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\mathrm{T}$, where $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ and $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values of $\\mathbf{A}$. The eigendecomposition of a diagonalizable matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is given by $\\mathbf{A} = \\mathbf{P}\\mathbf{\\Lambda}\\mathbf{P}^{-1}$, where $\\mathbf{P}$ is a matrix whose columns are the eigenvectors of $\\mathbf{A}$, and $\\mathbf{\\Lambda}$ is a diagonal matrix containing the corresponding eigenvalues.\nMultivariate Calculus # The gradient of a scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is denoted as $\\nabla f$ or $\\mathrm{grad}(f)$, resulting in a vector of partial derivatives $\\nabla f = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}]^\\mathrm{T}$. The Jacobian matrix of a vector-valued function $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is represented as $\\mathbf{J}_\\mathbf{f}$ or $\\nabla \\mathbf{f}^\\mathrm{T}$, where $ (\\mathbf{J}_\\mathbf{f})_{ij} = \\frac{\\partial f_i}{\\partial x_j} $.\nThe Hessian matrix of a twice-differentiable scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is denoted as $\\mathbf{H}_f$ or $\\nabla^2 f$, where $(\\mathbf{H}_f)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$.\nSpecial Matrices and Properties # A symmetric matrix satisfies $\\mathbf{A} = \\mathbf{A}^\\mathrm{T}$, while a skew-symmetric matrix has $\\mathbf{A} = -\\mathbf{A}^\\mathrm{T}$. An orthogonal matrix $\\mathbf{Q}$ satisfies $\\mathbf{Q}^\\mathrm{T}\\mathbf{Q} = \\mathbf{Q}\\mathbf{Q}^\\mathrm{T} = \\mathbf{I}$, meaning its inverse equals its transpose: $\\mathbf{Q}^{-1} = \\mathbf{Q}^\\mathrm{T}$. A matrix $\\mathbf{A}$ is positive definite if $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} \u0026gt; 0$ for all non-zero vectors $\\mathbf{x}$, and positive semidefinite if $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} \\geq 0$.\nDerivatives of Matrix Expressions # The derivative of a scalar function with respect to a vector $\\mathbf{x}$ is denoted as $\\frac{\\partial f}{\\partial \\mathbf{x}}$, resulting in a vector of the same dimension as $\\mathbf{x}$. For matrix functions, the derivative with respect to a matrix $\\mathbf{X}$ is written as $\\frac{\\partial f}{\\partial \\mathbf{X}}$, producing a matrix of the same dimensions as $\\mathbf{X}$. Common matrix derivatives include $\\frac{\\partial}{\\partial \\mathbf{X}}\\mathrm{tr}(\\mathbf{AX}) = \\mathbf{A}^\\mathrm{T}$ and $\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}) = \\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathrm{T}\\mathbf{x}$ (with $\\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathrm{T}\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}$ when $\\mathbf{A}$ is symmetric).\n"},{"id":10,"href":"/numerical_optimization/docs/lectures/fundamentals/convexity/","title":"3. Convexity theory","section":"I - Fundamentals","content":" Convexity theory # Convexity is a powerful property of functions and sets that simplifies the analysis of optimization problems and the characterization of global minimizers. In this chapter, we will explore the concepts of convex sets, convex functions, and their implications for unconstrained optimization.\nConvex sets # Let us first start by defining the convexity of a given set $\\mathcal{S}\\subset\\mathbb{R}^d$:\nDefinition 3.1 (Convex set)\nLet $\\mathcal{S}\\subset\\mathbb{R}^d$ be a set. The set $\\mathcal{S}$ is convex if, for any two points $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}$, the line segment that connects them is also contained in $\\mathcal{S}$, that is, \\begin{equation} \\mathbf{x}, \\mathbf{y} \\in \\mathcal{S} \\implies \\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y} \\in \\mathcal{S}, \\quad \\forall \\lambda \\in [0, 1]. \\label{eq:convex_set} \\end{equation} Figure 3.1: Convex set\nThis is illustrated by Figure 3.1 , where the set $\\mathcal{S}$ is convex, as the line segment between any two points $\\mathbf{x}$ and $\\mathbf{y}$ lies entirely within $\\mathcal{S}$. If this property does not hold, then the set is called non-convex.\nWhile we will not do deeper now, this property is desirable for the constraints of an optimization problem, .because it means that for a given algorithm, any subsequent step is feasible by staying true to the given constraints for the problem.\nConvex functions # A function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is convex if its domain is a convex set and it satisfies the following property:\nDefinition 3.2 (Convex function)\nA function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is convex if, for any two points $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ and for all $\\lambda \\in [0, 1]$, the following inequality holds: \\begin{equation} f(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}). \\label{eq:convex_function} \\end{equation} Figure 3.2: Convex function\nThis means that the line segment connecting the points $(\\mathbf{x}, f(\\mathbf{x}))$ and $(\\mathbf{y}, f(\\mathbf{y}))$ lies above the graph of the function $f$. In other words, the function is \u0026ldquo;bowl-shaped\u0026rdquo; or \u0026ldquo;curves upwards\u0026rdquo;. Such an illustration is given for a 1-dimensional function in Figure 3.2 , where the function $f$ is convex, as the line segment between any two points $(\\mathbf{x}, f(\\mathbf{x}))$ and $(\\mathbf{y}, f(\\mathbf{y}))$ lies above the graph of $f$.\nIn practice to show that a function is convex, we can make use of the following properties, given convex functions $f$ and $g$:\nlet $\\alpha, \\beta\u0026gt;0$, then $\\alpha f + \\beta g$ is convex $f \\circ g$ is convex Convexity and unconstrained optimization # When the objective function is convex, local and global minimizers are simple to characterize.\nTheorem 3.1 When $f$ is convex, any local minimizer $\\mathbf{x}^\\star$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $\\mathbf{x}^\\star$ is a global minimizer of $f$. Proof\nSuppose that $\\mathbf{x}^\\star$ is a local but not a global minimizer. Then we can find a point $\\mathbf{z} \\in \\mathbb{R}^n$ with $f(\\mathbf{z})\u0026lt;f\\left(\\mathbf{x}^\\star\\right)$. Consider the line segment that joins $\\mathbf{x}^\\star$ to $\\mathbf{z}$, that is, \\begin{equation} \\mathbf{x}=\\lambda \\mathbf{z}+(1-\\lambda) \\mathbf{x}^\\star, \\quad \\text { for some } \\lambda \\in(0,1] \\label{eq:line_segment} \\end{equation} By the convexity property for $f$, we have \\begin{equation} f(\\mathbf{x}) \\leq \\lambda f(\\mathbf{z})+(1-\\lambda) f\\left(\\mathbf{x}^\\star\\right)\u0026lt;f\\left(\\mathbf{x}^\\star\\right) \\label{eq:convexity} \\end{equation}\nAny neighborhood $\\mathcal{N}$ of $\\mathbf{x}^\\star$ contains a piece of the line segment \\eqref{eq:line_segment}, so there will always be points $\\mathbf{x} \\in \\mathcal{N}$ at which \\eqref{eq:convexity} is satisfied. Hence, $\\mathbf{x}^\\star$ is not a local minimizer. For the second part of the theorem, suppose that $\\mathbf{x}^\\star$ is not a global minimizer and choose $\\mathbf{z}$ as above. Then, from convexity, we have\n\\begin{equation} \\begin{aligned} \\nabla f\\left(\\mathbf{x}^\\star\\right)^{\\mathrm{T}}\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right) \u0026amp; =\\left.\\frac{d}{d \\lambda} f\\left(\\mathbf{x}^\\star+\\lambda\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right)\\right)\\right|_{\\lambda=0} \\\\ \u0026amp; =\\lim _{\\lambda \\downarrow 0} \\frac{f\\left(\\mathbf{x}^\\star+\\lambda\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right)\\right)-f\\left(\\mathbf{x}^\\star\\right)}{\\lambda} \\\\ \u0026amp; \\leq \\lim _{\\lambda \\downarrow 0} \\frac{\\lambda f(\\mathbf{z})+(1-\\lambda) f\\left(\\mathbf{x}^\\star\\right)-f\\left(\\mathbf{x}^\\star\\right)}{\\lambda} \\\\ \u0026amp; =f(\\mathbf{z})-f\\left(\\mathbf{x}^\\star\\right)\u0026lt;0 \\end{aligned} \\end{equation}\nTherefore, $\\nabla f\\left(\\mathbf{x}^\\star\\right) \\neq 0$, and so $\\mathbf{x}^\\star$ is not a stationary point.\n■ This result is fundamental in optimization, as it guarantees that if we find a local minimizer of a convex function, we can be sure that it is also the global minimizer. This property greatly simplifies the search for optimal solutions. As such, finding that the function we minimize is convex often means that the problem is easier to solve, as we can use algorithms that are guaranteed to converge to the global minimum.\nConversely, in the design stage, we might prefer to design a convex function, or try to find a convex approximation of a non-convex function, to ensure that the optimization problem is well-behaved and that we can find the global minimum efficiently.\n"},{"id":11,"href":"/numerical_optimization/docs/lectures/machine_learning/neural_networks/","title":"3. Neural Networks","section":"III - Machine Learning problems","content":" Neural Networks # Soon to be added.\n"},{"id":12,"href":"/numerical_optimization/docs/lectures/machine_learning/modern/","title":"4. Modern trends","section":"III - Machine Learning problems","content":" Modern trends # Soon to be added.\n"},{"id":13,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/","title":"4. Unconstrained optimization : linesearch","section":"I - Fundamentals","content":" Unconstrained optimization - Linesearch methods # Note : This is in part the content of the book \u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, with some modifications to the notations used in this lecture.\nAll algorithms for unconstrained minimization require the user to supply a starting point, which we usually denote by $\\mathbf{x}_0$. The user with knowledge about the application and the data set may be in a good position to choose $\\mathbf{x}_0$ to be a reasonable estimate of the solution. Otherwise, the starting point must be chosen in some arbitrary manner.\nBeginning at $\\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\\left\\{\\mathbf{x}_k\\right\\}_{k=0}^{\\infty}$ that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy. In deciding how to move from one iterate $\\mathbf{x}_k$ to the next, the algorithms use information about the function $f$ at $\\mathbf{x}_k$, and possibly also information from earlier iterates $\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{k-1}$. They use this information to find a new iterate $\\mathbf{x}_{k+1}$ with a lower function value than $\\mathbf{x}_k$. (There exist nonmonotone algorithms that do not insist on a decrease in $f$ at every step, but even these algorithms require $f$ to be decreased after some prescribed number $m$ of iterations. That is, they enforce $f\\left(\\mathbf{x}_k\\right)\u0026lt;f\\left(\\mathbf{x}_{k-m}\\right)$.)\nThere are two fundamental strategies for moving from the current point $\\mathbf{x}_k$ to a new iterate $\\mathbf{x}_{k+1}$. Most of the algorithms described in this book follow one of these approaches.\nTwo strategies: line search and trust region # In the line search strategy, the algorithm chooses a direction $\\mathbf{p}_k$ and searches along this direction from the current iterate $\\mathbf{x}_k$ for a new iterate with a lower function value. The distance to move along $\\mathbf{p}_k$ can be found by approximately solving the following one-dimensional minimization problem to find a step length $\\alpha$ :\n\\begin{equation} \\min _{\\alpha\u0026gt;0} f\\left(\\mathbf{x}_k+\\alpha \\mathbf{p}_k\\right) \\label{eq:line_search_min} \\end{equation}\nBy solving \\eqref{eq:line_search_min} exactly, we would derive the maximum benefit from the direction $\\mathbf{p}_k$, but an exact minimization is expensive and unnecessary. Instead, the line search algorithm generates a limited number of trial step lengths until it finds one that loosely approximates the minimum of \\eqref{eq:line_search_min}. At the new point a new search direction and step length are computed, and the process is repeated.\nIn the second algorithmic strategy, known as trust region, the information gathered about $f$ is used to construct a model function $m_k$ whose behavior near the current point $\\mathbf{x}_k$ is similar to that of the actual objective function $f$. Because the model $m_k$ may not be a good approximation of $f$ when $\\mathbf{x}$ is far from $\\mathbf{x}_k$, we restrict the search for a minimizer of $m_k$ to some region around $\\mathbf{x}_k$. In other words, we find the candidate step $\\mathbf{p}$ by approximately solving the following subproblem:\n\\begin{equation} \\min _{\\mathbf{p}} m_k\\left(\\mathbf{x}_k+\\mathbf{p}\\right), \\quad \\text { where } \\mathbf{x}_k+\\mathbf{p} \\text { lies inside the trust region. } \\label{eq:trust_region_subproblem} \\end{equation}\nIf the candidate solution does not produce a sufficient decrease in $f$, we conclude that the trust region is too large, and we shrink it and re-solve \\eqref{eq:trust_region_subproblem}. Usually, the trust region is a ball defined by $\\lVert\\mathbf{p}\\rVert_2 \\leq \\Delta$, where the scalar $\\Delta\u0026gt;0$ is called the trust-region radius. Elliptical and box-shaped trust regions may also be used.\nThe model $m_k$ in \\eqref{eq:trust_region_subproblem} is usually defined to be a quadratic function of the form\n\\begin{equation} m_k\\left(\\mathbf{x}_k+\\mathbf{p}\\right)=f_k+\\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\mathbf{p}^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{p} \\label{eq:quadratic_model} \\end{equation}\nwhere $f_k$, $\\nabla f_k$, and $\\mathbf{B}_k$ are a scalar, vector, and matrix, respectively. As the notation indicates, $f_k$ and $\\nabla f_k$ are chosen to be the function and gradient values at the point $\\mathbf{x}_k$, so that $m_k$ and $f$ are in agreement to first order at the current iterate $\\mathbf{x}_k$. The matrix $\\mathbf{B}_k$ is either the Hessian $\\nabla^2 f_k$ or some approximation to it.\nSuppose that the objective function is given by $f(\\mathbf{x})=10\\left(x_2-x_1^2\\right)^2+\\left(1-x_1\\right)^2$. At the point $\\mathbf{x}_k=(0,1)$ its gradient and Hessian are\n$$ \\nabla f_k=\\left[\\begin{array}{c} -2 \\\\ 20 \\end{array}\\right], \\quad \\nabla^2 f_k=\\left[\\begin{array}{cc} -38 \u0026amp; 0 \\\\ 0 \u0026amp; 20 \\end{array}\\right] $$\nNote that each time we decrease the size of the trust region after failure of a candidate iterate, the step from $\\mathbf{x}_k$ to the new candidate will be shorter, and it usually points in a different direction from the previous candidate. The trust-region strategy differs in this respect from line search, which stays with a single search direction.\nIn a sense, the line search and trust-region approaches differ in the order in which they choose the direction and distance of the move to the next iterate. Line search starts by fixing the direction $\\mathbf{p}_k$ and then identifying an appropriate distance, namely the step length $\\alpha_k$. In trust region, we first choose a maximum distance-the trust-region radius $\\Delta_k$-and then seek a direction and step that attain the best improvement possible subject to this distance constraint. If this step proves to be unsatisfactory, we reduce the distance measure $\\Delta_k$ and try again.\nThe line search approach is discussed in more detail in this lecture while the trust-region strategy, is left to the reader to study.\nSearch directions for line search methods # The steepest-descent direction $-\\nabla f_k$ is the most obvious choice for search direction for a line search method. It is intuitive; among all the directions we could move from $\\mathbf{x}_k$, it is the one along which $f$ decreases most rapidly. To verify this claim, we appeal again to Taylor\u0026rsquo;s theorem, which tells us that for any search direction $\\mathbf{p}$ and step-length parameter $\\alpha$, we have\n\\begin{equation} f\\left(\\mathbf{x}_k+\\alpha \\mathbf{p}\\right)=f\\left(\\mathbf{x}_k\\right)+\\alpha \\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\alpha^2 \\mathbf{p}^{\\mathrm{T}} \\nabla^2 f\\left(\\mathbf{x}_k+t \\mathbf{p}\\right) \\mathbf{p}, \\quad \\text { for some } t \\in(0, \\alpha) \\label{eq:taylor_expansion} \\end{equation}\nThe rate of change in $f$ along the direction $\\mathbf{p}$ at $\\mathbf{x}_k$ is simply the coefficient of $\\alpha$, namely, $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k$. Hence, the unit direction $\\mathbf{p}$ of most rapid decrease is the solution to the problem\n\\begin{equation} \\min _{\\mathbf{p}} \\mathbf{p}^{\\mathrm{T}} \\nabla f_k, \\quad \\text { subject to }\\lVert\\mathbf{p}\\rVert=1 \\label{eq:steepest_descent_problem} \\end{equation}\nSince $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k=\\lVert\\mathbf{p}\\rVert\\lVert\\nabla f_k\\rVert \\cos \\theta$, where $\\theta$ is the angle between $\\mathbf{p}$ and $\\nabla f_k$, we have from $\\lVert\\mathbf{p}\\rVert=1$ that $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k=\\lVert\\nabla f_k\\rVert \\cos \\theta$, so the objective in \\eqref{eq:steepest_descent_problem} is minimized when $\\cos \\theta$ takes on its minimum value of -1 at $\\theta=\\pi$ radians. In other words, the solution to \\eqref{eq:steepest_descent_problem} is\n$$ \\mathbf{p}=-\\nabla f_k /\\lVert\\nabla f_k\\rVert $$\nas claimed. This direction is orthogonal to the contours of the function.\nThe steepest descent method is a line search method that moves along $\\mathbf{p}_k=-\\nabla f_k$ at every step. It can choose the step length $\\alpha_k$ in a variety of ways, as we will see in next chapter. One advantage of the steepest descent direction is that it requires calculation of the gradient $\\nabla f_k$ but not of second derivatives. However, it can be excruciatingly slow on difficult problems.\nLine search methods may use search directions other than the steepest descent direction. In general, any descent direction-one that makes an angle of strictly less than $\\pi / 2$ radians with $-\\nabla f_k$-is guaranteed to produce a decrease in $f$, provided that the step length is sufficiently small. We can verify this claim by using Taylor\u0026rsquo;s theorem. From \\eqref{eq:taylor_expansion}, we have that\n$$ f\\left(\\mathbf{x}_k+\\epsilon \\mathbf{p}_k\\right)=f\\left(\\mathbf{x}_k\\right)+\\epsilon \\mathbf{p}_k^{\\mathrm{T}} \\nabla f_k+O\\left(\\epsilon^2\\right) $$\nWhen $\\mathbf{p}_k$ is a downhill direction, the angle $\\theta_k$ between $\\mathbf{p}_k$ and $\\nabla f_k$ has $\\cos \\theta_k\u0026lt;0$, so that\n$$ \\mathbf{p}_k^{\\mathrm{T}} \\nabla f_k=\\lVert\\mathbf{p}_k\\rVert\\lVert\\nabla f_k\\rVert \\cos \\theta_k\u0026lt;0 $$\nIt follows that $f\\left(\\mathbf{x}_k+\\epsilon \\mathbf{p}_k\\right)\u0026lt;f\\left(\\mathbf{x}_k\\right)$ for all positive but sufficiently small values of $\\epsilon$.\nAll of the search directions discussed so far can be used directly in a line search framework. They give rise to the steepest descent, Newton, quasi-Newton, and conjugate gradient line search methods. For Newton and quasi-Newton methods, see the next chapter.\nStep-length conditions # In computing the step length $\\alpha_{k}$, we face a tradeoff. We would like to choose $\\alpha_{k}$ to give a substantial reduction of $f$, but at the same time, we do not want to spend too much time making the choice. The ideal choice would be the global minimizer of the univariate function $\\phi(\\cdot)$ defined by\n\\begin{equation} \\phi(\\alpha)=f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right), \\quad \\alpha\u0026gt;0 \\label{eq:phi_def} \\end{equation}\nbut in general, it is too expensive to identify this value. To find even a local minimizer of $\\phi$ to moderate precision generally requires too many evaluations of the objective function $f$ and possibly the gradient $\\nabla f$. More practical strategies perform an inexact line search to identify a step length that achieves adequate reductions in $f$ at minimal cost.\nTypical line search algorithms try out a sequence of candidate values for $\\alpha$, stopping to accept one of these values when certain conditions are satisfied. The line search is done in two stages: A bracketing phase finds an interval containing desirable step lengths, and a bisection or interpolation phase computes a good step length within this interval. Sophisticated line search algorithms can be quite complicated, so we defer a full description until the end of this chapter. We now discuss various termination conditions for the line search algorithm and show that effective step lengths need not lie near minimizers of the univariate function $\\phi(\\alpha)$ defined in \\eqref{eq:phi_def}.\nA simple condition we could impose on $\\alpha_{k}$ is that it provide a reduction in $f$, i.e., $f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)\u0026lt;f\\left(\\mathbf{x}_{k}\\right)$. The difficulty is that we do not have sufficient reduction in $f$, a concept we discuss next.\nThe Wolfe conditions # A popular inexact line search condition stipulates that $\\alpha_{k}$ should first of all give sufficient decrease in the objective function $f$, as measured by the following inequality:\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:armijo} \\end{equation}\nfor some constant $c_{1} \\in(0,1)$. In other words, the reduction in $f$ should be proportional to both the step length $\\alpha_{k}$ and the directional derivative $\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$. Inequality \\eqref{eq:armijo} is sometimes called the Armijo condition.\nThe right-hand-side of \\eqref{eq:armijo}, which is a linear function, can be denoted by $l(\\alpha)$. The function $l(\\cdot)$ has negative slope $c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$, but because $c_{1} \\in(0,1)$, it lies above the graph of $\\phi$ for small positive values of $\\alpha$. The sufficient decrease condition states that $\\alpha$ is acceptable only if $\\phi(\\alpha) \\leq l(\\alpha)$. In practice, $c_{1}$ is chosen to be quite small, say $c_{1}=10^{-4}$.\nThe sufficient decrease condition is not enough by itself to ensure that the algorithm makes reasonable progress, because it is satisfied for all sufficiently small values of $\\alpha$. To rule out unacceptably short steps we introduce a second requirement, called the curvature condition, which requires $\\alpha_{k}$ to satisfy\n\\begin{equation} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \\geq c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:curvature} \\end{equation}\nfor some constant $c_{2} \\in\\left(c_{1}, 1\\right)$, where $c_{1}$ is the constant from \\eqref{eq:armijo}. Note that the left-handside is simply the derivative $\\phi^{\\prime}\\left(\\alpha_{k}\\right)$, so the curvature condition ensures that the slope of $\\phi\\left(\\alpha_{k}\\right)$ is greater than $c_{2}$ times the gradient $\\phi^{\\prime}(0)$. This makes sense because if the slope $\\phi^{\\prime}(\\alpha)$ is strongly negative, we have an indication that we can reduce $f$ significantly by moving further along the chosen direction. On the other hand, if the slope is only slightly negative or even positive, it is a sign that we cannot expect much more decrease in $f$ in this direction, so it might make sense to terminate the line search. Typical values of $c_{2}$ are 0.9 when the search direction $\\mathbf{p}_{k}$ is chosen by a Newton or quasi-Newton method, and 0.1 when $\\mathbf{p}_{k}$ is obtained from a nonlinear conjugate gradient method.\nThe sufficient decrease and curvature conditions are known collectively as the Wolfe conditions. We restate them here for future reference:\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \u0026amp; \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\\\ \\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \u0026amp; \\geq c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\end{aligned} \\label{eq:wolfe} \\end{equation}\nwith $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$.\nA step length may satisfy the Wolfe conditions without being particularly close to a minimizer of $\\phi$. We can, however, modify the curvature condition to force $\\alpha_{k}$ to lie in at least a broad neighborhood of a local minimizer or stationary point of $\\phi$. The strong Wolfe conditions require $\\alpha_{k}$ to satisfy\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \u0026amp; \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\\\ \\left|\\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k}\\right| \u0026amp; \\leq c_{2}\\left|\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\\right|, \\end{aligned} \\label{eq:strong_wolfe} \\end{equation}\nwith $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$. The only difference with the Wolfe conditions is that we no longer allow the derivative $\\phi^{\\prime}\\left(\\alpha_{k}\\right)$ to be too positive. Hence, we exclude points that are far from stationary points of $\\phi$.\nIt is not difficult to prove that there exist step lengths that satisfy the Wolfe conditions for every function $f$ that is smooth and bounded below.\nLemma 4.1 (Existence of step lengths)\nSuppose that $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ is continuously differentiable. Let $\\mathbf{p}_{k}$ be a descent direction at $\\mathbf{x}_{k}$, and assume that $f$ is bounded below along the ray $\\left\\{\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k} \\mid \\alpha\u0026gt;0\\right\\}$. Then if $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$, there exist intervals of step lengths satisfying the Wolfe conditions \\eqref{eq:wolfe} and the strong Wolfe conditions \\eqref{eq:strong_wolfe}. Proof\nSince $\\phi(\\alpha)=f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right)$ is bounded below for all $\\alpha\u0026gt;0$ and since $0\u0026lt;c_{1}\u0026lt;1$, the line $l(\\alpha)=f\\left(\\mathbf{x}_{k}\\right)+\\alpha c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$ must intersect the graph of $\\phi$ at least once. Let $\\alpha^{\\prime}\u0026gt;0$ be the smallest intersecting value of $\\alpha$, that is,\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime} \\mathbf{p}_{k}\\right)=f\\left(\\mathbf{x}_{k}\\right)+\\alpha^{\\prime} c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:intersection} \\end{equation}\nThe sufficient decrease condition \\eqref{eq:armijo} clearly holds for all step lengths less than $\\alpha^{\\prime}$.\nBy the mean value theorem, there exists $\\alpha^{\\prime \\prime} \\in\\left(0, \\alpha^{\\prime}\\right)$ such that\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime} \\mathbf{p}_{k}\\right)-f\\left(\\mathbf{x}_{k}\\right)=\\alpha^{\\prime} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime \\prime} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:mean_value} \\end{equation}\nBy combining \\eqref{eq:intersection} and \\eqref{eq:mean_value}, we obtain\n\\begin{equation} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime \\prime} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k}=c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\u0026gt;c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:inequality_proof} \\end{equation}\nsince $c_{1}\u0026lt;c_{2}$ and $\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\u0026lt;0$. Therefore, $\\alpha^{\\prime \\prime}$ satisfies the Wolfe conditions \\eqref{eq:wolfe}, and the inequalities hold strictly in both conditions. Hence, by our smoothness assumption on $f$, there is an interval around $\\alpha^{\\prime \\prime}$ for which the Wolfe conditions hold. Moreover, since the term in the left-hand side of \\eqref{eq:inequality_proof} is negative, the strong Wolfe conditions \\eqref{eq:strong_wolfe} hold in the same interval.\n■ The Wolfe conditions are scale-invariant in a broad sense: Multiplying the objective function by a constant or making an affine change of variables does not alter them. They can be used in most line search methods, and are particularly important in the implementation of quasi-Newton methods.\nTo summarize, see the following interactive visualisation of the Wolfe conditions, which illustrates the sufficient decrease and curvature conditions in action:\nThe Goldstein conditions # Like the Wolfe conditions, the Goldstein conditions also ensure that the step length $\\alpha$ achieves sufficient decrease while preventing $\\alpha$ from being too small. The Goldstein conditions can also be stated as a pair of inequalities, in the following way:\n\\begin{equation} f\\left(\\mathbf{x}_{k}\\right)+(1-c) \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\leq f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:goldstein} \\end{equation}\nwith $0\u0026lt;c\u0026lt;\\frac{1}{2}$. The second inequality is the sufficient decrease condition \\eqref{eq:armijo}, whereas the first inequality is introduced to control the step length from below.\nA disadvantage of the Goldstein conditions vis-à-vis the Wolfe conditions is that the first inequality in \\eqref{eq:goldstein} may exclude all minimizers of $\\phi$. However, the Goldstein and Wolfe conditions have much in common, and their convergence theories are quite similar. The Goldstein conditions are often used in Newton-type methods but are not well suited for quasi-Newton methods that maintain a positive definite Hessian approximation.\nAn illustration of the Goldstein conditions is shown in the following interactive visualisation:\nSufficient decrease and backtracking # We have mentioned that the sufficient decrease condition \\eqref{eq:armijo} alone is not sufficient to ensure that the algorithm makes reasonable progress along the given search direction. However, if the line search algorithm chooses its candidate step lengths appropriately, by using a so-called backtracking approach, we can dispense with the extra condition \\eqref{eq:curvature} and use just the sufficient decrease condition to terminate the line search procedure. In its most basic form, backtracking proceeds as follows.\nProcedure (Backtracking Line Search).\nChoose $\\bar{\\alpha}\u0026gt;0, \\rho, c \\in(0,1)$;\nset $\\alpha \\leftarrow \\bar{\\alpha}$;\nrepeat until $f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c \\alpha \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$\n$\\alpha \\leftarrow\\rho\\alpha$;\nend (repeat)\nterminate with $\\alpha_{k}=\\alpha$.\nIn this procedure, the initial step length $\\bar{\\alpha}$ is chosen to be 1 in Newton and quasi-Newton methods, but can have different values in other algorithms such as steepest descent or conjugate gradient. An acceptable step length will be found after a finite number of trials because $\\alpha_{k}$ will eventually become small enough that the sufficient decrease condition holds. In practice, the contraction factor $\\rho$ is often allowed to vary at each iteration of the line search. For example, it can be chosen by safeguarded interpolation, as we describe later. We need ensure only that at each iteration we have $\\rho \\in\\left[\\rho_{\\mathrm{lo}}, \\rho_{\\mathrm{hi}}\\right]$, for some fixed constants $0\u0026lt;\\rho_{\\text {lo }}\u0026lt;\\rho_{\\text {hi }}\u0026lt;1$.\nThe backtracking approach ensures either that the selected step length $\\alpha_{k}$ is some fixed value (the initial choice $\\bar{\\alpha}$ ), or else that it is short enough to satisfy the sufficient decrease condition but not too short. The latter claim holds because the accepted value $\\alpha_{k}$ is within striking distance of the previous trial value, $\\alpha_{k} / \\rho$, which was rejected for violating the sufficient decrease condition, that is, for being too long.\nConvergence of line search methods # To obtain global convergence, we must not only have well-chosen step lengths but also well-chosen search directions $\\mathbf{p}_k$. We discuss requirements on the search direction in this section, focusing on one key property: the angle $\\theta_k$ between $\\mathbf{p}_k$ and the steepest descent direction $-\\nabla f_k$, defined by\n$$ \\cos \\theta_k=\\frac{-\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k}{\\|\\nabla f_k\\|\\|\\mathbf{p}_k\\|} $$\nThe following theorem, due to Zoutendijk, has far-reaching consequences. It shows, for example, that the steepest descent method is globally convergent. For other algorithms it describes how far $\\mathbf{p}_k$ can deviate from the steepest descent direction and still give rise to a globally convergent iteration. Various line search termination conditions can be used to establish this result, but for concreteness we will consider only the Wolfe conditions. Though Zoutendijk\u0026rsquo;s result appears, at first, to be technical and obscure, its power will soon become evident.\nTheorem 4.1 (Zoutendijk\u0026#39;s theorem)\nConsider any iteration of the form $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, where $\\mathbf{p}_k$ is a descent direction and $\\alpha_k$ satisfies the Wolfe conditions. Suppose that $f$ is bounded below in $\\mathbb{R}^n$ and that $f$ is continuously differentiable in an open set $\\mathcal{N}$ containing the level set $\\mathcal{L} = \\{\\mathbf{x}: f(\\mathbf{x}) \\leq f(\\mathbf{x}_0)\\}$, where $\\mathbf{x}_0$ is the starting point of the iteration. Assume also that the gradient $\\nabla f$ is Lipschitz continuous on $\\mathcal{N}$, that is, there exists a constant $L\u0026gt;0$ such that\n\\begin{equation} \\|\\nabla f(\\mathbf{x})-\\nabla f(\\tilde{\\mathbf{x}})\\| \\leq L\\|\\mathbf{x}-\\tilde{\\mathbf{x}}\\|, \\quad \\text { for all } \\mathbf{x}, \\tilde{\\mathbf{x}} \\in \\mathcal{N} . \\label{eq:lipschitz} \\end{equation}\nThen\n\\begin{equation} \\sum_{k \\geq 0} \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2}\u0026lt;\\infty \\label{eq:zoutendijk_condition} \\end{equation}\nProof\nFrom the second Wolfe condition and the iteration formula we have that\n$$ (\\nabla f_{k+1}-\\nabla f_k)^{\\mathrm{T}} \\mathbf{p}_k \\geq(c_2-1) \\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k $$\nwhile the Lipschitz condition \\eqref{eq:lipschitz} implies that\n$$ (\\nabla f_{k+1}-\\nabla f_k)^{\\mathrm{T}} \\mathbf{p}_k \\leq \\alpha_k L\\|\\mathbf{p}_k\\|^{2} $$\nBy combining these two relations, we obtain\n$$ \\alpha_k \\geq \\frac{c_2-1}{L} \\frac{\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k}{\\|\\mathbf{p}_k\\|^{2}} $$\nBy substituting this inequality into the first Wolfe condition, we obtain\n$$ f_{k+1} \\leq f_k-c_1 \\frac{1-c_2}{L} \\frac{(\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k)^{2}}{\\|\\mathbf{p}_k\\|^{2}} $$\nFrom the definition of $\\cos \\theta_k$, we can write this relation as\n$$ f_{k+1} \\leq f_k-c \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2} $$\nwhere $c=c_1(1-c_2) / L$. By summing this expression over all indices less than or equal to $k$, we obtain\n$$ f_{k+1} \\leq f_0-c \\sum_{j=0}^{k} \\cos ^{2} \\theta_j\\|\\nabla f_j\\|^{2} $$\nSince $f$ is bounded below, we have that $f_0-f_{k+1}$ is less than some positive constant, for all $k$. Hence by taking limits, we obtain\n$$ \\sum_{k=0}^{\\infty} \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2}\u0026lt;\\infty $$\nwhich concludes the proof.\n■ Similar results to this theorem hold when the Goldstein conditions or strong Wolfe conditions are used in place of the Wolfe conditions.\nNote that the assumptions of Theorem 4.1 are not too restrictive. If the function $f$ were not bounded below, the optimization problem would not be well-defined. The smoothness assumption—Lipschitz continuity of the gradient—is implied by many of the smoothness conditions that are used in local convergence theorems and are often satisfied in practice.\nInequality \\eqref{eq:zoutendijk_condition}, which we call the Zoutendijk condition, implies that\n$$ \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2} \\rightarrow 0 $$\nThis limit can be used in turn to derive global convergence results for line search algorithms. If our method for choosing the search direction $\\mathbf{p}_k$ in the iteration ensures that the angle $\\theta_k$ is bounded away from $90^{\\circ}$, there is a positive constant $\\delta$ such that\n\\begin{equation} \\cos \\theta_k \\geq \\delta\u0026gt;0, \\quad \\text { for all } k \\label{eq:angle_bound} \\end{equation}\nIt follows immediately from \\eqref{eq:zoutendijk_condition} that\n\\begin{equation} \\lim _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 \\label{eq:global_convergence} \\end{equation}\nIn other words, we can be sure that the gradient norms $\\|\\nabla f_k\\|$ converge to zero, provided that the search directions are never too close to orthogonality with the gradient. In particular, the method of steepest descent (for which the search direction $\\mathbf{p}_k$ makes an angle of zero degrees with the negative gradient) produces a gradient sequence that converges to zero, provided that it uses a line search satisfying the Wolfe or Goldstein conditions.\nWe use the term globally convergent to refer to algorithms for which the property \\eqref{eq:global_convergence} is satisfied, but note that this term is sometimes used in other contexts to mean different things. For line search methods of the general form $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, the limit \\eqref{eq:global_convergence} is the strongest global convergence result that can be obtained: We cannot guarantee that the method converges to a minimizer, but only that it is attracted by stationary points. Only by making additional requirements on the search direction $\\mathbf{p}_k$—by introducing negative curvature information from the Hessian $\\nabla^{2} f(\\mathbf{x}_k)$, for example—can we strengthen these results to include convergence to a local minimum.\nNote that throughout this section we have used only the fact that Zoutendijk\u0026rsquo;s condition implies the limit \\eqref{eq:zoutendijk_condition}.\nRate of convergence # We refer the reader to the textbook\n\u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, 2nd edition, Springer, 2006, pages 47-51,\nfor a detailed discussion of the rate of convergence of line search methods.\nPeculiarly, see pages 47-51. In general, the rate of convergence depends on the choice of search direction and the step length conditions used.\n"},{"id":14,"href":"/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/","title":"5. Constrained optimization - Introduction","section":"I - Fundamentals","content":" Constrained optimization methods # Note : This is in part the content of the book \u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, with some modifications to the notations used in this lecture.\nThe second part of this lecture is about minimizing functions subject to constraints on the variables. A general formulation for these problems is\n$$ \\min_{\\mathbf{x} \\in \\mathrm{R}^{n}} f(\\mathbf{x}) \\quad \\text { subject to } \\quad \\begin{cases}c_{i}(\\mathbf{x})=0, \u0026amp; i \\in \\mathcal{E}, \\\\ c_{i}(\\mathbf{x}) \\geq 0, \u0026amp; i \\in \\mathcal{I},\\end{cases} $$\nwhere $f$ and the functions $c_{i}$ are all smooth, real-valued functions on a subset of $\\mathbb{R}^{n}$, and $\\mathcal{I}$ and $\\mathcal{E}$ are two finite sets of indices. As before, we call $f$ the objective function, while $c_{i}$, $i \\in \\mathcal{E}$ are the equality constraints and $c_{i}, i \\in \\mathcal{I}$ are the inequality constraints. We define the feasible set $\\Omega$ to be the set of points $\\mathbf{x}$ that satisfy the constraints; that is,\n$$ \\Omega=\\left\\{\\mathbf{x} \\mid c_{i}(\\mathbf{x})=0, \\quad i \\in \\mathcal{E} ; \\quad c_{i}(\\mathbf{x}) \\geq 0, \\quad i \\in \\mathcal{I}\\right\\} $$\nso that we can rewrite the problem more compactly as\n\\begin{equation} \\min_{\\mathbf{x} \\in \\Omega} f(\\mathbf{x}). \\label{eq:constrained_problem} \\end{equation}\nIn this chapter we derive mathematical characterizations of the solutions of \\eqref{eq:constrained_problem}. Recall that for the unconstrained optimization problem, we characterized solution points $\\mathbf{x}^{\\star}$ in the following way:\nNecessary conditions: Local minima of unconstrained problems have $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$ and $\\nabla^{2} f\\left(\\mathbf{x}^{\\star}\\right)$ positive semidefinite.\nSufficient conditions: Any point $\\mathbf{x}^{\\star}$ at which $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$ and $\\nabla^{2} f\\left(\\mathbf{x}^{\\star}\\right)$ is positive definite is a strong local minimizer of $f$.\nOur aim in this chapter is to derive similar conditions to characterize the solutions of constrained optimization problems.\nLocal and global solutions # We have seen already that global solutions are difficult to find even when there are no constraints. The situation may be improved when we add constraints, since the feasible set might exclude many of the local minima and it may be comparatively easy to pick the global minimum from those that remain. However, constraints can also make things much more difficult. As an example, consider the problem\n$$ \\min_{\\mathbf{x} \\in \\mathrm{R}^{n}}\\|\\mathbf{x}\\|_{2}^{2}, \\quad \\text { subject to }\\|\\mathbf{x}\\|_{2}^{2} \\geq 1 $$\nWithout the constraint, this is a convex quadratic problem with unique minimizer $\\mathbf{x}=\\mathbf{0}$. When the constraint is added, any vector $\\mathbf{x}$ with $\\|\\mathbf{x}\\|_{2}=1$ solves the problem. There are infinitely many such vectors (hence, infinitely many local minima) whenever $n \\geq 2$.\nA second example shows how addition of a constraint produces a large number of local solutions that do not form a connected set. Consider\n$$ \\min \\left(x_{2}+100\\right)^{2}+0.01 x_{1}^{2}, \\quad \\text { subject to } x_{2}-\\cos x_{1} \\geq 0 $$\nWithout the constraint, the problem has the unique solution $(-100,0)$. With the constraint there are local solutions near the points\n$$ \\left(x_{1}, x_{2}\\right)=(k \\pi,-1), \\quad \\text { for } \\quad k= \\pm 1, \\pm 3, \\pm 5, \\ldots $$\nDefinition 5.1 (Local solution)\nA vector $\\mathbf{x}^{\\star}$ is a local solution of the problem \\eqref{eq:constrained_problem} if $\\mathbf{x}^{\\star} \\in \\Omega$ and there is a neighborhood $\\mathcal{N}$ of $\\mathbf{x}^{\\star}$ such that $f(\\mathbf{x}) \\geq f\\left(\\mathbf{x}^{\\star}\\right)$ for $\\mathbf{x} \\in \\mathcal{N} \\cap \\Omega$. Similarly, we can make the following definitions:\nDefinition 5.2 (Strict local solution)\nA vector $\\mathbf{x}^{\\star}$ is a strict local solution (also called a strong local solution) if $\\mathbf{x}^{\\star} \\in \\Omega$ and there is a neighborhood $\\mathcal{N}$ of $\\mathbf{x}^{\\star}$ such that $f(\\mathbf{x})\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $\\mathbf{x} \\in \\mathcal{N} \\cap \\Omega$ with $\\mathbf{x} \\neq \\mathbf{x}^{\\star}$. Definition 5.3 (Isolated local solution)\nA point $\\mathbf{x}^{\\star}$ is an isolated local solution if $\\mathbf{x}^{\\star} \\in \\Omega$ and there is a neighborhood $\\mathcal{N}$ of $\\mathbf{x}^{\\star}$ such that $\\mathbf{x}^{\\star}$ is the only local minimizer in $\\mathcal{N} \\cap \\Omega$. At times, we replace the word \u0026ldquo;solution\u0026rdquo; by \u0026ldquo;minimizer\u0026rdquo; in our discussion. This alternative is frequently used in the literature, but it is slightly less satisfying because it does not account for the role of the constraints in defining the point in question.\nSmoothness # Smoothness of objective functions and constraints is an important issue in characterizing solutions, just as in the unconstrained case. It ensures that the objective function and the constraints all behave in a reasonably predictable way and therefore allows algorithms to make good choices for search directions.\nWe saw earlier that graphs of nonsmooth functions contain \u0026ldquo;kinks\u0026rdquo; or \u0026ldquo;jumps\u0026rdquo; where the smoothness breaks down. If we plot the feasible region for any given constrained optimization problem, we usually observe many kinks and sharp edges. Does this mean that the constraint functions that describe these regions are nonsmooth? The answer is often no, because the nonsmooth boundaries can often be described by a collection of smooth constraint functions. A diamond-shaped feasible region in $\\mathbb{R}^{2}$ could be described by the single nonsmooth constraint\n$$ \\|\\mathbf{x}\\|_{1}=\\left|x_{1}\\right|+\\left|x_{2}\\right| \\leq 1 . $$\nFigure 5.1: Nonsmooth constraints can be described by smooth constraints\nIt can also be described by the following set of smooth (in fact, linear) constraints:\n$$ x_{1}+x_{2} \\leq 1, \\quad x_{1}-x_{2} \\leq 1, \\quad -x_{1}+x_{2} \\leq 1, \\quad -x_{1}-x_{2} \\leq 1 $$\nEach of the four constraints represents one edge of the feasible polytope. In general, the constraint functions are chosen so that each one represents a smooth piece of the boundary of $\\Omega$.\nNonsmooth, unconstrained optimization problems can sometimes be reformulated as smooth constrained problems. An example is given by the unconstrained scalar problem of minimizing a nonsmooth function $f(x)$ defined by\n$$ f(x)=\\max \\left(x^{2}, x\\right), $$\nwhich has kinks at $x=0$ and $x=1$, and the solution at $x^{\\star}=0$. We obtain a smooth, constrained formulation of this problem by adding an artificial variable $t$ and writing\n$$ \\min t \\quad \\text { s.t. } \\quad t \\geq x, \\quad t \\geq x^{2} . $$\nReformulation techniques such as these are used often in cases where $f$ is a maximum of a collection of functions or when $f$ is a 1 -norm or $\\infty$-norm of a vector function.\nIn the examples above we expressed inequality constraints in a slightly different way from the form $c_{i}(\\mathbf{x}) \\geq 0$ that appears in the definition. However, any collection of inequality constraints with $\\geq$ and $\\leq$ and nonzero right-hand-sides can be expressed in the form $c_{i}(\\mathbf{x}) \\geq 0$ by simple rearrangement of the inequality. In general, it is good practice to state the constraint in a way that is intuitive and easy to understand.\nExamples # To introduce the basic principles behind the characterization of solutions of constrained optimization problems, we work through three simple examples. The ideas discussed here will be made rigorous in the sections that follow.\nWe start by noting one item of terminology that recurs throughout the rest of the lecture: At a feasible point $\\mathbf{x}$, the inequality constraint $i \\in \\mathcal{I}$ is said to be active if $c_{i}(\\mathbf{x})=0$ and inactive if the strict inequality $c_{i}(\\mathbf{x})\u0026gt;0$ is satisfied.\nA single equality constraint # Example 1\nOur first example is a two-variable problem with a single equality constraint:\n\\begin{equation} \\min x_{1}+x_{2} \\quad \\text { s.t. } \\quad x_{1}^{2}+x_{2}^{2}-2=0. \\label{eq:equality_example} \\end{equation}\nFigure 5.2: Constraints and gradient of function\nIn the general form, we have $f(\\mathbf{x})=x_{1}+x_{2}, \\mathcal{I}=\\emptyset, \\mathcal{E}=\\{1\\}$, and $c_{1}(\\mathbf{x})=x_{1}^{2}+x_{2}^{2}-2$. We can see by inspection that the feasible set for this problem is the circle of radius $\\sqrt{2}$ centered at the origin-just the boundary of this circle, not its interior. The solution $\\mathbf{x}^{\\star}$ is obviously $(-1,-1)^{\\mathrm{T}}$. From any other point on the circle, it is easy to find a way to move that stays feasible (that is, remains on the circle) while decreasing $f$. For instance, from the point $\\mathbf{x}=(\\sqrt{2}, 0)^{\\mathrm{T}}$ any move in the clockwise direction around the circle has the desired effect.\nWe also see that at the solution $\\mathbf{x}^{\\star}$, the constraint normal $\\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right)$ is parallel to $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)$. That is, there is a scalar $\\lambda_{1}^{\\star}$ such that\n\\begin{equation} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\lambda_{1}^{\\star} \\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right). \\label{eq:parallel_gradients} \\end{equation}\n(In this particular case, we have $\\lambda_{1}^{\\star}=-\\frac{1}{2}$.)\nWe can derive \\eqref{eq:parallel_gradients} by examining first-order Taylor series approximations to the objective and constraint functions. To retain feasibility with respect to the function $c_{1}(\\mathbf{x})=0$, we require that $c_{1}(\\mathbf{x}+\\mathbf{d})=0$; that is,\n$$ 0=c_{1}(\\mathbf{x}+\\mathbf{d}) \\approx c_{1}(\\mathbf{x})+\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}=\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} $$\nHence, the direction $\\mathbf{d}$ retains feasibility with respect to $c_{1}$, to first order, when it satisfies\n$$ \\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}=0 $$\nSimilarly, a direction of improvement must produce a decrease in $f$, so that\n$$ 0\u0026gt;f(\\mathbf{x}+\\mathbf{d})-f(\\mathbf{x}) \\approx \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} $$\nor, to first order,\n$$ \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0 $$\nIf there exists a direction $\\mathbf{d}$ that satisfies both conditions, we conclude that improvement on our current point $\\mathbf{x}$ is possible. It follows that a necessary condition for optimality for the problem \\eqref{eq:equality_example} is that there exist no direction $\\mathbf{d}$ satisfying both conditions.\nBy drawing a picture (see visualization below), the reader can check that the only way that such a direction cannot exist is if $\\nabla f(\\mathbf{x})$ and $\\nabla c_{1}(\\mathbf{x})$ are parallel, that is, if the condition $\\nabla f(\\mathbf{x})=\\lambda_{1} \\nabla c_{1}(\\mathbf{x})$ holds at $\\mathbf{x}$, for some scalar $\\lambda_{1}$. If this condition is not satisfied, the direction defined by\n$$ \\mathbf{d}=-\\left(\\mathbf{I}-\\frac{\\nabla c_{1}(\\mathbf{x}) \\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}}}{\\|\\nabla c_{1}(\\mathbf{x})\\|^{2}}\\right) \\nabla f(\\mathbf{x}) $$\nsatisfies both conditions.\nBy introducing the Lagrangian function\n$$ \\mathcal{L}\\left(\\mathbf{x}, \\lambda_{1}\\right)=f(\\mathbf{x})-\\lambda_{1} c_{1}(\\mathbf{x}), $$\nand noting that $\\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}, \\lambda_{1}\\right)=\\nabla f(\\mathbf{x})-\\lambda_{1} \\nabla c_{1}(\\mathbf{x})$, we can state the condition \\eqref{eq:parallel_gradients} equivalently as follows: At the solution $\\mathbf{x}^{\\star}$, there is a scalar $\\lambda_{1}^{\\star}$ such that\n\\begin{equation} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\lambda_{1}^{\\star}\\right)=0. \\label{eq:lagrangian_gradient_zero} \\end{equation}\nThis observation suggests that we can search for solutions of the equality-constrained problem \\eqref{eq:equality_example} by searching for stationary points of the Lagrangian function. The scalar quantity $\\lambda_{1}$ is called a Lagrange multiplier for the constraint $c_{1}(\\mathbf{x})=0$.\nThough the condition \\eqref{eq:parallel_gradients} (equivalently, \\eqref{eq:lagrangian_gradient_zero}) appears to be necessary for an optimal solution of the problem \\eqref{eq:equality_example}, it is clearly not sufficient. For instance, in this example, \\eqref{eq:parallel_gradients} is satisfied at the point $\\mathbf{x}=(1,1)$ (with $\\lambda_{1}=\\frac{1}{2}$ ), but this point is obviously not a solution-in fact, it maximizes the function $f$ on the circle. Moreover, in the case of equality-constrained problems, we cannot turn the condition \\eqref{eq:parallel_gradients} into a sufficient condition simply by placing some restriction on the sign of $\\lambda_{1}$. To see this, consider replacing the constraint $x_{1}^{2}+x_{2}^{2}-2=0$ by its negative $2-x_{1}^{2}-x_{2}^{2}=0$. The solution of the problem is not affected, but the value of $\\lambda_{1}^{\\star}$ that satisfies the condition \\eqref{eq:parallel_gradients} changes from $\\lambda_{1}^{\\star}=-\\frac{1}{2}$ to $\\lambda_{1}^{\\star}=\\frac{1}{2}$.\nThis situation is illustrated in following visualization:\nA single inequality constraint # Example 2\nThis is a slight modification of Example 1, in which the equality constraint is replaced by an inequality. Consider\n\\begin{equation} \\min x_{1}+x_{2} \\quad \\text { s.t. } \\quad 2-x_{1}^{2}-x_{2}^{2} \\geq 0, \\label{eq:inequality_example} \\end{equation}\nfor which the feasible region consists of the circle of problem \\eqref{eq:equality_example} and its interior. Note that the constraint normal $\\nabla c_{1}$ points toward the interior of the feasible region at each point on the boundary of the circle. By inspection, we see that the solution is still $(-1,-1)$ and that the condition \\eqref{eq:parallel_gradients} holds for the value $\\lambda_{1}^{\\star}=\\frac{1}{2}$. However, this inequality-constrained problem differs from the equality-constrained problem \\eqref{eq:equality_example} in that the sign of the Lagrange multiplier plays a significant role, as we now argue.\nAs before, we conjecture that a given feasible point $\\mathbf{x}$ is not optimal if we can find a step $\\mathbf{d}$ that both retains feasibility and decreases the objective function $f$ to first order. The main difference between problems \\eqref{eq:equality_example} and \\eqref{eq:inequality_example} comes in the handling of the feasibility condition. The direction $\\mathbf{d}$ improves the objective function, to first order, if $\\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0$. Meanwhile, the direction $\\mathbf{d}$ retains feasibility if\n$$ 0 \\leq c_{1}(\\mathbf{x}+\\mathbf{d}) \\approx c_{1}(\\mathbf{x})+\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} $$\nso, to first order, feasibility is retained if\n$$ c_{1}(\\mathbf{x})+\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0 $$\nIn determining whether a direction $\\mathbf{d}$ exists that satisfies both conditions, we consider the following two cases:\nCase I: Consider first the case in which $\\mathbf{x}$ lies strictly inside the circle, so that the strict inequality $c_{1}(\\mathbf{x})\u0026gt;0$ holds. In this case, any vector $\\mathbf{d}$ satisfies the feasibility condition, provided only that its length is sufficiently small. In particular, whenever $\\nabla f\\left(\\mathbf{x}^{\\star}\\right) \\neq \\mathbf{0}$, we can obtain a direction $\\mathbf{d}$ that satisfies both conditions by setting\n$$ \\mathbf{d}=-c_{1}(\\mathbf{x}) \\frac{\\nabla f(\\mathbf{x})}{\\|\\nabla f(\\mathbf{x})\\|} $$\nThe only situation in which such a direction fails to exist is when\n$$ \\nabla f(\\mathbf{x})=\\mathbf{0} . $$\nThis situation is summarized through the following interactive visualization:\nCase II: Consider now the case in which $\\mathbf{x}$ lies on the boundary of the circle, so that $c_{1}(\\mathbf{x})=0$. The conditions therefore become\n$$ \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0, \\quad \\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0 $$\nThe first of these conditions defines an open half-space, while the second defines a closed half-space. It is clear that the two regions fail to intersect only when $\\nabla f(\\mathbf{x})$ and $\\nabla c_{1}(\\mathbf{x})$ point in the same direction, that is, when\n\\begin{equation} \\nabla f(\\mathbf{x})=\\lambda_{1} \\nabla c_{1}(\\mathbf{x}), \\quad \\text { for some } \\lambda_{1} \\geq 0. \\label{eq:inequality_optimality} \\end{equation}\nNote that the sign of the multiplier is significant here. If \\eqref{eq:parallel_gradients} were satisfied with a negative value of $\\lambda_{1}$, then $\\nabla f(\\mathbf{x})$ and $\\nabla c_{1}(\\mathbf{x})$ would point in opposite directions, and we see that the set of directions that satisfy both conditions would make up an entire open half-plane.\nThe optimality conditions for both cases I and II can again be summarized neatly with reference to the Lagrangian function. When no first-order feasible descent direction exists at some point $\\mathbf{x}^{\\star}$, we have that\n\\begin{equation} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\lambda_{1}^{\\star}\\right)=\\mathbf{0}, \\quad \\text { for some } \\lambda_{1}^{\\star} \\geq 0, \\label{eq:kkt_gradient} \\end{equation}\nwhere we also require that\n\\begin{equation} \\lambda_{1}^{\\star} c_{1}\\left(\\mathbf{x}^{\\star}\\right)=0. \\label{eq:complementarity} \\end{equation}\nThis condition is known as a complementarity condition; it implies that the Lagrange multiplier $\\lambda_{1}$ can be strictly positive only when the corresponding constraint $c_{1}$ is active. Conditions of this type play a central role in constrained optimization, as we see in the sections that follow. In case I, we have that $c_{1}\\left(\\mathbf{x}^{\\star}\\right)\u0026gt;0$, so \\eqref{eq:complementarity} requires that $\\lambda_{1}^{\\star}=0$. Hence, \\eqref{eq:kkt_gradient} reduces to $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\mathbf{0}$, as required. In case II, \\eqref{eq:complementarity} allows $\\lambda_{1}^{\\star}$ to take on a nonnegative value, so \\eqref{eq:kkt_gradient} becomes equivalent to \\eqref{eq:inequality_optimality}.\nThis situation is summarized through the following interactive visualization:\nThis situation is also well visualized for quadratic functions:\nTwo inequality constraints # Example 3\nSuppose we add an extra constraint to the problem \\eqref{eq:inequality_example} to obtain\n\\begin{equation} \\min x_{1}+x_{2} \\quad \\text { s.t. } \\quad 2-x_{1}^{2}-x_{2}^{2} \\geq 0, \\quad x_{2} \\geq 0, \\label{eq:two_inequality_example} \\end{equation}\nfor which the feasible region is the half-disk. It is easy to see that the solution lies at $(-\\sqrt{2}, 0)^{\\mathrm{T}}$, a point at which both constraints are active. By repeating the arguments for the previous examples, we conclude that a direction $\\mathbf{d}$ is a feasible descent direction, to first order, if it satisfies the following conditions:\n$$ \\nabla c_{i}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, \\quad i \\in \\mathcal{I}=\\{1,2\\}, \\quad \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0 $$\nHowever, it is clear that no such direction can exist when $\\mathbf{x}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$. The conditions $\\nabla c_{i}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, i=1,2$, are both satisfied only if $\\mathbf{d}$ lies in the quadrant defined by $\\nabla c_{1}(\\mathbf{x})$ and $\\nabla c_{2}(\\mathbf{x})$, but it is clear by inspection that all vectors $\\mathbf{d}$ in this quadrant satisfy $\\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0$.\nLet us see how the Lagrangian and its derivatives behave for the problem \\eqref{eq:two_inequality_example} and the solution point $(-\\sqrt{2}, 0)^{\\mathrm{T}}$. First, we include an additional term $\\lambda_{i} c_{i}(\\mathbf{x})$ in the Lagrangian for each additional constraint, so we have\n\\begin{equation} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=f(\\mathbf{x})-\\lambda_{1} c_{1}(\\mathbf{x})-\\lambda_{2} c_{2}(\\mathbf{x}), \\label{eq:two_constraint_lagrangian} \\end{equation}\nwhere $\\boldsymbol{\\lambda}=\\left(\\lambda_{1}, \\lambda_{2}\\right)^{\\mathrm{T}}$ is the vector of Lagrange multipliers. The extension of condition \\eqref{eq:kkt_gradient} to this case is\n\\begin{equation} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)=\\mathbf{0}, \\quad \\text { for some } \\boldsymbol{\\lambda}^{\\star} \\geq \\mathbf{0}, \\label{eq:two_constraint_kkt} \\end{equation}\nwhere the inequality $\\boldsymbol{\\lambda}^{\\star} \\geq \\mathbf{0}$ means that all components of $\\boldsymbol{\\lambda}^{\\star}$ are required to be nonnegative. By applying the complementarity condition \\eqref{eq:complementarity} to both inequality constraints, we obtain\n\\begin{equation} \\lambda_{1}^{\\star} c_{1}\\left(\\mathbf{x}^{\\star}\\right)=0, \\quad \\lambda_{2}^{\\star} c_{2}\\left(\\mathbf{x}^{\\star}\\right)=0. \\label{eq:two_constraint_complementarity} \\end{equation}\nWhen $\\mathbf{x}^{\\star}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$, we have\n$$ \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad \\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} 2 \\sqrt{2} \\\\ 0 \\end{bmatrix}, \\quad \\nabla c_{2}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, $$\nso that it is easy to verify that $\\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)=\\mathbf{0}$ when we select $\\boldsymbol{\\lambda}^{\\star}$ as follows:\n$$ \\boldsymbol{\\lambda}^{\\star}=\\begin{bmatrix} 1 /(2 \\sqrt{2}) \\\\ 1 \\end{bmatrix} $$\nNote that both components of $\\boldsymbol{\\lambda}^{\\star}$ are positive.\nWe consider now some other feasible points that are not solutions of \\eqref{eq:two_inequality_example}, and examine the properties of the Lagrangian and its gradient at these points.\nFor the point $\\mathbf{x}=(\\sqrt{2}, 0)^{\\mathrm{T}}$, we again have that both constraints are active. However, the objective gradient $\\nabla f(\\mathbf{x})$ no longer lies in the quadrant defined by the conditions $\\nabla c_{i}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, i=1,2$. One first-order feasible descent direction from this point-a vector $\\mathbf{d}$ that satisfies the required conditions-is simply $\\mathbf{d}=(-1,0)^{\\mathrm{T}}$; there are many others. For this value of $\\mathbf{x}$ it is easy to verify that the condition $\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=\\mathbf{0}$ is satisfied only when $\\boldsymbol{\\lambda}=(-1 /(2 \\sqrt{2}), 1)$. Note that the first component $\\lambda_{1}$ is negative, so that the conditions \\eqref{eq:two_constraint_kkt} are not satisfied at this point.\nFinally, let us consider the point $\\mathbf{x}=(1,0)^{\\mathrm{T}}$, at which only the second constraint $c_{2}$ is active. At this point, linearization of $f$ and $c$ gives the following conditions, which must be satisfied for $\\mathbf{d}$ to be a feasible descent direction, to first order:\n$$ 1+\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, \\quad \\nabla c_{2}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, \\quad \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0 . $$\nIn fact, we need worry only about satisfying the second and third conditions, since we can always satisfy the first condition by multiplying $\\mathbf{d}$ by a sufficiently small positive quantity. By noting that\n$ \\nabla f(\\mathbf{x})=\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad \\nabla c_{2}(\\mathbf{x})=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $\nit is easy to verify that the vector $\\mathbf{d}=\\left(-\\frac{1}{2}, \\frac{1}{4}\\right)$ satisfies the required conditions and is therefore a descent direction.\nTo show that optimality conditions \\eqref{eq:two_constraint_kkt} and \\eqref{eq:two_constraint_complementarity} fail, we note first from \\eqref{eq:two_constraint_complementarity} that since $c_{1}(\\mathbf{x})\u0026gt;0$, we must have $\\lambda_{1}=0$. Therefore, in trying to satisfy $\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=\\mathbf{0}$, we are left to search for a value $\\lambda_{2}$ such that $\\nabla f(\\mathbf{x})-\\lambda_{2} \\nabla c_{2}(\\mathbf{x})=\\mathbf{0}$. No such $\\lambda_{2}$ exists, and thus this point fails to satisfy the optimality conditions.\nThe following visualization summarizes the discussion of this example on the active constraints:\nFirst-order optimality conditions # Statement of first-order necessary conditions # The three examples above suggest that a number of conditions are important in the characterization of solutions for the general problem. These include the relation $\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=\\mathbf{0}$, the nonnegativity of $\\lambda_{i}$ for all inequality constraints $c_{i}(\\mathbf{x})$, and the complementarity condition $\\lambda_{i} c_{i}(\\mathbf{x})=0$ that is required for all the inequality constraints. We now generalize the observations made in these examples and state the first-order optimality conditions in a rigorous fashion.\nIn general, the Lagrangian for the constrained optimization problem is defined as\n\\begin{equation} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=f(\\mathbf{x})-\\sum_{i \\in \\mathcal{E} \\cup \\mathcal{I}} \\lambda_{i} c_{i}(\\mathbf{x}). \\label{eq:general_lagrangian} \\end{equation}\nThe active set $\\mathcal{A}(\\mathbf{x})$ at any feasible $\\mathbf{x}$ is the union of the set $\\mathcal{E}$ with the indices of the active inequality constraints; that is,\n\\begin{equation} \\mathcal{A}(\\mathbf{x})=\\mathcal{E} \\cup\\left\\{i \\in \\mathcal{I} \\mid c_{i}(\\mathbf{x})=0\\right\\}. \\label{eq:active_set} \\end{equation}\nNext, we need to give more attention to the properties of the constraint gradients. The vector $\\nabla c_{i}(\\mathbf{x})$ is often called the normal to the constraint $c_{i}$ at the point $\\mathbf{x}$, because it is usually a vector that is perpendicular to the contours of the constraint $c_{i}$ at $\\mathbf{x}$, and in the case of an inequality constraint, it points toward the feasible side of this constraint. It is possible, however, that $\\nabla c_{i}(\\mathbf{x})$ vanishes due to the algebraic representation of $c_{i}$, so that the term $\\lambda_{i} \\nabla c_{i}(\\mathbf{x})$ vanishes for all values of $\\lambda_{i}$ and does not play a role in the Lagrangian gradient $\\nabla_{\\mathbf{x}} \\mathcal{L}$. For instance, if we replaced the constraint in \\eqref{eq:equality_example} by the equivalent condition\n$ c_{1}(\\mathbf{x})=\\left(x_{1}^{2}+x_{2}^{2}-2\\right)^{2}=0 $\nwe would have that $\\nabla c_{1}(\\mathbf{x})=\\mathbf{0}$ for all feasible points $\\mathbf{x}$, and in particular that the condition $\\nabla f(\\mathbf{x})=\\lambda_{1} \\nabla c_{1}(\\mathbf{x})$ no longer holds at the optimal point $(-1,-1)^{\\mathrm{T}}$. We usually make an assumption called a constraint qualification to ensure that such degenerate behavior does not occur at the value of $\\mathbf{x}$ in question. One such constraint qualification-probably the one most often used in the design of algorithms-is the one defined as follows:\nDefinition 5.4 (Linear independence constraint qualification (LICQ))\nGiven the point $\\mathbf{x}^{\\star}$ and the active set $\\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ defined by \\eqref{eq:active_set}, we say that the linear independence constraint qualification (LICQ) holds if the set of active constraint gradients $\\left\\{\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right), i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)\\right\\}$ is linearly independent. Note that if this condition holds, none of the active constraint gradients can be zero.\nThis condition allows us to state the following optimality conditions for a general nonlinear programming problem. These conditions provide the foundation for many of the algorithms described in the remaining chapters of the lecture. They are called first-order conditions because they concern themselves with properties of the gradients (first-derivative vectors) of the objective and constraint functions.\nTheorem 5.1 (First-order necessary conditions)\nSuppose that $\\mathbf{x}^{\\star}$ is a local solution and that the LICQ holds at $\\mathbf{x}^{\\star}$. Then there is a Lagrange multiplier vector $\\boldsymbol{\\lambda}^{\\star}$, with components $\\lambda_{i}^{\\star}, i \\in \\mathcal{E} \\cup \\mathcal{I}$, such that the following conditions are satisfied at $\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)$:\n\\begin{align} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\u0026amp;=\\mathbf{0}, \\label{eq:kkt_gradient_zero} \\\\ c_{i}\\left(\\mathbf{x}^{\\star}\\right)\u0026amp;=0, \\quad \u0026amp;\u0026amp; \\text { for all } i \\in \\mathcal{E}, \\label{eq:kkt_equality} \\\\ c_{i}\\left(\\mathbf{x}^{\\star}\\right)\u0026amp;\\geq 0, \\quad \u0026amp;\u0026amp; \\text { for all } i \\in \\mathcal{I}, \\label{eq:kkt_inequality} \\\\ \\lambda_{i}^{\\star} \u0026amp;\\geq 0, \\quad \u0026amp;\u0026amp; \\text { for all } i \\in \\mathcal{I}, \\label{eq:kkt_multiplier_sign} \\\\ \\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{x}^{\\star}\\right)\u0026amp;=0, \\quad \u0026amp;\u0026amp; \\text { for all } i \\in \\mathcal{E} \\cup \\mathcal{I}. \\label{eq:kkt_complementarity} \\end{align}\nThe conditions \\eqref{eq:kkt_gradient_zero}-\\eqref{eq:kkt_complementarity} are often known as the Karush-Kuhn-Tucker conditions, or KKT conditions for short. Because the complementarity condition implies that the Lagrange multipliers corresponding to inactive inequality constraints are zero, we can omit the terms for indices $i \\notin \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ from \\eqref{eq:kkt_gradient_zero} and rewrite this condition as\n\\begin{equation} \\mathbf{0}=\\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)=\\nabla f\\left(\\mathbf{x}^{\\star}\\right)-\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i}^{\\star} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right). \\label{eq:kkt_active_gradients} \\end{equation}\nA special case of complementarity is important and deserves its own definition:\nDefinition 5.5 (Strict complementarity)\nGiven a local solution $\\mathbf{x}^{\\star}$ and a vector $\\boldsymbol{\\lambda}^{\\star}$ satisfying \\eqref{eq:kkt_gradient_zero}-\\eqref{eq:kkt_complementarity}, we say that the strict complementarity condition holds if exactly one of $\\lambda_{i}^{\\star}$ and $c_{i}\\left(\\mathbf{x}^{\\star}\\right)$ is zero for each index $i \\in \\mathcal{I}$. In other words, we have that $\\lambda_{i}^{\\star}\u0026gt;0$ for each $i \\in \\mathcal{I} \\cap \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$. For a given problem and solution point $\\mathbf{x}^{\\star}$, there may be many vectors $\\boldsymbol{\\lambda}^{\\star}$ for which the conditions \\eqref{eq:kkt_gradient_zero}-\\eqref{eq:kkt_complementarity} are satisfied. When the LICQ holds, however, the optimal $\\boldsymbol{\\lambda}^{\\star}$ is unique.\nExample 4\nConsider the feasible region described by the four constraints:\n\\begin{equation} \\min_{\\mathbf{x}}\\left(x_{1}-\\frac{3}{2}\\right)^{2}+\\left(x_{2}-\\frac{1}{8}\\right)^{4} \\quad \\text { s.t. } \\quad\\begin{bmatrix} 1-x_{1}-x_{2} \\\\ 1-x_{1}+x_{2} \\\\ 1+x_{1}-x_{2} \\\\ 1+x_{1}+x_{2} \\end{bmatrix} \\geq \\mathbf{0}. \\label{eq:diamond_example} \\end{equation}\nIt is fairly clear that the solution is $\\mathbf{x}^{\\star}=(1,0)$. The first and second constraints are active at this point. Denoting them by $c_{1}$ and $c_{2}$ (and the inactive constraints by $c_{3}$ and $c_{4}$ ), we have\n$ \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} -1 \\\\ -\\frac{1}{2} \\end{bmatrix}, \\quad \\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}, \\quad \\nabla c_{2}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} . $\nTherefore, the KKT conditions \\eqref{eq:kkt_gradient_zero}-\\eqref{eq:kkt_complementarity} are satisfied when we set\n$ \\boldsymbol{\\lambda}^{\\star}=\\left(\\frac{3}{4}, \\frac{1}{4}, 0,0\\right)^{\\mathrm{T}}. $\nSensitivity # The convenience of using Lagrange multipliers should now be clear, but what of their intuitive significance? The value of each Lagrange multiplier $\\lambda_{i}^{\\star}$ tells us something about the sensitivity of the optimal objective value $f\\left(\\mathbf{x}^{\\star}\\right)$ to the presence of constraint $c_{i}$. To put it another way, $\\lambda_{i}^{\\star}$ indicates how hard $f$ is \u0026ldquo;pushing\u0026rdquo; or \u0026ldquo;pulling\u0026rdquo; against the particular constraint $c_{i}$. We illustrate this point with a little analysis. When we choose an inactive constraint $i \\notin \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ such that $c_{i}\\left(\\mathbf{x}^{\\star}\\right)\u0026gt;0$, the solution $\\mathbf{x}^{\\star}$ and function value $f\\left(\\mathbf{x}^{\\star}\\right)$ are quite indifferent to whether this constraint is present or not. If we perturb $c_{i}$ by a tiny amount, it will still be inactive and $\\mathbf{x}^{\\star}$ will still be a local solution of the optimization problem. Since $\\lambda_{i}^{\\star}=0$ from \\eqref{eq:kkt_complementarity}, the Lagrange multiplier indicates accurately that constraint $i$ is not significant.\nSuppose instead that constraint $i$ is active, and let us perturb the right-hand-side of this constraint a little, requiring, say, that $c_{i}(\\mathbf{x}) \\geq-\\epsilon\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$ instead of $c_{i}(\\mathbf{x}) \\geq 0$. Suppose that $\\epsilon$ is sufficiently small that the perturbed solution $\\mathbf{x}^{\\star}(\\epsilon)$ still has the same set of active constraints, and that the Lagrange multipliers are not much affected by the perturbation. (These conditions can be made more rigorous with the help of strict complementarity and second-order conditions, as discussed later in the lecture.) We then find that\n\\begin{equation} \\begin{aligned} -\\epsilon\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\| \u0026amp; =c_{i}\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)-c_{i}\\left(\\mathbf{x}^{\\star}\\right) \\approx\\left(\\mathbf{x}^{\\star}(\\epsilon)-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right), \\\\ 0 \u0026amp; =c_{j}\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)-c_{j}\\left(\\mathbf{x}^{\\star}\\right) \\approx\\left(\\mathbf{x}^{\\star}(\\epsilon)-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right), \\end{aligned} \\label{eq:sensitivity_perturbation} \\end{equation}\nfor all $j \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ with $j \\neq i$.\nThe value of $f\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)$, meanwhile, can be estimated with the help of \\eqref{eq:kkt_gradient_zero}. We have\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)-f\\left(\\mathbf{x}^{\\star}\\right) \u0026amp; \\approx\\left(\\mathbf{x}^{\\star}(\\epsilon)-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right) \\\\ \u0026amp; =\\sum_{j \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{j}^{\\star}\\left(\\mathbf{x}^{\\star}(\\epsilon)-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right) \\\\ \u0026amp; \\approx-\\epsilon\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\| \\lambda_{i}^{\\star} \\end{aligned} \\label{eq:sensitivity_objective} \\end{equation}\nBy taking limits, we see that the family of solutions $\\mathbf{x}^{\\star}(\\epsilon)$ satisfies\n\\begin{equation} \\frac{d f\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)}{d \\epsilon}=-\\lambda_{i}^{\\star}\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\| \\label{eq:sensitivity_derivative} \\end{equation}\nA sensitivity analysis of this problem would conclude that if $\\lambda_{i}^{\\star}\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$ is large, then the optimal value is sensitive to the placement of the $i$th constraint, while if this quantity is small, the dependence is not too strong. If $\\lambda_{i}^{\\star}$ is exactly zero for some active constraint, small perturbations to $c_{i}$ in some directions will hardly affect the optimal objective value at all; the change is zero, to first order.\nThis discussion motivates the definition below, which classifies constraints according to whether or not their corresponding Lagrange multiplier is zero.\nDefinition 5.6 (Strongly active and weakly active constraints)\nLet $\\mathbf{x}^{\\star}$ be a solution of the optimization problem, and suppose that the KKT conditions are satisfied. We say that an inequality constraint $c_{i}$ is strongly active or binding if $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ and $\\lambda_{i}^{\\star}\u0026gt;0$ for some Lagrange multiplier $\\boldsymbol{\\lambda}^{\\star}$ satisfying the KKT conditions. We say that $c_{i}$ is weakly active if $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ and $\\lambda_{i}^{\\star}=0$ for all $\\boldsymbol{\\lambda}^{\\star}$ satisfying the KKT conditions. Note that the analysis above is independent of scaling of the individual constraints. For instance, we might change the formulation of the problem by replacing some active constraint $c_{i}$ by $10 c_{i}$. The new problem will actually be equivalent (that is, it has the same feasible set and same solution), but the optimal multiplier $\\lambda_{i}^{\\star}$ corresponding to $c_{i}$ will be replaced by $\\lambda_{i}^{\\star} / 10$. However, since $\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$ is replaced by $10\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$, the product $\\lambda_{i}^{\\star}\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$ does not change. If, on the other hand, we replace the objective function $f$ by $10 f$, the multipliers $\\lambda_{i}^{\\star}$ in the KKT conditions all will need to be replaced by $10 \\lambda_{i}^{\\star}$. Hence in \\eqref{eq:sensitivity_derivative} we see that the sensitivity of $f$ to perturbations has increased by a factor of 10, which is exactly what we would expect.\nDerivation of the first-order conditions # Having studied some motivating examples, observed the characteristics of optimal and nonoptimal points, and stated the KKT conditions, we now describe a complete proof of Theorem 5.1. This analysis is not just of esoteric interest, but is rather the key to understanding all constrained optimization algorithms.\nFeasible sequences # The first concept we introduce is that of a feasible sequence. Given a feasible point $\\mathbf{x}^{\\star}$, a sequence $\\left\\{\\mathbf{z}_{k}\\right\\}_{k=0}^{\\infty}$ with $\\mathbf{z}_{k} \\in \\mathbb{R}^{n}$ is a feasible sequence if the following properties hold: (i) $\\mathbf{z}_{k} \\neq \\mathbf{x}^{\\star}$ for all $k$; (ii) $\\lim_{k \\rightarrow \\infty} \\mathbf{z}_{k}=\\mathbf{x}^{\\star}$; (iii) $\\mathbf{z}_{k}$ is feasible for all sufficiently large values of $k$.\nFor later reference, we denote the set of all possible feasible sequences approaching $\\mathbf{x}$ by $\\mathcal{T}(\\mathbf{x})$.\nWe characterize a local solution as a point $\\mathbf{x}$ at which all feasible sequences have the property that $f\\left(\\mathbf{z}_{k}\\right) \\geq f(\\mathbf{x})$ for all $k$ sufficiently large. We derive practical, verifiable conditions under which this property holds. To do so we will make use of the concept of a limiting direction of a feasible sequence.\nLimiting directions of a feasible sequence are vectors $\\mathbf{d}$ such that we have\n\\begin{equation} \\lim_{\\mathbf{z}_{k} \\in \\mathcal{S}_{\\mathbf{d}}} \\frac{\\mathbf{z}_{k}-\\mathbf{x}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}\\right\\|} \\rightarrow \\mathbf{d} \\label{eq:limiting_direction} \\end{equation}\nwhere $\\mathcal{S}_{\\mathbf{d}}$ is some subsequence of $\\left\\{\\mathbf{z}_{k}\\right\\}_{k=0}^{\\infty}$. In general, a feasible sequence has at least one limiting direction and may have more than one. To see this, note that the sequence of vectors defined by\n\\begin{equation} \\mathbf{d}_{k}=\\frac{\\mathbf{z}_{k}-\\mathbf{x}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}\\right\\|} \\label{eq:normalized_direction} \\end{equation}\nlies on the surface of the unit sphere, which is a compact set, and thus there is at least one limit point $\\mathbf{d}$. Moreover, all such points are limiting directions by the definition \\eqref{eq:limiting_direction}. If we have some sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$ with limiting direction $\\mathbf{d}$ and corresponding subsequence $\\mathcal{S}_{\\mathbf{d}}$, we can construct another feasible sequence $\\left\\{\\overline{\\mathbf{z}}_{k}\\right\\}$ such that\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\frac{\\overline{\\mathbf{z}}_{k}-\\mathbf{x}}{\\left\\|\\overline{\\mathbf{z}}_{k}-\\mathbf{x}\\right\\|}=\\mathbf{d} \\label{eq:unique_limiting_direction} \\end{equation}\n(that is, with a unique limit point) by simply defining each $\\overline{\\mathbf{z}}_{k}$ to be an element from the subsequence $\\mathcal{S}_{\\mathbf{d}}$.\nWe illustrate these concepts by revisiting the equality-constrained example.\nExample 1 (Equality-constrained example, revisited)\nThe figure shows a closeup of the equality-constrained problem in which the feasible set is a circle of radius $\\sqrt{2}$, near the nonoptimal point $\\mathbf{x}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$. The figure also shows a feasible sequence approaching $\\mathbf{x}$. This sequence could be defined analytically by the formula\n\\begin{equation} \\mathbf{z}_{k}=\\begin{bmatrix} -\\sqrt{2-1 / k^{2}} \\\\ -1 / k \\end{bmatrix}. \\label{eq:example_sequence_1} \\end{equation}\nThe vector $\\mathbf{d}=(0,-1)^{\\mathrm{T}}$ is a limiting direction of this feasible sequence. Note that $\\mathbf{d}$ is tangent to the feasible sequence at $\\mathbf{x}$ but points in the opposite direction. The objective function $f(\\mathbf{x})=x_{1}+x_{2}$ increases as we move along the sequence \\eqref{eq:example_sequence_1}; in fact, we have $f\\left(\\mathbf{z}_{k+1}\\right)\u0026gt;f\\left(\\mathbf{z}_{k}\\right)$ for all $k=2,3, \\ldots$. It follows that $f\\left(\\mathbf{z}_{k}\\right)\u0026lt;f(\\mathbf{x})$ for $k=2,3, \\ldots$. Hence, $\\mathbf{x}$ cannot be a solution.\nAnother feasible sequence is one that approaches $\\mathbf{x}^{\\star}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$ from the opposite direction. Its elements are defined by\n\\begin{equation} \\mathbf{z}_{k}=\\begin{bmatrix} -\\sqrt{2-1 / k^{2}} \\\\ 1 / k \\end{bmatrix}. \\label{eq:example_sequence_2} \\end{equation}\nIt is easy to show that $f$ decreases along this sequence and that its limiting direction is $\\mathbf{d}=(0,1)^{\\mathrm{T}}$. Other feasible sequences are obtained by combining elements from the two sequences already discussed, for instance\n\\begin{equation} \\mathbf{z}_{k}= \\begin{cases}\\left(-\\sqrt{2-1 / k^{2}}, 1 / k\\right)^{\\mathrm{T}}, \u0026amp; \\text { when } k \\text { is a multiple of } 3 \\\\ \\left(-\\sqrt{2-1 / k^{2}},-1 / k\\right)^{\\mathrm{T}}, \u0026amp; \\text { otherwise. }\\end{cases} \\label{eq:example_sequence_3} \\end{equation}\nIn general, feasible sequences of points approaching $(-\\sqrt{2}, 0)^{\\mathrm{T}}$ will have two limiting directions, $(0,1)^{\\mathrm{T}}$ and $(0,-1)^{\\mathrm{T}}$.\nWe now consider feasible sequences and limiting directions for an example that involves inequality constraints.\nExample 2 (Inequality-constrained example, revisited)\nWe now reconsider the inequality-constrained problem. The solution $\\mathbf{x}^{\\star}=(-1,-1)^{\\mathrm{T}}$ is the same as in the equality-constrained case, but there is a much more extensive collection of feasible sequences that converge to any given feasible point. From the point $\\mathbf{x}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$, the various feasible sequences defined above for the equality-constrained problem are still feasible for the inequality-constrained problem. There are also infinitely many feasible sequences that converge to $\\mathbf{x}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$ along a straight line from the interior of the circle. These are defined by\n\\begin{equation} \\mathbf{z}_{k}=(-1,0)^{\\mathrm{T}}+(1 / k) \\mathbf{w}, \\label{eq:straight_line_sequence} \\end{equation}\nwhere $\\mathbf{w}$ is any vector whose first component is positive ($w_{1}\u0026gt;0$). Now, $\\mathbf{z}_{k}$ is feasible, provided that $\\left\\|\\mathbf{z}_{k}\\right\\| \\leq 1$, that is,\n\\begin{equation} \\left(-1+w_{1} / k\\right)^{2}+\\left(w_{2} / k\\right)^{2} \\leq 1, \\label{eq:feasibility_condition} \\end{equation}\na condition that is satisfied, provided that $k\u0026gt;\\left(2 w_{1}\\right) /\\left(w_{1}^{2}+w_{2}^{2}\\right)$. In addition to these straight-line feasible sequences, we can also define an infinite variety of sequences that approach $(-\\sqrt{2}, 0)^{\\mathrm{T}}$ along a curve from the interior of the circle or that make the approach in a seemingly random fashion.\nGiven a point $\\mathbf{x}$, if it is possible to choose a feasible sequence from $\\mathcal{T}(\\mathbf{x})$ such that the first-order approximation to the objective function actually increases monotonically along the sequence, then $\\mathbf{x}$ must not be optimal. This condition is the fundamental first-order necessary condition, and we state it formally in the following theorem.\nTheorem 5.2 (First-order necessary condition for feasible sequences)\nIf $\\mathbf{x}^{\\star}$ is a local solution, then all feasible sequences $\\left\\{\\mathbf{z}_{k}\\right\\}$ in $\\mathcal{T}\\left(\\mathbf{x}^{\\star}\\right)$ must satisfy\n\\begin{equation} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d} \\geq 0 \\label{eq:first_order_feasible_condition} \\end{equation}\nwhere $\\mathbf{d}$ is any limiting direction of the feasible sequence.\nProof\nSuppose that there is a feasible sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$ with the property $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}\u0026lt;0$, for some limiting direction $\\mathbf{d}$, and let $\\mathcal{S}_{\\mathbf{d}}$ be the subsequence of $\\left\\{\\mathbf{z}_{k}\\right\\}$ that approaches $\\mathbf{x}^{\\star}$. By Taylor\u0026rsquo;s theorem, we have for any $\\mathbf{z}_{k} \\in \\mathcal{S}_{\\mathbf{d}}$ that\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{z}_{k}\\right) \u0026amp; =f\\left(\\mathbf{x}^{\\star}\\right)+\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\\\ \u0026amp; =f\\left(\\mathbf{x}^{\\star}\\right)+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\mathbf{d}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\end{aligned} \\label{eq:taylor_expansion_proof} \\end{equation}\nSince $\\mathbf{d}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)\u0026lt;0$, we have that the remainder term is eventually dominated by the first-order term, that is,\n\\begin{equation} f\\left(\\mathbf{z}_{k}\\right)\u0026lt;f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\mathbf{d}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right), \\quad \\text { for all } k \\text { sufficiently large. } \\label{eq:contradiction_inequality} \\end{equation}\nHence, given any open neighborhood of $\\mathbf{x}^{\\star}$, we can choose $k$ sufficiently large that $\\mathbf{z}_{k}$ lies within this neighborhood and has a lower value of the objective $f$. Therefore, $\\mathbf{x}^{\\star}$ is not a local solution.\n■ This theorem tells us why we can ignore constraints that are strictly inactive (that is, constraints for which $c_{i}(\\mathbf{x})\u0026gt;0$) in formulating optimality conditions. The theorem does not use the whole range of properties of the feasible sequence, but rather one specific property: the limiting directions of $\\left\\{\\mathbf{z}_{k}\\right\\}$. Because of the way in which the limiting directions are defined, it is clear that only the asymptotic behavior of the sequence is relevant, that is, its behavior for large values of the index $k$. If some constraint $i \\in \\mathcal{I}$ is inactive at $\\mathbf{x}$, then we have $c_{i}\\left(\\mathbf{z}_{k}\\right)\u0026gt;0$ for all $k$ sufficiently large, so that a constraint that is inactive at $\\mathbf{x}$ is also inactive at all sufficiently advanced elements of the feasible sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$.\nCharacterizing limiting directions: constraint qualifications # Theorem 5.2 is quite general, but it is not very useful as stated, because it seems to require knowledge of all possible limiting directions for all feasible sequences $\\mathcal{T}\\left(\\mathbf{x}^{\\star}\\right)$. In this section we show that constraint qualifications allow us to characterize the salient properties of $\\mathcal{T}\\left(\\mathbf{x}^{\\star}\\right)$, and therefore make the condition \\eqref{eq:first_order_feasible_condition} easier to verify.\nOne frequently used constraint qualification is the linear independence constrained qualification (LICQ) given in Definition 5.4 . The following lemma shows that when LICQ holds, there is a neat way to characterize the set of all possible limiting directions $\\mathbf{d}$ in terms of the gradients $\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)$ of the active constraints at $\\mathbf{x}^{\\star}$.\nIn subsequent results we introduce the notation $\\mathbf{A}$ to represent the matrix whose rows are the active constraint gradients at the optimal point, that is,\n\\begin{equation} \\nabla c_{i}^{\\star}=\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right), \\quad \\mathbf{A}^{\\mathrm{T}}=\\left[\\nabla c_{i}^{\\star}\\right]_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)}, \\quad \\nabla f^{\\star}=\\nabla f\\left(\\mathbf{x}^{\\star}\\right), \\label{eq:matrix_notation} \\end{equation}\nwhere the active set $\\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ is defined as in \\eqref{eq:active_set}.\nLemma 5.1 (Characterization of limiting directions)\nThe following two statements are true. (i) If $\\mathbf{d} \\in \\mathbb{R}^{n}$ is a limiting direction of a feasible sequence, then\n\\begin{equation} \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}=0, \\quad \\text { for all } i \\in \\mathcal{E}, \\quad \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0, \\quad \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}. \\label{eq:limiting_direction_conditions} \\end{equation}\n(ii) If \\eqref{eq:limiting_direction_conditions} holds with $\\left\\|\\mathbf{d}\\right\\|=1$ and the LICQ condition is satisfied, then $\\mathbf{d} \\in \\mathbb{R}^{n}$ is a limiting direction of some feasible sequence.\nProof\nWithout loss of generality, let us assume that all the constraints $c_{i}(\\cdot), i=1,2, \\ldots, m$, are active. (We can arrive at this convenient ordering by simply dropping all inactive constraints—which are irrelevant in some neighborhood of $\\mathbf{x}^{\\star}$—and renumbering the active constraints that remain.)\nTo prove (i), let $\\left\\{\\mathbf{z}_{k}\\right\\} \\in \\mathcal{T}\\left(\\mathbf{x}^{\\star}\\right)$ be some feasible sequence for which $\\mathbf{d}$ is a limiting direction, and assume (by taking a subsequence if necessary) that\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\frac{\\mathbf{z}_{k}-\\mathbf{x}^{\\star}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}=\\mathbf{d} \\label{eq:limiting_definition} \\end{equation}\nFrom this definition, we have that\n\\begin{equation} \\mathbf{z}_{k}=\\mathbf{x}^{\\star}+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right). \\label{eq:sequence_expansion} \\end{equation}\nBy taking $i \\in \\mathcal{E}$ and using Taylor\u0026rsquo;s theorem, we have that\n\\begin{equation} \\begin{aligned} 0 \u0026amp; =\\frac{1}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} c_{i}\\left(\\mathbf{z}_{k}\\right) \\\\ \u0026amp; =\\frac{1}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}\\left[c_{i}\\left(\\mathbf{x}^{\\star}\\right)+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)\\right] \\\\ \u0026amp; =\\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}+\\frac{o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} \\end{aligned} \\label{eq:equality_constraint_proof} \\end{equation}\nBy taking the limit as $k \\rightarrow \\infty$, the last term in this expression vanishes, and we have $\\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}=0$, as required. For the active inequality constraints $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$, we have similarly that\n\\begin{equation} \\begin{aligned} 0 \u0026amp; \\leq \\frac{1}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} c_{i}\\left(\\mathbf{z}_{k}\\right) \\\\ \u0026amp; =\\frac{1}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}\\left[c_{i}\\left(\\mathbf{x}^{\\star}\\right)+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)\\right] \\\\ \u0026amp; =\\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}+\\frac{o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} \\end{aligned} \\label{eq:inequality_constraint_proof} \\end{equation}\nHence, by a similar limiting argument, we have that $\\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d} \\geq 0$, as required.\nFor (ii), we use the implicit function theorem. First, since the LICQ holds, we have from\nDefinition 5.4\nthat the $m \\times n$ matrix $\\mathbf{A}$ of active constraint gradients has full row rank $m$. Let $\\mathbf{Z}$ be a matrix whose columns are a basis for the null space of $\\mathbf{A}$; that is,\n\\begin{equation} \\mathbf{Z} \\in \\mathbb{R}^{n \\times(n-m)}, \\quad \\mathbf{Z} \\text{ has full column rank }, \\quad \\mathbf{A} \\mathbf{Z}=\\mathbf{0}. \\label{eq:null_space_basis} \\end{equation}\nLet $\\mathbf{d}$ have the properties \\eqref{eq:limiting_direction_conditions}, and suppose that $\\left\\{t_{k}\\right\\}_{k=0}^{\\infty}$ is any sequence of positive scalars such $\\lim_{k \\rightarrow \\infty} t_{k}=0$. Define the parametrized system of equations $\\mathbf{R}: \\mathbb{R}^{n} \\times \\mathbb{R} \\rightarrow \\mathbb{R}^{n}$ by\n\\begin{equation} \\mathbf{R}(\\mathbf{z}, t)=\\begin{bmatrix} \\mathbf{c}(\\mathbf{z})-t \\mathbf{A} \\mathbf{d} \\\\ \\mathbf{Z}^{\\mathrm{T}}\\left(\\mathbf{z}-\\mathbf{x}^{\\star}-t \\mathbf{d}\\right) \\end{bmatrix}=\\begin{bmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{bmatrix} \\label{eq:parametrized_system} \\end{equation}\nWe claim that for each $t=t_{k}$, the solutions $\\mathbf{z}=\\mathbf{z}_{k}$ of this system for small $t\u0026gt;0$ give a feasible sequence that approaches $\\mathbf{x}^{\\star}$.\nClearly, for $t=0$, the solution of \\eqref{eq:parametrized_system} is $\\mathbf{z}=\\mathbf{x}^{\\star}$, and the Jacobian of $\\mathbf{R}$ at this point is\n\\begin{equation} \\nabla_{\\mathbf{z}} \\mathbf{R}\\left(\\mathbf{x}^{\\star}, 0\\right)=\\begin{bmatrix} \\mathbf{A} \\\\ \\mathbf{Z}^{\\mathrm{T}} \\end{bmatrix}, \\label{eq:jacobian_matrix} \\end{equation}\nwhich is nonsingular by construction of $\\mathbf{Z}$. Hence, according to the implicit function theorem, the system \\eqref{eq:parametrized_system} has a unique solution $\\mathbf{z}_{k}$ for all values of $t_{k}$ sufficiently small. Moreover, we have from \\eqref{eq:parametrized_system} and \\eqref{eq:limiting_direction_conditions} that\n\\begin{equation} \\begin{aligned} i \\in \\mathcal{E} \u0026amp; \\Rightarrow c_{i}\\left(\\mathbf{z}_{k}\\right)=t_{k} \\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}=0, \\\\ i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \u0026amp; \\Rightarrow c_{i}\\left(\\mathbf{z}_{k}\\right)=t_{k} \\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d} \\geq 0, \\end{aligned} \\label{eq:feasibility_proof} \\end{equation}\nso that $\\mathbf{z}_{k}$ is indeed feasible. Also, for any positive value $t=\\bar{t}\u0026gt;0$, we cannot have $\\mathbf{z}(t)=\\mathbf{x}^{\\star}$, since otherwise by substituting $(\\mathbf{z}, t)=\\left(\\mathbf{x}^{\\star}, \\bar{t}\\right)$ into \\eqref{eq:parametrized_system}, we obtain\n\\begin{equation} \\begin{bmatrix} \\mathbf{c}\\left(\\mathbf{x}^{\\star}\\right)-\\bar{t} \\mathbf{A} \\mathbf{d} \\\\ -\\mathbf{Z}^{\\mathrm{T}}(\\bar{t} \\mathbf{d}) \\end{bmatrix}=\\begin{bmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{bmatrix}. \\label{eq:contradiction_system} \\end{equation}\nSince $\\mathbf{c}\\left(\\mathbf{x}^{\\star}\\right)=\\mathbf{0}$ (we have assumed that all constraints are active) and $\\bar{t}\u0026gt;0$, we have from the full rank of the matrix in \\eqref{eq:jacobian_matrix} that $\\mathbf{d}=\\mathbf{0}$, which contradicts $\\left\\|\\mathbf{d}\\right\\|=1$. It follows that $\\mathbf{z}_{k}=\\mathbf{z}\\left(t_{k}\\right) \\neq \\mathbf{x}^{\\star}$ for all $k$.\nIt remains to show that $\\mathbf{d}$ is a limiting direction of $\\left\\{\\mathbf{z}_{k}\\right\\}$. Using the fact that $\\mathbf{R}\\left(\\mathbf{z}_{k}, t_{k}\\right)=\\mathbf{0}$ for all $k$ together with Taylor\u0026rsquo;s theorem, we find that\n\\begin{equation} \\begin{aligned} \\mathbf{0}=\\mathbf{R}\\left(\\mathbf{z}_{k}, t_{k}\\right) \u0026amp; =\\begin{bmatrix} \\mathbf{c}\\left(\\mathbf{z}_{k}\\right)-t_{k} \\mathbf{A} \\mathbf{d} \\\\ \\mathbf{Z}^{\\mathrm{T}}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}-t_{k} \\mathbf{d}\\right) \\end{bmatrix} \\\\ \u0026amp; =\\begin{bmatrix} \\mathbf{A}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)-t_{k} \\mathbf{A} \\mathbf{d} \\\\ \\mathbf{Z}^{\\mathrm{T}}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}-t_{k} \\mathbf{d}\\right) \\end{bmatrix} \\\\ \u0026amp; =\\begin{bmatrix} \\mathbf{A} \\\\ \\mathbf{Z}^{\\mathrm{T}} \\end{bmatrix}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}-t_{k} \\mathbf{d}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\end{aligned} \\label{eq:taylor_system} \\end{equation}\nBy dividing this expression by $\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|$ and using nonsingularity of the coefficient matrix in the first term, we obtain\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\mathbf{d}_{k}-\\frac{t_{k}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} \\mathbf{d}=\\mathbf{0}, \\quad \\text { where } \\mathbf{d}_{k}=\\frac{\\mathbf{z}_{k}-\\mathbf{x}^{\\star}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} \\label{eq:direction_limit} \\end{equation}\nSince $\\left\\|\\mathbf{d}_{k}\\right\\|=1$ for all $k$ and since $\\left\\|\\mathbf{d}\\right\\|=1$, we must have\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\frac{t_{k}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}=1 \\label{eq:scaling_limit} \\end{equation}\n(We leave the simple proof by contradiction of this statement as an exercise.) Hence, from \\eqref{eq:direction_limit}, we have $\\lim_{k \\rightarrow \\infty} \\mathbf{d}_{k}=\\mathbf{d}$, as required.\n■ The set of directions defined by \\eqref{eq:limiting_direction_conditions} plays a central role in the optimality conditions, so for future reference we give this set a name and define it formally.\nDefinition 5.7 (Linearized feasible directions)\nGiven a point $\\mathbf{x}^{\\star}$ and the active constraint set $\\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ defined by \\eqref{eq:active_set}, the set $F_{1}$ is defined by\n\\begin{equation} F_{1}=\\left\\{\\alpha \\mathbf{d} \\mid \\alpha\u0026gt;0, \\quad \\begin{array}{ll} \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}=0, \u0026amp; \\text { for all } i \\in \\mathcal{E} \\\\ \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0, \u0026amp; \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\end{array}\\right\\} \\label{eq:linearized_feasible_set} \\end{equation}\nNote that $F_{1}$ is a cone. In fact, when a constraint qualification is satisfied, $F_{1}$ is the tangent cone to the feasible set at $\\mathbf{x}^{\\star}$.\nIntroducing Lagrange multipliers # lemma 5.1 tells us that when the LICQ holds, the cone $F_{1}$ is simply the set of all positive multiples of all limiting directions of all possible feasible sequences. Therefore, the condition \\eqref{eq:first_order_feasible_condition} of Theorem 5.2 holds if $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}\u0026lt;0$ for all $\\mathbf{d} \\in F_{1}$. This condition, too, would appear to be impossible to check, since the set $F_{1}$ contains infinitely many vectors in general. The next lemma gives an alternative, practical way to check this condition that makes use of the Lagrange multipliers, the variables $\\lambda_{i}$ that were introduced in the definition of the Lagrangian $\\mathcal{L}$. Lemma 5.2 (Characterization using Lagrange multipliers)\nThere is no direction $\\mathbf{d} \\in F_{1}$ for which $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}\u0026lt;0$ if and only if there exists a vector $\\boldsymbol{\\lambda} \\in \\mathbb{R}^{m}$ with \\begin{equation} \\nabla f^{\\star}=\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i} \\nabla c_{i}^{\\star}=\\mathbf{A}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\boldsymbol{\\lambda}, \\quad \\lambda_{i} \\geq 0 \\text { for } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\label{eq:lagrange_condition} \\end{equation} Proof\nIf we define the cone $N$ by \\begin{equation} N=\\left\\{\\mathbf{s} \\mid \\mathbf{s}=\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i} \\nabla c_{i}^{\\star}, \\quad \\lambda_{i} \\geq 0 \\text { for } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}\\right\\} \\label{eq:cone_n_definition} \\end{equation} then the condition \\eqref{eq:lagrange_condition} is equivalent to $\\nabla f^{\\star} \\in N$. We note first that the set $N$ is closed—a fact that is intuitively clear but nontrivial to prove rigorously. We prove the forward implication by supposing that \\eqref{eq:lagrange_condition} holds and choosing $\\mathbf{d}$ to be any vector satisfying \\eqref{eq:limiting_direction_conditions}. We then have that \\begin{equation} \\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}=\\sum_{i \\in \\mathcal{E}} \\lambda_{i}\\left(\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}\\right)+\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}} \\lambda_{i}\\left(\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}\\right) \\label{eq:forward_implication} \\end{equation} The first summation is zero because $\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}=0$ for $i \\in \\mathcal{E}$, while the second term is nonnegative because $\\lambda_{i} \\geq 0$ and $\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0$ for $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$. Hence $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star} \\geq 0$. For the reverse implication, we show that if $\\nabla f^{\\star}$ does not satisfy \\eqref{eq:lagrange_condition} (that is, $\\nabla f^{\\star} \\notin N$), then we can find a vector $\\mathbf{d}$ for which $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}\u0026lt;0$ and \\eqref{eq:limiting_direction_conditions} holds. Let $\\hat{\\mathbf{s}}$ be the vector in $N$ that is closest to $\\nabla f^{\\star}$. Because $N$ is closed, $\\hat{\\mathbf{s}}$ is well-defined. In fact, $\\hat{\\mathbf{s}}$ solves the constrained optimization problem \\begin{equation} \\min \\left|\\mathbf{s}-\\nabla f^{\\star}\\right|_{2}^{2} \\quad \\text { subject to } \\mathbf{s} \\in N \\label{eq:projection_problem} \\end{equation} Since $\\hat{\\mathbf{s}} \\in N$, we also have $t \\hat{\\mathbf{s}} \\in N$ for all scalars $t \\geq 0$. Since $\\left|t \\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right|_{2}^{2}$ is minimized at $t=1$, we have \\begin{equation} \\begin{aligned} \\left.\\frac{d}{d t}\\left|t \\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right|{2}^{2}\\right|{t=1}=0 \u0026amp; \\Rightarrow\\left.\\left(-2 \\hat{\\mathbf{s}}^{\\mathrm{T}} \\nabla f^{\\star}+2 t \\hat{\\mathbf{s}}^{\\mathrm{T}} \\hat{\\mathbf{s}}\\right)\\right|_{t=1}=0 \\ \u0026amp; \\Rightarrow \\hat{\\mathbf{s}}^{\\mathrm{T}}\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right)=0 \\end{aligned} \\label{eq:orthogonality_condition} \\end{equation} Now, let $\\mathbf{s}$ be any other vector in $N$. Since $N$ is convex, we have by the minimizing property of $\\hat{\\mathbf{s}}$ that \\begin{equation} \\left|\\hat{\\mathbf{s}}+\\theta(\\mathbf{s}-\\hat{\\mathbf{s}})-\\nabla f^{\\star}\\right|{2}^{2} \\geq\\left|\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right|{2}^{2} \\quad \\text { for all } \\theta \\in[0,1] \\label{eq:convexity_property} \\end{equation} and hence \\begin{equation} 2 \\theta(\\mathbf{s}-\\hat{\\mathbf{s}})^{\\mathrm{T}}\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right)+\\theta^{2}\\left|\\mathbf{s}-\\hat{\\mathbf{s}}\\right|_{2}^{2} \\geq 0 \\label{eq:quadratic_expansion} \\end{equation} By dividing this expression by $\\theta$ and taking the limit as $\\theta \\downarrow 0$, we have $(\\mathbf{s}-\\hat{\\mathbf{s}})^{\\mathrm{T}}\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right) \\geq 0$. Therefore, because of \\eqref{eq:orthogonality_condition}, \\begin{equation} \\mathbf{s}^{\\mathrm{T}}\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right) \\geq 0, \\quad \\text { for all } \\mathbf{s} \\in N \\label{eq:separation_property} \\end{equation} We claim now that the vector \\begin{equation} \\mathbf{d}=\\hat{\\mathbf{s}}-\\nabla f^{\\star} \\label{eq:descent_direction} \\end{equation} satisfies both \\eqref{eq:limiting_direction_conditions} and $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}\u0026lt;0$. Note that $\\mathbf{d} \\neq \\mathbf{0}$ because $\\nabla f^{\\star}$ does not belong to the cone $N$. We have from \\eqref{eq:orthogonality_condition} that \\begin{equation} \\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}=\\mathbf{d}^{\\mathrm{T}}(\\hat{\\mathbf{s}}-\\mathbf{d})=\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right)^{\\mathrm{T}} \\hat{\\mathbf{s}}-\\mathbf{d}^{\\mathrm{T}} \\mathbf{d}=-\\left|\\mathbf{d}\\right|_{2}^{2}\u0026lt;0 \\label{eq:descent_property} \\end{equation} so that $\\mathbf{d}$ satisfies the descent property. By making appropriate choices of coefficients $\\lambda_{i}, i=1,2, \\ldots, m$, it is easy to see that \\begin{equation} \\begin{aligned} i \\in \\mathcal{E} \u0026amp; \\Rightarrow \\nabla c_{i}^{\\star} \\in N \\quad \\text { and } \\quad -\\nabla c_{i}^{\\star} \\in N \\ i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \u0026amp; \\Rightarrow \\nabla c_{i}^{\\star} \\in N \\end{aligned} \\label{eq:gradient_membership} \\end{equation} Hence, from \\eqref{eq:separation_property}, we have by substituting $\\mathbf{d}=\\hat{\\mathbf{s}}-\\nabla f^{\\star}$ and the particular choices $\\mathbf{s}=\\nabla c_{i}^{\\star}$ and $\\mathbf{s}=-\\nabla c_{i}^{\\star}$ that \\begin{equation} \\begin{aligned} i \\in \\mathcal{E} \u0026amp; \\Rightarrow \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0 \\quad \\text { and } \\quad -\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0 \\Rightarrow \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}=0 \\\\ i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \u0026amp; \\Rightarrow \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0 \\end{aligned} \\label{eq:direction_conditions_verified} \\end{equation} Therefore, $\\mathbf{d}$ also satisfies \\eqref{eq:limiting_direction_conditions}, so the reverse implication is proved. ■ Proof of the first-order necessary conditions lemma 5.1 and lemma 5.2 can be combined to give the KKT conditions described in Theorem 5.1. Suppose that $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ is a feasible point at which the LICQ holds. The theorem claims that if $\\mathbf{x}^{\\star}$ is a local solution, then there is a vector $\\boldsymbol{\\lambda}^{\\star} \\in \\mathbb{R}^{m}$ that satisfies the KKT conditions. We show first that there are multipliers $\\lambda_{i}, i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$, such that \\eqref{eq:lagrange_condition} is satisfied. Theorem 5.2 tells us that $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star} \\geq 0$ for all vectors $\\mathbf{d}$ that are limiting directions of feasible sequences. From lemma 5.1 , we know that when LICQ holds, the set of all possible limiting directions is exactly the set of vectors that satisfy the conditions \\eqref{eq:limiting_direction_conditions}. By putting these two statements together, we find that all directions $\\mathbf{d}$ that satisfy \\eqref{eq:limiting_direction_conditions} must also have $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star} \\geq 0$. Hence, from lemma 5.2 , we have that there is a vector $\\boldsymbol{\\lambda}$ for which \\eqref{eq:lagrange_condition} holds, as claimed. We now define the vector $\\boldsymbol{\\lambda}^{\\star}$ by \\begin{equation} \\lambda_{i}^{\\star}= \\begin{cases}\\lambda_{i}, \u0026amp; i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\ 0, \u0026amp; \\text { otherwise }\\end{cases} \\label{eq:multiplier_definition} \\end{equation} and show that this choice of $\\boldsymbol{\\lambda}^{\\star}$, together with our local solution $\\mathbf{x}^{\\star}$, satisfies the KKT conditions. We check these conditions in turn.\nThe condition \\eqref{eq:kkt_gradient_zero} follows immediately from \\eqref{eq:lagrange_condition} and the definitions of the Lagrangian function and \\eqref{eq:multiplier_definition} of $\\boldsymbol{\\lambda}^{\\star}$. Since $\\mathbf{x}^{\\star}$ is feasible, the conditions \\eqref{eq:kkt_equality} and \\eqref{eq:kkt_inequality} are satisfied. We have from \\eqref{eq:lagrange_condition} that $\\lambda_{i}^{\\star} \\geq 0$ for $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$, while from \\eqref{eq:multiplier_definition}, $\\lambda_{i}^{\\star}=0$ for $i \\in \\mathcal{I} \\backslash \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$. Hence, $\\lambda_{i}^{\\star} \\geq 0$ for $i \\in \\mathcal{I}$, so that \\eqref{eq:kkt_multiplier_sign} holds. We have for $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$ that $c_{i}\\left(\\mathbf{x}^{\\star}\\right)=0$, while for $i \\in \\mathcal{I} \\backslash \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$, we have $\\lambda_{i}^{\\star}=0$. Hence $\\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{x}^{\\star}\\right)=0$ for $i \\in \\mathcal{I}$, so that \\eqref{eq:kkt_complementarity} is satisfied as well.\nThis completes the proof.\nSecond-order conditions # So far, we have described the first-order conditions—the KKT conditions—which tell us how the first derivatives of $f$ and the active constraints $c_{i}$ are related at $\\mathbf{x}^{\\star}$. When these conditions are satisfied, a move along any vector $\\mathbf{w}$ from $F_{1}$ either increases the first-order approximation to the objective function (that is, $\\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)\u0026gt;0$), or else keeps this value the same (that is, $\\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$).\nWhat implications does optimality have for the second derivatives of $f$ and the constraints $c_{i}$? We see in this section that these derivatives play a \u0026ldquo;tiebreaking\u0026rdquo; role. For the directions $\\mathbf{w} \\in F_{1}$ for which $\\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$, we cannot determine from first derivative information alone whether a move along this direction will increase or decrease the objective function $f$. Second-order conditions examine the second derivative terms in the Taylor series expansions of $f$ and $c_{i}$, to see whether this extra information resolves the issue of increase or decrease in $f$. Essentially, the second-order conditions concern the curvature of the Lagrangian function in the \u0026ldquo;undecided\u0026rdquo; directions—the directions $\\mathbf{w} \\in F_{1}$ for which $\\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$.\nSince we are discussing second derivatives, stronger smoothness assumptions are needed here than in the previous sections. For the purpose of this section, $f$ and $c_{i}, i \\in \\mathcal{E} \\cup \\mathcal{I}$, are all assumed to be twice continuously differentiable.\nGiven $F_{1}$ from Definition 5.7 and some Lagrange multiplier vector $\\boldsymbol{\\lambda}^{\\star}$ satisfying the KKT conditions, we define a subset $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ of $F_{1}$ by\n\\begin{equation} F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)=\\left\\{\\mathbf{w} \\in F_{1} \\mid \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}=0, \\text { all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\text { with } \\lambda_{i}^{\\star}\u0026gt;0\\right\\} \\label{eq:f2_definition} \\end{equation}\nEquivalently,\n\\begin{equation} \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\Leftrightarrow \\begin{cases}\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}=0, \u0026amp; \\text { for all } i \\in \\mathcal{E}, \\\\ \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}=0, \u0026amp; \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\text { with } \\lambda_{i}^{\\star}\u0026gt;0, \\\\ \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w} \\geq 0, \u0026amp; \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\text { with } \\lambda_{i}^{\\star}=0 .\\end{cases} \\label{eq:f2_conditions} \\end{equation}\nThe subset $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ contains the directions $\\mathbf{w}$ that tend to \u0026ldquo;adhere\u0026rdquo; to the active inequality constraints for which the Lagrange multiplier component $\\lambda_{i}^{\\star}$ is positive, as well as to the equality constraints. From the definition \\eqref{eq:f2_definition} and the fact that $\\lambda_{i}^{\\star}=0$ for all inactive components $i \\in \\mathcal{I} \\backslash \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$, it follows immediately that\n\\begin{equation} \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\Rightarrow \\lambda_{i}^{\\star} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}=0 \\text { for all } i \\in \\mathcal{E} \\cup \\mathcal{I}. \\label{eq:f2_property} \\end{equation}\nHence, from the first KKT condition \\eqref{eq:kkt_gradient_zero} and the definition of the Lagrangian function, we have that\n\\begin{equation} \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\Rightarrow \\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\sum_{i \\in \\mathcal{E} \\cup \\mathcal{I}} \\lambda_{i}^{\\star} \\mathbf{w}^{\\mathrm{T}} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)=0 \\label{eq:f2_gradient_zero} \\end{equation}\nHence the set $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ contains directions from $F_{1}$ for which it is not clear from first derivative information alone whether $f$ will increase or decrease.\nThe first theorem defines a necessary condition involving the second derivatives: If $\\mathbf{x}^{\\star}$ is a local solution, then the curvature of the Lagrangian along directions in $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ must be nonnegative.\nTheorem 5.3 (Second-order necessary conditions)\nSuppose that $\\mathbf{x}^{\\star}$ is a local solution and that the LICQ condition is satisfied. Let $\\boldsymbol{\\lambda}^{\\star}$ be a Lagrange multiplier vector such that the KKT conditions are satisfied, and let $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ be defined as above. Then\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w} \\geq 0, \\quad \\text { for all } \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\label{eq:second_order_necessary} \\end{equation}\nProof\nSince $\\mathbf{x}^{\\star}$ is a local solution, all feasible sequences $\\left\\{\\mathbf{z}_{k}\\right\\}$ approaching $\\mathbf{x}^{\\star}$ must have $f\\left(\\mathbf{z}_{k}\\right) \\geq f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k$ sufficiently large. Our approach in this proof is to construct a feasible sequence whose limiting direction is $\\mathbf{w} /\\left\\|\\mathbf{w}\\right\\|$ and show that the property $f\\left(\\mathbf{z}_{k}\\right) \\geq f\\left(\\mathbf{x}^{\\star}\\right)$ implies that \\eqref{eq:second_order_necessary} holds.\nSince $\\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\subset F_{1}$, we can use the technique in the proof of\nlemma 5.1\nto construct a feasible sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$ such that\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\frac{\\mathbf{z}_{k}-\\mathbf{x}^{\\star}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}=\\frac{\\mathbf{w}}{\\left\\|\\mathbf{w}\\right\\|} \\label{eq:sequence_limit} \\end{equation}\nIn particular, we have from the construction that\n\\begin{equation} c_{i}\\left(\\mathbf{z}_{k}\\right)=\\frac{t_{k}}{\\left\\|\\mathbf{w}\\right\\|} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}, \\quad \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\label{eq:constraint_values} \\end{equation}\nwhere $\\left\\{t_{k}\\right\\}$ is some sequence of positive scalars decreasing to zero. Moreover, we have that\n\\begin{equation} \\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|=t_{k}+o\\left(t_{k}\\right) \\label{eq:distance_relation} \\end{equation}\nand so by substitution, we obtain\n\\begin{equation} \\mathbf{z}_{k}-\\mathbf{x}^{\\star}=\\frac{t_{k}}{\\left\\|\\mathbf{w}\\right\\|} \\mathbf{w}+o\\left(t_{k}\\right) \\label{eq:difference_expansion} \\end{equation}\nFrom the definition of the Lagrangian and \\eqref{eq:constraint_values}, we have that\n\\begin{equation} \\begin{aligned} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right) \u0026amp;=f\\left(\\mathbf{z}_{k}\\right)-\\sum_{i \\in \\mathcal{E} \\cup \\mathcal{I}} \\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{z}_{k}\\right) \\\\ \u0026amp;=f\\left(\\mathbf{z}_{k}\\right)-\\frac{t_{k}}{\\left\\|\\mathbf{w}\\right\\|} \\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i}^{\\star} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w} \\\\ \u0026amp;=f\\left(\\mathbf{z}_{k}\\right) \\end{aligned} \\label{eq:lagrangian_simplification} \\end{equation}\nwhere the last equality follows from the critical property \\eqref{eq:f2_property}. On the other hand, we can perform a Taylor series expansion to obtain an estimate of $\\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)$ near $\\mathbf{x}^{\\star}$. By using Taylor\u0026rsquo;s theorem and continuity of the Hessians $\\nabla^{2} f$ and $\\nabla^{2} c_{i}, i \\in \\mathcal{E} \\cup \\mathcal{I}$, we obtain\n\\begin{equation} \\begin{aligned} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)= \u0026amp; \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)+\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\\\ \u0026amp; +\\frac{1}{2}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\end{aligned} \\label{eq:taylor_expansion} \\end{equation}\nBy the complementarity conditions \\eqref{eq:kkt_complementarity} the first term on the right-hand-side of this expression is equal to $f\\left(\\mathbf{x}^{\\star}\\right)$. From \\eqref{eq:kkt_gradient_zero}, the second term is zero. Hence we can rewrite \\eqref{eq:taylor_expansion} as\n\\begin{equation} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)=f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\label{eq:simplified_taylor} \\end{equation}\nBy using \\eqref{eq:difference_expansion} and \\eqref{eq:distance_relation}, we have for the second-order term and the remainder term that\n\\begin{equation} \\begin{aligned} \u0026amp; \\frac{1}{2}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\\\ \u0026amp; =\\frac{1}{2}\\left(t_{k} /\\left\\|\\mathbf{w}\\right\\|\\right)^{2} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}+o\\left(t_{k}^{2}\\right) \\end{aligned} \\label{eq:second_order_term} \\end{equation}\nHence, by substituting this expression together with \\eqref{eq:lagrangian_simplification} into \\eqref{eq:simplified_taylor}, we obtain\n\\begin{equation} f\\left(\\mathbf{z}_{k}\\right)=f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left(t_{k} /\\left\\|\\mathbf{w}\\right\\|\\right)^{2} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}+o\\left(t_{k}^{2}\\right) \\label{eq:objective_expansion} \\end{equation}\nIf $\\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}\u0026lt;0$, then \\eqref{eq:objective_expansion} would imply that $f\\left(\\mathbf{z}_{k}\\right)\u0026lt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k$ sufficiently large, contradicting the fact that $\\mathbf{x}^{\\star}$ is a local solution. Hence, the condition \\eqref{eq:second_order_necessary} must hold, as claimed.\n■ Sufficient conditions are conditions on $f$ and $c_{i}, i \\in \\mathcal{E} \\cup \\mathcal{I}$, that ensure that $\\mathbf{x}^{\\star}$ is a local solution. (They take the opposite tack to necessary conditions, which assume that $\\mathbf{x}^{\\star}$ is a local solution and deduce properties of $f$ and $c_{i}$.) The second-order sufficient condition stated in the next theorem looks very much like the necessary condition just discussed, but it differs in that the constraint qualification is not required, and the inequality in \\eqref{eq:second_order_necessary} is replaced by a strict inequality.\nTheorem 5.4 (Second-order sufficient conditions)\nSuppose that for some feasible point $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ there is a Lagrange multiplier vector $\\boldsymbol{\\lambda}^{\\star}$ such that the KKT conditions are satisfied. Suppose also that\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}\u0026gt;0, \\quad \\text { for all } \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right), \\mathbf{w} \\neq \\mathbf{0} \\label{eq:second_order_sufficient} \\end{equation}\nThen $\\mathbf{x}^{\\star}$ is a strict local solution.\nProof\nThe result is proved if we can show that for any feasible sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$ approaching $\\mathbf{x}^{\\star}$, we have that $f\\left(\\mathbf{z}_{k}\\right)\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k$ sufficiently large.\nGiven any feasible sequence, we have from\nlemma 5.1\n(i) and\nDefinition 5.7\nthat all its limiting directions $\\mathbf{d}$ satisfy $\\mathbf{d} \\in F_{1}$. Choose a particular limiting direction $\\mathbf{d}$ whose associated subsequence $\\mathcal{S}_{\\mathbf{d}}$ satisfies \\eqref{eq:limiting_direction}. In other words, we have for all $k \\in \\mathcal{S}_{\\mathbf{d}}$ that\n\\begin{equation} \\mathbf{z}_{k}-\\mathbf{x}^{\\star}=\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\label{eq:limiting_direction_expansion} \\end{equation}\nFrom the definition of the Lagrangian, we have that\n\\begin{equation} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)=f\\left(\\mathbf{z}_{k}\\right)-\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{z}_{k}\\right) \\leq f\\left(\\mathbf{z}_{k}\\right) \\label{eq:lagrangian_bound} \\end{equation}\nwhile the Taylor series approximation \\eqref{eq:simplified_taylor} from the proof of Theorem 5.3 continues to hold.\nWe know that $\\mathbf{d} \\in F_{1}$, but suppose first that it is not in $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$. We can then identify some index $j \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$ such that the strict positivity condition\n\\begin{equation} \\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}\u0026gt;0 \\label{eq:strict_positivity} \\end{equation}\nis satisfied, while for the remaining indices $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$, we have\n\\begin{equation} \\lambda_{i}^{\\star} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d} \\geq 0 \\label{eq:nonnegativity} \\end{equation}\nFrom Taylor\u0026rsquo;s theorem and \\eqref{eq:limiting_direction_expansion}, we have for all $k \\in \\mathcal{S}_{\\mathbf{d}}$ and for this particular value of $j$ that\n\\begin{equation} \\begin{aligned} \\lambda_{j}^{\\star} c_{j}\\left(\\mathbf{z}_{k}\\right) \u0026amp; =\\lambda_{j}^{\\star} c_{j}\\left(\\mathbf{x}^{\\star}\\right)+\\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\\\ \u0026amp; =\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\end{aligned} \\label{eq:constraint_taylor} \\end{equation}\nHence, from \\eqref{eq:lagrangian_bound}, we have for $k \\in \\mathcal{S}_{\\mathbf{d}}$ that\n\\begin{equation} \\begin{aligned} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right) \u0026amp; =f\\left(\\mathbf{z}_{k}\\right)-\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{z}_{k}\\right) \\\\ \u0026amp; \\leq f\\left(\\mathbf{z}_{k}\\right)-\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\end{aligned} \\label{eq:lagrangian_inequality} \\end{equation}\nFrom the Taylor series estimate \\eqref{eq:simplified_taylor}, we have meanwhile that\n\\begin{equation} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)=f\\left(\\mathbf{x}^{\\star}\\right)+O\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\label{eq:lagrangian_taylor} \\end{equation}\nand by combining with \\eqref{eq:lagrangian_inequality}, we obtain\n\\begin{equation} f\\left(\\mathbf{z}_{k}\\right) \\geq f\\left(\\mathbf{x}^{\\star}\\right)+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\label{eq:objective_lower_bound} \\end{equation}\nTherefore, because of \\eqref{eq:strict_positivity}, we have $f\\left(\\mathbf{z}_{k}\\right)\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k \\in \\mathcal{S}_{\\mathbf{d}}$ sufficiently large.\nFor the other case of $\\mathbf{d} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$, we use \\eqref{eq:limiting_direction_expansion}, \\eqref{eq:lagrangian_bound}, and \\eqref{eq:simplified_taylor} to write\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{z}_{k}\\right) \u0026amp; \\geq f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\\\ \u0026amp; =f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2} \\mathbf{d}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\end{aligned} \\label{eq:second_order_case} \\end{equation}\nBecause of \\eqref{eq:second_order_sufficient}, we again have $f\\left(\\mathbf{z}_{k}\\right)\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k \\in \\mathcal{S}_{\\mathbf{d}}$ sufficiently large.\nSince this reasoning applies to all limiting directions of $\\left\\{\\mathbf{z}_{k}\\right\\}$, and since each element $\\mathbf{z}_{k}$ of the sequence can be assigned to one of the subsequences $\\mathcal{S}_{\\mathbf{d}}$ that converge to one of these limiting directions, we conclude that $f\\left(\\mathbf{z}_{k}\\right)\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k$ sufficiently large.\n■ Example 3 (Inequality-constrained example, one more time)\nWe now return to the inequality-constrained example to check the second-order conditions. In this problem we have $f(\\mathbf{x})=x_{1}+x_{2}$, $c_{1}(\\mathbf{x})=2-x_{1}^{2}-x_{2}^{2}$, $\\mathcal{E}=\\emptyset$, and $\\mathcal{I}=\\{1\\}$. The Lagrangian is\n\\begin{equation} \\mathcal{L}(\\mathbf{x}, \\lambda)=\\left(x_{1}+x_{2}\\right)-\\lambda_{1}\\left(2-x_{1}^{2}-x_{2}^{2}\\right) \\label{eq:example_lagrangian} \\end{equation}\nand it is easy to show that the KKT conditions are satisfied by $\\mathbf{x}^{\\star}=(-1,-1)^{\\mathrm{T}}$, with $\\lambda_{1}^{\\star}=\\frac{1}{2}$. The Lagrangian Hessian at this point is\n\\begin{equation} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)=\\begin{bmatrix} 2 \\lambda_{1}^{\\star} \u0026amp; 0 \\\\ 0 \u0026amp; 2 \\lambda_{1}^{\\star} \\end{bmatrix}=\\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\label{eq:example_hessian} \\end{equation}\nThis matrix is positive definite, that is, it satisfies $\\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}\u0026gt;0$ for all $\\mathbf{w} \\neq \\mathbf{0}$, so it certainly satisfies the conditions of Theorem 5.4. We conclude that $\\mathbf{x}^{\\star}=(-1,-1)^{\\mathrm{T}}$ is a strict local solution. (In fact, it is the global solution of this problem, since this problem is a convex programming problem.)\nExample 4\nFor an example in which the issues are more complex, consider the problem\n\\begin{equation} \\min -0.1\\left(x_{1}-4\\right)^{2}+x_{2}^{2} \\quad \\text { s.t. } \\quad x_{1}^{2}+x_{2}^{2}-1 \\geq 0, \\label{eq:nonconvex_example} \\end{equation}\nin which we seek to minimize a nonconvex function over the exterior of the unit circle. Obviously, the objective function is not bounded below on the feasible region, since we can take the feasible sequence\n\\begin{equation} \\begin{bmatrix} 10 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 20 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 30 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 40 \\\\ 0 \\end{bmatrix}, \\label{eq:unbounded_sequence} \\end{equation}\nand note that $f(\\mathbf{x})$ approaches $-\\infty$ along this sequence. Therefore, no global solution exists, but it may still be possible to identify a strict local solution on the boundary of the constraint. We search for such a solution by using the KKT conditions and the second-order conditions of Theorem 5.4.\nBy defining the Lagrangian for \\eqref{eq:nonconvex_example} in the usual way, it is easy to verify that\n\\begin{equation} \\begin{aligned} \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\lambda) \u0026amp; =\\begin{bmatrix} -0.2\\left(x_{1}-4\\right)-2 \\lambda x_{1} \\\\ 2 x_{2}-2 \\lambda x_{2} \\end{bmatrix}, \\\\ \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\lambda) \u0026amp; =\\begin{bmatrix} -0.2-2 \\lambda \u0026amp; 0 \\\\ 0 \u0026amp; 2-2 \\lambda \\end{bmatrix}. \\end{aligned} \\label{eq:example_derivatives} \\end{equation}\nThe point $\\mathbf{x}^{\\star}=(1,0)^{\\mathrm{T}}$ satisfies the KKT conditions with $\\lambda_{1}^{\\star}=0.3$ and the active set $\\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)=\\{1\\}$. To check that the second-order sufficient conditions are satisfied at this point, we note that\n\\begin{equation} \\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}, \\label{eq:constraint_gradient} \\end{equation}\nso that the space $F_{2}$ defined in \\eqref{eq:f2_definition} is simply\n\\begin{equation} F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)=\\left\\{\\mathbf{w} \\mid w_{1}=0\\right\\}=\\left\\{\\left(0, w_{2}\\right)^{\\mathrm{T}} \\mid w_{2} \\in \\mathbb{R}\\right\\} \\label{eq:f2_example} \\end{equation}\nNow, by substituting $\\mathbf{x}^{\\star}$ and $\\boldsymbol{\\lambda}^{\\star}$ into \\eqref{eq:example_derivatives}, we have for any $\\mathbf{w} \\in F_{2}$ with $\\mathbf{w} \\neq \\mathbf{0}$ that\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}=\\begin{bmatrix} 0 \\\\ w_{2} \\end{bmatrix}^{\\mathrm{T}}\\begin{bmatrix} -0.4 \u0026amp; 0 \\\\ 0 \u0026amp; 1.4 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ w_{2} \\end{bmatrix}=1.4 w_{2}^{2}\u0026gt;0 \\label{eq:positive_definiteness} \\end{equation}\nHence, the second-order sufficient conditions are satisfied, and we conclude from Theorem 5.4 that $(1,0)^{\\mathrm{T}}$ is a strict local solution for \\eqref{eq:nonconvex_example}.\nSecond-order conditions and projected Hessians # The second-order conditions are sometimes stated in a form that is weaker but easier to verify than \\eqref{eq:second_order_necessary} and \\eqref{eq:second_order_sufficient}. This form uses a two-sided projection of the Lagrangian Hessian $\\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)$ onto subspaces that are related to $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$.\nThe simplest case is obtained when the multiplier $\\boldsymbol{\\lambda}^{\\star}$ that satisfies the KKT conditions is unique (as happens, for example, when the LICQ condition holds) and strict complementarity holds. In this case, the definition \\eqref{eq:f2_definition} of $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ reduces to\n\\begin{equation} F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)=\\operatorname{Null}\\left[\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}}\\right]_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)}=\\operatorname{Null} \\mathbf{A}, \\label{eq:f2_null_space} \\end{equation}\nwhere $\\mathbf{A}$ is defined as in \\eqref{eq:matrix_notation}. In other words, $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ is the null space of the matrix whose rows are the active constraint gradients at $\\mathbf{x}^{\\star}$. As in \\eqref{eq:null_space_basis}, we can define the matrix $\\mathbf{Z}$ with full column rank whose columns span the space $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$. Any vector $\\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ can be written as $\\mathbf{w}=\\mathbf{Z} \\mathbf{u}$ for some vector $\\mathbf{u}$, and conversely, we have that $\\mathbf{Z} \\mathbf{u} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ for all $\\mathbf{u}$. Hence, the condition \\eqref{eq:second_order_necessary} in Theorem 5.3 can be restated as\n\\begin{equation} \\mathbf{u}^{\\mathrm{T}} \\mathbf{Z}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{Z} \\mathbf{u} \\geq 0 \\text { for all } \\mathbf{u} \\label{eq:projected_necessary} \\end{equation}\nor, more succinctly,\n\\begin{equation} \\mathbf{Z}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{Z} \\text { is positive semidefinite.} \\label{eq:projected_psd} \\end{equation}\nSimilarly, the condition \\eqref{eq:second_order_sufficient} in Theorem 5.4 can be restated as\n\\begin{equation} \\mathbf{Z}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{Z} \\text { is positive definite.} \\label{eq:projected_pd} \\end{equation}\nWe see at the end of this section that $\\mathbf{Z}$ can be computed numerically, so that the positive (semi)definiteness conditions can actually be checked by forming these matrices and finding their eigenvalues.\nWhen the optimal multiplier $\\boldsymbol{\\lambda}^{\\star}$ is unique but the strict complementarity condition is not satisfied, $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ is no longer a subspace. Instead, it is an intersection of planes (defined by the first two conditions in \\eqref{eq:f2_conditions}) and half-spaces (defined by the third condition in \\eqref{eq:f2_conditions}). We can still, however, define two subspaces $\\overline{F}_{2}$ and $\\underline{F}_{2}$ that \u0026ldquo;bound\u0026rdquo; $F_{2}$ above and below, in the sense that $\\overline{F}_{2}$ is the smallest-dimensional subspace that contains $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$, while $\\underline{F}_{2}$ is the largest-dimensional subspace contained in $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$. To be precise, we have\n\\begin{equation} \\underline{F}_{2}=\\left\\{\\mathbf{d} \\in F_{1} \\mid \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}=0, \\text { all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)\\right\\} \\label{eq:f2_lower} \\end{equation}\n\\begin{equation} \\overline{F}_{2}=\\left\\{\\mathbf{d} \\in F_{1} \\mid \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}=0, \\text { all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\text { with } i \\in \\mathcal{E} \\text { or } \\lambda_{i}^{\\star}\u0026gt;0\\right\\} \\label{eq:f2_upper} \\end{equation}\nso that\n\\begin{equation} \\underline{F}_{2} \\subset F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\subset \\overline{F}_{2}. \\label{eq:f2_containment} \\end{equation}\nAs in the previous case, we can construct matrices $\\underline{\\mathbf{Z}}$ and $\\overline{\\mathbf{Z}}$ whose columns span the subspaces $\\underline{F}_{2}$ and $\\overline{F}_{2}$, respectively. If the condition \\eqref{eq:second_order_necessary} of Theorem 5.3 holds, we can be sure that\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w} \\geq 0, \\quad \\text { for all } \\mathbf{w} \\in \\underline{F}_{2}, \\label{eq:lower_bound_condition} \\end{equation}\nbecause $\\underline{F}_{2} \\subset F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$. Therefore, an immediate consequence of \\eqref{eq:second_order_necessary} is that the matrix $\\underline{\\mathbf{Z}}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\underline{\\mathbf{Z}}$ is positive semidefinite.\nAnalogously, we have from $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\subset \\overline{F}_{2}$ that condition \\eqref{eq:second_order_sufficient} is implied by the condition\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}\u0026gt;0, \\quad \\text { for all } \\mathbf{w} \\in \\overline{F}_{2} \\label{eq:upper_bound_condition} \\end{equation}\nHence, given that the $\\boldsymbol{\\lambda}^{\\star}$ satisfying the KKT conditions is unique, a sufficient condition for \\eqref{eq:second_order_sufficient} is that the matrix $\\overline{\\mathbf{Z}}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\overline{\\mathbf{Z}}$ be positive definite. Again, this condition provides a practical way to check the second-order sufficient condition.\nThe matrices $\\underline{\\mathbf{Z}}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\underline{\\mathbf{Z}}$ and $\\overline{\\mathbf{Z}}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\overline{\\mathbf{Z}}$ are sometimes called two-sided projected Hessian matrices, or simply projected Hessians for short.\nOne way to compute the matrix $\\mathbf{Z}$ (and its counterparts $\\underline{\\mathbf{Z}}$ and $\\overline{\\mathbf{Z}}$) is to apply a QR factorization to the matrix of active constraint gradients whose null space we seek. In the simplest case above (in which the multiplier $\\boldsymbol{\\lambda}^{\\star}$ is unique and strictly complementary), we define $\\mathbf{A}$ as in \\eqref{eq:matrix_notation} and write the QR factorization of $\\mathbf{A}^{\\mathrm{T}}$ as\n\\begin{equation} \\mathbf{A}^{\\mathrm{T}}=\\mathbf{Q}\\begin{bmatrix} \\mathbf{R} \\\\ \\mathbf{0} \\end{bmatrix}=\\begin{bmatrix} \\mathbf{Q}_{1} \u0026amp; \\mathbf{Q}_{2} \\end{bmatrix}\\begin{bmatrix} \\mathbf{R} \\\\ \\mathbf{0} \\end{bmatrix}=\\mathbf{Q}_{1} \\mathbf{R} \\label{eq:qr_factorization} \\end{equation}\nwhere $\\mathbf{R}$ is a square upper triangular matrix, and $\\mathbf{Q}$ is $n \\times n$ orthogonal. If $\\mathbf{R}$ is nonsingular, we can set $\\mathbf{Z}=\\mathbf{Q}_{2}$. If $\\mathbf{R}$ is singular (indicating that the active constraint gradients are linearly dependent), a slight enhancement of this procedure that makes use of column pivoting during the QR procedure can be used to identify $\\mathbf{Z}$.\n"},{"id":15,"href":"/numerical_optimization/docs/lectures/fundamentals/","title":"I - Fundamentals","section":"Lectures","content":" Fundamentals # Content 1. Optimization problems 2. Unconstrained optimization : basics 3. Convexity theory 4. Unconstrained optimization : linesearch 5. Constrained optimization - Introduction "},{"id":16,"href":"/numerical_optimization/docs/practical_labs/linear_regression/","title":"I - Linear Regression models","section":"Practical labs","content":" Linear Regression models # Introduction # In this lab session, we will explore the fundamental concepts of numerical optimization through the lens of linear regression. We\u0026rsquo;ll begin with the simplest case and gradually build up to more complex scenarios, comparing analytical solutions with numerical methods at each step.\nLinear regression is perhaps the most fundamental problem in machine learning and statistics. While it has a closed-form solution, implementing numerical optimization methods for this problem provides excellent intuition for more complex optimization scenarios where analytical solutions don\u0026rsquo;t exist.\nLearning objectives # By the end of this session, you should be able to:\nDerive the analytical solution for simple linear regression Implement gradient descent with various step size strategies Understand the connection between the one-dimensional and multi-dimensional cases Apply line search techniques to improve convergence I - One dimensional case # Let us first start with the form that most people are familiar with, the linear regression model in one dimension. The setup is as follows:\nWe have a set of data points $\\{(x_i, y_i)\\}_{i=1}^n$. Here the $x_i$ are the input features and the $y_i$ are the target values. Assuming there is a linear relationship between and target because of some underlying phenomenon, we model the observations as: \\begin{equation} y_i = \\alpha x_i + \\beta + \\epsilon \\label{eq:linear_model_1d} \\end{equation} where $\\epsilon$ is a random noise term that we assume to be normally distributed with mean 0 and variance $\\sigma^2$. Our goal is then to find the parameters $\\alpha$ and $\\beta$ that \u0026ldquo;best match\u0026rdquo; the data points. Such a program can is illustrated with following interactive plot.\n1. Modeling and solving the problem # Propose a loss function that quantifies the difference between the observed $y_i$ and the predicted values $\\hat{y}_i = \\alpha x_i + \\beta$. Hint The most common loss function for regression problems is the mean squared error (MSE): $$ L(\\alpha, \\beta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - (\\alpha x_i + \\beta))^2 $$ Show that the loss function is convex in the parameters $\\alpha$ and $\\beta$. Hint To show convexity, we need to demonstrate that the Hessian matrix of second derivatives is positive semi-definite. Or that the function is a positive linear combination of convex functions. The loss function is a quadratic function in $\\alpha$ and $\\beta$, which is convex. The Hessian matrix will have positive eigenvalues, confirming convexity. Derive the analytical solution for the parameters $\\alpha$ and $\\beta$ by setting the gradients of the loss function with respect to these parameters to zero. Hint It is often useful to express the gradients in terms of the means and variances of the data points:\nmeans : $$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$ variance: $$s_{xx} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2$$ covariance: $$s_{xy} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$$ Implement the analytical solution in Python and compute the optimal parameters for a given dataset. To generate dataset, you can use following code snippet: import numpy as np import matplotlib.pyplot as plt # Set random seed for reproducibility rng = np.random.default_rng(42) # Generate synthetic data n_samples = 50 x = np.linspace(0, 10, n_samples) # True parameters alpha = 2.5 beta = 1.0 # Add Gaussian noise noise = rng.normal(0, 1, n_samples) y = alpha * x + beta + noise # Visualize the data plt.figure(figsize=(8, 6)) plt.scatter(x, y, alpha=0.7, label=\u0026#39;Data points\u0026#39;) plt.xlabel(\u0026#39;x\u0026#39;) plt.ylabel(\u0026#39;y\u0026#39;) plt.title(\u0026#39;Synthetic linear data with noise\u0026#39;) plt.grid(True, alpha=0.3) plt.legend() plt.show() (Bonus) Show that doing a Maximum Likelihood Estimation (MLE) for the parameters $\\alpha$ and $\\beta$ leads to the same solution as minimizing the loss function derived above. Hint The MLE for the parameters in a linear regression model with Gaussian noise leads to minimizing the negative log-likelihood, which is equivalent to minimizing the mean squared error loss function. The derivation involves taking the logarithm of the Gaussian probability density function and simplifying it, leading to the same equations for $\\alpha$ and $\\beta$ as derived from the loss function. 2. Gradient descent for the one-dimensional case # Now that we have a good understanding of the problem and have implemented the analytical solution, let\u0026rsquo;s explore how we can solve this problem using numerical optimization techniques, specifically steepest gradient descent, i.e always taking the gradient as the direction.\nRecalling the update rule for steepest gradient descent of a point $\\theta_k$ at iteration $k$: $$ \\theta_{k+1} = \\theta_k - \\alpha_k \\nabla L(\\theta_k) $$ where $\\alpha_k$ is the step size at iteration $k$, give the update rule for the parameters $\\alpha$ and $\\beta$ in the context of our linear regression problem.\nHint The update rules for the parameters $\\alpha$ and $\\beta$ can be derived from the gradients of the loss function: $$ \\begin{align*} \\alpha_{k+1} \u0026amp;= \\alpha_k - \\alpha_k \\frac{\\partial L}{\\partial \\alpha}(\\alpha_k, \\beta_k) \\\\ \\beta_{k+1} \u0026amp;= \\beta_k - \\alpha_k \\frac{\\partial L}{\\partial \\beta}(\\alpha_k, \\beta_k) \\end{align*} $$ where the gradients are computed as follows: $$ \\begin{align*} \\frac{\\partial L}{\\partial \\alpha} \u0026amp;= -\\sum_{i=1}^n (y_i - (\\alpha_k x_i + \\beta_k)) x_i \\\\ \\frac{\\partial L}{\\partial \\beta} \u0026amp;= -\\sum_{i=1}^n (y_i - (\\alpha_k x_i + \\beta_k)) \\end{align*} $$ Implement gradient descent with a constant step size $\\alpha_k = \\alpha$ for all iterations. Your function should:\nTake initial parameters $(\\alpha_0, \\beta_0)$, step size $\\alpha$, and number of iterations as inputs. Return the trajectory of parameters and loss values. Include a stopping criterion based on gradient magnitude. Experiment with different step sizes: $\\alpha \\in \\{0.0001, 0.001, 0.01, 0.1\\}$. Plot the loss function over iterations for each case. What do you observe?\nHint The loss function should decrease over iterations, but the rate of decrease will depend on the step size. A very small step size will lead to slow convergence, while a very large step size may cause divergence or oscillations. For a fixed number of iterations (say 1000), plot the final error as a function of step size on a logarithmic scale. What is the optimal range for $\\alpha$? Hint The optimal range for $\\alpha$ is typically small enough to ensure convergence but large enough to allow for reasonable speed of convergence. You may find that values around $0.001$ to $0.01$ work well, but this can depend on the specific dataset and problem. Let\u0026rsquo;s try a first experiment with a decreasing step size. Implement a linear decay strategy: $$ \\alpha_k = \\alpha_0 - k \\cdot \\gamma, $$ where $\\gamma$ is a small constant (e.g., $0.0001$). Experiment with different values of $\\alpha_0$ and $\\gamma$. Compare the convergence behavior with constant step size. Plot the loss function and parameter trajectories over iterations.\nWhy might decreasing step sizes be beneficial? What are the trade-offs between aggressive and conservative decay rates? Hint Decreasing step sizes can help avoid overshooting the minimum and allow for finer adjustments as the algorithm converges.\nIn nonconvex problmes, aggressive decay rates may lead to faster convergence initially but can cause the algorithm to get stuck in local minima, while conservative rates may lead to slower convergence but better exploration of the parameter space.\nTry implementing an exponential decay strategy: $$ \\alpha_k = \\alpha_0 \\cdot \\gamma^k$$ where $\\gamma \\in (0, 1)$ is the decay rate. Experiment with different values of $\\gamma$ (e.g., $0.9$, $0.95$, $0.99$) and compare the convergence behavior with constant and linear decay strategies. Hint Exponential decay is more aggressive, thus it may also cause the step size to become too small too quickly, leading to slow convergence in later iterations. The choice of $\\gamma$ can significantly affect the convergence behavior. II - Multiple variables case # Now that we have a good understanding of the one-dimensional case, let\u0026rsquo;s generalize our approach to multiple dimensions. The setup is similar, but now we have multiple features and parameters:\nWe have a set of data points $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ are the input features and $y_i \\in \\mathbb{R}$ are the target values. We model the observations as: \\begin{equation} y_i = \\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_i + \\beta + \\epsilon \\label{eq:linear_model_d} \\end{equation} where $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector, $\\beta \\in \\mathbb{R}$ is the bias term, and $\\epsilon$ is a random noise term that we assume to be normally distributed with mean 0 and variance $\\sigma^2$. Figure 0.1: 2D linear regression\nAn example of this model is illustrated in Figure 0.1 , where the data points are represented in a two-dimensional space, and the linear regression model fits a plane to the data.\nThis model is more general than the one-dimensional case, as it allows for multiple features to influence the target variable. Notably, we can also augment the feature vectors with a constant term to simplify the notation so that we don\u0026rsquo;t have to deal with the bias term separately. We define: \\begin{equation} \\tilde{\\mathbf{x}}_i = [1, \\mathbf{x}_i^{\\mathrm{T}}]^{\\mathrm{T}} \\in \\mathbb{R}^{d+1} \\end{equation} and \\begin{equation} \\tilde{\\mathbf{w}} = [\\beta, \\mathbf{w}^{\\mathrm{T}}]^{\\mathrm{T}} \\in \\mathbb{R}^{d+1} \\end{equation} so that we can rewrite the model as: \\begin{equation} y_i = \\tilde{\\mathbf{w}}^{\\mathrm{T}} \\tilde{\\mathbf{x}}_i + \\epsilon \\label{eq:linear_model_d_augmented} \\end{equation}\nOur goal is then to find the parameters $\\mathbf{w}$ and $\\beta$ that \u0026ldquo;best match\u0026rdquo; the data points, or in augmented notation, to find $\\tilde{\\mathbf{w}}$ that minimizes the loss function.\n1. Modeling and solving the problem # While the augmented formulation is nice, we can also express the model in matrix form for the observed data. We define the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times (d+1)}$ as: \\begin{equation} \\mathbf{X} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; x_{12} \u0026amp; \\ldots \u0026amp; x_{1d} \\\\ 1 \u0026amp; x_{21} \u0026amp; x_{22} \u0026amp; \\ldots \u0026amp; x_{2d} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n1} \u0026amp; x_{n2} \u0026amp; \\ldots \u0026amp; x_{nd} \\end{bmatrix} \\end{equation} and the target vector $\\mathbf{y} \\in \\mathbb{R}^n$ as: \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\end{equation}\nThen we can express the model as: \\begin{equation} \\mathbf{y} = \\mathbf{X} \\tilde{\\mathbf{w}} + \\boldsymbol{\\epsilon}, \\label{eq:linear_model_matrix} \\end{equation}\nwhere $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n$ is the noise vector. This more compact formulation is interesting for several reasons:\nIt already encapsulates the observed data in the model and we consider all the $y_i$ as a vector, which allows us to work with the entire dataset at once. the matrix form allow us to obtain solutions that will be expressed as matrix operations, which is more efficient for larger datasets it allows us to use linear algebra techniques to derive the solution Propose a loss function that quantifies the difference between the observed $y_i$ and the predicted values $\\hat{y}_i = \\mathbf{X} \\tilde{\\mathbf{w}}$. Hint The most common loss function for regression problems is the mean squared error (MSE): $$ L(\\tilde{\\mathbf{w}}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}\\|^2_2 $$ Show that the loss function is convex in the parameters $\\tilde{\\mathbf{w}}$. Hint As in the one-dimensional case, the loss function is a quadratic function in $\\tilde{\\mathbf{w}}$, which is convex. The Hessian matrix of second derivatives will be positive semi-definite, confirming convexity. Derive the gradient of the loss function with respect to $\\tilde{\\mathbf{w}}$ in matrix form. To help yourselves, you can use the properties of matrix derivatives from matrix cookbook available here and identity of vector norms: $$ \\lVert \\mathbf{u} - \\mathbf{v} \\rVert^2_2 = \\mathbf{u}^{\\mathrm{T}} \\mathbf{u} - 2 \\mathbf{u}^{\\mathrm{T}} \\mathbf{v} + \\mathbf{v}^{\\mathrm{T}} \\mathbf{v}. $$ and show that optimal solution $\\tilde{\\mathbf{w}}$ satisfies the normal equations: $$ \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} \\tilde{\\mathbf{w}} = \\mathbf{X}^{\\mathrm{T}} \\mathbf{y}. $$\nHint Make use of\n$\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^{\\mathrm{T}} \\mathbf{a} = \\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{a}^{\\mathrm{T}} \\mathbf{x} = \\mathbf{a}$ $\\frac{\\partial}{\\partial \\mathbf{x}} \\|\\mathbf{x}\\|^2_2 = \\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^{\\mathrm{T}} \\mathbf{x} = 2 \\mathbf{x}$ $\\frac{\\partial}{\\partial \\mathbf{x}} \\lVert \\mathbf{A} \\mathbf{x} \\rVert^2_2 = 2 \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}$ Thus, to obtain optimal parameters $\\tilde{\\mathbf{w}}$, we can solve the normal equations: $$ \\tilde{\\mathbf{w}} = (\\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{-1} \\mathbf{X}^{\\mathrm{T}} \\mathbf{y}. $$\nNote: The matrix $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ is known as the Gram matrix, and it is positive semi-definite. If $\\mathbf{X}$ has full column rank, then $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ is invertible, and we can compute the unique solution for $\\tilde{\\mathbf{w}}$. Otherwise, the solution is not unique, and we may need to use regularization techniques (e.g., ridge regression) to obtain a stable solution, but we will not cover this in this lab.\nImplement the analytical solution using NumPy\u0026rsquo;s linear algebra functions. Compare your result with np.linalg.lstsq. To generate dataset, you can use following code snippet: import numpy as np # Generate multi-dimensional data d = 5 # number of features n_samples = 100 # Generate random features X = np.random.randn(n_samples, d) # Add intercept term X_augmented = np.column_stack([np.ones(n_samples), X]) # True parameters w_true = np.random.randn(d + 1) # including bias # Generate targets with noise y = X_augmented @ w_true + 0.5 * np.random.randn(n_samples) print(f\u0026#34;Data shape: {X.shape}\u0026#34;) print(f\u0026#34;Augmented data shape: {X_augmented.shape}\u0026#34;) print(f\u0026#34;True parameters: {w_true}\u0026#34;) 2. Gradient descent for the multiple variables case # Rather than inverting the matrix $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$, which can be computationally expensive for large datasets, we can use gradient descent to find the optimal parameters $\\tilde{\\mathbf{w}}$.\nGive the update rule for the parameters $\\tilde{\\mathbf{w}}$ in the context of our linear regression problem using steepest gradient descent. Hint The update rule for the parameters $\\tilde{\\mathbf{w}}$ can be derived from the gradient of the loss function: $$ \\tilde{\\mathbf{w}}_{k+1} = \\tilde{\\mathbf{w}}_k - \\alpha_k \\nabla L(\\tilde{\\mathbf{w}}_k) $$ where the gradient is given by: $$ \\nabla L(\\tilde{\\mathbf{w}}) = -\\mathbf{X}^{\\mathrm{T}} (\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}) $$ Implement gradient descent with a constant step size $\\alpha_k = \\alpha$ for all iterations. Your function should: Take initial parameters $\\tilde{\\mathbf{w}}_0$, step size $\\alpha$, and number of iterations as inputs. Return the trajectory of parameters and loss values. Include a stopping criterion based on gradient magnitude. 3. Experimenting with backtracking line search # From implementing gradient descent, we have seen that the choice of step size $\\alpha$ can significantly affect the convergence behavior. A fixed step size may not be optimal for all iterations, leading to slow convergence or oscillations. On the other hand, a decreasing step size is not the best choice as it may lead to very small step sizes in later iterations, causing slow convergence. Let us put in practice the theory we have set in place around line search techniques to adaptively choose the step size at each iteration.\nImplement a backtracking line search algorithm to adaptively choose the step size $\\alpha_k$ at each iteration. For a reminder, check the memo here.\nUse the backtracking line search to find the optimal step size for each iteration of gradient descent. Compare the convergence behavior with constant and decreasing step sizes.\nExperiment with different parameters for the backtracking line search, such as the initial step size $\\alpha_0$, reduction factor $\\rho$, and Armijo parameter $c_1$. How do these parameters affect the convergence behavior?\nHint The backtracking line search will adaptively adjust the step size based on the Armijo condition, allowing for more efficient convergence. The choice of $\\alpha_0$, $\\rho$, and $c_1$ can significantly affect the speed of convergence and stability of the algorithm. 4. Using more complex linesearch techniques using toolboxes # In practice, we often use more sophisticated line search techniques that are not so easy to implement from scratch. One such technique is the line_search function from SciPy\u0026rsquo;s optimization module, which implements interpolation techniques to find an optimal step size.\nUse the line_search function from SciPy\u0026rsquo;s optimization module to find the optimal step size for each iteration of gradient descent. Compare the convergence behavior with the backtracking line search. Documentation is available here. III - (Bonus) The general case # Consider the general case where the target is also a vector, i.e., we have a multi-output linear regression problem. The model can be expressed as: \\begin{equation} \\mathbf{Y} = \\mathbf{X} \\tilde{\\mathbf{W}} + \\boldsymbol{\\epsilon}, \\label{eq:linear_model_multi_output} \\end{equation} where $\\mathbf{Y} \\in \\mathbb{R}^{n \\times m}$ is the target matrix with $m$ outputs, and $\\tilde{\\mathbf{W}} \\in \\mathbb{R}^{(d+1) \\times m}$ is the weight matrix.\nDerive all the necessary tools to solve this problem using the same techniques as in the previous sections.\nIV - (Bonus) Regularization # In practice, we often encounter situations where the model is overfitting the data, especially in high-dimensional settings. Regularization techniques are used to prevent overfitting by adding a penalty term to the loss function.\nImplement L2 regularization (ridge regression) by adding a penalty term to the loss function: \\begin{equation} L(\\tilde{\\mathbf{w}}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}\\|^2_2 + \\frac{\\lambda}{2} \\|\\tilde{\\mathbf{w}}\\|^2_2, \\end{equation} where $\\lambda \u0026gt; 0$ is the regularization parameter.\nV - (Bonus) Nonlinear regression # It\u0026rsquo;s actually possible to extend the linear regression model to nonlinear regression by setting up the design matrix $\\mathbf{X}$ to include nonlinear features of the input data. For example, we can include polynomial features suc as: \\begin{equation} \\mathbf{X} = \\begin{bmatrix} 1 \u0026amp; x_1 \u0026amp; x_1^2 \u0026amp; \\ldots \u0026amp; x_1^d \\\\ 1 \u0026amp; x_2 \u0026amp; x_2^2 \u0026amp; \\ldots \u0026amp; x_2^d \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_n \u0026amp; x_n^2 \u0026amp; \\ldots \u0026amp; x_n^d \\end{bmatrix} \\end{equation}\nFrom this information and your own research , implement a nonlinear regression model using polynomial features. You can use the PolynomialFeatures class from sklearn.preprocessing to generate polynomial features.\n"},{"id":17,"href":"/numerical_optimization/docs/lectures/advanced/","title":"II - Advanced problems","section":"Lectures","content":" Advanced problems # Content 1. Unconstrained optimization : Second-order 2. Proximal methods "},{"id":18,"href":"/numerical_optimization/docs/practical_labs/remote_sensing/","title":"II - Remote Sensing project","section":"Practical labs","content":" Remote sensing project # Soon to be added.\n"},{"id":19,"href":"/numerical_optimization/docs/practical_labs/mnist/","title":"III - Digit recognition","section":"Practical labs","content":" Digit recognition with multi-layer perceptron # Soon to be added.\n"},{"id":20,"href":"/numerical_optimization/docs/lectures/machine_learning/","title":"III - Machine Learning problems","section":"Lectures","content":" Machine Learning problems # Content 1. From Linear regression to perceptron 2. Support Vector Machine 3. Neural Networks 4. Modern trends "},{"id":21,"href":"/numerical_optimization/docs/lectures/reminders/","title":"Reminders","section":"Lectures","content":" Reminders # Content Linear Algebra Differentiation "},{"id":22,"href":"/numerical_optimization/docs/practical_labs/environment/","title":"Lab environment","section":"Practical labs","content":" Lab Environment Setup # Welcome to the numerical optimization course! This page will guide you through setting up a modern, efficient Python environment using uv.\nPrerequisites # Good news! uv doesn\u0026rsquo;t require Python to be pre-installed - it can manage Python installations for you. However, having Python already installed won\u0026rsquo;t hurt.\nInstalling uv # 🐧 Linux \u0026amp; 🍎 macOS # The fastest way to install uv is using the official installer:\ncurl -LsSf https://astral.sh/uv/install.sh | sh This will:\nDownload and install the latest version of uv Add uv to your PATH automatically Work on both Linux and macOS Alternative installation methods:\nUsing pipx (if you have it): pipx install uv Using pip: pip install uv Using Homebrew (macOS): brew install uv 🪟 Windows # Option 1: PowerShell Installer (Recommended) Open PowerShell and run:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Option 2: Using pipx or pip If you have Python already installed:\npipx install uv # or pip install uv Option 3: Download from GitHub You can also download the installer or binaries directly from the GitHub releases page.\nVerify Installation # After installation, restart your terminal and verify uv is working:\nuv --version You should see output like uv 0.7.8 or similar (version numbers may vary).\nSetting Up the Lab Project # Now let\u0026rsquo;s create a dedicated project for all your numerical optimization lab sessions.\nStep 1: Create the Project # Navigate to where you want to store your course materials and run:\nuv init --bare numerical-optimization-labs cd numerical-optimization-labs The --bare flag creates a minimal project structure with only essential files: pyproject.toml, .python-version, and README.md (no default main.py file).\nStep 2: Install Required Packages # Now we\u0026rsquo;ll install all the packages you\u0026rsquo;ll need for the course. The uv add command will automatically create a virtual environment, install packages, and update both pyproject.toml and the lock file:\n# Core scientific computing packages uv add numpy scipy scikit-learn # Visualization libraries uv add matplotlib plotly # Machine learning framework uv add torch # Development and interactive tools uv add rich jupyter ipython # Optional: Add some useful development tools uv add pytest black ruff What\u0026rsquo;s happening behind the scenes:\nuv creates a virtual environment at .venv/ in your project directory All packages are installed into this isolated environment A lockfile (uv.lock) is generated containing exact versions of all dependencies for reproducible installations Your pyproject.toml is updated with the new dependencies Step 3: Verify Installation # Check that everything installed correctly:\nuv run python -c \u0026#34;import numpy, scipy, sklearn, matplotlib, plotly, torch, rich, jupyter, IPython; print(\u0026#39;✅ All packages imported successfully!\u0026#39;)\u0026#34; Using Your Lab Environment # Running Python Scripts # To run any Python script in your lab environment:\nuv run python your_script.py The uv run command ensures your script runs in the project\u0026rsquo;s virtual environment with all dependencies available.\nStarting Jupyter Lab/Notebook # To start Jupyter for interactive development:\n# For Jupyter Lab (recommended) uv run jupyter lab # For classic Jupyter Notebook uv run jupyter notebook Interactive Python (IPython) # For an enhanced interactive Python experience:\nuv run ipython Adding More Packages Later # If you need additional packages during the course:\nuv add package-name To remove packages you no longer need:\nuv remove package-name Project Structure # Your lab project will look like this:\nnumerical-optimization-labs/ ├── .venv/ # Virtual environment (auto-created) ├── .python-version # Pinned Python version ├── pyproject.toml # Project configuration and dependencies ├── uv.lock # Exact dependency versions (for reproducibility) ├── README.md # Project documentation └── your_lab_files.py # Your lab work goes here Sharing and Collaboration # Setting up the Environment on Another Machine # If you clone this project or share it with others, they can recreate the exact environment by running:\ncd numerical-optimization-labs uv sync This command reads the lockfile and installs the exact same versions of all dependencies.\nVersion Control # Make sure to commit these files to Git:\n✅ pyproject.toml ✅ uv.lock ✅ .python-version ❌ .venv/ (add this to .gitignore) Troubleshooting # Permission Issues # If you encounter permission errors, use sudo on macOS/Linux or run your command prompt as administrator on Windows.\nPython Version Issues # If you need a specific Python version:\n# Install a specific Python version uv python install 3.11 # Pin it to your project uv python pin 3.11 Environment Issues # If something goes wrong with your environment:\n# Sync environment with lockfile uv sync # Force recreate environment rm -rf .venv uv sync Package Conflicts # uv\u0026rsquo;s dependency resolver is much more robust than pip and should handle conflicts automatically. If you encounter issues, check the error message and try updating conflicting packages.\nQuick Reference Commands # Task Command Create new project uv init project-name Add packages uv add package1 package2 Remove packages uv remove package-name Run Python script uv run python script.py Start Jupyter uv run jupyter lab Install from lockfile uv sync List installed packages uv tree Update a package uv add package-name --upgrade Getting Help # uv Documentation: docs.astral.sh/uv Command Help: uv help or uv command --help Course Forum: [link to your course forum/discussion board] 🎉 You\u0026rsquo;re all set! Your lab environment is ready for numerical optimization adventures. If you encounter any issues during setup, don\u0026rsquo;t hesitate to ask for help during class or by mail.\n"},{"id":23,"href":"/numerical_optimization/docs/practical_labs/backtracking/","title":"Backtracking memo","section":"Practical labs","content":" Backtracking procedure for step size selection # Introduction # The backtracking line search is a fundamental technique in optimization algorithms for determining an appropriate step size that ensures sufficient decrease in the objective function. This procedure is particularly useful in gradient-based methods where choosing an optimal step size analytically is difficult or computationally expensive.\nMathematical setup # Consider the optimization problem: $\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x})$\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a continuously differentiable function. At iteration $k$, we have:\nCurrent point: $\\mathbf{x}_k$ Search direction: $\\mathbf{p}_k$ (typically $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ for steepest descent) Step size: $\\alpha_k \u0026gt; 0$ The Armijo condition # The backtracking procedure is based on the Armijo condition, which requires: $f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\leq f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k$\nwhere $c_1 \\in (0, 1)$ is a constant, typically $c_1 = 10^{-4}$.\nBacktracking algorithm steps # Step 1: Initialize parameters # Choose initial step size $\\alpha_0 \u0026gt; 0$ (e.g., $\\alpha_0 = 1$) Set reduction factor $\\rho \\in (0, 1)$ (typically $\\rho = 0.5$) Set Armijo parameter $c_1 \\in (0, 1)$ (typically $c_1 = 10^{-4}$) Set $\\alpha = \\alpha_0$ Step 2: Check Armijo condition # Evaluate the condition: $f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\leq f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k$\nStep 3: Backtrack if necessary # If the Armijo condition is satisfied:\nAccept $\\alpha_k = \\alpha$ Go to Step 4 Else:\nUpdate $\\alpha \\leftarrow \\rho \\alpha$ Return to Step 2 Step 4: Update iteration # Compute the new iterate: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$\nAlgorithmic description # Algorithm: Backtracking Line Search Input: x_k, p_k, α₀, ρ, c₁ Output: α_k 1. Set α = α₀ 2. While f(x_k + α·p_k) \u0026gt; f(x_k) + c₁·α·∇f(x_k)ᵀ·p_k do 3. α ← ρ·α 4. End while 5. Return α_k = α Theoretical properties # Convergence guarantee # Under mild conditions on $f$ and $\\mathbf{p}_k$, the backtracking procedure terminates in finite steps. Specifically, if:\n$f$ is continuously differentiable $\\mathbf{p}_k$ is a descent direction: $\\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k \u0026lt; 0$ Then there exists a step size $\\alpha \u0026gt; 0$ satisfying the Armijo condition.\nSufficient decrease property # The accepted step size $\\alpha_k$ ensures: $f(\\mathbf{x}_{k+1}) - f(\\mathbf{x}_k) \\leq c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k \u0026lt; 0$\nThis guarantees that each iteration decreases the objective function value.\nImplementation considerations # Choice of parameters # Initial step size $\\alpha_0$: Common choices are $\\alpha_0 = 1$ for Newton-type methods, or $\\alpha_0 = 1/|\\nabla f(\\mathbf{x}_k)|$ for gradient methods Reduction factor $\\rho$: Typically $\\rho = 0.5$ or $\\rho = 0.8$ Armijo parameter $c_1$: Usually $c_1 = 10^{-4}$ or $c_1 = 10^{-3}$ Computational complexity # Each backtracking iteration requires:\nOne function evaluation: $f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$ One gradient evaluation: $\\nabla f(\\mathbf{x}_k)$ (if not already computed) One vector operation: $\\mathbf{x}_k + \\alpha \\mathbf{p}_k$ Practical modifications # Maximum iterations: Limit the number of backtracking steps to prevent infinite loops:\nmax_backtracks = 50 iter = 0 while (Armijo condition not satisfied) and (iter \u0026lt; max_backtracks): α ← ρ·α iter ← iter + 1 Minimum step size: Set a lower bound $\\alpha_{min}$ to avoid numerical issues:\nif α \u0026lt; α_min: α = α_min break Applications # The backtracking procedure is widely used in:\nGradient descent: $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ Newton\u0026rsquo;s method: $\\mathbf{p}_k = -(\\mathbf{H}_k)^{-1} \\nabla f(\\mathbf{x}_k)$ where $\\mathbf{H}_k$ is the Hessian Quasi-Newton methods: $\\mathbf{p}_k = -\\mathbf{B}_k^{-1} \\nabla f(\\mathbf{x}_k)$ where $\\mathbf{B}_k$ approximates the Hessian Conjugate gradient methods Example implementation # def backtracking_line_search(f, grad_f, x_k, p_k, alpha_0=1.0, rho=0.5, c1=1e-4): \u0026#34;\u0026#34;\u0026#34; Backtracking line search for step size selection Parameters: - f: objective function - grad_f: gradient function - x_k: current point - p_k: search direction - alpha_0: initial step size - rho: reduction factor - c1: Armijo parameter Returns: - alpha_k: accepted step size \u0026#34;\u0026#34;\u0026#34; alpha = alpha_0 f_k = f(x_k) grad_k = grad_f(x_k) # Armijo condition right-hand side armijo_rhs = f_k + c1 * alpha * np.dot(grad_k, p_k) while f(x_k + alpha * p_k) \u0026gt; armijo_rhs: alpha *= rho armijo_rhs = f_k + c1 * alpha * np.dot(grad_k, p_k) return alpha Exercises\nImplement the backtracking line search for the quadratic function $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\mathrm{T}} \\mathbf{Q} \\mathbf{x} - \\mathbf{b}^{\\mathrm{T}} \\mathbf{x}$, where $\\mathbf{Q}$ is positive definite.\nCompare the performance of different values of $\\rho$ and $c_1$ on a test optimization problem.\nAnalyze the number of backtracking steps required as a function of the condition number of the Hessian matrix.\n"},{"id":24,"href":"/numerical_optimization/docs/practical_labs/","title":"Practical labs","section":"Docs","content":" Practical labs # Content I - Linear Regression models II - Remote Sensing project III - Digit recognition Lab environment Backtracking memo "}]