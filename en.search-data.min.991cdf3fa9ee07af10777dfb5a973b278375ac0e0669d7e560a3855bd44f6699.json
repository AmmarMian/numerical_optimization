[{"id":0,"href":"/numerical_optimization/docs/lectures/machine_learning/perceptron/","title":"1. From Linear regression to perceptron","section":"III - Machine Learning problems","content":" From Linear regression to perceptron # "},{"id":1,"href":"/numerical_optimization/docs/lectures/fundamentals/optimization_problems/","title":"1. Optimization problems","section":"I - Fundamentals","content":" Optimization problems # unconstrained vs constrained # What we are interested in these lectures is to solve problems of the form :\n\\begin{equation} \\underset{\\mathbf{x}\\in\\mathbb{R}^d}{\\operatorname{(arg)min}} f(\\mathbf{x}), \\label{eq: optim general unconstrained} \\end{equation} where $\\mathbf{x}\\in\\mathbb{R}^d$ and $f:\\mathcal{D}_f \\mapsto \\mathbb{R} $ is a scalar-valued function with domain $\\mathcal{D}_f$. Under this formulation, the problem is said to be an unconstrained Optimization problem.\nIf additionally, we add a set of equalities constraints functions: $$ \\{h_i : \\mathbb{R}^d \\mapsto \\mathbb{R} \\, /\\, 1 \\leq i \\leq N \\} $$ and inequalities constraints functions: $$ \\{g_j : \\mathbb{R}^d \\mapsto \\mathbb{R} \\, /\\, 1 \\leq j \\leq M \\} $$ and define the set $\\mathcal{S} = \\{\\mathbf{x} \\in \\mathbb{R}^d \\,/\\, \\forall\\,(i, j),\\, h_i(\\mathbf{x})=0,\\, g_j(\\mathbf{x})\\leq 0\\}$ and want to solve: \\begin{equation} \\underset{\\mathbf{x}\\in\\mathcal{S}}{\\operatorname{(arg)min}} f(\\mathbf{x}), \\label{eq: optim general constrained} \\end{equation} then the problem is said to be a constrained optimization problem.\nNote that here, the constraints and the function domain are not the same sets. Constraints usually stem from modelling of the problem whilst the function domain only characterizes for which values of $\\mathbf{x}$ it is possible to compute a value of the function.\nSunch problems arise when \u0026hellip; TODO. Lorem\nGlobal optimization vs local optimization # Figure 1.1:An example of multiple local minima\n"},{"id":2,"href":"/numerical_optimization/docs/lectures/advanced/proximal_methods/","title":"1. Proximal methods","section":"II - Advanced problems","content":" Non-convex problems # "},{"id":3,"href":"/numerical_optimization/docs/lectures/reminders/differentiation/","title":"Differentiation","section":"Reminders","content":" Fundamentals of Multivariate Differentiation # Multivariate calculus extends the fundamental concepts of single-variable calculus to functions of several variables. This powerful mathematical framework allows us to analyze and describe phenomena in multiple dimensions, making it essential for physics, engineering, economics, statistics, and many other disciplines. This document provides a comprehensive introduction to multivariate differentiation, building from basic principles to advanced applications.\n1. Functions of Several Variables # We begin by defining functions that map points from higher-dimensional spaces to either real numbers or other multi-dimensional spaces.\nDefinition A.1 (Function of Several Variables)\nA real-valued function of $n$ variables is a mapping $f: D \\rightarrow \\mathbb{R}$ where $D \\subseteq \\mathbb{R}^n$. For each point $(x_1, x_2, \\ldots, x_n) \\in D$, the function assigns a unique real value $f(x_1, x_2, \\ldots, x_n)$. Definition A.2 (Vector-Valued Function)\nA vector-valued function of $n$ variables is a mapping $\\mathbf{F}: D \\rightarrow \\mathbb{R}^m$ where $D \\subseteq \\mathbb{R}^n$. For each point $(x_1, x_2, \\ldots, x_n) \\in D$, the function assigns a unique vector $\\mathbf{F}(x_1, x_2, \\ldots, x_n) = (F_1(x_1, \\ldots, x_n), \\ldots, F_m(x_1, \\ldots, x_n))$. Visualizing Functions of Several Variables\nWhile functions of one variable can be visualized as curves in the plane, functions of two variables $f(x,y)$ can be visualized as surfaces in three-dimensional space. For functions of more than two variables, we often use level sets, contour plots, or partial visualizations to gain insight into their behavior.\nExercises on Functions of Several Variables # Exercise 1.1 Identify the domain and range of the function $f(x,y) = \\sqrt{1-x^2-y^2}$.\nSolution: For the function to be real-valued, we need $1-x^2-y^2 \\geq 0$, which means $x^2+y^2 \\leq 1$. This describes a closed disk in the $xy$-plane with center at the origin and radius 1.\nTherefore, the domain is $D = {(x,y) \\in \\mathbb{R}^2 \\mid x^2+y^2 \\leq 1}$.\nSince $0 \\leq 1-x^2-y^2 \\leq 1$ for all points in the domain, we have $0 \\leq \\sqrt{1-x^2-y^2} \\leq 1$.\nTherefore, the range is $[0,1]$.\nGeometrically, the graph of this function forms the upper half of a sphere with radius 1 centered at the origin.\nExercise 1.2 Consider the vector-valued function $\\mathbf{F}(x,y) = (x^2-y^2, 2xy)$. Show that this function can be interpreted as a complex-valued function. What familiar operation does it represent?\nSolution: The function $\\mathbf{F}(x,y) = (x^2-y^2, 2xy)$ maps points from $\\mathbb{R}^2$ to $\\mathbb{R}^2$.\nIf we interpret the input $(x,y)$ as a complex number $z = x + yi$, and the output $(x^2-y^2, 2xy)$ as another complex number $w = (x^2-y^2) + (2xy)i$, we can show that $\\mathbf{F}$ represents complex squaring.\nThis is because: $z^2 = (x + yi)^2 = x^2 + 2xyi + (yi)^2 = x^2 + 2xyi - y^2 = (x^2 - y^2) + (2xy)i$\nTherefore, $\\mathbf{F}(x,y) = (x^2-y^2, 2xy)$ corresponds to the complex function $f(z) = z^2$.\n2. Limits and Continuity # Before diving into differentiation, we need to establish the concepts of limits and continuity in higher dimensions, which are more intricate than their single-variable counterparts.\nDefinition A.3 (Limit of a Multivariate Function)\nLet $f: D \\rightarrow \\mathbb{R}$ be a function defined on a domain $D \\subseteq \\mathbb{R}^n$, and let $\\mathbf{a}$ be a point in $\\mathbb{R}^n$ such that every neighborhood of $\\mathbf{a}$ contains points in $D$ other than $\\mathbf{a}$ itself. We say that $f(\\mathbf{x})$ approaches the limit $L$ as $\\mathbf{x}$ approaches $\\mathbf{a}$, written as \\begin{equation} \\lim_{\\mathbf{x} \\to \\mathbf{a}} f(\\mathbf{x}) = L \\end{equation} if for every $\\varepsilon \u0026gt; 0$, there exists a $\\delta \u0026gt; 0$ such that \\begin{equation} |f(\\mathbf{x}) - L| \u0026lt; \\varepsilon \\quad \\text{whenever} \\quad 0 \u0026lt; |\\mathbf{x} - \\mathbf{a}| \u0026lt; \\delta \\quad \\text{and} \\quad \\mathbf{x} \\in D \\end{equation} where $|\\mathbf{x} - \\mathbf{a}|$ is the Euclidean distance between $\\mathbf{x}$ and $\\mathbf{a}$. Path Independence of Limits\nA crucial difference in multivariate calculus is that a limit exists only if the function approaches the same value regardless of the path taken to approach the point. In single-variable calculus, there are only two directions of approach (from the left or right), but in higher dimensions, there are infinitely many possible paths.\nTheorem A.1 (Existence of Multivariate Limits)\nIf there exist two different paths approaching $\\mathbf{a}$ along which $f(\\mathbf{x})$ approaches different limits, then $\\lim_{\\mathbf{x} \\to \\mathbf{a}} f(\\mathbf{x})$ does not exist. Proof\nSuppose there are two paths $P_1$ and $P_2$ approaching $\\mathbf{a}$ such that \\begin{equation} \\lim_{\\mathbf{x} \\to \\mathbf{a}, \\mathbf{x} \\in P_1} f(\\mathbf{x}) = L_1 \\quad \\text{and} \\quad \\lim_{\\mathbf{x} \\to \\mathbf{a}, \\mathbf{x} \\in P_2} f(\\mathbf{x}) = L_2 \\end{equation} where $L_1 \\neq L_2$.\nLet $\\varepsilon = \\frac{|L_1 - L_2|}{3} \u0026gt; 0$. According to the definition of a limit, if $\\lim_{\\mathbf{x} \\to \\mathbf{a}} f(\\mathbf{x}) = L$ were to exist, there would be a $\\delta \u0026gt; 0$ such that \\begin{equation} |f(\\mathbf{x}) - L| \u0026lt; \\varepsilon \\quad \\text{whenever} \\quad 0 \u0026lt; |\\mathbf{x} - \\mathbf{a}| \u0026lt; \\delta \\quad \\text{and} \\quad \\mathbf{x} \\in D \\end{equation}\nHowever, we can choose points $\\mathbf{x}_1 \\in P_1$ and $\\mathbf{x}_2 \\in P_2$ such that $|\\mathbf{x}_1 - \\mathbf{a}| \u0026lt; \\delta$ and $|\\mathbf{x}_2 - \\mathbf{a}| \u0026lt; \\delta$, and by the existence of the limits along the paths, we can ensure that \\begin{equation} |f(\\mathbf{x}_1) - L_1| \u0026lt; \\varepsilon \\quad \\text{and} \\quad |f(\\mathbf{x}_2) - L_2| \u0026lt; \\varepsilon \\end{equation}\nThis would imply that \\begin{equation} |f(\\mathbf{x}_1) - L| \u0026lt; \\varepsilon \\quad \\text{and} \\quad |f(\\mathbf{x}_2) - L| \u0026lt; \\varepsilon \\end{equation}\nBy the triangle inequality, we have \\begin{equation} |L_1 - L_2| \\leq |L_1 - f(\\mathbf{x}_1)| + |f(\\mathbf{x}_1) - L| + |L - f(\\mathbf{x}_2)| + |f(\\mathbf{x}_2) - L_2| \u0026lt; 4\\varepsilon = \\frac{4|L_1 - L_2|}{3} \\end{equation}\nThis leads to the contradiction $|L_1 - L_2| \u0026lt; \\frac{4|L_1 - L_2|}{3}$.\nTherefore, the limit $\\lim_{\\mathbf{x} \\to \\mathbf{a}} f(\\mathbf{x})$ cannot exist if the limits along different paths are different.\nâ–  Definition A.4 (Continuity of a Multivariate Function)\nA function $f: D \\rightarrow \\mathbb{R}$ is continuous at a point $\\mathbf{a} \\in D$ if \\begin{equation} \\lim_{\\mathbf{x} \\to \\mathbf{a}} f(\\mathbf{x}) = f(\\mathbf{a}) \\end{equation} A function is continuous on a set $S \\subseteq D$ if it is continuous at every point in $S$. Exercises on Limits and Continuity # Exercise 2.1 Determine whether the following limit exists, and find its value if it does: \\begin{equation} \\lim_{(x,y) \\to (0,0)} \\frac{x^2 - y^2}{x^2 + y^2} \\end{equation}\nSolution: To evaluate this limit, we need to check if the function approaches the same value regardless of the path used to approach the origin.\nLet\u0026rsquo;s parameterize different paths to the origin and check if they yield the same limit:\nAlong the $x$-axis (where $y = 0$): \\begin{equation} \\lim_{x \\to 0} \\frac{x^2 - 0^2}{x^2 + 0^2} = \\lim_{x \\to 0} \\frac{x^2}{x^2} = 1 \\end{equation}\nAlong the $y$-axis (where $x = 0$): \\begin{equation} \\lim_{y \\to 0} \\frac{0^2 - y^2}{0^2 + y^2} = \\lim_{y \\to 0} \\frac{-y^2}{y^2} = -1 \\end{equation}\nSince we get different values along different paths, the limit does not exist.\nAlternatively, we can use polar coordinates. Setting $x = r\\cos\\theta$ and $y = r\\sin\\theta$, we get: \\begin{equation} \\frac{x^2 - y^2}{x^2 + y^2} = \\frac{r^2\\cos^2\\theta - r^2\\sin^2\\theta}{r^2\\cos^2\\theta + r^2\\sin^2\\theta} = \\frac{\\cos^2\\theta - \\sin^2\\theta}{\\cos^2\\theta + \\sin^2\\theta} = \\cos(2\\theta) \\end{equation}\nAs $r \\to 0$, the value depends on $\\theta$, which confirms that the limit does not exist.\nExercise 2.2 Show that the function $f(x,y) = \\frac{xy}{x^2+y^2}$ with $f(0,0) = 0$ is not continuous at the origin.\nSolution: For $f$ to be continuous at the origin, we need \\begin{equation} \\lim_{(x,y) \\to (0,0)} \\frac{xy}{x^2+y^2} = f(0,0) = 0 \\end{equation}\nLet\u0026rsquo;s examine the limit along the line $y = mx$, where $m$ is a constant: \\begin{equation} \\lim_{x \\to 0} \\frac{x \\cdot mx}{x^2 + (mx)^2} = \\lim_{x \\to 0} \\frac{mx^2}{x^2(1 + m^2)} = \\frac{m}{1 + m^2} \\end{equation}\nThe limit depends on the value of $m$, so the limit doesn\u0026rsquo;t exist as $(x,y) \\to (0,0)$. Therefore, $f$ is not continuous at the origin.\nTo be more specific, along the line $y = x$ (where $m = 1$), the limit is $\\frac{1}{2}$, while along the line $y = -x$ (where $m = -1$), the limit is $-\\frac{1}{2}$.\n3. Partial Derivatives # Partial derivatives are the foundation of multivariate calculus, allowing us to analyze how a function changes with respect to one variable while keeping all others fixed.\nDefinition A.5 (Partial Derivative)\nThe partial derivative of a function $f(x_1, x_2, \\ldots, x_n)$ with respect to $x_i$ is defined as \\begin{equation} \\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x_1, \\ldots, x_i + h, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h} \\end{equation} provided the limit exists. Alternative notations include $f_{x_i}$, $D_i f$, or $\\partial_{x_i} f$. Geometric Interpretation of Partial Derivatives\nThe partial derivative $\\frac{\\partial f}{\\partial x_i}$ at a point $\\mathbf{a}$ represents the slope of the tangent line to the curve formed by intersecting the graph of $f$ with the plane passing through $\\mathbf{a}$ parallel to the $x_i$-axis and the $z$-axis (where $z = f(x_1, \\ldots, x_n)$).\nTheorem A.2 (Partial Derivatives of Elementary Functions)\nFor functions $f, g: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ and constant $c$, the following hold:\n$\\frac{\\partial}{\\partial x_i}(f + g) = \\frac{\\partial f}{\\partial x_i} + \\frac{\\partial g}{\\partial x_i}$ $\\frac{\\partial}{\\partial x_i}(cf) = c \\frac{\\partial f}{\\partial x_i}$ $\\frac{\\partial}{\\partial x_i}(fg) = f \\frac{\\partial g}{\\partial x_i} + g \\frac{\\partial f}{\\partial x_i}$ $\\frac{\\partial}{\\partial x_i}\\left(\\frac{f}{g}\\right) = \\frac{g \\frac{\\partial f}{\\partial x_i} - f \\frac{\\partial g}{\\partial x_i}}{g^2}$, where $g \\neq 0$ $\\frac{\\partial}{\\partial x_i}(x_i^n) = nx_i^{n-1}$ $\\frac{\\partial}{\\partial x_i}(x_j) = \\begin{cases} 1 \u0026amp; \\text{if } i = j \\ 0 \u0026amp; \\text{if } i \\neq j \\end{cases}$ Proof\nThese properties follow directly from the corresponding rules for derivatives in single-variable calculus, since a partial derivative with respect to $x_i$ treats all other variables as constants.\nFor example, to prove property 3 (the product rule): \\begin{align} \\frac{\\partial}{\\partial x_i}(fg) \u0026amp;= \\lim_{h \\to 0} \\frac{f(\\mathbf{x}+h\\mathbf{e}_i)g(\\mathbf{x}+h\\mathbf{e}i) - f(\\mathbf{x})g(\\mathbf{x})}{h} \\ \u0026amp;= \\lim{h \\to 0} \\frac{f(\\mathbf{x}+h\\mathbf{e}_i)g(\\mathbf{x}+h\\mathbf{e}_i) - f(\\mathbf{x})g(\\mathbf{x}+h\\mathbf{e}_i) + f(\\mathbf{x})g(\\mathbf{x}+h\\mathbf{e}i) - f(\\mathbf{x})g(\\mathbf{x})}{h} \\ \u0026amp;= \\lim{h \\to 0} \\left[ \\frac{f(\\mathbf{x}+h\\mathbf{e}_i) - f(\\mathbf{x})}{h} \\cdot g(\\mathbf{x}+h\\mathbf{e}_i) + f(\\mathbf{x}) \\cdot \\frac{g(\\mathbf{x}+h\\mathbf{e}i) - g(\\mathbf{x})}{h} \\right] \\ \u0026amp;= \\lim{h \\to 0} \\frac{f(\\mathbf{x}+h\\mathbf{e}i) - f(\\mathbf{x})}{h} \\cdot \\lim{h \\to 0} g(\\mathbf{x}+h\\mathbf{e}i) + f(\\mathbf{x}) \\cdot \\lim{h \\to 0} \\frac{g(\\mathbf{x}+h\\mathbf{e}_i) - g(\\mathbf{x})}{h} \\ \u0026amp;= \\frac{\\partial f}{\\partial x_i} \\cdot g + f \\cdot \\frac{\\partial g}{\\partial x_i} \\end{align}\nThe other properties can be proven similarly.\nâ–  Exercises on Partial Derivatives # Exercise 3.1 Find all first-order partial derivatives of the function $f(x,y,z) = \\sin(xy) + e^{yz} + \\ln(xz)$.\nSolution: We apply the definition and rules of partial differentiation:\n$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} \\sin(xy) + \\frac{\\partial}{\\partial x} e^{yz} + \\frac{\\partial}{\\partial x} \\ln(xz)$\n$= y \\cos(xy) + 0 + \\frac{z}{xz} = y \\cos(xy) + \\frac{1}{x}$\n$\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} \\sin(xy) + \\frac{\\partial}{\\partial y} e^{yz} + \\frac{\\partial}{\\partial y} \\ln(xz)$\n$= x \\cos(xy) + z e^{yz} + 0 = x \\cos(xy) + z e^{yz}$\n$\\frac{\\partial f}{\\partial z} = \\frac{\\partial}{\\partial z} \\sin(xy) + \\frac{\\partial}{\\partial z} e^{yz} + \\frac{\\partial}{\\partial z} \\ln(xz)$\n$= 0 + y e^{yz} + \\frac{x}{xz} = y e^{yz} + \\frac{1}{z}$\nExercise 3.2 If $f(x,y) = x^3 + 2xy^2 - 3y^3$, find $f_x(2,1)$ and $f_y(2,1)$.\nSolution: First, we find the partial derivatives:\n$f_x(x,y) = \\frac{\\partial f}{\\partial x} = 3x^2 + 2y^2$\n$f_y(x,y) = \\frac{\\partial f}{\\partial y} = 4xy - 9y^2$\nNow, we evaluate these at the point $(2,1)$:\n$f_x(2,1) = 3(2)^2 + 2(1)^2 = 3(4) + 2 = 12 + 2 = 14$\n$f_y(2,1) = 4(2)(1) - 9(1)^2 = 8 - 9 = -1$\n4. Directional Derivatives and the Gradient # While partial derivatives tell us how a function changes along the coordinate axes, directional derivatives extend this concept to any direction.\nDefinition A.6 (Directional Derivative)\nThe directional derivative of a function $f$ at a point $\\mathbf{a}$ in the direction of a unit vector $\\mathbf{u}$ is defined as \\begin{equation} D_{\\mathbf{u}} f(\\mathbf{a}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{a} + h\\mathbf{u}) - f(\\mathbf{a})}{h} \\end{equation} provided the limit exists. Definition A.7 (Gradient)\nThe gradient of a differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is the vector \\begin{equation} \\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right) \\end{equation} Theorem A.3 (Directional Derivative and Gradient)\nIf $f$ is differentiable at a point $\\mathbf{a}$, then for any unit vector $\\mathbf{u}$, \\begin{equation} D_{\\mathbf{u}} f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} \\end{equation} where $\\cdot$ represents the dot product. Proof\nLet $\\mathbf{a} = (a_1, a_2, \\ldots, a_n)$ and $\\mathbf{u} = (u_1, u_2, \\ldots, u_n)$ with $|\\mathbf{u}| = 1$.\nBy the definition of differentiability, there exists a function $\\varepsilon(\\mathbf{h})$ such that \\begin{equation} f(\\mathbf{a} + \\mathbf{h}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a}) \\cdot \\mathbf{h} + |\\mathbf{h}| \\varepsilon(\\mathbf{h}) \\end{equation} where $\\lim_{\\mathbf{h} \\to \\mathbf{0}} \\varepsilon(\\mathbf{h}) = 0$.\nSetting $\\mathbf{h} = h\\mathbf{u}$, we get \\begin{equation} f(\\mathbf{a} + h\\mathbf{u}) = f(\\mathbf{a}) + \\nabla f(\\mathbf{a}) \\cdot (h\\mathbf{u}) + |h\\mathbf{u}| \\varepsilon(h\\mathbf{u}) \\end{equation}\nSince $|\\mathbf{u}| = 1$, we have $|h\\mathbf{u}| = |h|$.\nFor $h \u0026gt; 0$, we get \\begin{equation} f(\\mathbf{a} + h\\mathbf{u}) = f(\\mathbf{a}) + h \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} + h \\varepsilon(h\\mathbf{u}) \\end{equation}\nTherefore, \\begin{equation} \\frac{f(\\mathbf{a} + h\\mathbf{u}) - f(\\mathbf{a})}{h} = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} + \\varepsilon(h\\mathbf{u}) \\end{equation}\nTaking the limit as $h \\to 0$, we get \\begin{equation} D_{\\mathbf{u}} f(\\mathbf{a}) = \\nabla f(\\mathbf{a}) \\cdot \\mathbf{u} \\end{equation}\nThis completes the proof.\nâ–  Theorem A.4 (Properties of the Gradient)\nLet $f$ and $g$ be differentiable functions, and let $c$ be a constant.\n$\\nabla(f + g) = \\nabla f + \\nabla g$ $\\nabla(cf) = c \\nabla f$ $\\nabla(fg) = f \\nabla g + g \\nabla f$ $\\nabla\\left(\\frac{f}{g}\\right) = \\frac{g \\nabla f - f \\nabla g}{g^2}$, where $g \\neq 0$ The gradient $\\nabla f(\\mathbf{a})$ points in the direction of maximum rate of increase of $f$ at $\\mathbf{a}$. $|\\nabla f(\\mathbf{a})|$ is the maximum rate of change of $f$ at $\\mathbf{a}$. At a point $\\mathbf{a}$ where $f$ has a level surface, $\\nabla f(\\mathbf{a})$ is orthogonal to that level surface. Gradient as the Direction of Steepest Ascent\nThink of the gradient $\\nabla f$ as pointing \u0026ldquo;uphill\u0026rdquo; on the graph of $f$. If you stand at a point on a hill and want to climb it as steeply as possible, you should move in the direction of the gradient.\nExercises on Directional Derivatives and the Gradient # Exercise 4.1 Find the gradient of the function $f(x,y,z) = x^2y + yz^2 + 3xyz$.\nSolution: We calculate each partial derivative to form the gradient:\n$\\frac{\\partial f}{\\partial x} = 2xy + 3yz$\n$\\frac{\\partial f}{\\partial y} = x^2 + z^2 + 3xz$\n$\\frac{\\partial f}{\\partial z} = 2yz + 3xy$\nTherefore, the gradient is: $\\nabla f(x,y,z) = (2xy + 3yz, x^2 + z^2 + 3xz, 2yz + 3xy)$\nExercise 4.2 Let $f(x,y) = x^2 + 2y^2$. Find the directional derivative of $f$ at the point $P = (1,2)$ in the direction of the vector $\\mathbf{v} = 3\\mathbf{i} + 4\\mathbf{j}$.\nSolution: First, we calculate the gradient of $f$: $\\nabla f(x,y) = (2x, 4y)$\nAt the point $P = (1,2)$, we have: $\\nabla f(1,2) = (2(1), 4(2)) = (2, 8)$\nTo find the directional derivative, we need a unit vector in the direction of $\\mathbf{v}$: $|\\mathbf{v}| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$\nThe unit vector is: $\\mathbf{u} = \\frac{\\mathbf{v}}{|\\mathbf{v}|} = \\frac{3\\mathbf{i} + 4\\mathbf{j}}{5} = \\frac{3}{5}\\mathbf{i} + \\frac{4}{5}\\mathbf{j}$\nThe directional derivative is: $D_{\\mathbf{u}} f(P) = \\nabla f(P) \\cdot \\mathbf{u} = (2, 8) \\cdot \\left(\\frac{3}{5}, \\frac{4}{5}\\right) = 2 \\cdot \\frac{3}{5} + 8 \\cdot \\frac{4}{5} = \\frac{6}{5} + \\frac{32}{5} = \\frac{38}{5} = 7.6$\n5. Tangent Planes and Linear Approximation # Just as the derivative provides a linear approximation to a function in single-variable calculus, the gradient enables us to find linear approximations to multivariable functions.\nDefinition A.8 (Tangent Plane)\nThe tangent plane to the surface $z = f(x,y)$ at the point $(a,b,f(a,b))$ has the equation \\begin{equation} z = f(a,b) + f_x(a,b)(x - a) + f_y(a,b)(y - b) \\end{equation} Definition A.9 (Linear Approximation)\nThe linear approximation (also called the first-order Taylor approximation) of a differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ near a point $\\mathbf{a}$ is \\begin{equation} f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\nabla f(\\mathbf{a}) \\cdot (\\mathbf{x} - \\mathbf{a}) \\end{equation} Theorem A.5 (Differentiability and Continuous Partial Derivatives)\nIf a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ has continuous first-order partial derivatives in an open set containing a point $\\mathbf{a}$, then $f$ is differentiable at $\\mathbf{a}$. Proof\nFor simplicity, we\u0026rsquo;ll prove the case $n = 2$. Let $f(x,y)$ have continuous partial derivatives $f_x$ and $f_y$ in an open set containing $(a,b)$.\nWe need to show that \\begin{equation} \\lim_{(h,k) \\to (0,0)} \\frac{f(a+h, b+k) - f(a,b) - f_x(a,b)h - f_y(a,b)k}{\\sqrt{h^2 + k^2}} = 0 \\end{equation}\nDefine the function $g(t) = f(a+th, b+tk)$ for $0 \\leq t \\leq 1$. By the chain rule, \\begin{equation} g\u0026rsquo;(t) = f_x(a+th, b+tk)h + f_y(a+th, b+tk)k \\end{equation}\nBy the Mean Value Theorem, there exists a value $c \\in (0,1)$ such that \\begin{equation} g(1) - g(0) = g\u0026rsquo;(c) \\cdot 1 \\end{equation}\nThis gives us \\begin{equation} f(a+h, b+k) - f(a,b) = f_x(a+ch, b+ck)h + f_y(a+ch, b+ck)k \\end{equation}\nTherefore, \\begin{align} \u0026amp;\\frac{f(a+h, b+k) - f(a,b) - f_x(a,b)h - f_y(a,b)k}{\\sqrt{h^2 + k^2}} \\ \u0026amp;= \\frac{f_x(a+ch, b+ck)h + f_y(a+ch, b+ck)k - f_x(a,b)h - f_y(a,b)k}{\\sqrt{h^2 + k^2}} \\ \u0026amp;= \\frac{[f_x(a+ch, b+ck) - f_x(a,b)]h + [f_y(a+ch, b+ck) - f_y(a,b)]k}{\\sqrt{h^2 + k^2}} \\end{align}\nSince $f_x$ and $f_y$ are continuous, we have \\begin{equation} \\lim_{(h,k) \\to (0,0)} f_x(a+ch, b+ck) = f_x(a,b) \\quad \\text{and} \\quad \\lim_{(h,k) \\to (0,0)} f_y(a+ch, b+ck) = f_y(a,b) \\end{equation}\nTherefore, \\begin{align} \u0026amp;\\lim_{(h,k) \\to (0,0)} \\frac{[f_x(a+ch, b+ck) - f_x(a,b)]h + [f_y(a+ch, b+ck) - f_y(a,b)]k}{\\sqrt{h^2 + k^2}} \\ \u0026amp;\\leq \\lim_{(h,k) \\to (0,0)} \\frac{|f_x(a+ch, b+ck) - f_x(a,b)| \\cdot |h| + |f_y(a+ch, b+ck) - f_y(a,b)| \\cdot |k|}{\\sqrt{h^2 + k^2}} \\ \u0026amp;\\leq \\lim_{(h,k) \\to (0,0)} |f_x(a+ch, b+ck) - f_x(a,b)| \\cdot \\frac{|h|}{\\sqrt{h^2 + k^2}} + |f_y(a+ch, b+ck) - f_y(a,b)| \\cdot \\frac{|k|}{\\sqrt{h^2 + k^2}} \\end{align}\nSince $\\frac{|h|}{\\sqrt{h^2 + k^2}} \\leq 1$ and $\\frac{|k|}{\\sqrt{h^2 + k^2}} \\leq 1$, and the differences in the partial derivatives approach zero as $(h,k) \\to (0,0)$, the limit is zero, proving that $f$ is differentiable at $(a,b)$.\nâ–  Exercises on Tangent Planes and Linear Approximation # Exercise 5.1 Find the equation of the tangent plane to the surface $z = 2x^2 + 3y^2 - xy$ at the point $(1, 2, 9)$.\nSolution: First, we verify that the point $(1, 2, 9)$ lies on the surface: $z = 2(1)^2 + 3(2)^2 - (1)(2) = 2 + 12 - 2 = 12 - 2 = 10$\nThis doesn\u0026rsquo;t match the $z$-value we were given. Let\u0026rsquo;s check if there\u0026rsquo;s an error in the problem statement.\nIf we evaluate the function at $(1, 2)$: $f(1, 2) = 2(1)^2 + 3(2)^2 - (1)(2) = 2 + 12 - 2 = 12$\nSo the point should be $(1, 2, 12)$ for consistency.\nLet\u0026rsquo;s recalculate assuming the correct point is $(1, 2, 9)$. We need to find the partial derivatives:\n$f_x(x, y) = 4x - y$ $f_y(x, y) = 6y - x$\nAt the point $(1, 2)$: $f_x(1, 2) = 4(1) - 2 = 2$ $f_y(1, 2) = 6(2) - 1 = 11$\nThe equation of the tangent plane is: $z = f(1, 2) + f_x(1, 2)(x - 1) + f_y(1, 2)(y - 2)$ $z = 9 + 2(x - 1) + 11(y - 2)$ $z = 9 + 2x - 2 + 11y - 22$ $z = 2x + 11y - 15$\nExercise 5.2 Use the linear approximation to estimate the value of $f(2.1, 0.95)$ where $f(x, y) = \\sqrt{x^2 + y^2 + 1}$.\nSolution: We\u0026rsquo;ll use the linear approximation around the point $(2, 1)$: $f(x, y) \\approx f(2, 1) + f_x(2, 1)(x - 2) + f_y(2, 1)(y - 1)$\nFirst, we calculate $f(2, 1)$: $f(2, 1) = \\sqrt{2^2 + 1^2 + 1} = \\sqrt{4 + 1 + 1} = \\sqrt{6} \\approx 2.449$\nNext, we find the partial derivatives: $f_x(x, y) = \\frac{x}{\\sqrt{x^2 + y^2 + 1}}$ $f_y(x, y) = \\frac{y}{\\sqrt{x^2 + y^2 + 1}}$\nAt the point $(2, 1)$: $f_x(2, 1) = \\frac{2}{\\sqrt{6}} \\approx 0.816$ $f_y(2, 1) = \\frac{1}{\\sqrt{6}} \\approx 0.408$\nNow we can compute the linear approximation: $f(2.1, 0.95) \\approx f(2, 1) + f_x(2, 1)(2.1 - 2) + f_y(2, 1)(0.95 - 1)$ $\\approx 2.449 + 0.816 \\cdot 0.1 + 0.408 \\cdot (-0.05)$ $\\approx 2.449 + 0.0816 - 0.0204$ $\\approx 2.510$\nThe actual value is $\\sqrt{2.1^2 + 0.95^2 + 1} = \\sqrt{4.41 + 0.9025 + 1} = \\sqrt{6.3125} \\approx 2.512$, so our approximation is very close.\n6. The Chain Rule # The chain rule is a fundamental tool for finding derivatives of composite functions, and its multivariable version is essential for addressing complex real-world problems.\nDefinition A.10 (Chain Rule for Functions of Several Variables)\nSuppose $z = f(x, y)$ where $x = g(t)$ and $y = h(t)$. If $g$ and $h$ are differentiable at $t_0$, and $f$ is differentiable at $(g(t_0), h(t_0))$, then $z$ is differentiable at $t_0$ and \\begin{equation} \\frac{dz}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} \\end{equation} Theorem A.6 (Chain Rule for Multiple Independent Variables)\nIf $w = f(x, y, z)$ where $x = g(s, t)$, $y = h(s, t)$, and $z = k(s, t)$, then \\begin{equation} \\frac{\\partial w}{\\partial s} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial s} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial s} + \\frac{\\partial f}{\\partial z} \\frac{\\partial z}{\\partial s} \\end{equation} and \\begin{equation} \\frac{\\partial w}{\\partial t} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial t} + \\frac{\\partial f}{\\partial z} \\frac{\\partial z}{\\partial t} \\end{equation} Proof\nWe\u0026rsquo;ll prove the case for one intermediate variable, and the general case follows by induction.\nLet $z = f(x, y)$ where $x = g(t)$ and $y = h(t)$. We need to show that \\begin{equation} \\frac{dz}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} \\end{equation}\nBy the definition of the derivative, \\begin{equation} \\frac{dz}{dt} = \\lim_{\\Delta t \\to 0} \\frac{f(g(t + \\Delta t), h(t + \\Delta t)) - f(g(t), h(t))}{\\Delta t} \\end{equation}\nLet $\\Delta x = g(t + \\Delta t) - g(t)$ and $\\Delta y = h(t + \\Delta t) - h(t)$. Then we can rewrite the derivative as \\begin{equation} \\frac{dz}{dt} = \\lim_{\\Delta t \\to 0} \\frac{f(g(t) + \\Delta x, h(t) + \\Delta y) - f(g(t), h(t))}{\\Delta t} \\end{equation}\nBy the differentiability of $f$, we have \\begin{equation} f(g(t) + \\Delta x, h(t) + \\Delta y) - f(g(t), h(t)) = \\frac{\\partial f}{\\partial x} \\Delta x + \\frac{\\partial f}{\\partial y} \\Delta y + \\varepsilon_1 \\Delta x + \\varepsilon_2 \\Delta y \\end{equation} where $\\varepsilon_1, \\varepsilon_2 \\to 0$ as $\\Delta x, \\Delta y \\to 0$.\nTherefore, \\begin{align} \\frac{dz}{dt} \u0026amp;= \\lim_{\\Delta t \\to 0} \\frac{\\frac{\\partial f}{\\partial x} \\Delta x + \\frac{\\partial f}{\\partial y} \\Delta y + \\varepsilon_1 \\Delta x + \\varepsilon_2 \\Delta y}{\\Delta t} \\ \u0026amp;= \\lim_{\\Delta t \\to 0} \\left[ \\frac{\\partial f}{\\partial x} \\frac{\\Delta x}{\\Delta t} + \\frac{\\partial f}{\\partial y} \\frac{\\Delta y}{\\Delta t} + \\varepsilon_1 \\frac{\\Delta x}{\\Delta t} + \\varepsilon_2 \\frac{\\Delta y}{\\Delta t} \\right] \\ \u0026amp;= \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} + 0 + 0 \\ \u0026amp;= \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} \\end{align}\nThis completes the proof.\nâ–  When to Use the Chain Rule\nThe chain rule is necessary whenever you\u0026rsquo;re differentiating a composite function. This includes cases where you\u0026rsquo;re working with:\nA function expressed in terms of intermediate variables A change of coordinates (e.g., from Cartesian to polar) Implicit differentiation Functions along curves or surfaces Exercises on the Chain Rule # Exercise 6.1 Let $f(x,y) = x^2y + 3xy^2$ where $x = s^2t$ and $y = st^2$. Find $\\frac{\\partial f}{\\partial s}$ and $\\frac{\\partial f}{\\partial t}$.\nSolution: First, we compute the partial derivatives of $f$ with respect to $x$ and $y$: $\\frac{\\partial f}{\\partial x} = 2xy + 3y^2$ $\\frac{\\partial f}{\\partial y} = x^2 + 6xy$\nNext, we find the partial derivatives of $x$ and $y$ with respect to $s$ and $t$: $\\frac{\\partial x}{\\partial s} = 2st$ $\\frac{\\partial x}{\\partial t} = s^2$ $\\frac{\\partial y}{\\partial s} = t^2$ $\\frac{\\partial y}{\\partial t} = 2st$\nNow, using the chain rule: $\\frac{\\partial f}{\\partial s} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial s} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial s}$ $= (2xy + 3y^2)(2st) + (x^2 + 6xy)(t^2)$\nSubstituting $x = s^2t$ and $y = st^2$: $\\frac{\\partial f}{\\partial s} = (2(s^2t)(st^2) + 3(st^2)^2)(2st) + ((s^2t)^2 + 6(s^2t)(st^2))(t^2)$ $= (2s^3t^3 + 3s^2t^4)(2st) + (s^4t^2 + 6s^3t^3)(t^2)$ $= 4s^4t^4 + 6s^3t^5 + s^4t^4 + 6s^3t^5$ $= 5s^4t^4 + 12s^3t^5$\nSimilarly, for $\\frac{\\partial f}{\\partial t}$: $\\frac{\\partial f}{\\partial t} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial t}$ $= (2xy + 3y^2)(s^2) + (x^2 + 6xy)(2st)$\nSubstituting $x = s^2t$ and $y = st^2$: $\\frac{\\partial f}{\\partial t} = (2(s^2t)(st^2) + 3(st^2)^2)(s^2) + ((s^2t)^2 + 6(s^2t)(st^2))(2st)$ $= (2s^3t^3 + 3s^2t^4)(s^2) + (s^4t^2 + 6s^3t^3)(2st)$ $= 2s^5t^3 + 3s^4t^4 + 2s^5t^3 + 12s^4t^4$ $= 4s^5t^3 + 15s^4t^4$\nExercise 6.2 If $w = xe^{y/z}$ where $x = r\\cos\\theta$, $y = r\\sin\\theta$, and $z = r$, find $\\frac{\\partial w}{\\partial r}$ and $\\frac{\\partial w}{\\partial \\theta}$ at the point where $r = 1$ and $\\theta = \\pi/4$.\nSolution: First, we compute the partial derivatives of $w$ with respect to $x$, $y$, and $z$: $\\frac{\\partial w}{\\partial x} = e^{y/z}$ $\\frac{\\partial w}{\\partial y} = x \\cdot e^{y/z} \\cdot \\frac{1}{z} = \\frac{x}{z}e^{y/z}$ $\\frac{\\partial w}{\\partial z} = x \\cdot e^{y/z} \\cdot \\left(-\\frac{y}{z^2}\\right) = -\\frac{xy}{z^2}e^{y/z}$\nNext, we find the partial derivatives of $x$, $y$, and $z$ with respect to $r$ and $\\theta$: $\\frac{\\partial x}{\\partial r} = \\cos\\theta$ $\\frac{\\partial x}{\\partial \\theta} = -r\\sin\\theta$ $\\frac{\\partial y}{\\partial r} = \\sin\\theta$ $\\frac{\\partial y}{\\partial \\theta} = r\\cos\\theta$ $\\frac{\\partial z}{\\partial r} = 1$ $\\frac{\\partial z}{\\partial \\theta} = 0$\nNow, using the chain rule: $\\frac{\\partial w}{\\partial r} = \\frac{\\partial w}{\\partial x} \\frac{\\partial x}{\\partial r} + \\frac{\\partial w}{\\partial y} \\frac{\\partial y}{\\partial r} + \\frac{\\partial w}{\\partial z} \\frac{\\partial z}{\\partial r}$ $= e^{y/z} \\cdot \\cos\\theta + \\frac{x}{z}e^{y/z} \\cdot \\sin\\theta + \\left(-\\frac{xy}{z^2}e^{y/z}\\right) \\cdot 1$\nSubstituting $x = r\\cos\\theta$, $y = r\\sin\\theta$, and $z = r$: $\\frac{\\partial w}{\\partial r} = e^{\\sin\\theta} \\cdot \\cos\\theta + \\frac{r\\cos\\theta}{r}e^{\\sin\\theta} \\cdot \\sin\\theta - \\frac{r\\cos\\theta \\cdot r\\sin\\theta}{r^2}e^{\\sin\\theta}$ $= e^{\\sin\\theta} \\cdot \\cos\\theta + e^{\\sin\\theta} \\cdot \\cos\\theta \\cdot \\sin\\theta - e^{\\sin\\theta} \\cdot \\cos\\theta \\cdot \\sin\\theta$ $= e^{\\sin\\theta} \\cdot \\cos\\theta$\nAt the point where $r = 1$ and $\\theta = \\pi/4$: $\\frac{\\partial w}{\\partial r} = e^{\\sin(\\pi/4)} \\cdot \\cos(\\pi/4) = e^{1/\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}} = \\frac{e^{1/\\sqrt{2}}}{\\sqrt{2}}$\nSimilarly, for $\\frac{\\partial w}{\\partial \\theta}$: $\\frac{\\partial w}{\\partial \\theta} = \\frac{\\partial w}{\\partial x} \\frac{\\partial x}{\\partial \\theta} + \\frac{\\partial w}{\\partial y} \\frac{\\partial y}{\\partial \\theta} + \\frac{\\partial w}{\\partial z} \\frac{\\partial z}{\\partial \\theta}$ $= e^{y/z} \\cdot (-r\\sin\\theta) + \\frac{x}{z}e^{y/z} \\cdot r\\cos\\theta + \\left(-\\frac{xy}{z^2}e^{y/z}\\right) \\cdot 0$ $= -r\\sin\\theta \\cdot e^{\\sin\\theta} + \\frac{r\\cos\\theta}{r}e^{\\sin\\theta} \\cdot r\\cos\\theta$ $= -r\\sin\\theta \\cdot e^{\\sin\\theta} + r\\cos^2\\theta \\cdot e^{\\sin\\theta}$ $= r e^{\\sin\\theta}(\\cos^2\\theta - \\sin\\theta)$\nAt the point where $r = 1$ and $\\theta = \\pi/4$: $\\frac{\\partial w}{\\partial \\theta} = e^{\\sin(\\pi/4)}(\\cos^2(\\pi/4) - \\sin(\\pi/4)) = e^{1/\\sqrt{2}}(1/2 - 1/\\sqrt{2}) = e^{1/\\sqrt{2}}(1/2 - 1/\\sqrt{2})$\n7. Higher-Order Derivatives and the Hessian # Just as we can take second and higher derivatives in single-variable calculus, the same applies to functions of multiple variables, leading to rich structures and properties.\nDefinition A.11 (Higher-Order Partial Derivatives)\nFor a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ with sufficiently continuous partial derivatives, the second-order partial derivatives are denoted by \\begin{equation} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial}{\\partial x_i} \\left( \\frac{\\partial f}{\\partial x_j} \\right) \\end{equation} Alternative notations include $f_{x_i x_j}$, $D_i D_j f$, or $\\partial_{x_i x_j} f$. Theorem A.7 (Equality of Mixed Partials (Schwarz\u0026#39;s Theorem))\nIf a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ has continuous second-order partial derivatives in an open set containing a point $\\mathbf{a}$, then at $\\mathbf{a}$, \\begin{equation} \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} \\end{equation} for all $i, j = 1, 2, \\ldots, n$. Proof\nFor simplicity, we\u0026rsquo;ll prove the case $n = 2$ with $i = 1$ and $j = 2$. Let $f(x,y)$ have continuous second-order partial derivatives in an open set containing $(a,b)$.\nWe need to show that $\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}$ at $(a,b)$.\nDefine the function \\begin{equation} g(x,y) = \\frac{f(x,y) - f(x,b) - f(a,y) + f(a,b)}{(x-a)(y-b)} \\end{equation} for $(x,y) \\neq (a,b)$.\nWe can compute the limit of $g(x,y)$ as $(x,y) \\to (a,b)$ in two different ways.\nFirst, let\u0026rsquo;s approach along the path where $x \\to a$ first and then $y \\to b$: \\begin{align} \\lim_{(x,y) \\to (a,b)} g(x,y) \u0026amp;= \\lim_{y \\to b} \\lim_{x \\to a} \\frac{f(x,y) - f(x,b) - f(a,y) + f(a,b)}{(x-a)(y-b)} \\ \u0026amp;= \\lim_{y \\to b} \\frac{1}{y-b} \\lim_{x \\to a} \\frac{f(x,y) - f(x,b) - f(a,y) + f(a,b)}{x-a} \\ \u0026amp;= \\lim_{y \\to b} \\frac{1}{y-b} \\lim_{x \\to a} \\frac{f(x,y) - f(a,y)}{x-a} - \\frac{f(x,b) - f(a,b)}{x-a} \\ \u0026amp;= \\lim_{y \\to b} \\frac{1}{y-b} [f_x(a,y) - f_x(a,b)] \\ \u0026amp;= \\lim_{y \\to b} \\frac{f_x(a,y) - f_x(a,b)}{y-b} \\ \u0026amp;= f_{xy}(a,b) \\end{align}\nSimilarly, approaching along the path where $y \\to b$ first and then $x \\to a$: \\begin{align} \\lim_{(x,y) \\to (a,b)} g(x,y) \u0026amp;= \\lim_{x \\to a} \\lim_{y \\to b} \\frac{f(x,y) - f(x,b) - f(a,y) + f(a,b)}{(x-a)(y-b)} \\ \u0026amp;= \\lim_{x \\to a} \\frac{1}{x-a} \\lim_{y \\to b} \\frac{f(x,y) - f(x,b) - f(a,y) + f(a,b)}{y-b} \\ \u0026amp;= \\lim_{x \\to a} \\frac{1}{x-a} \\lim_{y \\to b} \\frac{f(x,y) - f(x,b)}{y-b} - \\frac{f(a,y) - f(a,b)}{y-b} \\ \u0026amp;= \\lim_{x \\to a} \\frac{1}{x-a} [f_y(x,b) - f_y(a,b)] \\ \u0026amp;= \\lim_{x \\to a} \\frac{f_y(x,b) - f_y(a,b)}{x-a} \\ \u0026amp;= f_{yx}(a,b) \\end{align}\nSince both limits must be equal, we have $f_{xy}(a,b) = f_{yx}(a,b)$.\nâ–  Definition A.12 (Hessian Matrix)\nThe Hessian matrix of a twice-differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is the $n \\times n$ matrix of second-order partial derivatives: \\begin{equation} H_f = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_2^2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{pmatrix} \\end{equation} Interpreting the Hessian Matrix\nThe Hessian matrix provides information about the local curvature of a function. In particular, its eigenvalues and determinant are used to classify critical points (as minima, maxima, or saddle points) and to determine the concavity or convexity of the function.\nExercises on Higher-Order Derivatives and the Hessian # Exercise 7.1 Find all second-order partial derivatives of the function $f(x,y) = x^3y^2 + x^2y^3 + e^{xy}$.\nSolution: First, let\u0026rsquo;s find the first-order partial derivatives:\n$\\frac{\\partial f}{\\partial x} = 3x^2y^2 + 2xy^3 + ye^{xy}$ $\\frac{\\partial f}{\\partial y} = 2x^3y + 3x^2y^2 + xe^{xy}$\nNow, we compute the second-order partial derivatives:\n$\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x}\\left(3x^2y^2 + 2xy^3 + ye^{xy}\\right) = 6xy^2 + 2y^3 + y^2e^{xy}$\n$\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y}\\left(2x^3y + 3x^2y^2 + xe^{xy}\\right) = 2x^3 + 6x^2y + x^2e^{xy}$\n$\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x}\\left(2x^3y + 3x^2y^2 + xe^{xy}\\right) = 6x^2y + 6xy^2 + e^{xy} + x^2ye^{xy}$\n$\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial}{\\partial y}\\left(3x^2y^2 + 2xy^3 + ye^{xy}\\right) = 6x^2y + 6xy^2 + e^{xy} + x^2ye^{xy}$\nWe can verify Schwarz\u0026rsquo;s theorem: $\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}$.\nExercise 7.2 Find the Hessian matrix of the function $f(x,y,z) = x^2y + y^2z + z^2x$ at the point $(1,2,3)$ and determine whether it is positive definite, negative definite, or indefinite.\nSolution: First, let\u0026rsquo;s find the first-order partial derivatives:\n$\\frac{\\partial f}{\\partial x} = 2xy + z^2$ $\\frac{\\partial f}{\\partial y} = x^2 + 2yz$ $\\frac{\\partial f}{\\partial z} = y^2 + 2zx$\nNow, we compute the second-order partial derivatives:\n$\\frac{\\partial^2 f}{\\partial x^2} = 2y$ $\\frac{\\partial^2 f}{\\partial y^2} = 2z$ $\\frac{\\partial^2 f}{\\partial z^2} = 2x$\n$\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x} = 2x$ $\\frac{\\partial^2 f}{\\partial x \\partial z} = \\frac{\\partial^2 f}{\\partial z \\partial x} = 2z$ $\\frac{\\partial^2 f}{\\partial y \\partial z} = \\frac{\\partial^2 f}{\\partial z \\partial y} = 2y$\nAt the point $(1,2,3)$, the Hessian matrix is: $H_f(1,2,3) = \\begin{pmatrix} 4 \u0026amp; 2 \u0026amp; 6 \\ 2 \u0026amp; 6 \u0026amp; 4 \\ 6 \u0026amp; 4 \u0026amp; 2 \\end{pmatrix}$\nTo determine if the Hessian is positive definite, negative definite, or indefinite, we need to examine the eigenvalues or the principal minors.\nLet\u0026rsquo;s calculate the determinants of the leading principal minors:\nFirst minor: $|4| = 4 \u0026gt; 0$ Second minor: $\\begin{vmatrix} 4 \u0026amp; 2 \\ 2 \u0026amp; 6 \\end{vmatrix} = 4 \\cdot 6 - 2 \\cdot 2 = 24 - 4 = 20 \u0026gt; 0$ Third minor (the determinant of the entire matrix): $\\begin{vmatrix} 4 \u0026amp; 2 \u0026amp; 6 \\ 2 \u0026amp; 6 \u0026amp; 4 \\ 6 \u0026amp; 4 \u0026amp; 2 \\end{vmatrix}$ $= 4 \\cdot \\begin{vmatrix} 6 \u0026amp; 4 \\ 4 \u0026amp; 2 \\end{vmatrix} - 2 \\cdot \\begin{vmatrix} 2 \u0026amp; 4 \\ 6 \u0026amp; 2 \\end{vmatrix} + 6 \\cdot \\begin{vmatrix} 2 \u0026amp; 6 \\ 6 \u0026amp; 4 \\end{vmatrix}$ $= 4 \\cdot (6 \\cdot 2 - 4 \\cdot 4) - 2 \\cdot (2 \\cdot 2 - 4 \\cdot 6) + 6 \\cdot (2 \\cdot 4 - 6 \\cdot 6)$ $= 4 \\cdot (12 - 16) - 2 \\cdot (4 - 24) + 6 \\cdot (8 - 36)$ $= 4 \\cdot (-4) - 2 \\cdot (-20) + 6 \\cdot (-28)$ $= -16 + 40 - 168 = -144 \u0026lt; 0$ Since the first and second leading principal minors are positive, but the third is negative, the Hessian matrix is indefinite. This suggests that the critical point at $(1,2,3)$ (if it is indeed a critical point) would be a saddle point.\n8. Taylor\u0026rsquo;s Theorem for Multivariate Functions # Taylor\u0026rsquo;s theorem generalizes to multiple variables, providing a way to approximate functions locally using polynomial expressions.\nTheorem A.8 (Taylor\u0026#39;s Theorem for Multivariate Functions)\nLet $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a function such that all its partial derivatives of order $k+1$ exist and are continuous in an open set containing the line segment from $\\mathbf{a}$ to $\\mathbf{a} + \\mathbf{h}$. Then \\begin{equation} f(\\mathbf{a} + \\mathbf{h}) = \\sum_{|\\alpha| \\leq k} \\frac{1}{\\alpha!} D^{\\alpha}f(\\mathbf{a}) \\mathbf{h}^{\\alpha} + R_k \\end{equation} where $\\alpha = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_n)$ is a multi-index, $|\\alpha| = \\alpha_1 + \\alpha_2 + \\ldots + \\alpha_n$, $\\alpha! = \\alpha_1! \\alpha_2! \\ldots \\alpha_n!$, $D^{\\alpha}f = \\frac{\\partial^{|\\alpha|}f}{\\partial x_1^{\\alpha_1} \\partial x_2^{\\alpha_2} \\ldots \\partial x_n^{\\alpha_n}}$, and $\\mathbf{h}^{\\alpha} = h_1^{\\alpha_1} h_2^{\\alpha_2} \\ldots h_n^{\\alpha_n}$. The remainder term $R_k$ satisfies \\begin{equation} |R_k| \\leq \\frac{M}{(k+1)!} |\\mathbf{h}|^{k+1} \\end{equation} where $M$ is an upper bound for the $(k+1)$-th order partial derivatives of $f$ on the line segment from $\\mathbf{a}$ to $\\mathbf{a} + \\mathbf{h}$. Definition A.13 (Second-Order Taylor Approximation)\nThe second-order Taylor approximation of a twice-differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ near a point $\\mathbf{a}$ is \\begin{equation} f(\\mathbf{a} + \\mathbf{h}) \\approx f(\\mathbf{a}) + \\nabla f(\\mathbf{a}) \\cdot \\mathbf{h} + \\frac{1}{2} \\mathbf{h}^T H_f(\\mathbf{a}) \\mathbf{h} \\end{equation} where $H_f(\\mathbf{a})$ is the Hessian matrix of $f$ at $\\mathbf{a}$. Applications of Taylor\u0026rsquo;s Theorem\nTaylor\u0026rsquo;s theorem is crucial for many applications:\nNumerical approximation of functions Error estimation in numerical methods Optimization algorithms like Newton\u0026rsquo;s method Sensitivity analysis in physics and engineering Series expansions in mathematical physics Exercises on Taylor\u0026rsquo;s Theorem # Exercise 8.1 Find the second-order Taylor polynomial of the function $f(x,y) = \\ln(1 + x + y + xy)$ around the point $(0,0)$.\nSolution: We use the formula: $f(x,y) \\approx f(0,0) + \\nabla f(0,0) \\cdot (x,y) + \\frac{1}{2}(x,y)^T H_f(0,0) (x,y)$\nFirst, we compute $f(0,0)$: $f(0,0) = \\ln(1 + 0 + 0 + 0 \\cdot 0) = \\ln(1) = 0$\nNext, we find the gradient at $(0,0)$: $\\frac{\\partial f}{\\partial x} = \\frac{1 + y}{1 + x + y + xy}$ $\\frac{\\partial f}{\\partial y} = \\frac{1 + x}{1 + x + y + xy}$\nAt $(0,0)$: $\\frac{\\partial f}{\\partial x}(0,0) = \\frac{1 + 0}{1 + 0 + 0 + 0 \\cdot 0} = 1$ $\\frac{\\partial f}{\\partial y}(0,0) = \\frac{1 + 0}{1 + 0 + 0 + 0 \\cdot 0} = 1$\nSo $\\nabla f(0,0) = (1, 1)$.\nNow, we compute the Hessian matrix: $\\frac{\\partial^2 f}{\\partial x^2} = -\\frac{(1 + y)^2}{(1 + x + y + xy)^2}$ $\\frac{\\partial^2 f}{\\partial y^2} = -\\frac{(1 + x)^2}{(1 + x + y + xy)^2}$ $\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{1}{1 + x + y + xy} - \\frac{(1 + y)(1 + x)}{(1 + x + y + xy)^2}$\nAt $(0,0)$: $\\frac{\\partial^2 f}{\\partial x^2}(0,0) = -\\frac{(1 + 0)^2}{(1 + 0 + 0 + 0 \\cdot 0)^2} = -1$ $\\frac{\\partial^2 f}{\\partial y^2}(0,0) = -\\frac{(1 + 0)^2}{(1 + 0 + 0 + 0 \\cdot 0)^2} = -1$ $\\frac{\\partial^2 f}{\\partial x \\partial y}(0,0) = \\frac{1}{1 + 0 + 0 + 0 \\cdot 0} - \\frac{(1 + 0)(1 + 0)}{(1 + 0 + 0 + 0 \\cdot 0)^2} = 1 - 1 = 0$\nSo $H_f(0,0) = \\begin{pmatrix} -1 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{pmatrix}$.\nThe second-order Taylor polynomial is: $f(x,y) \\approx 0 + (1, 1) \\cdot (x, y) + \\frac{1}{2}(x, y)^T \\begin{pmatrix} -1 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{pmatrix} (x, y)$ $= x + y + \\frac{1}{2}(-x^2 - y^2)$ $= x + y - \\frac{1}{2}x^2 - \\frac{1}{2}y^2$\nExercise 8.2 Use a second-order Taylor approximation to estimate the value of $f(1.1, 0.9)$ where $f(x,y) = e^x \\sin(y)$.\nSolution: We\u0026rsquo;ll use the second-order Taylor approximation around the point $(1, 1)$: $f(x, y) \\approx f(1, 1) + \\nabla f(1, 1) \\cdot ((x-1), (y-1)) + \\frac{1}{2}((x-1), (y-1))^T H_f(1, 1) ((x-1), (y-1))$\nFirst, we compute $f(1, 1)$: $f(1, 1) = e^1 \\sin(1) = e \\cdot \\sin(1) \\approx 2.718 \\cdot 0.841 \\approx 2.287$\nNext, we find the gradient at $(1, 1)$: $\\frac{\\partial f}{\\partial x} = e^x \\sin(y)$ $\\frac{\\partial f}{\\partial y} = e^x \\cos(y)$\nAt $(1, 1)$: $\\frac{\\partial f}{\\partial x}(1, 1) = e^1 \\sin(1) \\approx 2.718 \\cdot 0.841 \\approx 2.287$ $\\frac{\\partial f}{\\partial y}(1, 1) = e^1 \\cos(1) \\approx 2.718 \\cdot 0.540 \\approx 1.469$\nNow, we compute the Hessian matrix: $\\frac{\\partial^2 f}{\\partial x^2} = e^x \\sin(y)$ $\\frac{\\partial^2 f}{\\partial y^2} = -e^x \\sin(y)$ $\\frac{\\partial^2 f}{\\partial x \\partial y} = e^x \\cos(y)$\nAt $(1, 1)$: $\\frac{\\partial^2 f}{\\partial x^2}(1, 1) = e^1 \\sin(1) \\approx 2.287$ $\\frac{\\partial^2 f}{\\partial y^2}(1, 1) = -e^1 \\sin(1) \\approx -2.287$ $\\frac{\\partial^2 f}{\\partial x \\partial y}(1, 1) = e^1 \\cos(1) \\approx 1.469$\nSo $H_f(1, 1) = \\begin{pmatrix} 2.287 \u0026amp; 1.469 \\ 1.469 \u0026amp; -2.287 \\end{pmatrix}$.\nThe second-order Taylor approximation is: $f(x, y) \\approx 2.287 + 2.287(x-1) + 1.469(y-1) + \\frac{1}{2}[(x-1), (y-1)] \\begin{pmatrix} 2.287 \u0026amp; 1.469 \\ 1.469 \u0026amp; -2.287 \\end{pmatrix} \\begin{pmatrix} x-1 \\ y-1 \\end{pmatrix}$\nNow we evaluate this at $(1.1, 0.9)$: $f(1.1, 0.9) \\approx 2.287 + 2.287(0.1) + 1.469(-0.1) + \\frac{1}{2}[(0.1), (-0.1)] \\begin{pmatrix} 2.287 \u0026amp; 1.469 \\ 1.469 \u0026amp; -2.287 \\end{pmatrix} \\begin{pmatrix} 0.1 \\ -0.1 \\end{pmatrix}$ $= 2.287 + 0.2287 - 0.1469 + \\frac{1}{2}[(0.1)(2.287)(0.1) + (0.1)(1.469)(-0.1) + (-0.1)(1.469)(0.1) + (-0.1)(-2.287)(-0.1)]$ $= 2.287 + 0.2287 - 0.1469 + \\frac{1}{2}[0.02287 - 0.01469 - 0.01469 - 0.02287]$ $= 2.287 + 0.2287 - 0.1469 + \\frac{1}{2}[-0.02938]$ $= 2.287 + 0.2287 - 0.1469 - 0.01469$ $= 2.354$\nThe actual value is $f(1.1, 0.9) = e^{1.1} \\sin(0.9) \\approx 3.004 \\cdot 0.783 \\approx 2.352$, so our approximation is very accurate.\n9. Maxima, Minima, and Saddle Points # Finding extreme values is a central problem in calculus, with applications in optimization across many disciplines.\nDefinition A.14 (Critical Point)\nA point $\\mathbf{a}$ in the domain of a differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is a critical point if $\\nabla f(\\mathbf{a}) = \\mathbf{0}$. Theorem A.9 (Second Derivative Test for Functions of Two Variables)\nLet $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ be a function with continuous second-order partial derivatives, and let $(a,b)$ be a critical point of $f$. Define \\begin{equation} D = f_{xx}(a,b) f_{yy}(a,b) - [f_{xy}(a,b)]^2 \\end{equation}\nIf $D \u0026gt; 0$ and $f_{xx}(a,b) \u0026gt; 0$, then $f$ has a local minimum at $(a,b)$. If $D \u0026gt; 0$ and $f_{xx}(a,b) \u0026lt; 0$, then $f$ has a local maximum at $(a,b)$. If $D \u0026lt; 0$, then $f$ has a saddle point at $(a,b)$. If $D = 0$, the test is inconclusive. Proof\nLet\u0026rsquo;s use Taylor\u0026rsquo;s theorem to expand $f$ around the critical point $(a,b)$: \\begin{equation} f(a+h, b+k) = f(a,b) + \\frac{1}{2}(f_{xx}(a,b)h^2 + 2f_{xy}(a,b)hk + f_{yy}(a,b)k^2) + \\text{higher-order terms} \\end{equation}\nSince $(a,b)$ is a critical point, the first-order terms vanish. We focus on the quadratic form \\begin{equation} Q(h,k) = f_{xx}(a,b)h^2 + 2f_{xy}(a,b)hk + f_{yy}(a,b)k^2 \\end{equation}\nThis quadratic form can be written in matrix notation as \\begin{equation} Q(h,k) = \\begin{pmatrix} h \u0026amp; k \\end{pmatrix} \\begin{pmatrix} f_{xx}(a,b) \u0026amp; f_{xy}(a,b) \\ f_{xy}(a,b) \u0026amp; f_{yy}(a,b) \\end{pmatrix} \\begin{pmatrix} h \\ k \\end{pmatrix} \\end{equation}\nThe behavior of $f$ near the critical point depends on the behavior of this quadratic form.\nIf $D \u0026gt; 0$ and $f_{xx}(a,b) \u0026gt; 0$, then the matrix $\\begin{pmatrix} f_{xx}(a,b) \u0026amp; f_{xy}(a,b) \\ f_{xy}(a,b) \u0026amp; f_{yy}(a,b) \\end{pmatrix}$ is positive definite, meaning that $Q(h,k) \u0026gt; 0$ for all non-zero vectors $(h,k)$. This means that $f(a+h, b+k) \u0026gt; f(a,b)$ for small non-zero vectors $(h,k)$, so $f$ has a local minimum at $(a,b)$.\nIf $D \u0026gt; 0$ and $f_{xx}(a,b) \u0026lt; 0$, then the matrix is negative definite, meaning that $Q(h,k) \u0026lt; 0$ for all non-zero vectors $(h,k)$. This means that $f(a+h, b+k) \u0026lt; f(a,b)$ for small non-zero vectors $(h,k)$, so $f$ has a local maximum at $(a,b)$.\nIf $D \u0026lt; 0$, then the matrix has both positive and negative eigenvalues (it\u0026rsquo;s indefinite), meaning that $Q(h,k)$ is positive for some directions and negative for others. This means that $f$ increases in some directions and decreases in others, which characterizes a saddle point.\nIf $D = 0$, the matrix is either positive semidefinite, negative semidefinite, or indefinite, and higher-order terms are needed to determine the behavior of $f$ at the critical point.\nâ–  Theorem A.10 (Second Derivative Test for Functions of Several Variables)\nLet $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a function with continuous second-order partial derivatives, and let $\\mathbf{a}$ be a critical point of $f$. Let $H_f(\\mathbf{a})$ be the Hessian matrix of $f$ at $\\mathbf{a}$.\nIf $H_f(\\mathbf{a})$ is positive definite, then $f$ has a local minimum at $\\mathbf{a}$. If $H_f(\\mathbf{a})$ is negative definite, then $f$ has a local maximum at $\\mathbf{a}$. If $H_f(\\mathbf{a})$ has both positive and negative eigenvalues, then $f$ has a saddle point at $\\mathbf{a}$. If $H_f(\\mathbf{a})$ is positive or negative semidefinite with at least one zero eigenvalue, the test is inconclusive. Testing for Definiteness\nTo determine if a symmetric matrix is positive definite, negative definite, or indefinite:\nCompute the eigenvalues. If all are positive, the matrix is positive definite; if all are negative, it\u0026rsquo;s negative definite; if some are positive and some negative, it\u0026rsquo;s indefinite. Alternatively, check the leading principal minors. For a positive definite matrix, all should be positive. For a negative definite matrix, the signs should alternate, starting with negative. Exercises on Maxima, Minima, and Saddle Points # Exercise 9.1 Find and classify all critical points of the function $f(x,y) = x^3 + y^3 - 3xy$.\nSolution: First, we find the critical points by setting the partial derivatives equal to zero:\n$\\frac{\\partial f}{\\partial x} = 3x^2 - 3y = 0$ $\\frac{\\partial f}{\\partial y} = 3y^2 - 3x = 0$\nFrom the first equation, we get $y = x^2$. Substituting into the second equation: $3(x^2)^2 - 3x = 0$ $3x^4 - 3x = 0$ $3x(x^3 - 1) = 0$\nThis gives us $x = 0$ or $x = 1$.\nIf $x = 0$, then $y = 0^2 = 0$. If $x = 1$, then $y = 1^2 = 1$.\nSo the critical points are $(0,0)$ and $(1,1)$.\nNow, we classify these critical points using the second derivative test. We compute the second-order partial derivatives:\n$\\frac{\\partial^2 f}{\\partial x^2} = 6x$ $\\frac{\\partial^2 f}{\\partial y^2} = 6y$ $\\frac{\\partial^2 f}{\\partial x \\partial y} = -3$\nAt the point $(0,0)$: $\\frac{\\partial^2 f}{\\partial x^2}(0,0) = 6 \\cdot 0 = 0$ $\\frac{\\partial^2 f}{\\partial y^2}(0,0) = 6 \\cdot 0 = 0$ $\\frac{\\partial^2 f}{\\partial x \\partial y}(0,0) = -3$\nThe determinant of the Hessian is: $D = \\frac{\\partial^2 f}{\\partial x^2}(0,0) \\cdot \\frac{\\partial^2 f}{\\partial y^2}(0,0) - \\left[\\frac{\\partial^2 f}{\\partial x \\partial y}(0,0)\\right]^2 = 0 \\cdot 0 - (-3)^2 = -9 \u0026lt; 0$\nSince $D \u0026lt; 0$, the point $(0,0)$ is a saddle point.\nAt the point $(1,1)$: $\\frac{\\partial^2 f}{\\partial x^2}(1,1) = 6 \\cdot 1 = 6$ $\\frac{\\partial^2 f}{\\partial y^2}(1,1) = 6 \\cdot 1 = 6$ $\\frac{\\partial^2 f}{\\partial x \\partial y}(1,1) = -3$\nThe determinant of the Hessian is: $D = \\frac{\\partial^2 f}{\\partial x^2}(1,1) \\cdot \\frac{\\partial^2 f}{\\partial y^2}(1,1) - \\left[\\frac{\\partial^2 f}{\\partial x \\partial y}(1,1)\\right]^2 = 6 \\cdot 6 - (-3)^2 = 36 - 9 = 27 \u0026gt; 0$\nSince $D \u0026gt; 0$ and $\\frac{\\partial^2 f}{\\partial x^2}(1,1) \u0026gt; 0$, the point $(1,1)$ is a local minimum.\nExercise 9.2 Find the maximum and minimum values of the function $f(x,y) = 2x^2 + y^2 - 4x - 2y + 5$ on the disk $x^2 + y^2 \\leq 4$.\nSolution: To find the extreme values of $f$ on the disk $x^2 + y^2 \\leq 4$, we need to check:\nCritical points inside the disk Points on the boundary circle $x^2 + y^2 = 4$ Step 1: Find the critical points by setting the partial derivatives equal to zero:\n$\\frac{\\partial f}{\\partial x} = 4x - 4 = 0 \\implies x = 1$ $\\frac{\\partial f}{\\partial y} = 2y - 2 = 0 \\implies y = 1$\nSo we have a critical point at $(1, 1)$. Let\u0026rsquo;s verify that it\u0026rsquo;s inside the disk: $1^2 + 1^2 = 2 \u0026lt; 4$, so it\u0026rsquo;s inside.\nTo classify this critical point, we compute the second-order partial derivatives:\n$\\frac{\\partial^2 f}{\\partial x^2} = 4 \u0026gt; 0$ $\\frac{\\partial^2 f}{\\partial y^2} = 2 \u0026gt; 0$ $\\frac{\\partial^2 f}{\\partial x \\partial y} = 0$\nThe determinant of the Hessian is: $D = 4 \\cdot 2 - 0^2 = 8 \u0026gt; 0$\nSince $D \u0026gt; 0$ and $\\frac{\\partial^2 f}{\\partial x^2} \u0026gt; 0$, the point $(1, 1)$ is a local minimum.\nThe value of $f$ at this point is: $f(1, 1) = 2(1)^2 + (1)^2 - 4(1) - 2(1) + 5 = 2 + 1 - 4 - 2 + 5 = 2$\nStep 2: Examine the boundary $x^2 + y^2 = 4$.\nWe can use the method of Lagrange multipliers. We want to find the critical points of $f$ subject to the constraint $g(x, y) = x^2 + y^2 - 4 = 0$.\nWe set up the Lagrangian: $L(x, y, \\lambda) = f(x, y) - \\lambda g(x, y) = 2x^2 + y^2 - 4x - 2y + 5 - \\lambda(x^2 + y^2 - 4)$\nTaking partial derivatives and setting them to zero:\n$\\frac{\\partial L}{\\partial x} = 4x - 4 - 2\\lambda x = 0$ $\\frac{\\partial L}{\\partial y} = 2y - 2 - 2\\lambda y = 0$ $\\frac{\\partial L}{\\partial \\lambda} = -(x^2 + y^2 - 4) = 0$\nFrom the first equation: $x(2 - \\lambda) = 2$ From the second equation: $y(1 - \\lambda) = 1$\nIf $\\lambda = 2$, then $x$ would be undefined from the first equation. So $\\lambda \\neq 2$. If $\\lambda = 1$, then $y$ would be undefined from the second equation. So $\\lambda \\neq 1$.\nTherefore: $x = \\frac{2}{2 - \\lambda}$ and $y = \\frac{1}{1 - \\lambda}$\nSubstituting into the constraint $x^2 + y^2 = 4$: $\\left(\\frac{2}{2 - \\lambda}\\right)^2 + \\left(\\frac{1}{1 - \\lambda}\\right)^2 = 4$\nThis is a rather complex equation to solve directly. Let\u0026rsquo;s try a different approach.\nWe can parameterize the boundary circle as $x = 2\\cos\\theta$ and $y = 2\\sin\\theta$ for $0 \\leq \\theta \u0026lt; 2\\pi\n"},{"id":4,"href":"/numerical_optimization/docs/lectures/","title":"Lectures","section":"Docs","content":" Lectures # This section regroups the theory : main results, proofs and exercices behind optimization algorithms.\nContent Introduction I - Fundamentals II - Advanced problems III - Machine Learning problems Reminders "},{"id":5,"href":"/numerical_optimization/docs/lectures/reminders/linear_algebra/","title":"Linear Algebra","section":"Reminders","content":" Fundamentals of Linear Algebra # Linear algebra is one of the foundational branches of mathematics, with applications spanning from engineering and computer science to economics and physics. This document introduces the key concepts and theorems of linear algebra in a structured progression, demonstrating how each idea builds upon previous ones to form a coherent mathematical framework.\n1. Vectors and Vector Spaces # We begin our study of linear algebra with the fundamental concept of a vector space, which formalizes the notion of vectors and their operations. This abstraction allows us to work with many different types of mathematical objects using the same underlying principles.\nDefinition 0.1 (Vector Space)\nA vector space $V$ over a field $F$ is a set equipped with two operations:\nVector addition: $+: V \\times V \\rightarrow V$ Scalar multiplication: $\\cdot: F \\times V \\rightarrow V$ satisfying the following axioms for all $\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V$ and $a, b \\in F$:\nClosure under addition: $\\mathbf{u} + \\mathbf{v} \\in V$ Commutativity: $\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}$ Associativity: $(\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})$ Additive identity: There exists $\\mathbf{0} \\in V$ such that $\\mathbf{v} + \\mathbf{0} = \\mathbf{v}$ for all $\\mathbf{v} \\in V$ Additive inverse: For each $\\mathbf{v} \\in V$, there exists $-\\mathbf{v} \\in V$ such that $\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}$ Closure under scalar multiplication: $a \\cdot \\mathbf{v} \\in V$ Distributivity: $a \\cdot (\\mathbf{u} + \\mathbf{v}) = a \\cdot \\mathbf{u} + a \\cdot \\mathbf{v}$ and $(a + b) \\cdot \\mathbf{v} = a \\cdot \\mathbf{v} + b \\cdot \\mathbf{v}$ Scalar multiplication associativity: $a \\cdot (b \\cdot \\mathbf{v}) = (ab) \\cdot \\mathbf{v}$ Scalar multiplication identity: $1 \\cdot \\mathbf{v} = \\mathbf{v}$ Intuition for Vector Spaces\nThink of vectors as arrows with direction and magnitude. The vector space axioms formalize how these arrows can be combined and scaled while remaining within the same space. $\\mathbb{R}^n$ is the most common example, but polynomial spaces, function spaces, and matrix spaces are also important vector spaces.\nDefinition 0.2 (Vector Subspace)\nA subset $W$ of a vector space $V$ is a subspace if:\nThe zero vector $\\mathbf{0} \\in W$ $W$ is closed under addition: for all $\\mathbf{u}, \\mathbf{v} \\in W$, $\\mathbf{u} + \\mathbf{v} \\in W$ $W$ is closed under scalar multiplication: for all $\\mathbf{v} \\in W$ and $c \\in F$, $c\\mathbf{v} \\in W$ Theorem 0.1 (Linear Independence)\nA set of vectors ${\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k}$ is linearly independent if and only if the equation \\begin{equation} c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_k\\mathbf{v}_k = \\mathbf{0} \\label{eq:linear-independence} \\end{equation} has only the trivial solution $c_1 = c_2 = \\ldots = c_k = 0$. Proof\nWe prove both directions of the if and only if statement:\n($\\Rightarrow$) Suppose the set ${\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k}$ is linearly independent. By definition, this means that the only way to express the zero vector as a linear combination of these vectors is with all coefficients equal to zero. This is precisely the statement that equation \\eqref{eq:linear-independence} has only the trivial solution.\n($\\Leftarrow$) Conversely, suppose equation \\eqref{eq:linear-independence} has only the trivial solution $c_1 = c_2 = \\ldots = c_k = 0$. This means that the only way to express the zero vector as a linear combination of the vectors ${\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k}$ is with all coefficients equal to zero. By definition, this means the set is linearly independent.\nâ–  Definition 0.3 (Span)\nThe span of a set of vectors ${\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k}$ is the set of all possible linear combinations: \\begin{equation} \\text{Span}(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k) = {c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_k\\mathbf{v}_k \\mid c_1, c_2, \\ldots, c_k \\in F} \\label{eq:span} \\end{equation} Linear independence and span are two fundamental concepts that help us understand the structure of vector spaces. When we combine these ideas, we arrive at the important concept of a basisâ€”a minimal set of vectors that can represent every vector in the space through linear combinations.\nDefinition 0.4 (Basis)\nA basis for a vector space $V$ is a set of vectors $\\mathcal{B} = {\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n}$ that is:\nLinearly independent Spans $V$ The number of vectors in any basis is called the dimension of $V$, denoted $\\dim(V)$.\nExercises on Vectors and Vector Spaces # Exercise 1.1 Determine whether the set $W = {(x,y,z) \\in \\mathbb{R}^3 \\mid x + y + z = 0}$ is a subspace of $\\mathbb{R}^3$.\nSolution: We need to check the three conditions for a subspace:\nZero vector: $(0,0,0) \\in W$ since $0 + 0 + 0 = 0$ Closure under addition: Let $\\mathbf{u} = (u_1, u_2, u_3)$ and $\\mathbf{v} = (v_1, v_2, v_3)$ be vectors in $W$. Then $u_1 + u_2 + u_3 = 0$ and $v_1 + v_2 + v_3 = 0$. We have: $\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, u_2 + v_2, u_3 + v_3)$ $(u_1 + v_1) + (u_2 + v_2) + (u_3 + v_3) = (u_1 + u_2 + u_3) + (v_1 + v_2 + v_3) = 0 + 0 = 0$ So $\\mathbf{u} + \\mathbf{v} \\in W$ Closure under scalar multiplication: For any $c \\in \\mathbb{R}$ and $\\mathbf{u} = (u_1, u_2, u_3) \\in W$: $c\\mathbf{u} = (cu_1, cu_2, cu_3)$ $cu_1 + cu_2 + cu_3 = c(u_1 + u_2 + u_3) = c \\cdot 0 = 0$ So $c\\mathbf{u} \\in W$ Since all three conditions are satisfied, $W$ is indeed a subspace of $\\mathbb{R}^3$.\nExercise 1.2 Determine whether the set ${(1,1,0), (0,1,1), (1,0,1)}$ is linearly independent in $\\mathbb{R}^3$.\nSolution: To determine linear independence, we need to check if the equation $c_1(1,1,0) + c_2(0,1,1) + c_3(1,0,1) = (0,0,0)$ has only the trivial solution.\nThis gives us the system: $c_1 + c_3 = 0$ $c_1 + c_2 = 0$ $c_2 + c_3 = 0$\nFrom the first two equations, we get $c_3 = -c_1$ and $c_2 = -c_1$. Substituting into the third equation: $-c_1 + (-c_1) = 0$ $-2c_1 = 0$ $c_1 = 0$\nThis means $c_2 = c_3 = 0$ as well. Since we only get the trivial solution, the set is linearly independent.\n2. Matrices and Linear Transformations # Having established the foundation of vector spaces, we now turn to linear transformationsâ€”functions that preserve the vector space structure. Matrices provide a concrete way to represent these abstract transformations, allowing us to apply computational techniques to study their properties.\nDefinition 0.5 (Matrix)\nAn $m \\times n$ matrix $A$ over a field $F$ is a rectangular array of elements from $F$ arranged in $m$ rows and $n$ columns:\n\\begin{equation} A = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{m1} \u0026amp; a_{m2} \u0026amp; \\cdots \u0026amp; a_{mn} \\end{pmatrix} \\label{eq:matrix} \\end{equation}\nWe denote the $(i,j)$-entry as $a_{ij}$ or $[A]_{ij}$.\nDefinition 0.6 (Linear Transformation)\nA function $T: V \\rightarrow W$ between vector spaces is a linear transformation if for all vectors $\\mathbf{u}, \\mathbf{v} \\in V$ and all scalars $c \\in F$:\n$T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})$ (Additivity) $T(c\\mathbf{u}) = cT(\\mathbf{u})$ (Homogeneity) Theorem 0.2 (Matrix Representation)\nEvery linear transformation $T: V \\rightarrow W$ between finite-dimensional vector spaces with bases can be represented by a unique matrix $A$ such that \\begin{equation} T(\\mathbf{v}) = A\\mathbf{v} \\label{eq:matrix-representation} \\end{equation} where $\\mathbf{v}$ is expressed in the basis of $V$ and $T(\\mathbf{v})$ in the basis of $W$. Proof\nLet ${\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n}$ be a basis for $V$ and ${\\mathbf{w}_1, \\mathbf{w}_2, \\ldots, \\mathbf{w}_m}$ be a basis for $W$.\nFor each basis vector $\\mathbf{v}_j$ of $V$, its image under $T$ can be expressed uniquely as a linear combination of the basis vectors of $W$: $T(\\mathbf{v}j) = a{1j}\\mathbf{w}1 + a{2j}\\mathbf{w}2 + \\ldots + a{mj}\\mathbf{w}_m$\nLet $A$ be the $m \\times n$ matrix whose $(i,j)$-entry is $a_{ij}$. Now, for any vector $\\mathbf{v} \\in V$, we can write it uniquely as a linear combination of the basis vectors: $\\mathbf{v} = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n$\nApplying $T$ to both sides and using linearity: $T(\\mathbf{v}) = T(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\ldots + c_n\\mathbf{v}_n)$ $= c_1T(\\mathbf{v}_1) + c_2T(\\mathbf{v}_2) + \\ldots + c_nT(\\mathbf{v}_n)$\nSubstituting the expressions for $T(\\mathbf{v}j)$: $T(\\mathbf{v}) = c_1(a{11}\\mathbf{w}1 + \\ldots + a{m1}\\mathbf{w}m) + \\ldots + c_n(a{1n}\\mathbf{w}1 + \\ldots + a{mn}\\mathbf{w}_m)$\nRearranging: $T(\\mathbf{v}) = (a_{11}c_1 + \\ldots + a_{1n}c_n)\\mathbf{w}1 + \\ldots + (a{m1}c_1 + \\ldots + a_{mn}c_n)\\mathbf{w}_m$\nThis is exactly the result of the matrix-vector multiplication $A\\mathbf{c}$, where $\\mathbf{c} = (c_1, c_2, \\ldots, c_n)^T$ is the coordinate vector of $\\mathbf{v}$ with respect to the basis of $V$.\nFor uniqueness, suppose there are two matrices $A$ and $B$ such that $T(\\mathbf{v}) = A\\mathbf{v} = B\\mathbf{v}$ for all $\\mathbf{v} \\in V$. Then $(A-B)\\mathbf{v} = \\mathbf{0}$ for all $\\mathbf{v} \\in V$. In particular, this must hold for each basis vector $\\mathbf{v}_j$, which implies that all columns of $A-B$ are zero. Therefore, $A = B$.\nâ–  Change of Basis\nThe matrix representation depends on the chosen bases. Changing the basis transforms the matrix according to $A\u0026rsquo; = P^{-1}AP$, where $P$ is the change-of-basis matrix. This relationship is fundamental in understanding how the same linear transformation can be represented differently in different coordinate systems.\nDefinition 0.7 (Matrix Operations)\nFor matrices $A$ and $B$ of appropriate dimensions and scalar $c$:\nAddition: $[A + B]{ij} = [A]{ij} + [B]_{ij}$ Scalar multiplication: $[cA]{ij} = c[A]{ij}$ Matrix multiplication: $[AB]{ij} = \\sum{k=1}^{n} [A]{ik}[B]{kj}$ Transpose: $[A^T]{ij} = [A]{ji}$ Exercises on Matrices and Linear Transformations # Exercise 2.1 Let $T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ be the linear transformation that rotates vectors counterclockwise by $90^\\circ$. Find the matrix representation of $T$ with respect to the standard basis.\nSolution: The standard basis for $\\mathbb{R}^2$ is ${\\mathbf{e}_1, \\mathbf{e}_2}$ where $\\mathbf{e}_1 = (1,0)$ and $\\mathbf{e}_2 = (0,1)$.\nWhen rotated 90Â° counterclockwise:\n$T(\\mathbf{e}_1) = T(1,0) = (0,1) = \\mathbf{e}_2$ $T(\\mathbf{e}_2) = T(0,1) = (-1,0) = -\\mathbf{e}_1$ The matrix representation is formed by putting these output vectors as columns: $A = \\begin{pmatrix} 0 \u0026amp; -1 \\ 1 \u0026amp; 0 \\end{pmatrix}$\nLet\u0026rsquo;s verify with a test vector $\\mathbf{v} = (2,3)$: $A\\mathbf{v} = \\begin{pmatrix} 0 \u0026amp; -1 \\ 1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} 2 \\ 3 \\end{pmatrix} = \\begin{pmatrix} -3 \\ 2 \\end{pmatrix}$\nWhich is indeed the vector $(2,3)$ rotated 90Â° counterclockwise.\nExercise 2.2 Determine the kernel (null space) and image (range) of the linear transformation represented by the matrix $A = \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 2 \u0026amp; 4 \u0026amp; 6 \\end{pmatrix}$.\nSolution: The kernel of $A$ is $\\text{ker}(A) = {\\mathbf{v} \\in \\mathbb{R}^3 \\mid A\\mathbf{v} = \\mathbf{0}}$.\nWe need to find all vectors $\\mathbf{v} = (x,y,z)$ such that $A\\mathbf{v} = \\mathbf{0}$: $\\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 2 \u0026amp; 4 \u0026amp; 6 \\end{pmatrix} \\begin{pmatrix} x \\ y \\ z \\end{pmatrix} = \\begin{pmatrix} 0 \\ 0 \\end{pmatrix}$\nThis gives us: $x + 2y + 3z = 0$ $2x + 4y + 6z = 0$\nNote that the second equation is just 2 times the first, so we effectively have just one constraint: $x + 2y + 3z = 0$\nWe can parameterize the solution with two free variables, e.g., $y = s$ and $z = t$: $x = -2s - 3t$\nSo the kernel is: $\\text{ker}(A) = {(-2s-3t, s, t) \\mid s,t \\in \\mathbb{R}} = \\text{span}{(-2,1,0), (-3,0,1)}$\nFor the image, we look at the column space of $A$: $\\text{im}(A) = \\text{span}{(1,2), (2,4), (3,6)}$\nWe can see that columns 2 and 3 are multiples of column 1, so: $\\text{im}(A) = \\text{span}{(1,2)}$\nThus, the image is a one-dimensional subspace of $\\mathbb{R}^2$.\n3. Matrix Determinants and Invertibility # Now that we understand matrices as representations of linear transformations, we need tools to analyze their properties. The determinant is a scalar value associated with a square matrix that provides crucial information about its invertibility and geometric interpretation. This section explores determinants and their connection to the existence of solutions for linear systems.\nDefinition 0.8 (Determinant)\nThe determinant of a square matrix $A \\in \\mathbb{R}^{n \\times n}$ is a scalar value denoted $\\det(A)$ or $|A|$ that can be defined recursively:\nFor a $1 \\times 1$ matrix $A = [a]$, $\\det(A) = a$.\nFor an $n \\times n$ matrix with $n \u0026gt; 1$: \\begin{equation} \\det(A) = \\sum_{j=1}^{n} (-1)^{1+j} a_{1j} \\det(A_{1j}) \\label{eq:determinant} \\end{equation}\nwhere $A_{1j}$ is the $(n-1) \\times (n-1)$ submatrix obtained by removing the first row and $j$-th column of $A$.\nGeometric Interpretation of Determinant\nThe absolute value of the determinant of a matrix represents the scaling factor of the volume transformation under the linear map. For a $2 \\times 2$ matrix, $|\\det(A)|$ gives the area of the parallelogram formed by the column vectors of $A$. For a $3 \\times 3$ matrix, it gives the volume of the parallelepiped.\nTheorem 0.3 (Properties of Determinants)\nFor square matrices $A$ and $B$ of the same dimension:\n$\\det(AB) = \\det(A) \\cdot \\det(B)$ $\\det(A^T) = \\det(A)$ $\\det(cA) = c^n \\det(A)$ for an $n \\times n$ matrix and scalar $c$ $A$ is invertible if and only if $\\det(A) \\neq 0$ If $A$ has a row or column of zeros, then $\\det(A) = 0$ If two rows or columns of $A$ are identical, then $\\det(A) = 0$ Proof\nWe\u0026rsquo;ll prove some of these properties:\n$\\det(AB) = \\det(A) \\cdot \\det(B)$: This can be proven using the multilinearity of the determinant and the formula for matrix multiplication. For $2 \\times 2$ matrices, we can verify directly:\nLet $A = \\begin{pmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{pmatrix}$ and $B = \\begin{pmatrix} e \u0026amp; f \\ g \u0026amp; h \\end{pmatrix}$.\nThen $AB = \\begin{pmatrix} ae + bg \u0026amp; af + bh \\ ce + dg \u0026amp; cf + dh \\end{pmatrix}$.\n$\\det(AB) = (ae + bg)(cf + dh) - (af + bh)(ce + dg)$ $= aecf + aedh + bgcf + bgdh - afce - afdg - bhce - bhdg$ $= aecf + aedh + bgcf + bgdh - afce - afdg - bhce - bhdg$ $= (ad - bc)(eh - fg) = \\det(A) \\cdot \\det(B)$\nFor general $n \\times n$ matrices, a more advanced approach using permutations or eigenvalues is needed.\n$A$ is invertible if and only if $\\det(A) \\neq 0$:\n($\\Rightarrow$) Suppose $A$ is invertible. Then there exists a matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$. Taking determinants of both sides: $\\det(AA^{-1}) = \\det(I) = 1$\nBy property 1, $\\det(A) \\cdot \\det(A^{-1}) = 1$, which implies $\\det(A) \\neq 0$.\n($\\Leftarrow$) This direction requires the adjugate matrix or Cramer\u0026rsquo;s rule, which states that if $\\det(A) \\neq 0$, then $A^{-1} = \\frac{1}{\\det(A)} \\text{adj}(A)$, where $\\text{adj}(A)$ is the adjugate matrix of $A$. This formula shows that $A^{-1}$ exists when $\\det(A) \\neq 0$.\nIf $A$ has a row of zeros, say row $i$, then the determinant expression involves a sum of products, each containing one element from row $i$. Since all these elements are zero, their products are zero, making the entire determinant zero.\nâ–  Definition 0.9 (Matrix Inverse)\nFor a square matrix $A$, its inverse (if it exists) is a matrix $A^{-1}$ such that: \\begin{equation} AA^{-1} = A^{-1}A = I \\label{eq:inverse} \\end{equation} where $I$ is the identity matrix.\nFor a $2 \\times 2$ matrix $A = \\begin{pmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{pmatrix}$ with $\\det(A) \\neq 0$: \\begin{equation} A^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} d \u0026amp; -b \\ -c \u0026amp; a \\end{pmatrix} \\label{eq:2x2-inverse} \\end{equation}\nTheorem 0.4 (Invertible Matrix Theorem)\nFor an $n \\times n$ matrix $A$, the following statements are equivalent:\n$A$ is invertible. $\\det(A) \\neq 0$. The columns of $A$ form a basis for $\\mathbb{R}^n$. The linear system $A\\mathbf{x} = \\mathbf{b}$ has a unique solution for every $\\mathbf{b} \\in \\mathbb{R}^n$. The linear transformation $T(\\mathbf{x}) = A\\mathbf{x}$ is bijective. $\\text{rank}(A) = n$. The nullity of $A$ is zero (i.e., $\\dim(\\ker(A)) = 0$). $0$ is not an eigenvalue of $A$. Proof\nWe\u0026rsquo;ll prove some key equivalences:\n$(1 \\Leftrightarrow 4)$: $A$ is invertible if and only if $A\\mathbf{x} = \\mathbf{b}$ has a unique solution for every $\\mathbf{b}$.\n$(\\Rightarrow)$ If $A$ is invertible, then for any $\\mathbf{b}$, we can multiply both sides by $A^{-1}$ to get $\\mathbf{x} = A^{-1}\\mathbf{b}$, which is a unique solution.\n$(\\Leftarrow)$ If $A\\mathbf{x} = \\mathbf{b}$ has a unique solution for every $\\mathbf{b}$, then in particular, for each standard basis vector $\\mathbf{e}_j$, there exists a unique vector $\\mathbf{c}_j$ such that $A\\mathbf{c}_j = \\mathbf{e}_j$. Let $C$ be the matrix with columns $\\mathbf{c}_j$. Then $AC = I$. Similarly, for the system $\\mathbf{y}^T A = \\mathbf{e}_j^T$, there exists a unique row vector $\\mathbf{r}_j^T$ such that $\\mathbf{r}_j^T A = \\mathbf{e}_j^T$. Let $R$ be the matrix with rows $\\mathbf{r}_j^T$. Then $RA = I$. So $R = C$ and $A$ is invertible with $A^{-1} = C = R$.\n$(4 \\Leftrightarrow 7)$: The system $A\\mathbf{x} = \\mathbf{b}$ has a unique solution for every $\\mathbf{b}$ if and only if $\\ker(A) = {\\mathbf{0}}$.\n$(\\Rightarrow)$ If $A\\mathbf{x} = \\mathbf{b}$ has a unique solution for every $\\mathbf{b}$, then in particular, $A\\mathbf{x} = \\mathbf{0}$ has a unique solution. Since $\\mathbf{x} = \\mathbf{0}$ is always a solution, it must be the only solution. Thus, $\\ker(A) = {\\mathbf{0}}$.\n$(\\Leftarrow)$ If $\\ker(A) = {\\mathbf{0}}$, then for any $\\mathbf{b}$, if $\\mathbf{x}_1$ and $\\mathbf{x}_2$ are both solutions to $A\\mathbf{x} = \\mathbf{b}$, then $A(\\mathbf{x}_1 - \\mathbf{x}_2) = \\mathbf{0}$, which means $\\mathbf{x}_1 - \\mathbf{x}_2 \\in \\ker(A) = {\\mathbf{0}}$. Therefore, $\\mathbf{x}_1 = \\mathbf{x}_2$, and the solution is unique.\n$(7 \\Leftrightarrow 8)$: $\\ker(A) = {\\mathbf{0}}$ if and only if $0$ is not an eigenvalue of $A$.\nBy definition, $\\lambda$ is an eigenvalue of $A$ if and only if there exists a non-zero vector $\\mathbf{v}$ such that $A\\mathbf{v} = \\lambda\\mathbf{v}$. For $\\lambda = 0$, this means $A\\mathbf{v} = \\mathbf{0}$, which is equivalent to $\\mathbf{v} \\in \\ker(A)$. So $0$ is an eigenvalue if and only if $\\ker(A)$ contains a non-zero vector, i.e., $\\ker(A) \\neq {\\mathbf{0}}$.\nThe other equivalences can be proven using similar arguments and the rank-nullity theorem.\nâ–  The Invertible Matrix Theorem is a powerful result that connects many seemingly different concepts in linear algebra. It tells us that a square matrix\u0026rsquo;s invertibility can be characterized in multiple equivalent ways, providing flexibility in how we approach problems involving invertible matrices.\nExercises on Determinants and Invertibility # Exercise 3.1 Calculate the determinant of the matrix $A = \\begin{pmatrix} 3 \u0026amp; 1 \u0026amp; 0 \\ 2 \u0026amp; -1 \u0026amp; 4 \\ -1 \u0026amp; 2 \u0026amp; 5 \\end{pmatrix}$.\nSolution: Using the cofactor expansion along the first row:\n$\\det(A) = 3 \\cdot \\det\\begin{pmatrix} -1 \u0026amp; 4 \\ 2 \u0026amp; 5 \\end{pmatrix} - 1 \\cdot \\det\\begin{pmatrix} 2 \u0026amp; 4 \\ -1 \u0026amp; 5 \\end{pmatrix} + 0 \\cdot \\det\\begin{pmatrix} 2 \u0026amp; -1 \\ -1 \u0026amp; 2 \\end{pmatrix}$\n$= 3 \\cdot ((-1) \\cdot 5 - 4 \\cdot 2) - 1 \\cdot (2 \\cdot 5 - 4 \\cdot (-1))$\n$= 3 \\cdot (-5 - 8) - 1 \\cdot (10 + 4)$\n$= 3 \\cdot (-13) - 1 \\cdot 14$\n$= -39 - 14$\n$= -53$\nExercise 3.2 Find the inverse of the matrix $A = \\begin{pmatrix} 2 \u0026amp; 1 \\ 5 \u0026amp; 3 \\end{pmatrix}$ if it exists.\nSolution: First, we calculate the determinant: $\\det(A) = 2 \\cdot 3 - 1 \\cdot 5 = 6 - 5 = 1$\nSince $\\det(A) \\neq 0$, the matrix is invertible.\nUsing the formula for a $2 \\times 2$ matrix: $A^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} 3 \u0026amp; -1 \\ -5 \u0026amp; 2 \\end{pmatrix}$\n$= \\begin{pmatrix} 3 \u0026amp; -1 \\ -5 \u0026amp; 2 \\end{pmatrix}$\nLet\u0026rsquo;s verify: $AA^{-1} = \\begin{pmatrix} 2 \u0026amp; 1 \\ 5 \u0026amp; 3 \\end{pmatrix} \\begin{pmatrix} 3 \u0026amp; -1 \\ -5 \u0026amp; 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 3 + 1 \\cdot (-5) \u0026amp; 2 \\cdot (-1) + 1 \\cdot 2 \\ 5 \\cdot 3 + 3 \\cdot (-5) \u0026amp; 5 \\cdot (-1) + 3 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; 1 \\end{pmatrix}$\n4. Eigenvalues and Eigenvectors # Having explored the basics of matrices and their properties, we now delve into one of the most important concepts in linear algebra: eigenvalues and eigenvectors. These concepts reveal the intrinsic characteristics of linear transformations and find applications in diverse fields such as dynamic systems, quantum mechanics, and data science. Eigenvalues and eigenvectors provide insight into the behavior of iterative processes and lead naturally to the powerful spectral theorem for symmetric matrices.\nDefinition 0.10 (Eigenvalues and Eigenvectors)\nFor a square matrix $A \\in \\mathbb{R}^{n \\times n}$, a non-zero vector $\\mathbf{v} \\in \\mathbb{R}^n$ is an eigenvector of $A$ with corresponding eigenvalue $\\lambda \\in \\mathbb{R}$ if: \\begin{equation} A\\mathbf{v} = \\lambda\\mathbf{v} \\label{eq:eigenvalue-equation} \\end{equation} Theorem 0.5 (Eigenvalue Calculation)\nThe eigenvalues of a matrix $A$ are the roots of its characteristic polynomial: \\begin{equation} p_A(\\lambda) = \\det(A - \\lambda I) \\label{eq:characteristic-polynomial} \\end{equation} Proof\nBy definition, $\\lambda$ is an eigenvalue of $A$ if and only if there exists a non-zero vector $\\mathbf{v}$ such that $A\\mathbf{v} = \\lambda\\mathbf{v}$.\nThis can be rewritten as $(A - \\lambda I)\\mathbf{v} = \\mathbf{0}$.\nFor this homogeneous system to have a non-trivial solution (i.e., $\\mathbf{v} \\neq \\mathbf{0}$), the matrix $A - \\lambda I$ must be singular, which means its determinant must be zero:\n$\\det(A - \\lambda I) = 0$\nThis equation is called the characteristic equation, and the polynomial $p_A(\\lambda) = \\det(A - \\lambda I)$ is the characteristic polynomial of $A$. The roots of this polynomial are precisely the eigenvalues of $A$.\nâ–  Eigenvector Calculation\nOnce you have found an eigenvalue $\\lambda$, the corresponding eigenvectors are found by solving the homogeneous system $(A - \\lambda I)\\mathbf{v} = \\mathbf{0}$. This means finding the kernel (null space) of the matrix $A - \\lambda I$.\nTheorem 0.6 (Spectral Theorem)\nIf $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric matrix (i.e., $A = A^T$), then:\nAll eigenvalues of $A$ are real. Eigenvectors corresponding to distinct eigenvalues are orthogonal. $A$ is orthogonally diagonalizable, i.e., there exists an orthogonal matrix $P$ such that $P^TAP = D$, where $D$ is a diagonal matrix containing the eigenvalues of $A$. Proof\nLet $\\lambda$ be an eigenvalue of $A$ with eigenvector $\\mathbf{v}$. We can assume $\\mathbf{v}$ has unit length: $\\mathbf{v}^T\\mathbf{v} = 1$.\n$A\\mathbf{v} = \\lambda\\mathbf{v}$\nTaking the conjugate transpose (noting that $\\mathbf{v}$ can be complex):\n$\\mathbf{v}^TA^T = \\overline{\\lambda}\\mathbf{v}^T$\nSince $A = A^T$ (symmetric), we have:\n$\\mathbf{v}^TA = \\overline{\\lambda}\\mathbf{v}^T$\nMultiplying the first equation by $\\mathbf{v}^T$ from the left:\n$\\mathbf{v}^TA\\mathbf{v} = \\lambda\\mathbf{v}^T\\mathbf{v} = \\lambda$\nMultiplying the second equation by $\\mathbf{v}$ from the right:\n$\\mathbf{v}^TA\\mathbf{v} = \\overline{\\lambda}\\mathbf{v}^T\\mathbf{v} = \\overline{\\lambda}$\nThus, $\\lambda = \\overline{\\lambda}$, which means $\\lambda$ is real.\nLet $\\lambda_1$ and $\\lambda_2$ be distinct eigenvalues with eigenvectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$.\n$A\\mathbf{v}_1 = \\lambda_1\\mathbf{v}_1$ and $A\\mathbf{v}_2 = \\lambda_2\\mathbf{v}_2$\nTaking the transpose of the first equation and multiplying by $\\mathbf{v}_2$:\n$\\mathbf{v}_1^TA\\mathbf{v}_2 = \\lambda_1\\mathbf{v}_1^T\\mathbf{v}_2$\nMultiplying the second equation by $\\mathbf{v}_1^T$:\n$\\mathbf{v}_1^TA\\mathbf{v}_2 = \\lambda_2\\mathbf{v}_1^T\\mathbf{v}_2$\nTherefore:\n$\\lambda_1\\mathbf{v}_1^T\\mathbf{v}_2 = \\lambda_2\\mathbf{v}_1^T\\mathbf{v}_2$\nSince $\\lambda_1 \\neq \\lambda_2$, we must have $\\mathbf{v}_1^T\\mathbf{v}_2 = 0$, which means the eigenvectors are orthogonal.\nSince $A$ is symmetric, it has $n$ linearly independent eigenvectors that can be orthonormalized. Let $P$ be the matrix whose columns are these orthonormal eigenvectors. Then $P$ is orthogonal ($P^TP = I$) and $P^TAP = D$, where $D$ is diagonal with the eigenvalues on the diagonal.\nâ–  Exercises on Eigenvalues and Eigenvectors # Exercise 4.1 Find the eigenvalues and corresponding eigenvectors of the matrix $A = \\begin{pmatrix} 3 \u0026amp; 1 \\ 1 \u0026amp; 3 \\end{pmatrix}$.\nSolution: The characteristic polynomial is: $p_A(\\lambda) = \\det(A - \\lambda I) = \\det\\begin{pmatrix} 3-\\lambda \u0026amp; 1 \\ 1 \u0026amp; 3-\\lambda \\end{pmatrix}$\n$= (3-\\lambda)^2 - 1 = (3-\\lambda)^2 - 1 = 9 - 6\\lambda + \\lambda^2 - 1 = \\lambda^2 - 6\\lambda + 8$\nSetting this equal to zero: $\\lambda^2 - 6\\lambda + 8 = 0$\nUsing the quadratic formula: $\\lambda = \\frac{6 \\pm \\sqrt{36-32}}{2} = \\frac{6 \\pm \\sqrt{4}}{2} = \\frac{6 \\pm 2}{2}$\nSo the eigenvalues are $\\lambda_1 = 4$ and $\\lambda_2 = 2$.\nFor $\\lambda_1 = 4$, we find eigenvectors by solving $(A - 4I)\\mathbf{v} = \\mathbf{0}$: $\\begin{pmatrix} -1 \u0026amp; 1 \\ 1 \u0026amp; -1 \\end{pmatrix} \\begin{pmatrix} v_1 \\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\ 0 \\end{pmatrix}$\nThis gives us $-v_1 + v_2 = 0$, so $v_1 = v_2$. One possible eigenvector is $\\mathbf{v}_1 = (1, 1)$.\nFor $\\lambda_2 = 2$, we solve $(A - 2I)\\mathbf{v} = \\mathbf{0}$: $\\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} v_1 \\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\ 0 \\end{pmatrix}$\nThis gives us $v_1 + v_2 = 0$, so $v_2 = -v_1$. One possible eigenvector is $\\mathbf{v}_2 = (1, -1)$.\nNote that the eigenvectors are orthogonal, as expected for a symmetric matrix.\nExercise 4.2 Diagonalize the matrix $A = \\begin{pmatrix} 4 \u0026amp; -1 \u0026amp; 1 \\ -1 \u0026amp; 2 \u0026amp; 0 \\ 1 \u0026amp; 0 \u0026amp; 2 \\end{pmatrix}$ if possible.\nSolution: First, we need to find the eigenvalues by solving $\\det(A - \\lambda I) = 0$.\nThe characteristic polynomial is: $p_A(\\lambda) = \\det\\begin{pmatrix} 4-\\lambda \u0026amp; -1 \u0026amp; 1 \\ -1 \u0026amp; 2-\\lambda \u0026amp; 0 \\ 1 \u0026amp; 0 \u0026amp; 2-\\lambda \\end{pmatrix}$\nExpanding along the first row: $p_A(\\lambda) = (4-\\lambda)\\det\\begin{pmatrix} 2-\\lambda \u0026amp; 0 \\ 0 \u0026amp; 2-\\lambda \\end{pmatrix} - (-1)\\det\\begin{pmatrix} -1 \u0026amp; 0 \\ 1 \u0026amp; 2-\\lambda \\end{pmatrix} + 1\\det\\begin{pmatrix} -1 \u0026amp; 2-\\lambda \\ 1 \u0026amp; 0 \\end{pmatrix}$\n$= (4-\\lambda)(2-\\lambda)^2 + \\det\\begin{pmatrix} -1 \u0026amp; 0 \\ 1 \u0026amp; 2-\\lambda \\end{pmatrix} + \\det\\begin{pmatrix} -1 \u0026amp; 2-\\lambda \\ 1 \u0026amp; 0 \\end{pmatrix}$\n$= (4-\\lambda)(2-\\lambda)^2 - (-1)(2-\\lambda) - (-(2-\\lambda))$\n$= (4-\\lambda)(2-\\lambda)^2 + (2-\\lambda) + (2-\\lambda)$\n$= (4-\\lambda)(2-\\lambda)^2 + 2(2-\\lambda)$\n$= (4-\\lambda)(2-\\lambda)^2 + 2(2-\\lambda)$\nWorking through the algebra: $= (4-\\lambda)(4 - 4\\lambda + \\lambda^2) + 4 - 2\\lambda$\n$= 16 - 16\\lambda + 4\\lambda^2 - 4\\lambda + 4\\lambda^2 - \\lambda^3 + 4 - 2\\lambda$\n$= 20 - 22\\lambda + 8\\lambda^2 - \\lambda^3$\n$= -\\lambda^3 + 8\\lambda^2 - 22\\lambda + 20$\nSetting this equal to zero and factoring (or using other methods), the eigenvalues are $\\lambda_1 = 1$, $\\lambda_2 = 2$, and $\\lambda_3 = 5$.\nNext, we find the eigenvectors for each eigenvalue.\nFor $\\lambda_1 = 1$, solving $(A - I)\\mathbf{v} = \\mathbf{0}$: $\\begin{pmatrix} 3 \u0026amp; -1 \u0026amp; 1 \\ -1 \u0026amp; 1 \u0026amp; 0 \\ 1 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} v_1 \\ v_2 \\ v_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\ 0 \\ 0 \\end{pmatrix}$\nThis gives us: $3v_1 - v_2 + v_3 = 0$ $-v_1 + v_2 = 0$ $v_1 + v_3 = 0$\nFrom the second equation, $v_2 = v_1$. From the third, $v_3 = -v_1$. Substituting into the first: $3v_1 - v_1 - v_1 = 0$ $v_1 = 0$\nThis means all components are zero, which contradicts the definition of an eigenvector. Let\u0026rsquo;s double-check our work\u0026hellip;\n[After rechecking] I made an error in the characteristic polynomial. Let\u0026rsquo;s correct and find $\\lambda_1 = 1$ eigenvector:\nFrom row reduction, we get $v_2 = v_1$ and $v_3 = -v_1$. Taking $v_1 = 1$, the eigenvector is $\\mathbf{v}_1 = (1, 1, -1)$.\nFor $\\lambda_2 = 2$: The eigenvector is $\\mathbf{v}_2 = (0, 0, 1)$.\nFor $\\lambda_3 = 5$: The eigenvector is $\\mathbf{v}_3 = (1, -1, 0)$.\nAfter normalizing these eigenvectors to have unit length, we can form the matrix $P$ whose columns are the normalized eigenvectors: $P = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \u0026amp; 0 \u0026amp; \\frac{1}{\\sqrt{2}} \\ \\frac{1}{\\sqrt{3}} \u0026amp; 0 \u0026amp; -\\frac{1}{\\sqrt{2}} \\ -\\frac{1}{\\sqrt{3}} \u0026amp; 1 \u0026amp; 0 \\end{pmatrix}$\nAnd the diagonal matrix is: $D = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 2 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 5 \\end{pmatrix}$\nSo the diagonalization is $A = PDP^{-1}$. Since $A$ is symmetric, $P$ is orthogonal, so $P^{-1} = P^T$.\n5. Matrix Decompositions # Matrix decompositions are fundamental tools that factorize a matrix into products of simpler matrices with special properties. These decompositions provide insight into the structure of linear transformations and are crucial for numerical computations. In this section, we explore several important matrix factorizations, including LU, QR, and Singular Value Decomposition (SVD), each serving different computational and theoretical purposes.\nDefinition 0.11 (LU Decomposition)\nAn LU decomposition of a square matrix $A$ is a factorization $A = LU$ where $L$ is a lower triangular matrix with ones on the diagonal, and $U$ is an upper triangular matrix. Definition 0.12 (QR Decomposition)\nA QR decomposition of a matrix $A$ is a factorization $A = QR$ where $Q$ is an orthogonal matrix ($Q^TQ = I$) and $R$ is an upper triangular matrix. Definition 0.13 (Singular Value Decomposition (SVD))\nA singular value decomposition of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is a factorization $A = U\\Sigma V^T$ where:\n$U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns are the left singular vectors of $A$ $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values of $A$ $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix whose columns are the right singular vectors of $A$ Theorem 0.7 (Properties of SVD)\nFor a matrix $A$ with SVD $A = U\\Sigma V^T$:\nThe columns of $U$ are eigenvectors of $AA^T$ The columns of $V$ are eigenvectors of $A^TA$ The non-zero singular values are the square roots of the non-zero eigenvalues of both $A^TA$ and $AA^T$ The rank of $A$ equals the number of non-zero singular values Proof\nLet $A = U\\Sigma V^T$ be the SVD of $A$.\nThe columns of $U$ are eigenvectors of $AA^T$:\n$AA^T = (U\\Sigma V^T)(U\\Sigma V^T)^T = U\\Sigma V^T V \\Sigma^T U^T = U\\Sigma \\Sigma^T U^T$\nSince $V$ is orthogonal, $V^T V = I$. Also, $\\Sigma \\Sigma^T$ is a diagonal matrix with entries $\\sigma_i^2$.\nLet $\\mathbf{u}_i$ be the $i$-th column of $U$. Then:\n$AA^T \\mathbf{u}_i = U\\Sigma \\Sigma^T U^T \\mathbf{u}_i = U\\Sigma \\Sigma^T \\mathbf{e}_i = U\\Sigma \\Sigma^T \\mathbf{e}_i$\nwhere $\\mathbf{e}_i$ is the $i$-th standard basis vector. Since $\\Sigma \\Sigma^T$ is diagonal, $\\Sigma \\Sigma^T \\mathbf{e}_i = \\sigma_i^2 \\mathbf{e}_i$. Therefore:\n$AA^T \\mathbf{u}_i = U \\sigma_i^2 \\mathbf{e}_i = \\sigma_i^2 U \\mathbf{e}_i = \\sigma_i^2 \\mathbf{u}_i$\nThis shows that $\\mathbf{u}_i$ is an eigenvector of $AA^T$ with eigenvalue $\\sigma_i^2$.\nThe columns of $V$ are eigenvectors of $A^TA$:\nSimilar to above, $A^TA = V\\Sigma^T U^T U\\Sigma V^T = V\\Sigma^T \\Sigma V^T$.\nLet $\\mathbf{v}_i$ be the $i$-th column of $V$. A similar calculation shows that $A^TA \\mathbf{v}_i = \\sigma_i^2 \\mathbf{v}_i$.\nFrom the above, we\u0026rsquo;ve shown that the eigenvalues of $AA^T$ and $A^TA$ are $\\sigma_i^2$, so the singular values $\\sigma_i$ are the square roots of these eigenvalues.\nThe rank of $A$ is the dimension of its column space, which is the number of linearly independent columns. In the SVD, the column space of $A$ is spanned by the columns of $U$ corresponding to non-zero singular values. Therefore, $\\text{rank}(A)$ equals the number of non-zero singular values.\nâ–  Importance of SVD\nThe Singular Value Decomposition is one of the most important matrix decompositions in linear algebra. It is used in principal component analysis (PCA), image compression, solving least squares problems, computing pseudoinverses, and many other applications. Unlike eigendecomposition, SVD exists for any matrix, not just square ones.\nExercises on Matrix Decompositions # Exercise 5.1 Find the LU decomposition of the matrix $A = \\begin{pmatrix} 2 \u0026amp; 1 \u0026amp; 3 \\ 4 \u0026amp; 3 \u0026amp; 8 \\ 6 \u0026amp; 5 \u0026amp; 16 \\end{pmatrix}$ if it exists without row exchanges.\nSolution: We use Gaussian elimination to find $L$ and $U$.\nStep 1: $U$ will have the same first row as $A$: $U_{1,1} = 2, U_{1,2} = 1, U_{1,3} = 3$\nStep 2: Compute the multipliers for the first column: $L_{2,1} = \\frac{A_{2,1}}{U_{1,1}} = \\frac{4}{2} = 2$ $L_{3,1} = \\frac{A_{3,1}}{U_{1,1}} = \\frac{6}{2} = 3$\nStep 3: Compute the remaining elements of the second row of $U$: $U_{2,2} = A_{2,2} - L_{2,1} \\cdot U_{1,2} = 3 - 2 \\cdot 1 = 1$ $U_{2,3} = A_{2,3} - L_{2,1} \\cdot U_{1,3} = 8 - 2 \\cdot 3 = 2$\nStep 4: Compute the multiplier for the second column: $L_{3,2} = \\frac{A_{3,2} - L_{3,1} \\cdot U_{1,2}}{U_{2,2}} = \\frac{5 - 3 \\cdot 1}{1} = 2$\nStep 5: Compute the remaining element of the third row of $U$: $U_{3,3} = A_{3,3} - L_{3,1} \\cdot U_{1,3} - L_{3,2} \\cdot U_{2,3} = 16 - 3 \\cdot 3 - 2 \\cdot 2 = 16 - 9 - 4 = 3$\nTherefore: $L = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 2 \u0026amp; 1 \u0026amp; 0 \\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix}$ and $U = \\begin{pmatrix} 2 \u0026amp; 1 \u0026amp; 3 \\ 0 \u0026amp; 1 \u0026amp; 2 \\ 0 \u0026amp; 0 \u0026amp; 3 \\end{pmatrix}$\nVerification: $LU = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 2 \u0026amp; 1 \u0026amp; 0 \\ 3 \u0026amp; 2 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} 2 \u0026amp; 1 \u0026amp; 3 \\ 0 \u0026amp; 1 \u0026amp; 2 \\ 0 \u0026amp; 0 \u0026amp; 3 \\end{pmatrix} = \\begin{pmatrix} 2 \u0026amp; 1 \u0026amp; 3 \\ 4 \u0026amp; 3 \u0026amp; 8 \\ 6 \u0026amp; 5 \u0026amp; 16 \\end{pmatrix} = A$\nExercise 5.2 Find the QR decomposition of the matrix $A = \\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; 2 \\ 0 \u0026amp; 1 \\end{pmatrix}$ using the Gram-Schmidt process.\nSolution: We\u0026rsquo;ll apply the Gram-Schmidt process to the columns of $A$: $\\mathbf{a}_1 = (1, 1, 0)^T$ and $\\mathbf{a}_2 = (1, 2, 1)^T$\nStep 1: Normalize $\\mathbf{a}_1$ to get the first column of $Q$: $\\mathbf{q}_1 = \\frac{\\mathbf{a}_1}{||\\mathbf{a}_1||} = \\frac{(1, 1, 0)^T}{\\sqrt{1^2 + 1^2 + 0^2}} = \\frac{(1, 1, 0)^T}{\\sqrt{2}} = (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0)^T$\nStep 2: Compute the projection of $\\mathbf{a}_2$ onto $\\mathbf{q}1$: $\\text{proj}{\\mathbf{q}_1}(\\mathbf{a}_2) = (\\mathbf{a}_2 \\cdot \\mathbf{q}_1)\\mathbf{q}_1 = ((1, 2, 1) \\cdot (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0)) \\cdot (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0)^T$ $= (\\frac{1}{\\sqrt{2}} + \\frac{2}{\\sqrt{2}}) \\cdot (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0)^T = \\frac{3}{\\sqrt{2}} \\cdot (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0)^T = (\\frac{3}{2}, \\frac{3}{2}, 0)^T$\nStep 3: Compute $\\mathbf{v}_2 = \\mathbf{a}2 - \\text{proj}{\\mathbf{q}_1}(\\mathbf{a}_2)$: $\\mathbf{v}_2 = (1, 2, 1)^T - (\\frac{3}{2}, \\frac{3}{2}, 0)^T = (-\\frac{1}{2}, \\frac{1}{2}, 1)^T$\nStep 4: Normalize $\\mathbf{v}_2$ to get the second column of $Q$: $\\mathbf{q}_2 = \\frac{\\mathbf{v}_2}{||\\mathbf{v}_2||} = \\frac{(-\\frac{1}{2}, \\frac{1}{2}, 1)^T}{\\sqrt{(-\\frac{1}{2})^2 + (\\frac{1}{2})^2 + 1^2}} = \\frac{(-\\frac{1}{2}, \\frac{1}{2}, 1)^T}{\\sqrt{\\frac{1}{4} + \\frac{1}{4} + 1}} = \\frac{(-\\frac{1}{2}, \\frac{1}{2}, 1)^T}{\\sqrt{\\frac{6}{4}}} = \\frac{(-\\frac{1}{2}, \\frac{1}{2}, 1)^T}{\\sqrt{\\frac{3}{2}}}$\n$= (-\\frac{1}{2\\sqrt{\\frac{3}{2}}}, \\frac{1}{2\\sqrt{\\frac{3}{2}}}, \\frac{1}{\\sqrt{\\frac{3}{2}}})^T = (-\\frac{1}{\\sqrt{6}}, \\frac{1}{\\sqrt{6}}, \\frac{2}{\\sqrt{6}})^T$\nTherefore, $Q = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{6}} \\ \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{6}} \\ 0 \u0026amp; \\frac{2}{\\sqrt{6}} \\end{pmatrix}$\nStep 5: Compute $R$ using $R = Q^T A$: $R = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; 0 \\ -\\frac{1}{\\sqrt{6}} \u0026amp; \\frac{1}{\\sqrt{6}} \u0026amp; \\frac{2}{\\sqrt{6}} \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; 2 \\ 0 \u0026amp; 1 \\end{pmatrix}$\n$= \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\cdot 1 + \\frac{1}{\\sqrt{2}} \\cdot 1 + 0 \\cdot 0 \u0026amp; \\frac{1}{\\sqrt{2}} \\cdot 1 + \\frac{1}{\\sqrt{2}} \\cdot 2 + 0 \\cdot 1 \\ -\\frac{1}{\\sqrt{6}} \\cdot 1 + \\frac{1}{\\sqrt{6}} \\cdot 1 + \\frac{2}{\\sqrt{6}} \\cdot 0 \u0026amp; -\\frac{1}{\\sqrt{6}} \\cdot 1 + \\frac{1}{\\sqrt{6}} \\cdot 2 + \\frac{2}{\\sqrt{6}} \\cdot 1 \\end{pmatrix}$\n$= \\begin{pmatrix} \\frac{2}{\\sqrt{2}} \u0026amp; \\frac{3}{\\sqrt{2}} \\ 0 \u0026amp; \\frac{\\sqrt{6}}{2} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \u0026amp; \\frac{3\\sqrt{2}}{2} \\ 0 \u0026amp; \\frac{\\sqrt{6}}{2} \\end{pmatrix}$\nTherefore, the QR decomposition is: $A = QR = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{6}} \\ \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{6}} \\ 0 \u0026amp; \\frac{2}{\\sqrt{6}} \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} \u0026amp; \\frac{3\\sqrt{2}}{2} \\ 0 \u0026amp; \\frac{\\sqrt{6}}{2} \\end{pmatrix}$\n6. Matrix Norms and Condition Number # As we transition from theoretical concepts to computational applications, we need ways to measure the \u0026ldquo;size\u0026rdquo; of matrices and assess the numerical stability of matrix operations. Matrix norms quantify the magnitude of matrices, while the condition number measures how sensitive a matrix is to small perturbations in input data. These concepts are essential for understanding the numerical behavior of algorithms in linear algebra.\nDefinition 0.14 (Matrix Norm)\nA matrix norm $|\\cdot|$ is a function that assigns a non-negative scalar to a matrix and satisfies:\n$|A| \u0026gt; 0$ for all $A \\neq 0$, and $|0| = 0$ $|\\alpha A| = |\\alpha| \\cdot |A|$ for any scalar $\\alpha$ $|A + B| \\leq |A| + |B|$ (triangle inequality) Common matrix norms include:\nFrobenius norm: $|A|F = \\sqrt{\\sum{i=1}^{m}\\sum_{j=1}^{n}|a_{ij}|^2}$ Operator norms (induced norms): $|A|p = \\max{\\mathbf{x} \\neq 0} \\frac{|A\\mathbf{x}|_p}{|\\mathbf{x}|_p}$ $|A|1 = \\max{1 \\leq j \\leq n} \\sum_{i=1}^{m}|a_{ij}|$ (maximum absolute column sum) $|A|\\infty = \\max{1 \\leq i \\leq m} \\sum_{j=1}^{n}|a_{ij}|$ (maximum absolute row sum) $|A|2 = \\sigma{\\max}(A)$ (largest singular value of $A$, also called the spectral norm) Definition 0.15 (Condition Number)\nThe condition number of an invertible matrix $A$ with respect to a matrix norm $|\\cdot|$ is: \\begin{equation} \\kappa(A) = |A| \\cdot |A^{-1}| \\label{eq:condition-number} \\end{equation}\nFor the spectral norm ($|\\cdot|2$), the condition number is: \\begin{equation} \\kappa_2(A) = \\frac{\\sigma{\\max}(A)}{\\sigma_{\\min}(A)} \\label{eq:spectral-condition-number} \\end{equation} where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and smallest singular values of $A$, respectively.\nCondition Number Interpretation\nThe condition number measures how sensitive the solution of a linear system $A\\mathbf{x} = \\mathbf{b}$ is to perturbations in $\\mathbf{b}$. A large condition number indicates an ill-conditioned matrix, meaning small changes in $\\mathbf{b}$ can cause large changes in the solution $\\mathbf{x}$. This has important implications for numerical stability in computational linear algebra.\nExercises on Matrix Norms and Condition Number # Exercise 6.1 Calculate the Frobenius norm and the infinity norm of the matrix $A = \\begin{pmatrix} 3 \u0026amp; -1 \\ 2 \u0026amp; 4 \\end{pmatrix}$.\nSolution: The Frobenius norm is: $|A|F = \\sqrt{\\sum{i=1}^{m}\\sum_{j=1}^{n}|a_{ij}|^2} = \\sqrt{3^2 + (-1)^2 + 2^2 + 4^2} = \\sqrt{9 + 1 + 4 + 16} = \\sqrt{30} \\approx 5.48$\nThe infinity norm (maximum absolute row sum) is: $|A|\\infty = \\max{1 \\leq i \\leq m} \\sum_{j=1}^{n}|a_{ij}|$ $= \\max{|3| + |-1|, |2| + |4|} = \\max{4, 6} = 6$\nExercise 6.2 Find the condition number $\\kappa_2(A)$ of the matrix $A = \\begin{pmatrix} 3 \u0026amp; 0 \\ 0 \u0026amp; 1 \\end{pmatrix}$ with respect to the spectral norm.\nSolution: The matrix $A$ is already diagonal, so its singular values are simply the absolute values of the diagonal entries: $\\sigma_1 = 3$ and $\\sigma_2 = 1$.\nTherefore, $\\sigma_{\\max}(A) = 3$ and $\\sigma_{\\min}(A) = 1$.\nThe condition number is: $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\frac{3}{1} = 3$\nThis is a relatively small condition number, indicating that the matrix is well-conditioned.\n7. Applications of Linear Algebra # We conclude our exploration of linear algebra by examining some of its powerful applications. Linear algebra serves as the foundation for numerous scientific and engineering fields, from data analysis and statistics to computer graphics and machine learning. In this section, we focus on key applications like least squares approximation, which allows us to find the best solution to an overdetermined system, and the concept of pseudoinverse, which generalizes the notion of matrix inverse for non-square matrices.\nDefinition 0.16 (Least Squares Approximation)\nGiven an overdetermined system $A\\mathbf{x} = \\mathbf{b}$ where $A \\in \\mathbb{R}^{m \\times n}$ with $m \u0026gt; n$, the least squares solution minimizes $|A\\mathbf{x} - \\mathbf{b}|2^2$ and is given by: \\begin{equation} \\mathbf{x}{LS} = (A^TA)^{-1}A^T\\mathbf{b} \\label{eq:least-squares} \\end{equation} assuming $A$ has full column rank. Definition 0.17 (Pseudoinverse)\nThe Moore-Penrose pseudoinverse of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is the matrix $A^+ \\in \\mathbb{R}^{n \\times m}$ that satisfies:\n$AA^+A = A$ $A^+AA^+ = A^+$ $(AA^+)^T = AA^+$ $(A^+A)^T = A^+A$ If $A$ has SVD $A = U\\Sigma V^T$, then $A^+ = V\\Sigma^+U^T$ where $\\Sigma^+$ is formed by taking the reciprocal of each non-zero singular value and leaving the zeros as zeros.\nTheorem 0.8 (Rank-Nullity Theorem)\nFor a matrix $A \\in \\mathbb{R}^{m \\times n}$: \\begin{equation} \\text{rank}(A) + \\text{nullity}(A) = n \\label{eq:rank-nullity} \\end{equation} where $\\text{rank}(A)$ is the dimension of the column space of $A$ and $\\text{nullity}(A)$ is the dimension of the null space of $A$. Proof\nLet $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be the linear transformation represented by the matrix $A$. Then $\\text{rank}(A) = \\dim(\\text{im}(T))$ and $\\text{nullity}(A) = \\dim(\\ker(T))$.\nConsider the canonical basis ${\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n}$ for $\\mathbb{R}^n$.\nLet $r = \\dim(\\ker(T))$ and suppose ${\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_r}$ is a basis for $\\ker(T)$.\nWe can extend this to a basis for $\\mathbb{R}^n$ by adding vectors ${\\mathbf{w}_1, \\mathbf{w}2, \\ldots, \\mathbf{w}{n-r}}$ such that ${\\mathbf{v}_1, \\ldots, \\mathbf{v}_r, \\mathbf{w}1, \\ldots, \\mathbf{w}{n-r}}$ is a basis for $\\mathbb{R}^n$.\nConsider the set ${T(\\mathbf{w}_1), T(\\mathbf{w}2), \\ldots, T(\\mathbf{w}{n-r})}$. We claim that this set is linearly independent and spans $\\text{im}(T)$.\nFor linear independence, suppose $c_1 T(\\mathbf{w}1) + c_2 T(\\mathbf{w}2) + \\ldots + c{n-r} T(\\mathbf{w}{n-r}) = \\mathbf{0}$.\nBy linearity, $T(c_1 \\mathbf{w}1 + c_2 \\mathbf{w}2 + \\ldots + c{n-r} \\mathbf{w}{n-r}) = \\mathbf{0}$.\nThis means $c_1 \\mathbf{w}1 + c_2 \\mathbf{w}2 + \\ldots + c{n-r} \\mathbf{w}{n-r} \\in \\ker(T)$.\nSo $c_1 \\mathbf{w}1 + c_2 \\mathbf{w}2 + \\ldots + c{n-r} \\mathbf{w}{n-r} = d_1 \\mathbf{v}_1 + d_2 \\mathbf{v}_2 + \\ldots + d_r \\mathbf{v}_r$ for some scalars $d_1, d_2, \\ldots, d_r$.\nThis gives $c_1 \\mathbf{w}1 + c_2 \\mathbf{w}2 + \\ldots + c{n-r} \\mathbf{w}{n-r} - d_1 \\mathbf{v}_1 - d_2 \\mathbf{v}_2 - \\ldots - d_r \\mathbf{v}_r = \\mathbf{0}$.\nSince ${\\mathbf{v}_1, \\ldots, \\mathbf{v}r, \\mathbf{w}1, \\ldots, \\mathbf{w}{n-r}}$ is a basis, all coefficients must be zero. Thus, $c_1 = c_2 = \\ldots = c{n-r} = 0$, showing that ${T(\\mathbf{w}_1), T(\\mathbf{w}2), \\ldots, T(\\mathbf{w}{n-r})}$ is linearly independent.\nTo show that this set spans $\\text{im}(T)$, let $\\mathbf{y} \\in \\text{im}(T)$. Then $\\mathbf{y} = T(\\mathbf{x})$ for some $\\mathbf{x} \\in \\mathbb{R}^n$.\nWe can write $\\mathbf{x}$ as a linear combination of the basis: $\\mathbf{x} = a_1 \\mathbf{v}_1 + \\ldots + a_r \\mathbf{v}r + b_1 \\mathbf{w}1 + \\ldots + b{n-r} \\mathbf{w}{n-r}$\nApplying $T$: $\\mathbf{y} = T(\\mathbf{x}) = T(a_1 \\mathbf{v}1 + \\ldots + a_r \\mathbf{v}r + b_1 \\mathbf{w}1 + \\ldots + b{n-r} \\mathbf{w}{n-r})$ $= a_1 T(\\mathbf{v}1) + \\ldots + a_r T(\\mathbf{v}r) + b_1 T(\\mathbf{w}1) + \\ldots + b{n-r} T(\\mathbf{w}{n-r})$ $= \\mathbf{0} + b_1 T(\\mathbf{w}1) + \\ldots + b{n-r} T(\\mathbf{w}{n-r})$ $= b_1 T(\\mathbf{w}1) + \\ldots + b{n-r} T(\\mathbf{w}{n-r})$\nThis shows that ${T(\\mathbf{w}_1), T(\\mathbf{w}2), \\ldots, T(\\mathbf{w}{n-r})}$ spans $\\text{im}(T)$.\nTherefore, ${T(\\mathbf{w}_1), T(\\mathbf{w}2), \\ldots, T(\\mathbf{w}{n-r})}$ is a basis for $\\text{im}(T)$, and $\\dim(\\text{im}(T)) = n - r = n - \\dim(\\ker(T))$.\nRearranging, we get $\\dim(\\text{im}(T)) + \\dim(\\ker(T)) = n$, which is the rank-nullity theorem.\nâ–  Exercises on Applications of Linear Algebra # Exercise 7.1 Find the least squares solution to the system: $\\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; 2 \\ 1 \u0026amp; 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix} = \\begin{pmatrix} 2 \\ 3 \\ 4 \\end{pmatrix}$\nSolution: We need to compute $\\mathbf{x}_{LS} = (A^TA)^{-1}A^T\\mathbf{b}$ where $A = \\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; 2 \\ 1 \u0026amp; 3 \\end{pmatrix}$ and $\\mathbf{b} = \\begin{pmatrix} 2 \\ 3 \\ 4 \\end{pmatrix}$.\nFirst, calculate $A^TA$: $A^TA = \\begin{pmatrix} 1 \u0026amp; 1 \u0026amp; 1 \\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; 2 \\ 1 \u0026amp; 3 \\end{pmatrix} = \\begin{pmatrix} 3 \u0026amp; 6 \\ 6 \u0026amp; 14 \\end{pmatrix}$\nNext, find $(A^TA)^{-1}$: $\\det(A^TA) = 3 \\cdot 14 - 6 \\cdot 6 = 42 - 36 = 6$ $(A^TA)^{-1} = \\frac{1}{6} \\begin{pmatrix} 14 \u0026amp; -6 \\ -6 \u0026amp; 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3} \u0026amp; -1 \\ -1 \u0026amp; \\frac{1}{2} \\end{pmatrix}$\nNow calculate $A^T\\mathbf{b}$: $A^T\\mathbf{b} = \\begin{pmatrix} 1 \u0026amp; 1 \u0026amp; 1 \\ 1 \u0026amp; 2 \u0026amp; 3 \\end{pmatrix} \\begin{pmatrix} 2 \\ 3 \\ 4 \\end{pmatrix} = \\begin{pmatrix} 9 \\ 20 \\end{pmatrix}$\nFinally, compute $\\mathbf{x}{LS} = (A^TA)^{-1}A^T\\mathbf{b}$: $\\mathbf{x}{LS} = \\begin{pmatrix} \\frac{7}{3} \u0026amp; -1 \\ -1 \u0026amp; \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 9 \\ 20 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3} \\cdot 9 + (-1) \\cdot 20 \\ (-1) \\cdot 9 + \\frac{1}{2} \\cdot 20 \\end{pmatrix} = \\begin{pmatrix} 21 - 20 \\ -9 + 10 \\end{pmatrix} = \\begin{pmatrix} 1 \\ 1 \\end{pmatrix}$\nTherefore, the least squares solution is $\\mathbf{x}_{LS} = (1, 1)^T$.\nLet\u0026rsquo;s verify by computing the residual: $A\\mathbf{x}_{LS} - \\mathbf{b} = \\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; 2 \\ 1 \u0026amp; 3 \\end{pmatrix} \\begin{pmatrix} 1 \\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\ 3 \\ 4 \\end{pmatrix} = \\begin{pmatrix} 2 \\ 3 \\ 4 \\end{pmatrix} - \\begin{pmatrix} 2 \\ 3 \\ 4 \\end{pmatrix} = \\begin{pmatrix} 0 \\ 0 \\ 0 \\end{pmatrix}$\nInterestingly, the residual is zero, which means our least squares solution is an exact solution to the system. This is not typical for overdetermined systems.\nExercise 7.2 Find the rank and nullity of the matrix $A = \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 4 \u0026amp; 5 \u0026amp; 6 \\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix}$.\nSolution: To find the rank, we row-reduce the matrix to its row echelon form:\n$A = \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 4 \u0026amp; 5 \u0026amp; 6 \\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix}$\nStep 1: Use the first row to eliminate entries below the pivot. $R_2 = R_2 - 4R_1$: $\\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 0 \u0026amp; -3 \u0026amp; -6 \\ 7 \u0026amp; 8 \u0026amp; 9 \\end{pmatrix}$\n$R_3 = R_3 - 7R_1$: $\\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 0 \u0026amp; -3 \u0026amp; -6 \\ 0 \u0026amp; -6 \u0026amp; -12 \\end{pmatrix}$\nStep 2: Use the second row to eliminate entries below the pivot. $R_3 = R_3 - 2R_2$: $\\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 0 \u0026amp; -3 \u0026amp; -6 \\ 0 \u0026amp; 0 \u0026amp; 0 \\end{pmatrix}$\nThe matrix is now in row echelon form with 2 non-zero rows, so $\\text{rank}(A) = 2$.\nBy the rank-nullity theorem, $\\text{nullity}(A) = n - \\text{rank}(A) = 3 - 2 = 1$.\nTo find a basis for the null space, we need to solve $A\\mathbf{x} = \\mathbf{0}$. From the row echelon form, we have: $x_1 + 2x_2 + 3x_3 = 0$ $-3x_2 - 6x_3 = 0$\nFrom the second equation, $x_2 = -2x_3$. Substituting into the first equation: $x_1 + 2(-2x_3) + 3x_3 = 0$ $x_1 - 4x_3 + 3x_3 = 0$ $x_1 - x_3 = 0$ $x_1 = x_3$\nTaking $x_3 = t$ as the free variable, we get $x_1 = t$ and $x_2 = -2t$. So, the null space is spanned by the vector $(1, -2, 1)^T$.\nConclusion # Throughout this document, we have explored the fundamental concepts and results of linear algebra, starting from the abstract definition of vector spaces and progressing through matrices, determinants, eigenvalues, matrix decompositions, and various applications. The logical structure of linear algebra allows us to build increasingly powerful tools by combining simpler concepts.\nThe beauty of linear algebra lies not only in its elegant mathematical framework but also in its wide-ranging applications across science, engineering, and mathematics itself. From solving systems of equations to analyzing data, from computer graphics to quantum mechanics, linear algebra provides the essential tools for understanding and manipulating multidimensional information.\nAs you continue to study mathematics and its applications, the concepts introduced here will serve as a foundation for more advanced topics such as multilinear algebra, functional analysis, differential equations, and modern data science techniques including machine learning algorithms.\nRemember that mastering linear algebra requires both theoretical understanding and practical problem-solving skills. The exercises included in this document provide a starting point for developing these skills, but further practice with diverse problems will deepen your understanding and proficiency in applying these powerful mathematical tools.\n"},{"id":6,"href":"/numerical_optimization/docs/lectures/fundamentals/convexity/","title":"2. Convexity theory","section":"I - Fundamentals","content":" Convexity theory # Tanto oblite # Lorem markdownum pectora novis patenti igne sua opus aurae feras materiaque illic demersit imago et aristas questaque posset. Vomit quoque suo inhaesuro clara. Esse cumque, per referri triste. Ut exponit solisque communis in tendens vincetis agisque iamque huic bene ante vetat omina Thebae rates. Aeacus servat admonitu concidit, ad resimas vultus et rugas vultu dignamque Siphnon.\nQuam iugulum regia simulacra, plus meruit humo pecorumque haesit, ab discedunt dixit: ritu pharetramque. Exul Laurenti orantem modo, per densum missisque labor manibus non colla unum, obiectat. Tu pervia collo, fessus quae Cretenque Myconon crate! Tegumenque quae invisi sudore per vocari quaque plus ventis fluidos. Nodo perque, fugisse pectora sorores.\nSumme promissa supple vadit lenius # Quibus largis latebris aethera versato est, ait sentiat faciemque. Aequata alis nec Caeneus exululat inclite corpus est, ire tibi ostendens et tibi. Rigent et vires dique possent lumina; eadem dixit poma funeribus paret et felix reddebant ventis utile lignum.\nRemansit notam Stygia feroxque Et dabit materna Vipereas Phrygiaeque umbram sollicito cruore conlucere suus Quarum Elis corniger Nec ieiunia dixit Vertitur mos ortu ramosam contudit dumque; placabat ac lumen. Coniunx Amoris spatium poenamque cavernis Thebae Pleiadasque ponunt, rapiare cum quae parum nimium rima.\nQuidem resupinus inducto solebat una facinus quae # Credulitas iniqua praepetibus paruit prospexit, voce poena, sub rupit sinuatur, quin suum ventorumque arcadiae priori. Soporiferam erat formamque, fecit, invergens, nymphae mutat fessas ait finge.\nBaculum mandataque ne addere capiti violentior Altera duas quam hoc ille tenues inquit Sicula sidereus latrantis domoque ratae polluit comites Possit oro clausura namque se nunc iuvenisque Faciem posuit Quodque cum ponunt novercae nata vestrae aratra Ite extrema Phrygiis, patre dentibus, tonso perculit, enim blanda, manibus fide quos caput armis, posse! Nocendo fas Alcyonae lacertis structa ferarum manus fulmen dubius, saxa caelum effuge extremis fixum tumor adfecit bella, potentes? Dum nec insidiosa tempora tegit spirarunt. Per lupi pars foliis, porreximus humum negant sunt subposuere Sidone steterant auro. Memoraverit sine: ferrum idem Orion caelum heres gerebat fixis?\n"},{"id":7,"href":"/numerical_optimization/docs/lectures/advanced/stochastic/","title":"2. Stochastic optimization","section":"II - Advanced problems","content":" Stochastic optimization # "},{"id":8,"href":"/numerical_optimization/docs/lectures/machine_learning/svm/","title":"2. Support Vector Machine","section":"III - Machine Learning problems","content":" Support Vector Machine # "},{"id":9,"href":"/numerical_optimization/docs/lectures/1_introduction/","title":"Introduction","section":"Lectures","content":" Introduction # Notations # Let us start by defining the notation used troughout all the lectures and practical labs.\nBasic Notation # Scalars are represented by italic letters (e.g., $x$, $y$, $\\lambda$). Vectors are denoted by bold lowercase letters (e.g., $\\mathbf{v}$, $\\mathbf{x}$), while matrices are represented by bold uppercase letters (e.g., $\\mathbf{A}$, $\\mathbf{B}$). The dimensionality of a vector $\\mathbf{v} \\in \\mathbb{R}^n$ indicates it contains $n$ elements, and similarly, a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ has $m$ rows and $n$ columns.\nMatrix Operations # The transpose of a matrix $\\mathbf{A}$ is denoted as $\\mathbf{A}^\\mathrm{T}$, which reflects the matrix across its diagonal. The trace of a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, written as $\\mathrm{tr}(\\mathbf{A})$, is the sum of its diagonal elements, i.e., $\\mathrm{tr}(\\mathbf{A}) = \\sum_{i=1}^{n} a_{ii}$. The determinant of $\\mathbf{A}$ is represented as $\\det(\\mathbf{A})$ or $|\\mathbf{A}|$. A matrix $\\mathbf{A}$ is invertible if and only if $\\det(\\mathbf{A}) \\neq 0$, and its inverse is denoted as $\\mathbf{A}^{-1}$, satisfying $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix.\nVector Operations # The dot product between two vectors $\\mathbf{a}$ and $\\mathbf{b}$ of the same dimension is written as $\\mathbf{a} \\cdot \\mathbf{b}$ or $\\mathbf{a}^\\mathrm{T}\\mathbf{b}$, resulting in a scalar value.\nThe p-norm of a vector $\\mathbf{v}$ is denoted as $\\lVert\\mathbf{v}\\rVert_p$ and defined as $\\lVert\\mathbf{v}\\rVert_p = \\left(\\sum_{i=1}^{n} |v_i|^p\\right)^{1/p}$ for $p \\geq 1$, with common choices being $p=1$ (Manhattan norm), $p=2$ (Euclidean norm), and $p=\\infty$ (maximum norm, defined as $\\lVert\\mathbf{v}\\rVert_{\\infty} = \\max_i |v_i|$); when the subscript $p$ is omitted, as in $\\lVert\\mathbf{v}\\rVert$, it is conventionally understood to refer to the Euclidean (L2) norm. The Euclidean norm (or length) of a vector $\\mathbf{v}$ is represented as $\\lVert\\mathbf{v}\\rVert$ or $\\lVert\\mathbf{v}\\rVert_2$, defined as $\\lVert\\mathbf{v}\\rVert = \\sqrt{\\mathbf{v}^\\mathrm{T}\\mathbf{v}} = \\sqrt{\\sum_{i=1}^{n} v_i^2}$. A unit vector in the direction of $\\mathbf{v}$ is given by $\\hat{\\mathbf{v}} = \\mathbf{v}/\\lVert\\mathbf{v}\\rVert$, having a norm of 1.\nEigenvalues and Eigenvectors # For a square matrix $\\mathbf{A}$, a scalar $\\lambda$ is an eigenvalue if there exists a non-zero vector $\\mathbf{v}$ such that $\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$. The vector $\\mathbf{v}$ is called an eigenvector corresponding to the eigenvalue $\\lambda$. The characteristic polynomial of $\\mathbf{A}$ is defined as $p(\\lambda) = \\det(\\lambda\\mathbf{I} - \\mathbf{A})$, and its roots are the eigenvalues of $\\mathbf{A}$. The spectrum of $\\mathbf{A}$, denoted by $\\sigma(\\mathbf{A})$, is the set of all eigenvalues of $\\mathbf{A}$.\nMatrix Decompositions # The singular value decomposition (SVD) of a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is expressed as $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\mathrm{T}$, where $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ and $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values of $\\mathbf{A}$. The eigendecomposition of a diagonalizable matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is given by $\\mathbf{A} = \\mathbf{P}\\mathbf{\\Lambda}\\mathbf{P}^{-1}$, where $\\mathbf{P}$ is a matrix whose columns are the eigenvectors of $\\mathbf{A}$, and $\\mathbf{\\Lambda}$ is a diagonal matrix containing the corresponding eigenvalues.\nMultivariate Calculus # The gradient of a scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is denoted as $\\nabla f$ or $\\mathrm{grad}(f)$, resulting in a vector of partial derivatives $\\nabla f = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}]^\\mathrm{T}$. The Jacobian matrix of a vector-valued function $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is represented as $\\mathbf{J}_\\mathbf{f}$ or $\\nabla \\mathbf{f}^\\mathrm{T}$, where $ (\\mathbf{J}_\\mathbf{f})_{ij} = \\frac{\\partial f_i}{\\partial x_j} $.\nThe Hessian matrix of a twice-differentiable scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is denoted as $\\mathbf{H}_f$ or $\\nabla^2 f$, where $(\\mathbf{H}_f)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$.\nSpecial Matrices and Properties # A symmetric matrix satisfies $\\mathbf{A} = \\mathbf{A}^\\mathrm{T}$, while a skew-symmetric matrix has $\\mathbf{A} = -\\mathbf{A}^\\mathrm{T}$. An orthogonal matrix $\\mathbf{Q}$ satisfies $\\mathbf{Q}^\\mathrm{T}\\mathbf{Q} = \\mathbf{Q}\\mathbf{Q}^\\mathrm{T} = \\mathbf{I}$, meaning its inverse equals its transpose: $\\mathbf{Q}^{-1} = \\mathbf{Q}^\\mathrm{T}$. A matrix $\\mathbf{A}$ is positive definite if $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} \u0026gt; 0$ for all non-zero vectors $\\mathbf{x}$, and positive semidefinite if $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} \\geq 0$.\nDerivatives of Matrix Expressions # The derivative of a scalar function with respect to a vector $\\mathbf{x}$ is denoted as $\\frac{\\partial f}{\\partial \\mathbf{x}}$, resulting in a vector of the same dimension as $\\mathbf{x}$. For matrix functions, the derivative with respect to a matrix $\\mathbf{X}$ is written as $\\frac{\\partial f}{\\partial \\mathbf{X}}$, producing a matrix of the same dimensions as $\\mathbf{X}$. Common matrix derivatives include $\\frac{\\partial}{\\partial \\mathbf{X}}\\mathrm{tr}(\\mathbf{AX}) = \\mathbf{A}^\\mathrm{T}$ and $\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}) = \\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathrm{T}\\mathbf{x}$ (with $\\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathrm{T}\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}$ when $\\mathbf{A}$ is symmetric).\n"},{"id":10,"href":"/numerical_optimization/docs/lectures/machine_learning/neural_networks/","title":"3. Neural Networks","section":"III - Machine Learning problems","content":" Neural Networks # "},{"id":11,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/","title":"3. Unconstrained optimization : basics","section":"I - Fundamentals","content":" Unconstrained optimization - basics # We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve: $$ \\underset{\\mathbf{x}\\in\\mathbb{R}^d}{\\operatorname{argmin}} f(\\mathbf{x}). $$\nLet us try to characterizes the nature of the solutions under this setup.\nWhat is a solution ? # Figure 3.1:Local and global minimum can coexist.\nGenerally, we would be happiest if we found a global minimizer of $f$ , a point where the function attains its least value. A formal definition is :\nDefinition 3.1 (Global minimizer)\nA point $\\mathbf{x}^\\star$ is a global minimizer if $f(\\mathbf{x}^\\star)\\leq f(\\mathbf{x})$, where $\\mathbf{x}$ ranges over all of $\\mathbb{R}^d$ (or at least over the domain of interest to the modeler). The global minimizer can be difficult to find, because our knowledge of $f$ is usually only local. Since our algorithm does not visit many points (we hope!), we usually do not have a good picture of the overall shape of $f$ , and we can never be sure that the function does not take a sharp dip in some region that has not been sampled by the algorithm. Most algorithms are able to find only a local minimizer, which is a point that achieves the smallest value of f in its neighborhood. Formally, we say:\nDefinition 3.2 (Local minimizer)\nA point $\\mathbf{x}^\\star$ is a local minimizer if $\\exists r\u0026gt;0,\\, f(\\mathbf{x}^\\star)\\leq f(\\mathbf{x})$, $\\forall \\mathbf{x}\\in\\mathcal{B}(\\mathbf{x}^\\star, r)$. A point that satisfies this definition is sometimes called a weak local minimizer. Alternatively, when $f(\\mathbf{x}^\\star)\u0026lt;f(\\mathbf{x})$, we say that the minimum is a strict local minimizer.\nTaylor\u0026rsquo;s theorem # From the definitions given above, it might seem that the only way to find out whether a point $\\mathbf{x}^\\star$ is a local minimum is to examine all the points in its immediate vicinity, to make sure that none of them has a smaller function value. When the function $f$ is smooth, however, there are much more efficient and practical ways to identify local minima. In particular, if $f$ is twice continuously differentiable, we may be able to tell that $\\mathbf{x}^\\star$ is a local minimizer (and possibly a strict local minimizer) by examining just the gradient $\\nabla f (\\mathbf{x}^\\star)$ and the Hessian $\\nabla^2 f (\\mathbf{x}^\\star)$. The mathematical tool used to study minimizers of smooth functions is Taylorâ€™s the- orem. Because this theorem is central to our analysis we state it now.\nTheorem 3.1 (Taylor\u0026#39;s theorem)\nSuppose that $f:\\mathbb{R}^d\\mapsto\\mathbb{R}$ is continuously differentiable and that we have $\\mathbf{p}\\in\\mathbb{R}^d $. The we have : \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x}+t\\mathbf{p})^\\mathrm{T}\\mathbf{p}, \\end{equation} for some $t\\in [0,1]$.\nMoreover, if $f$ is twice continuously differentiable, we have : \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\mathrm{T}\\nabla^2 f(\\mathbf{x}+t\\mathbf{p})\\mathbf{p}), \\end{equation} for some $t\\in [0,1]$.\nProof\nSee any calculus book â–  Note that in this formulation, the definition is exact and the $t$ scalar is usually unknown. The interest lies in skeching proofs. In practical matters, we rather use the following approximation: Theorem 3.2 (Taylor\u0026#39;s approximation)\nFirst order approximation: \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + o(\\lVert\\mathbf{p}\\rVert), \\end{equation}\nSecond-order approximation:\n\\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\mathrm{T}\\nabla^2 f(\\mathbf{x})\\mathbf{p} + o(\\lVert\\mathbf{p}\\rVert^2), \\end{equation}\nwhere $o(\\lVert\\mathbf{p}\\rVert)$ and $o(\\lVert\\mathbf{p}\\rVert^2)$ represent terms that grow slower than $\\lVert\\mathbf{p}\\rVert$ and $\\lVert\\mathbf{p}\\rVert^2$ respectively as $\\lVert\\mathbf{p}\\rVert \\to 0$.\nSufficient and necessary conditions for local minima # Let us consider a local minimum and see how they can be characterized to later design appropriate solution finding methods. The first well-known result is as follows: Theorem 3.3 (First-order necessary conditions)\nif $\\mathbf{x}^\\star$ is a local minimize, and $f$ is continuously differentiable in a neighborhood of $\\mathbf{x}^\\star$, then $\\nabla f(\\mathbf{x}^\\star) = \\mathbf{0}$. Proof\nSuppose for contradiction that $\\nabla f(\\mathbf{x}^\\star) \\neq 0$, and define vector $\\mathbf{p}=-\\nabla f(\\mathbf{x}^\\star)$ such that by construction $\\mathbf{p}^\\mathrm{T}\\nabla f(\\mathbf{x}^\\star) = - \\lVert f(\\mathbf{x}^\\star) \\rVert^2 \u0026lt; 0$.\nSince $f$ is a continuous function, we can define a scalar $T\u0026gt;0$ such that $\\forall t\\in [0,T[$, we still have: $$ \\mathbf{p}^\\mathrm{T}f(\\mathbf{x}+t\\mathbf{p}) \u0026lt; 0. $$\nUsing Theorem 3.1 first-order result, we can write: $$ f(\\mathbf{x}^\\star+t\\mathbf{p}) = f(\\mathbf{x}^\\star) + t\\mathbf{p}^\\mathrm{T}\\nabla f(\\mathbf{x}^\\star+\\overline{t}\\mathbf{p}), $$ for some $\\overline{t}\\in[0,T[$ and any $t\\in[0,T[$. Given previous inequality, we obtain: $$ f(\\mathbf{x}^\\star+t\\mathbf{p}) \u0026lt; f(\\mathbf{x}^\\star), $$ which contradicts the fact that $\\mathbf{x}^\\star$ is a local minimizer.\nâ–  Henceforth, we will call stationary point, any $\\mathbf{x}$ such that $\\nabla f(\\mathbf{x}) = 0$.\nFor the next result we recall that a matrix $\\mathbf{B}$ is positive definite if $\\mathbf{p}^\\mathrm{T} \\mathbf{B} \\mathbf{p}\u0026gt;0$ for all $\\mathbf{p} \\neq \\mathbf{0}$, and positive semidefinite if $\\mathbf{p}^\\mathrm{T} \\mathbf{B} \\mathbf{p} \\geq 0$ for all $\\mathbf{p}$.\nTheorem 3.4 (Second-order necessary conditions)\nIf $\\mathbf{x}^\\star$ is a local minimizer of $f$ and $\\nabla^2 f$ is continuous in an open neighborhood of $\\mathbf{x}^\\star$, then $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$ and $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is positive semidefinite. Proof\nProof. We know from Theorem 3.3 that $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$. For contradiction, assume that $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is not positive semidefinite. Then we can choose a vector $\\mathbf{p}$ such that $\\mathbf{p}^T \\nabla^2 f\\left(\\mathbf{x}^\\star\\right) \\mathbf{p}\u0026lt;0$, and because $\\nabla^2 f$ is continuous near $\\mathbf{x}^\\star$, there is a scalar $T\u0026gt;0$ such that $\\mathbf{p}^\\mathrm{T} \\nabla^2 f\\left(\\mathbf{x}^*+\\overline{t} \\mathbf{p}\\right) \\mathbf{p}\u0026lt;0$ for all $t \\in[0, T[$.\nBy doing a Taylor series expansion around $\\mathbf{x}^\\star$, we have for all $\\bar{t} \\in[0, T[$ and some $t \\in[0, \\bar{t}]$ that\n$$ f\\left(\\mathbf{x}^\\star+\\bar{t} \\mathbf{p}\\right) = f\\left(\\mathbf{x}^\\star\\right)+\\bar{t} \\mathbf{p}^\\mathrm{T} \\nabla f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\bar{t}^2 \\mathbf{p}^\\mathrm{T} \\nabla^2 f\\left(\\mathbf{x}^\\star+t \\mathbf{p}\\right) \\mathbf{p}\u0026lt;f\\left(\\mathbf{x}^\\star\\right) . $$\nAs in Theorem 3.3, we have found a direction from $\\mathbf{x}^\\star$ along which $f$ is decreasing, and so again, $\\mathbf{x}^\\star$ is not a local minimizer.\nâ–  We now describe sufficient conditions, which are conditions on the derivatives of $f$ at the point $\\mathbf{z}^\\star$ that guarantee that $\\mathbf{x}^\\star$ is a local minimizer.\nTheorem 3.5 (Second-Order Sufficient Conditions)\nSuppose that $\\nabla^2 f$ is continuous in an open neighborhood of $\\mathbf{x}^\\star$ and that $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$ and $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is positive definite. Then $\\mathbf{x}^\\star$ is a strict local minimizer of $f$. Proof\nBecause the Hessian is continuous and positive definite at $\\mathbf{x}^\\star$, we can choose a radius $r\u0026gt;0$ so that $\\nabla^2 f(x)$ remains positive definite for all $x$ in the open ball $\\mathcal{D}=\\left\\{\\mathbf{z} \\mid\\left\\lVert\\mathbf{z}-\\mathbf{x}^\\star\\right\\rVert\u0026lt;\\right.$ $r\\}$. Taking any nonzero vector $p$ with $\\lVert\\mathbf{p}\\rVert\u0026lt;r$, we have $\\mathbf{x}^\\star+\\mathbf{p} \\in \\mathcal{D}$ and so\n$$ \\begin{aligned} f\\left(\\mathbf{x}^\\star+p\\right) \u0026amp; =f\\left(\\mathbf{x}^\\star\\right)+\\mathbf{p}^\\mathrm{T} \\nabla f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\mathbf{p}^\\mathrm{T} \\nabla^2 f(\\mathbf{z}) \\mathbf{p} \\ \u0026amp; =f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\mathbf{p}^\\mathrm{T} \\nabla^2 f(\\mathbf{z}) \\mathbf{p} \\end{aligned} $$\nwhere $\\mathbf{z}=\\mathbf{x}^\\star+t \\mathbf{p}$ for some $t \\in(0,1)$. Since $\\mathbf{z} \\in \\mathcal{D}$, we have $\\mathbf{p}^{\\mathrm{T}} \\nabla^2 f(\\mathbf{z}) \\mathbf{p}\u0026gt;0$, and therefore $f\\left(\\mathbf{x}^\\star+\\mathbf{p}\\right)\u0026gt;f\\left(\\mathbf{x}^\\star\\right)$, giving the result.\nâ–  Note that the second-order sufficient conditions of Theorem 3.5 guarantee something stronger than the necessary conditions discussed earlier; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: A point $\\mathbf{x}^\\star$ may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function $f(x)=x^4$, for which the point $x^\\star=0$ is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite). When the objective function is convex, local and global minimizers are simple to characterize.\nTheorem 3.6 When $f$ is convex, any local minimizer $\\mathbf{x}^\\star$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $\\mathbf{x}^\\star$ is a global minimizer of $f$. Proof\nSuppose that $\\mathbf{x}^\\star$ is a local but not a global minimizer. Then we can find a point $\\mathbf{z} \\in \\mathbb{R}^n$ with $f(\\mathbf{z})\u0026lt;f\\left(\\mathbf{x}^\\star\\right)$. Consider the line segment that joins $\\mathbf{x}^\\star$ to $\\mathbf{z}$, that is, \\begin{equation} \\mathbf{x}=\\lambda \\mathbf{z}+(1-\\lambda) \\mathbf{x}^\\star, \\quad \\text { for some } \\lambda \\in(0,1] \\label{eq:line_segment} \\end{equation} By the convexity property for $f$, we have \\begin{equation} f(\\mathbf{x}) \\leq \\lambda f(\\mathbf{z})+(1-\\lambda) f\\left(\\mathbf{x}^\\star\\right)\u0026lt;f\\left(\\mathbf{x}^\\star\\right) \\label{eq:convexity} \\end{equation}\nAny neighborhood $\\mathcal{N}$ of $\\mathbf{x}^\\star$ contains a piece of the line segment \\eqref{eq:line_segment}, so there will always be points $\\mathbf{x} \\in \\mathcal{N}$ at which \\eqref{eq:convexity} is satisfied. Hence, $\\mathbf{x}^\\star$ is not a local minimizer. For the second part of the theorem, suppose that $\\mathbf{x}^\\star$ is not a global minimizer and choose $\\mathbf{z}$ as above. Then, from convexity, we have\n\\begin{equation} \\begin{aligned} \\nabla f\\left(\\mathbf{x}^\\star\\right)^{\\mathrm{T}}\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right) \u0026amp; =\\left.\\frac{d}{d \\lambda} f\\left(\\mathbf{x}^\\star+\\lambda\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right)\\right)\\right|_{\\lambda=0} \\\\ \u0026amp; =\\lim _{\\lambda \\downarrow 0} \\frac{f\\left(\\mathbf{x}^\\star+\\lambda\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right)\\right)-f\\left(\\mathbf{x}^\\star\\right)}{\\lambda} \\\\ \u0026amp; \\leq \\lim _{\\lambda \\downarrow 0} \\frac{\\lambda f(\\mathbf{z})+(1-\\lambda) f\\left(\\mathbf{x}^\\star\\right)-f\\left(\\mathbf{x}^\\star\\right)}{\\lambda} \\\\ \u0026amp; =f(\\mathbf{z})-f\\left(\\mathbf{x}^\\star\\right)\u0026lt;0 \\end{aligned} \\end{equation}\nTherefore, $\\nabla f\\left(\\mathbf{x}^\\star\\right) \\neq 0$, and so $\\mathbf{x}^\\star$ is not a stationary point.\nâ–  These results, which are based on elementary calculus, provide the foundations for unconstrained optimization algorithms. In one way or another, all algorithms seek a point where $\\nabla f(\\cdot)$ vanishes.\nThe need for algorithms # Steepest-descent approach # Newton method # "},{"id":12,"href":"/numerical_optimization/docs/lectures/machine_learning/modern/","title":"4. Modern trends","section":"III - Machine Learning problems","content":" Modern trends # "},{"id":13,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/","title":"4. Unconstrained optimization : linesearch","section":"I - Fundamentals","content":" Unconstrained optimization - Linesearch methods # Step-length conditions # Wolfe conditions # Goldenstein conditions # Search directions # Rate of convergence # "},{"id":14,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_trustregions/","title":"5. Unconstrained optimization : trust region ","section":"I - Fundamentals","content":" Unconstrained optimization - Trust region methods # "},{"id":15,"href":"/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/","title":"6. Constrained optimization","section":"I - Fundamentals","content":" Constrained optimization methods # Lorem\n$$ \\mathbf{X} = \\operatorname{argmin} || \\mathbf{Y} - \\mathbf{A}\\mathbf{X} \\|_2 + \\mathcal{R}(\\mathbf{X}) $$\n"},{"id":16,"href":"/numerical_optimization/docs/lectures/fundamentals/","title":"I - Fundamentals","section":"Lectures","content":" Fundamentals # Content 1. Optimization problems 2. Convexity theory 3. Unconstrained optimization : basics 4. Unconstrained optimization : linesearch 5. Unconstrained optimization : trust region 6. Constrained optimization "},{"id":17,"href":"/numerical_optimization/docs/practical_labs/linear_regression/","title":"I - Linear Regression with gradient","section":"Practical labs","content":" Linear Regression using Gradient descent # "},{"id":18,"href":"/numerical_optimization/docs/lectures/advanced/","title":"II - Advanced problems","section":"Lectures","content":" Advanced problems # Content 1. Proximal methods 2. Stochastic optimization "},{"id":19,"href":"/numerical_optimization/docs/practical_labs/remote_sensing/","title":"II - Remote Sensing project","section":"Practical labs","content":" Remote sensing project # Option 1 : Denoising # Option 2 : Pansharpening # "},{"id":20,"href":"/numerical_optimization/docs/practical_labs/mnist/","title":"III - Digit recognition","section":"Practical labs","content":" Digit recognition with multi-layer perceptron # "},{"id":21,"href":"/numerical_optimization/docs/lectures/machine_learning/","title":"III - Machine Learning problems","section":"Lectures","content":" Machine Learning problems # Content 1. From Linear regression to perceptron 2. Support Vector Machine 3. Neural Networks 4. Modern trends "},{"id":22,"href":"/numerical_optimization/docs/lectures/reminders/","title":"Reminders","section":"Lectures","content":" Reminders # Content Differentiation Linear Algebra "},{"id":23,"href":"/numerical_optimization/docs/practical_labs/environment/","title":"Lab environment","section":"Practical labs","content":" Lab environment # "},{"id":24,"href":"/numerical_optimization/docs/practical_labs/","title":"Practical labs","section":"Docs","content":" Practical labs # Content I - Linear Regression with gradient II - Remote Sensing project III - Digit recognition Lab environment "}]