<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on Numerical optimization</title><link>http://ammarmian.github.io/numerical_optimization/</link><description>Recent content in Introduction on Numerical optimization</description><generator>Hugo</generator><language>fr</language><atom:link href="http://ammarmian.github.io/numerical_optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>1. From Linear regression to perceptron</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/perceptron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/perceptron/</guid><description>&lt;h1 id="from-linear-regression-to-perceptron">
 From Linear regression to perceptron
 &lt;a class="anchor" href="#from-linear-regression-to-perceptron">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p></description></item><item><title>1. Optimization problems</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/optimization_problems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/optimization_problems/</guid><description>&lt;h1 id="optimization-problems">
 Optimization problems
 &lt;a class="anchor" href="#optimization-problems">#&lt;/a>
&lt;/h1>
&lt;h2 id="unconstrained-vs-constrained">
 Unconstrained vs constrained
 &lt;a class="anchor" href="#unconstrained-vs-constrained">#&lt;/a>
&lt;/h2>
&lt;p>What we are interested in these lectures is to solve problems of the form :&lt;/p>
&lt;p>\begin{equation}
\underset{\mathbf{x}\in\mathbb{R}^d}{\operatorname{(arg)min}} f(\mathbf{x}),
\label{eq: optim general unconstrained}
\end{equation}
where $\mathbf{x}\in\mathbb{R}^d$ and $f:\mathcal{D}_f \mapsto \mathbb{R} $ is a scalar-valued function with domain $\mathcal{D}_f$. Under this formulation, the problem is said to be an &lt;strong>unconstrained optimization&lt;/strong> problem.&lt;/p>
&lt;p>If additionally, we add a set of equalities constraints functions:
$$
\{h_i : \mathbb{R}^d \mapsto \mathbb{R} \, /\, 1 \leq i \leq N \}
$$
and inequalities constraints functions:
$$
\{g_j : \mathbb{R}^d \mapsto \mathbb{R} \, /\, 1 \leq j \leq M \}
$$
and define the set $\mathcal{S} = \{\mathbf{x} \in \mathbb{R}^d \,/\, \forall\,(i, j),\, h_i(\mathbf{x})=0,\, g_j(\mathbf{x})\leq 0\}$ and want to solve:
\begin{equation}
\underset{\mathbf{x}\in\mathcal{S}}{\operatorname{(arg)min}} f(\mathbf{x}),
\label{eq: optim general constrained}
\end{equation}
then the problem is said to be a &lt;strong>constrained optimization&lt;/strong> problem.&lt;/p></description></item><item><title>1. Unconstrained optimization : Second-order</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/advanced/unconstrained_newton/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/advanced/unconstrained_newton/</guid><description>&lt;h1 id="unconstrained-optimization----second-order-methods">
 Unconstrained optimization - Second-order methods
 &lt;a class="anchor" href="#unconstrained-optimization----second-order-methods">#&lt;/a>
&lt;/h1>
&lt;p>We have seen in the previous chapter that first-order methods, such as steepest descent, are often used to find a local minimum of a function $f(\mathbf{x})$. However, these methods can be slow to converge, especially when the function has ill-conditioned Hessian or when the initial guess is far from the solution. Second-order methods, which use information about the curvature of the function, can provide faster convergence rates.&lt;/p></description></item><item><title>Differentiation</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/reminders/differentiation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/reminders/differentiation/</guid><description>&lt;h1 id="fundamentals-of-multivariate-differentiation">
 Fundamentals of Multivariate Differentiation
 &lt;a class="anchor" href="#fundamentals-of-multivariate-differentiation">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p>
&lt;!-- Multivariate calculus extends the fundamental concepts of single-variable calculus to functions of several variables. This powerful mathematical framework allows us to analyze and describe phenomena in multiple dimensions, making it essential for physics, engineering, economics, statistics, and many other disciplines. This document provides a comprehensive introduction to multivariate differentiation, building from basic principles to advanced applications. -->
&lt;!---->
&lt;!-- ## 1. Functions of Several Variables -->
&lt;!---->
&lt;!-- We begin by defining functions that map points from higher-dimensional spaces to either real numbers or other multi-dimensional spaces. -->
&lt;!---->
&lt;!-- 









&lt;div id="multivariate-function" class="theorem-box">
 &lt;p class="theorem-title">&lt;strong>Definition A.1 (Function of Several Variables)&lt;/strong>&lt;/p></description></item><item><title>Linear Algebra</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/reminders/linear_algebra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/reminders/linear_algebra/</guid><description>&lt;h1 id="fundamentals-of-linear-algebra">
 Fundamentals of Linear Algebra
 &lt;a class="anchor" href="#fundamentals-of-linear-algebra">#&lt;/a>
&lt;/h1>
&lt;h2 id="1---introduction">
 1 - Introduction
 &lt;a class="anchor" href="#1---introduction">#&lt;/a>
&lt;/h2>
&lt;blockquote>
&lt;p>Linear algebra is one of the foundational branches of mathematics, with applications spanning from engineering and computer science to economics and physics. This document is extracted from the textbook &lt;strong>Matrix Differential Calculus with Applications in Statistics and Econometrics&lt;/strong> from Jan R. Magnus; Heinz Neudecker, adapting the notations to the ones used in the lectures.&lt;/p>
&lt;/blockquote>
&lt;p>In this chapter, we summarize some of the well-known definitions and theorems of matrix algebra. Most of the theorems will be proved.&lt;/p></description></item><item><title>2. Proximal methods</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/advanced/proximal_methods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/advanced/proximal_methods/</guid><description>&lt;h1 id="proximal-methods">
 Proximal methods
 &lt;a class="anchor" href="#proximal-methods">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p></description></item><item><title>2. Support Vector Machine</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/svm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/svm/</guid><description>&lt;h1 id="support-vector-machine">
 Support Vector Machine
 &lt;a class="anchor" href="#support-vector-machine">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p></description></item><item><title>2. Unconstrained optimization : basics</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/</guid><description>&lt;h1 id="unconstrained-optimization---basics">
 Unconstrained optimization - basics
 &lt;a class="anchor" href="#unconstrained-optimization---basics">#&lt;/a>
&lt;/h1>
&lt;p>We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve:
$$
\underset{\mathbf{x}\in\mathbb{R}^d}{\operatorname{argmin}} f(\mathbf{x}).
$$&lt;/p>
&lt;p>Let us try to characterizes the nature of the solutions under this setup.&lt;/p>
&lt;h2 id="what-is-a-solution-">
 What is a solution ?
 &lt;a class="anchor" href="#what-is-a-solution-">#&lt;/a>
&lt;/h2>








&lt;figure id="figure-%!s(int=2)-1">&lt;img src="%20../../../../../../tikZ/local_global_minima/main.svg"
 alt="Local vs global" width="600px">
 &lt;figcaption>
 &lt;p>
 &lt;strong>Figure 2.1: &lt;/strong>Local and global minimum can coexist.&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>Generally, we would be happiest if we found a global minimizer of $f$ , a point where the
function attains its least value. A formal definition is :&lt;/p></description></item><item><title>Introduction</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/1_introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/1_introduction/</guid><description>&lt;h1 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h1>
&lt;h2 id="notations">
 Notations
 &lt;a class="anchor" href="#notations">#&lt;/a>
&lt;/h2>
&lt;p>Let us start by defining the notation used troughout all the lectures and practical labs.&lt;/p>
&lt;h3 id="basic-notation">
 Basic Notation
 &lt;a class="anchor" href="#basic-notation">#&lt;/a>
&lt;/h3>
&lt;p>Scalars are represented by italic letters (e.g., $x$, $y$, $\lambda$). Vectors are denoted by bold lowercase letters (e.g., $\mathbf{v}$, $\mathbf{x}$), while matrices are represented by bold uppercase letters (e.g., $\mathbf{A}$, $\mathbf{B}$). The dimensionality of a vector $\mathbf{v} \in \mathbb{R}^n$ indicates it contains $n$ elements, and similarly, a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$ has $m$ rows and $n$ columns.&lt;/p></description></item><item><title>3. Convexity theory</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/convexity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/convexity/</guid><description>&lt;h1 id="convexity-theory">
 Convexity theory
 &lt;a class="anchor" href="#convexity-theory">#&lt;/a>
&lt;/h1>
&lt;p>Convexity is a powerful property of functions and sets that simplifies the analysis of optimization problems and the characterization of global minimizers. In this chapter, we will explore the concepts of convex sets, convex functions, and their implications for unconstrained optimization.&lt;/p>
&lt;h2 id="convex-sets">
 Convex sets
 &lt;a class="anchor" href="#convex-sets">#&lt;/a>
&lt;/h2>
&lt;p>Let us first start by defining the convexity of a given set $\mathcal{S}\subset\mathbb{R}^d$:&lt;/p>










&lt;div id="convex_set" class="theorem-box">
 &lt;p class="theorem-title">&lt;strong>Definition 3.1 (Convex set)&lt;/strong>&lt;/p>
 &lt;div class="theorem-content">
 Let $\mathcal{S}\subset\mathbb{R}^d$ be a set. The set $\mathcal{S}$ is convex if, for any two points $\mathbf{x}, \mathbf{y} \in \mathcal{S}$, the line segment that connects them is also contained in $\mathcal{S}$, that is,
\begin{equation}
\mathbf{x}, \mathbf{y} \in \mathcal{S} \implies \lambda \mathbf{x} + (1-\lambda) \mathbf{y} \in \mathcal{S}, \quad \forall \lambda \in [0, 1].
\label{eq:convex_set}
\end{equation}
 &lt;/div>
&lt;/div>


&lt;div class="center-container">
 &lt;div class="center-content">
 








&lt;figure id="convex_set">&lt;img src="%20../../../../../../tikZ/convex_set/main.svg"
 alt="Zig zag" width="400px">
 &lt;figcaption>
 &lt;p>
 &lt;strong>Figure 3.1: &lt;/strong>Convex set&lt;/p></description></item><item><title>3. Neural Networks</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/neural_networks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/neural_networks/</guid><description>&lt;h1 id="neural-networks">
 Neural Networks
 &lt;a class="anchor" href="#neural-networks">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p></description></item><item><title>4. Modern trends</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/modern/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/modern/</guid><description>&lt;h1 id="modern-trends">
 Modern trends
 &lt;a class="anchor" href="#modern-trends">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p></description></item><item><title>4. Unconstrained optimization : linesearch</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/</guid><description>&lt;h1 id="unconstrained-optimization---linesearch-methods">
 Unconstrained optimization - Linesearch methods
 &lt;a class="anchor" href="#unconstrained-optimization---linesearch-methods">#&lt;/a>
&lt;/h1>
&lt;p>All algorithms for unconstrained minimization require the user to supply a starting point, which we usually denote by $\mathbf{x}_0$. The user with knowledge about the application and the data set may be in a good position to choose $\mathbf{x}_0$ to be a reasonable estimate of the solution. Otherwise, the starting point must be chosen in some arbitrary manner.&lt;/p>
&lt;p>Beginning at $\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\left\{\mathbf{x}_k\right\}_{k=0}^{\infty}$ that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy. In deciding how to move from one iterate $\mathbf{x}_k$ to the next, the algorithms use information about the function $f$ at $\mathbf{x}_k$, and possibly also information from earlier iterates $\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_{k-1}$. They use this information to find a new iterate $\mathbf{x}_{k+1}$ with a lower function value than $\mathbf{x}_k$. (There exist nonmonotone algorithms that do not insist on a decrease in $f$ at every step, but even these algorithms require $f$ to be decreased after some prescribed number $m$ of iterations. That is, they enforce $f\left(\mathbf{x}_k\right)&amp;lt;f\left(\mathbf{x}_{k-m}\right)$.)&lt;/p></description></item><item><title>6. Constrained optimization</title><link>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/</guid><description>&lt;h1 id="constrained-optimization-methods">
 Constrained optimization methods
 &lt;a class="anchor" href="#constrained-optimization-methods">#&lt;/a>
&lt;/h1>
&lt;p>Lorem&lt;/p>
&lt;p>$$
\mathbf{X} = \operatorname{argmin} || \mathbf{Y} - \mathbf{A}\mathbf{X} \|_2 + \mathcal{R}(\mathbf{X})
$$&lt;/p></description></item><item><title>I - Linear Regression models</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/linear_regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/linear_regression/</guid><description>&lt;h1 id="linear-regression-models">
 Linear Regression models
 &lt;a class="anchor" href="#linear-regression-models">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>In this lab session, we will explore the fundamental concepts of numerical optimization through the lens of linear regression. We&amp;rsquo;ll begin with the simplest case and gradually build up to more complex scenarios, comparing analytical solutions with numerical methods at each step.&lt;/p>
&lt;p>Linear regression is perhaps the most fundamental problem in machine learning and statistics. While it has a closed-form solution, implementing numerical optimization methods for this problem provides excellent intuition for more complex optimization scenarios where analytical solutions don&amp;rsquo;t exist.&lt;/p></description></item><item><title>II - Remote Sensing project</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/remote_sensing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/remote_sensing/</guid><description>&lt;h1 id="remote-sensing-project">
 Remote sensing project
 &lt;a class="anchor" href="#remote-sensing-project">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p></description></item><item><title>III - Digit recognition</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/mnist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/mnist/</guid><description>&lt;h1 id="digit-recognition-with-multi-layer-perceptron">
 Digit recognition with multi-layer perceptron
 &lt;a class="anchor" href="#digit-recognition-with-multi-layer-perceptron">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p></description></item><item><title>Lab environment</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/environment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/environment/</guid><description>&lt;h1 id="lab-environment-setup">
 Lab Environment Setup
 &lt;a class="anchor" href="#lab-environment-setup">#&lt;/a>
&lt;/h1>
&lt;p>Welcome to the numerical optimization course! This page will guide you through setting up a modern, efficient Python environment using &lt;strong>uv&lt;/strong>, a fast and reliable Python package manager written in Rust.&lt;/p>
&lt;h2 id="why-uv">
 Why uv?
 &lt;a class="anchor" href="#why-uv">#&lt;/a>
&lt;/h2>
&lt;p>For this course, we&amp;rsquo;re using &lt;a href="https://docs.astral.sh/uv/">uv&lt;/a> instead of traditional tools like pip or conda because:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>⚡ Blazing Fast&lt;/strong>: uv is 10-100x faster than traditional package managers like pip, making dependency installation nearly instantaneous&lt;/li>
&lt;li>&lt;strong>🔧 All-in-One&lt;/strong>: A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more&lt;/li>
&lt;li>&lt;strong>🔒 Reproducible&lt;/strong>: Automatic lock files ensure everyone has identical environments&lt;/li>
&lt;li>&lt;strong>🐍 Python Version Management&lt;/strong>: Automatically downloads and manages Python versions when needed&lt;/li>
&lt;li>&lt;strong>📦 Modern Standards&lt;/strong>: Built around &lt;code>pyproject.toml&lt;/code> and modern Python packaging practices&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">
 Prerequisites
 &lt;a class="anchor" href="#prerequisites">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Good news!&lt;/strong> uv doesn&amp;rsquo;t require Python to be pre-installed - it can manage Python installations for you. However, having Python already installed won&amp;rsquo;t hurt.&lt;/p></description></item><item><title>Backtracking memo</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/backtracking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/backtracking/</guid><description>&lt;h1 id="backtracking-procedure-for-step-size-selection">
 Backtracking procedure for step size selection
 &lt;a class="anchor" href="#backtracking-procedure-for-step-size-selection">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>The backtracking line search is a fundamental technique in optimization algorithms for determining an appropriate step size that ensures sufficient decrease in the objective function. This procedure is particularly useful in gradient-based methods where choosing an optimal step size analytically is difficult or computationally expensive.&lt;/p>
&lt;h2 id="mathematical-setup">
 Mathematical setup
 &lt;a class="anchor" href="#mathematical-setup">#&lt;/a>
&lt;/h2>
&lt;p>Consider the optimization problem:
$\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$&lt;/p>
&lt;p>where $f: \mathbb{R}^n \to \mathbb{R}$ is a continuously differentiable function. At iteration $k$, we have:&lt;/p></description></item></channel></rss>