[{"id":0,"href":"/numerical_optimization/docs/lectures/machine_learning/perceptron/","title":"1. From Linear regression to perceptron","section":"III - Machine Learning problems","content":" From Linear regression to perceptron # Soon to be added.\n"},{"id":1,"href":"/numerical_optimization/docs/lectures/fundamentals/optimization_problems/","title":"1. Optimization problems","section":"I - Fundamentals","content":" Optimization problems # Unconstrained vs constrained # What we are interested in these lectures is to solve problems of the form :\n\\begin{equation} \\underset{\\mathbf{x}\\in\\mathbb{R}^d}{\\operatorname{(arg)min}} f(\\mathbf{x}), \\label{eq: optim general unconstrained} \\end{equation} where $\\mathbf{x}\\in\\mathbb{R}^d$ and $f:\\mathcal{D}_f \\mapsto \\mathbb{R} $ is a scalar-valued function with domain $\\mathcal{D}_f$. Under this formulation, the problem is said to be an unconstrained optimization problem.\nIf additionally, we add a set of equalities constraints functions: $$ \\{h_i : \\mathbb{R}^d \\mapsto \\mathbb{R} \\, /\\, 1 \\leq i \\leq N \\} $$ and inequalities constraints functions: $$ \\{g_j : \\mathbb{R}^d \\mapsto \\mathbb{R} \\, /\\, 1 \\leq j \\leq M \\} $$ and define the set $\\mathcal{S} = \\{\\mathbf{x} \\in \\mathbb{R}^d \\,/\\, \\forall\\,(i, j),\\, h_i(\\mathbf{x})=0,\\, g_j(\\mathbf{x})\\leq 0\\}$ and want to solve: \\begin{equation} \\underset{\\mathbf{x}\\in\\mathcal{S}}{\\operatorname{(arg)min}} f(\\mathbf{x}), \\label{eq: optim general constrained} \\end{equation} then the problem is said to be a constrained optimization problem.\nNote that here, the constraints and the function domain are not the same sets. Constraints usually stem from modelling of the problem whilst the function domain only characterizes for which values of $\\mathbf{x}$ it is possible to compute a value of the function.\nGlobal optimization vs local optimization # Figure 1.1: An example of multiple local minima\nIn the context of optimization, we can distinguish between global optimization and local optimization:\nGlobal optimization refers to the process of finding the best solution (minimum or maximum) across the entire search space. This means identifying the point where the function achieves its absolute minimum or maximum value, regardless of how many local minima or maxima exist. Local optimization, on the other hand, focuses on finding a solution that is optimal within a limited neighborhood of the search space. This means identifying a point where the function achieves a minimum or maximum value relative to nearby points, but not necessarily the absolute best solution across the entire space. Often, global optimization is not feasible unless the function is convex, or the search space is small enough. In practice, we often use local optimization methods to find a good enough solution, which may not be the global optimum. This is peculiarly true in machine learning, where the loss function is often non-convex and may have many local minima.\n"},{"id":2,"href":"/numerical_optimization/docs/lectures/advanced/unconstrained_newton/","title":"1. Unconstrained optimization : Second-order ","section":"II - Advanced problems","content":" Unconstrained optimization - Second-order methods # Note : This is in part the content of the book \u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, with some modifications to the notations used in this lecture.\nWe have seen in the previous chapter that first-order methods, such as steepest descent, are often used to find a local minimum of a function $f(\\mathbf{x})$. However, these methods can be slow to converge, especially when the function has ill-conditioned Hessian or when the initial guess is far from the solution. Second-order methods, which use information about the curvature of the function, can provide faster convergence rates.\nSearch directions # Another important search direction-perhaps the most important one of all-is the Newton direction. This direction is derived from the second-order Taylor series approximation to $f\\left(\\mathbf{x}_k+\\mathbf{p}\\right)$, which is $$ f\\left(\\mathbf{x}_k+\\mathbf{p}\\right) \\approx f_k+\\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\mathbf{p}^{\\mathrm{T}} \\nabla^2 f_k \\mathbf{p} \\stackrel{\\text { def }}{=} m_k(\\mathbf{p}) $$\nAssuming for the moment that $\\nabla^2 f_k$ is positive definite, we obtain the Newton direction by finding the vector $\\mathbf{p}$ that minimizes $m_k(\\mathbf{p})$. By simply setting the derivative of $m_k(\\mathbf{p})$ to zero, we obtain the following explicit formula:\n\\begin{equation} \\mathbf{p}_k^{\\mathrm{N}}=-\\nabla^2 f_k^{-1} \\nabla f_k \\label{eq:newton_direction} \\end{equation}\nThe Newton direction is reliable when the difference between the true function $f\\left(\\mathbf{x}_k+ \\mathbf{p}\\right)$ and its quadratic model $m_k(\\mathbf{p})$ is not too large. By comparing \\eqref{eq:newton_direction} with traditional Taylor expansion, we see that the only difference between these functions is that the matrix $\\nabla^2 f\\left(\\mathbf{x}_k+t \\mathbf{p}\\right)$ in the third term of the expansion has been replaced by $\\nabla^2 f_k=\\nabla^2 f\\left(\\mathbf{x}_k\\right)$. If $\\nabla^2 f(\\cdot)$ is sufficiently smooth, this difference introduces a perturbation of only $O\\left(\\lVert\\mathbf{p}\\rVert^3\\right)$ into the expansion, so that when $\\lVert\\mathbf{p}\\rVert$ is small, the approximation $f\\left(\\mathbf{x}_k+\\mathbf{p}\\right) \\approx m_k(\\mathbf{p})$ is very accurate indeed.\nThe Newton direction can be used in a line search method when $\\nabla^2 f_k$ is positive definite, for in this case we have\n$$ \\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}=-\\mathbf{p}_k^{\\mathrm{N} \\mathrm{T}} \\nabla^2 f_k \\mathbf{p}_k^{\\mathrm{N}} \\leq-\\sigma_k\\lVert\\mathbf{p}_k^{\\mathrm{N}}\\rVert^2 $$\nfor some $\\sigma_k\u0026gt;0$. Unless the gradient $\\nabla f_k$ (and therefore the step $\\mathbf{p}_k^N$) is zero, we have that $\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}\u0026lt;0$, so the Newton direction is a descent direction. Unlike the steepest descent direction, there is a \u0026ldquo;natural\u0026rdquo; step length of 1 associated with the Newton direction. Most line search implementations of Newton\u0026rsquo;s method use the unit step $\\alpha=1$ where possible and adjust this step length only when it does not produce a satisfactory reduction in the value of $f$.\nWhen $\\nabla^2 f_k$ is not positive definite, the Newton direction may not even be defined, since $\\nabla^2 f_k^{-1}$ may not exist. Even when it is defined, it may not satisfy the descent property $\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k^{\\mathrm{N}}\u0026lt;0$, in which case it is unsuitable as a search direction. In these situations, line search methods modify the definition of $\\mathbf{p}_k$ to make it satisfy the downhill condition while retaining the benefit of the second-order information contained in $\\nabla^2 f_k$.\nMethods that use the Newton direction have a fast rate of local convergence, typically quadratic. When a neighborhood of the solution is reached, convergence to high accuracy often occurs in just a few iterations. The main drawback of the Newton direction is the need for the Hessian $\\nabla^2 f(\\mathbf{x})$. Explicit computation of this matrix of second derivatives is sometimes, though not always, a cumbersome, error-prone, and expensive process.\nQuasi-Newton search directions provide an attractive alternative in that they do not require computation of the Hessian and yet still attain a superlinear rate of convergence. In place of the true Hessian $\\nabla^2 f_k$, they use an approximation $\\mathbf{B}_k$, which is updated after each step to take account of the additional knowledge gained during the step. The updates make use of the fact that changes in the gradient $\\mathbf{g}$ provide information about the second derivative of $f$ along the search direction. By using the expression from our statement of Taylor\u0026rsquo;s theorem, we have by adding and subtracting the term $\\nabla^2 f(\\mathbf{x}) \\mathbf{p}$ that\n$$ \\nabla f(\\mathbf{x}+\\mathbf{p})=\\nabla f(\\mathbf{x})+\\nabla^2 f(\\mathbf{x}) \\mathbf{p}+\\int_0^1\\left[\\nabla^2 f(\\mathbf{x}+t \\mathbf{p})-\\nabla^2 f(\\mathbf{x})\\right] \\mathbf{p} d t $$\nBecause $\\nabla f(\\cdot)$ is continuous, the size of the final integral term is $o(\\lVert\\mathbf{p}\\rVert)$. By setting $\\mathbf{x}=\\mathbf{x}_k$ and $\\mathbf{p}=\\mathbf{x}_{k+1}-\\mathbf{x}_k$, we obtain\n$$ \\nabla f_{k+1}=\\nabla f_k+\\nabla^2 f_{k+1}\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right)+o\\left(\\lVert\\mathbf{x}_{k+1}-\\mathbf{x}_k\\rVert\\right) $$\nWhen $\\mathbf{x}_k$ and $\\mathbf{x}_{k+1}$ lie in a region near the solution $\\mathbf{x}^*$, within which $\\nabla f$ is positive definite, the final term in this expansion is eventually dominated by the $\\nabla^2 f_k\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right)$ term, and we can write\n$$ \\nabla^2 f_{k+1}\\left(\\mathbf{x}_{k+1}-\\mathbf{x}_k\\right) \\approx \\nabla f_{k+1}-\\nabla f_k $$\nWe choose the new Hessian approximation $\\mathbf{B}_{k+1}$ so that it mimics this property of the true Hessian, that is, we require it to satisfy the following condition, known as the secant equation:\n\\begin{equation} \\mathbf{B}_{k+1} \\mathbf{s}_k=\\mathbf{y}_k \\label{eq:secant_equation} \\end{equation}\nwhere\n$$ \\mathbf{s}_k=\\mathbf{x}_{k+1}-\\mathbf{x}_k, \\quad \\mathbf{y}_k=\\nabla f_{k+1}-\\nabla f_k $$\nTypically, we impose additional requirements on $\\mathbf{B}_{k+1}$, such as symmetry (motivated by symmetry of the exact Hessian), and a restriction that the difference between successive approximation $\\mathbf{B}_k$ to $\\mathbf{B}_{k+1}$ have low rank. The initial approximation $\\mathbf{B}_0$ must be chosen by the user.\nTwo of the most popular formulae for updating the Hessian approximation $\\mathbf{B}_k$ are the symmetric-rank-one (SR1) formula, defined by\n\\begin{equation} \\mathbf{B}_{k+1}=\\mathbf{B}_k+\\frac{\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}}}{\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:sr1_formula} \\end{equation}\nand the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno, which is defined by\n\\begin{equation} \\mathbf{B}_{k+1}=\\mathbf{B}_k-\\frac{\\mathbf{B}_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k}+\\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:bfgs_formula} \\end{equation}\nNote that the difference between the matrices $\\mathbf{B}_k$ and $\\mathbf{B}_{k+1}$ is a rank-one matrix in the case of \\eqref{eq:sr1_formula}, and a rank-two matrix in the case of \\eqref{eq:bfgs_formula}. Both updates satisfy the secant equation and both maintain symmetry. One can show that BFGS update \\eqref{eq:bfgs_formula} generates positive definite approximations whenever the initial approximation $\\mathbf{B}_0$ is positive definite and $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k\u0026gt;0$.\nThe quasi-Newton search direction is given by using $\\mathbf{B}_k$ in place of the exact Hessian in the formula \\eqref{eq:newton_direction}, that is,\n\\begin{equation} \\mathbf{p}_k=-\\mathbf{B}_k^{-1} \\nabla f_k \\label{eq:quasi_newton_direction} \\end{equation}\nSome practical implementations of quasi-Newton methods avoid the need to factorize $\\mathbf{B}_k$ at each iteration by updating the inverse of $\\mathbf{B}_k$, instead of $\\mathbf{B}_k$ itself. In fact, the equivalent formula for \\eqref{eq:sr1_formula} and \\eqref{eq:bfgs_formula}, applied to the inverse approximation $\\mathbf{H}_k \\stackrel{\\text { def }}{=} \\mathbf{B}_k^{-1}$, is\n\\begin{equation} \\mathbf{H}_{k+1}=\\left(\\mathbf{I}-\\rho_k \\mathbf{s}_k \\mathbf{y}_k^{\\mathrm{T}}\\right) \\mathbf{H}_k\\left(\\mathbf{I}-\\rho_k \\mathbf{y}_k \\mathbf{s}_k^{\\mathrm{T}}\\right)+\\rho_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}}, \\quad \\rho_k=\\frac{1}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:inverse_bfgs} \\end{equation}\nCalculation of $\\mathbf{p}_k$ can then be performed by using the formula $\\mathbf{p}_k=-\\mathbf{H}_k \\nabla f_k$. This can be implemented as a matrix-vector multiplication, which is typically simpler than the factorization/back-substitution procedure that is needed to implement the formula \\eqref{eq:quasi_newton_direction}.\nStep-size selection # Contrarily to the steepest descent, Newton methods have a \u0026ldquo;natural\u0026rdquo; step size of 1 associated with the Newton direction. This is because the Newton direction is derived from the second-order Taylor series approximation, which is designed to minimize the quadratic model of the function. However, in practice, it is often necessary to adjust this step size to ensure sufficient decrease in the function value.\nWhen using a line search method, we can set $\\alpha_k=1$ and check if this step size leads to a sufficient decrease in the function value. If it does not, we can use a backtracking line search to find a suitable step size that satisfies the Armijo condition. The Armijo condition ensures that the step size leads to a sufficient decrease in the function value, which is crucial for convergence of the method.\nConvergence of Newton methods # As in first-order methods, we make use of Zoutendijk\u0026rsquo;s condition, that still apllies.\nConsider now the Newton-like method with $\\mathbf{p}_k = -\\mathbf{B}_k^{-1} \\nabla f_k$ and assume that the matrices $\\mathbf{B}_k$ are positive definite with a uniformly bounded condition number. That is, there is a constant $M$ such that\n\\begin{equation} \\|\\mathbf{B}_k\\|\\|\\mathbf{B}_k^{-1}\\| \\leq M, \\quad \\text { for all } k . \\label{eq:condition_bound} \\end{equation}\nIt is easy to show from the definition that\n$$ \\cos \\theta_k \\geq 1 / M $$\nBy combining this bound with (4.16) we find that\n$$ \\lim _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 $$\nTherefore, we have shown that Newton and quasi-Newton methods are globally convergent if the matrices $\\mathbf{B}_k$ have a bounded condition number and are positive definite (which is needed to ensure that $\\mathbf{p}_k$ is a descent direction), and if the step lengths satisfy the Wolfe conditions.\nFor some algorithms, such as conjugate gradient methods, we will not be able to prove the limit (4.18), but only the weaker result\n\\begin{equation} \\liminf _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 \\label{eq:weak_convergence} \\end{equation}\nIn other words, just a subsequence of the gradient norms $\\|\\nabla f_{k_j}\\|$ converges to zero, rather than the whole sequence. This result, too, can be proved by using Zoutendijk\u0026rsquo;s condition (4.16), but instead of a constructive proof, we outline a proof by contradiction. Suppose that \\eqref{eq:weak_convergence} does not hold, so that the gradients remain bounded away from zero, that is, there exists $\\gamma\u0026gt;0$ such that\n$$ \\|\\nabla f_k\\| \\geq \\gamma, \\quad \\text { for all } k \\text { sufficiently large. } $$\nThen from (4.16) we conclude that\n$$ \\cos \\theta_k \\rightarrow 0 $$\nthat is, the entire sequence $\\{\\cos \\theta_k\\}$ converges to 0. To establish \\eqref{eq:weak_convergence}, therefore, it is enough to show that a subsequence $\\{\\cos \\theta_{k_j}\\}$ is bounded away from zero.\nRate of convergence # We refer the reader to the textbook:\n\u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, 2nd edition, Springer, 2006, pages 47-51, Peculiarly, see pages 51-53.\nQuasi-newton methods # Quasi-Newton methods, like steepest descent, require only the gradient of the objective function to be supplied at each iterate. By measuring the changes in gradients, they construct a model of the objective function that is good enough to produce superlinear convergence. The improvement over steepest descent is dramatic, especially on difficult problems. Moreover, since second derivatives are not required, quasi-Newton methods are sometimes more efficient than Newton\u0026rsquo;s method. Today, optimization software libraries contain a variety of quasi-Newton algorithms for solving unconstrained, constrained, and large-scale optimization problems. In this chapter we discuss quasi-Newton methods for small and medium-sized problems.\nThe development of automatic differentiation techniques has diminished the appeal of quasi-Newton methods, but only to a limited extent. Automatic differentiation eliminates the tedium of computing second derivatives by hand, as well as the risk of introducing errors in the calculation. Nevertheless, quasi-Newton methods remain competitive on many types of problems.\nThe BFGS method # The most popular quasi-Newton algorithm is the BFGS method, named for its discoverers Broyden, Fletcher, Goldfarb, and Shanno. In this section we derive this algorithm (and its close relative, the DFP algorithm) and describe its theoretical properties and practical implementation.\nWe begin the derivation by forming the following quadratic model of the objective function at the current iterate $\\mathbf{x}_k$:\n$$ m_k(\\mathbf{p})=f_k+\\nabla f_k^{\\mathrm{T}} \\mathbf{p}+\\frac{1}{2} \\mathbf{p}^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{p} $$\nHere $\\mathbf{B}_k$ is an $n \\times n$ symmetric positive definite matrix that will be revised or updated at every iteration. Note that the value and gradient of this model at $\\mathbf{p}=\\mathbf{0}$ match $f_k$ and $\\nabla f_k$, respectively. The minimizer $\\mathbf{p}_k$ of this convex quadratic model, which we can write explicitly as\n$$ \\mathbf{p}_k=-\\mathbf{B}_k^{-1} \\nabla f_k, $$\nis used as the search direction, and the new iterate is\n$$ \\mathbf{x}_{k+1}=\\mathbf{x}_k+\\alpha_k \\mathbf{p}_k $$\nwhere the step length $\\alpha_k$ is chosen to satisfy the Wolfe conditions. This iteration is quite similar to the line search Newton method; the key difference is that the approximate Hessian $\\mathbf{B}_k$ is used in place of the true Hessian.\nInstead of computing $\\mathbf{B}_k$ afresh at every iteration, Davidon proposed to update it in a simple manner to account for the curvature measured during the most recent step. Suppose that we have generated a new iterate $\\mathbf{x}_{k+1}$ and wish to construct a new quadratic model, of the form\n$$ m_{k+1}(\\mathbf{p})=f_{k+1}+\\nabla f_{k+1}^{\\mathrm{T}} \\mathbf{p}+\\frac{1}{2} \\mathbf{p}^{\\mathrm{T}} \\mathbf{B}_{k+1} \\mathbf{p} . $$\nWhat requirements should we impose on $\\mathbf{B}_{k+1}$, based on the knowledge we have gained during the latest step? One reasonable requirement is that the gradient of $m_{k+1}$ should match the gradient of the objective function $f$ at the latest two iterates $\\mathbf{x}_k$ and $\\mathbf{x}_{k+1}$. Since $\\nabla m_{k+1}(\\mathbf{0})$ is precisely $\\nabla f_{k+1}$, the second of these conditions is satisfied automatically. The first condition can be written mathematically as\n$$ \\nabla m_{k+1}\\left(-\\alpha_k \\mathbf{p}_k\\right)=\\nabla f_{k+1}-\\alpha_k \\mathbf{B}_{k+1} \\mathbf{p}_k=\\nabla f_k . $$\nBy rearranging, we obtain\n$$ \\mathbf{B}_{k+1} \\alpha_k \\mathbf{p}_k=\\nabla f_{k+1}-\\nabla f_k . $$\nTo simplify the notation it is useful to define the vectors\n$$ \\mathbf{s}_k=\\mathbf{x}_{k+1}-\\mathbf{x}_k, \\quad \\mathbf{y}_k=\\nabla f_{k+1}-\\nabla f_k, $$\nso that the equation becomes\n\\begin{equation} \\mathbf{B}_{k+1} \\mathbf{s}_k=\\mathbf{y}_k . \\label{eq:secant_equation_bfgs} \\end{equation}\nDefinition 1.1 (Secant equation)\nWe refer to \\eqref{eq:secant_equation_bfgs} as the secant equation. Given the displacement $\\mathbf{s}_k$ and the change of gradients $\\mathbf{y}_k$, the secant equation requires that the symmetric positive definite matrix $\\mathbf{B}_{k+1}$ map $\\mathbf{s}_k$ into $\\mathbf{y}_k$. This will be possible only if $\\mathbf{s}_k$ and $\\mathbf{y}_k$ satisfy the curvature condition\n\\begin{equation} \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k\u0026gt;0, \\label{eq:curvature_condition} \\end{equation}\nas is easily seen by premultiplying \\eqref{eq:secant_equation_bfgs} by $\\mathbf{s}_k^{\\mathrm{T}}$. When $f$ is strongly convex, the inequality \\eqref{eq:curvature_condition} will be satisfied for any two points $\\mathbf{x}_k$ and $\\mathbf{x}_{k+1}$. However, this condition will not always hold for nonconvex functions, and in this case we need to enforce \\eqref{eq:curvature_condition} explicitly, by imposing restrictions on the line search procedure that chooses $\\alpha$. In fact, the condition \\eqref{eq:curvature_condition} is guaranteed to hold if we impose the Wolfe or strong Wolfe conditions on the line search. To verify this claim, we note from the definition and the Wolfe condition that $\\nabla f_{k+1}^{\\mathrm{T}} \\mathbf{s}_k \\geq c_2 \\nabla f_k^{\\mathrm{T}} \\mathbf{s}_k$, and therefore\n$$ \\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k \\geq\\left(c_2-1\\right) \\alpha_k \\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k . $$\nSince $c_2\u0026lt;1$ and since $\\mathbf{p}_k$ is a descent direction, the term on the right will be positive, and the curvature condition \\eqref{eq:curvature_condition} holds.\nWhen the curvature condition is satisfied, the secant equation \\eqref{eq:secant_equation_bfgs} always has a solution $\\mathbf{B}_{k+1}$. In fact, it admits an infinite number of solutions, since there are $n(n+1) / 2$ degrees of freedom in a symmetric matrix, and the secant equation represents only $n$ conditions. The requirement of positive definiteness imposes $n$ additional inequalities—all principal minors must be positive—but these conditions do not absorb the remaining degrees of freedom.\nTo determine $\\mathbf{B}_{k+1}$ uniquely, then, we impose the additional condition that among all symmetric matrices satisfying the secant equation, $\\mathbf{B}_{k+1}$ is, in some sense, closest to the current matrix $\\mathbf{B}_k$. In other words, we solve the problem\n$$ \\begin{gathered} \\min _{\\mathbf{B}}\\|\\mathbf{B}-\\mathbf{B}_k\\| \\\\ \\text { subject to } \\quad \\mathbf{B}=\\mathbf{B}^{\\mathrm{T}}, \\quad \\mathbf{B} \\mathbf{s}_k=\\mathbf{y}_k \\end{gathered} $$\nwhere $\\mathbf{s}_k$ and $\\mathbf{y}_k$ satisfy \\eqref{eq:curvature_condition} and $\\mathbf{B}_k$ is symmetric and positive definite. Many matrix norms can be used in the objective, and each norm gives rise to a different quasi-Newton method. A norm that allows easy solution of the minimization problem, and that gives rise to a scale-invariant optimization method, is the weighted Frobenius norm\n$$ \\|\\mathbf{A}\\|_{\\mathbf{W}} \\equiv\\|\\mathbf{W}^{1 / 2} \\mathbf{A} \\mathbf{W}^{1 / 2}\\|_F $$\nwhere $\\|\\cdot\\|_F$ is defined by $\\|\\mathbf{C}\\|_F^2=\\sum_{i=1}^{n} \\sum_{j=1}^{n} c_{ij}^2$. The weight $\\mathbf{W}$ can be chosen as any matrix satisfying the relation $\\mathbf{W} \\mathbf{y}_k=\\mathbf{s}_k$. For concreteness, the reader can assume that $\\mathbf{W}=\\overline{\\mathbf{G}}_k^{-1}$ where $\\overline{\\mathbf{G}}_k$ is the average Hessian defined by\n$$ \\overline{\\mathbf{G}}_k=\\left[\\int_{0}^{1} \\nabla^2 f\\left(\\mathbf{x}_k+\\tau \\alpha_k \\mathbf{p}_k\\right) d \\tau\\right] $$\nThe property\n$$ \\mathbf{y}_k=\\overline{\\mathbf{G}}_k \\alpha_k \\mathbf{p}_k=\\overline{\\mathbf{G}}_k \\mathbf{s}_k $$\nfollows from Taylor\u0026rsquo;s theorem. With this choice of weighting matrix $\\mathbf{W}$, the norm is adimensional, which is a desirable property, since we do not wish the solution to depend on the units of the problem.\nWith this weighting matrix and this norm, the unique solution is\n$$ \\text { (DFP) } \\quad \\mathbf{B}_{k+1}=\\left(\\mathbf{I}-\\gamma_k \\mathbf{y}_k \\mathbf{s}_k^{\\mathrm{T}}\\right) \\mathbf{B}_k\\left(\\mathbf{I}-\\gamma_k \\mathbf{s}_k \\mathbf{y}_k^{\\mathrm{T}}\\right)+\\gamma_k \\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}, $$\nwith\n$$ \\gamma_k=\\frac{1}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} $$\nThis formula is called the DFP updating formula, since it is the one originally proposed by Davidon in 1959, and subsequently studied, implemented, and popularized by Fletcher and Powell.\nThe inverse of $\\mathbf{B}_k$, which we denote by\n$$ \\mathbf{H}_k=\\mathbf{B}_k^{-1}, $$\nis useful in the implementation of the method, since it allows the search direction to be calculated by means of a simple matrix-vector multiplication. Using the Sherman-Morrison-Woodbury formula, we can derive the following expression for the update of the inverse Hessian approximation $\\mathbf{H}_k$ that corresponds to the DFP update of $\\mathbf{B}_k$:\n$$ \\text { (DFP) } \\quad \\mathbf{H}_{k+1}=\\mathbf{H}_k-\\frac{\\mathbf{H}_k \\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}} \\mathbf{H}_k}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{H}_k \\mathbf{y}_k}+\\frac{\\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} \\text {. } $$\nNote that the last two terms in the right-hand-side are rank-one matrices, so that $\\mathbf{H}_k$ undergoes a rank-two modification. It is easy to see that the DFP formula is also a rank-two modification of $\\mathbf{B}_k$. This is the fundamental idea of quasi-Newton updating: Instead of recomputing the iteration matrices from scratch at every iteration, we apply a simple modification that combines the most recently observed information about the objective function with the existing knowledge embedded in our current Hessian approximation.\nThe DFP updating formula is quite effective, but it was soon superseded by the BFGS formula, which is presently considered to be the most effective of all quasi-Newton updating formulae. BFGS updating can be derived by making a simple change in the argument that led to DFP. Instead of imposing conditions on the Hessian approximations $\\mathbf{B}_k$, we impose similar conditions on their inverses $\\mathbf{H}_k$. The updated approximation $\\mathbf{H}_{k+1}$ must be symmetric and positive definite, and must satisfy the secant equation, now written as\n$$ \\mathbf{H}_{k+1} \\mathbf{y}_k=\\mathbf{s}_k . $$\nThe condition of closeness to $\\mathbf{H}_k$ is now specified by the following analogue:\n$$ \\begin{gathered} \\min _{\\mathbf{H}}\\|\\mathbf{H}-\\mathbf{H}_k\\| \\\\ \\text { subject to } \\quad \\mathbf{H}=\\mathbf{H}^{\\mathrm{T}}, \\quad \\mathbf{H} \\mathbf{y}_k=\\mathbf{s}_k . \\end{gathered} $$\nThe norm is again the weighted Frobenius norm described above, where the weight matrix $\\mathbf{W}$ is now any matrix satisfying $\\mathbf{W} \\mathbf{s}_k=\\mathbf{y}_k$. (For concreteness, we assume again that $\\mathbf{W}$ is given by the average Hessian $\\overline{\\mathbf{G}}_k$.) The unique solution $\\mathbf{H}_{k+1}$ is given by\n\\begin{equation} \\mathbf{H}_{k+1}=\\left(\\mathbf{I}-\\rho_k \\mathbf{s}_k \\mathbf{y}_k^{\\mathrm{T}}\\right) \\mathbf{H}_k\\left(\\mathbf{I}-\\rho_k \\mathbf{y}_k \\mathbf{s}_k^{\\mathrm{T}}\\right)+\\rho_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}}, \\label{eq:bfgs_update} \\end{equation}\nwhere\n$$ \\rho_k=\\frac{1}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} $$\nJust one issue has to be resolved before we can define a complete BFGS algorithm: How should we choose the initial approximation $\\mathbf{H}_0$ ? Unfortunately, there is no magic formula that works well in all cases. We can use specific information about the problem, for instance by setting it to the inverse of an approximate Hessian calculated by finite differences at $\\mathbf{x}_0$. Otherwise, we can simply set it to be the identity matrix, or a multiple of the identity matrix, where the multiple is chosen to reflect the scaling of the variables.\nAlgorithm 8.1 (BFGS Method). Given starting point $\\mathbf{x}_0$, convergence tolerance $\\epsilon\u0026gt;0$, inverse Hessian approximation $\\mathbf{H}_0$; $k \\leftarrow 0$; while $\\|\\nabla f_k\\|\u0026gt;\\epsilon$; Compute search direction $\\mathbf{p}_k=-\\mathbf{H}_k \\nabla f_k ;$ Set $\\mathbf{x}_{k+1}=\\mathbf{x}_k+\\alpha_k \\mathbf{p}_k$ where $\\alpha_k$ is computed from a line search procedure to satisfy the Wolfe conditions; Define $\\mathbf{s}_k=\\mathbf{x}_{k+1}-\\mathbf{x}_k$ and $\\mathbf{y}_k=\\nabla f_{k+1}-\\nabla f_k$; Compute $\\mathbf{H}_{k+1}$ by means of \\eqref{eq:bfgs\\_update}; $k \\leftarrow k+1$; end (while) Each iteration can be performed at a cost of $O\\left(n^{2}\\right)$ arithmetic operations (plus the cost of function and gradient evaluations); there are no $O\\left(n^{3}\\right)$ operations such as linear system solves or matrix-matrix operations. The algorithm is robust, and its rate of convergence is superlinear, which is fast enough for most practical purposes. Even though Newton\u0026rsquo;s method converges more rapidly (that is, quadratically), its cost per iteration is higher because it requires the solution of a linear system. A more important advantage for BFGS is, of course, that it does not require calculation of second derivatives.\nWe can derive a version of the BFGS algorithm that works with the Hessian approximation $\\mathbf{B}_k$ rather than $\\mathbf{H}_k$. The update formula for $\\mathbf{B}_k$ is obtained by simply applying the Sherman-Morrison-Woodbury formula to \\eqref{eq:bfgs_update} to obtain\n\\begin{equation} \\text { (BFGS) } \\quad \\mathbf{B}_{k+1}=\\mathbf{B}_k-\\frac{\\mathbf{B}_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k}+\\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k} \\text {. } \\label{eq:bfgs_b_update} \\end{equation}\nA naive implementation of this variant is not efficient for unconstrained minimization, because it requires the system $\\mathbf{B}_k \\mathbf{p}_k=-\\nabla f_k$ to be solved for the step $\\mathbf{p}_k$, thereby increasing the cost of the step computation to $O\\left(n^{3}\\right)$. We discuss later, however, that less expensive implementations of this variant are possible by updating Cholesky factors of $\\mathbf{B}_k$.\nProperties of the BFGS method # It is usually easy to observe the superlinear rate of convergence of the BFGS method on practical problems. Below, we report the last few iterations of the steepest descent, BFGS, and an inexact Newton method on Rosenbrock\u0026rsquo;s function. The table gives the value of $\\|\\mathbf{x}_k-\\mathbf{x}^\\star\\|$. The Wolfe conditions were imposed on the step length in all three methods. From the starting point $(-1.2,1)$, the steepest descent method required 5264 iterations, whereas BFGS and Newton took only 34 and 21 iterations, respectively to reduce the gradient norm to $10^{-5}$.\nsteep. desc. BFGS Newton $1.827 \\mathrm{e}-04$ $1.70 \\mathrm{e}-03$ $3.48 \\mathrm{e}-02$ $1.826 \\mathrm{e}-04$ $1.17 \\mathrm{e}-03$ $1.44 \\mathrm{e}-02$ $1.824 \\mathrm{e}-04$ $1.34 \\mathrm{e}-04$ $1.82 \\mathrm{e}-04$ $1.823 \\mathrm{e}-04$ $1.01 \\mathrm{e}-06$ $1.17 \\mathrm{e}-08$ A few points in the derivation of the BFGS and DFP methods merit further discussion. Note that the minimization problem that gives rise to the BFGS update formula does not explicitly require the updated Hessian approximation to be positive definite. It is easy to show, however, that $\\mathbf{H}_{k+1}$ will be positive definite whenever $\\mathbf{H}_k$ is positive definite, by using the following argument. First, note that $\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k$ is positive, so that the updating formula \\eqref{eq:bfgs_update} is well-defined. For any nonzero vector $\\mathbf{z}$, we have\n$$ \\mathbf{z}^{\\mathrm{T}} \\mathbf{H}_{k+1} \\mathbf{z}=\\mathbf{w}^{\\mathrm{T}} \\mathbf{H}_k \\mathbf{w}+\\rho_k\\left(\\mathbf{z}^{\\mathrm{T}} \\mathbf{s}_k\\right)^{2} \\geq 0 $$\nwhere we have defined $\\mathbf{w}=\\mathbf{z}-\\rho_k \\mathbf{y}_k\\left(\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{z}\\right)$. The right hand side can be zero only if $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{z}=0$, but in this case $\\mathbf{w}=\\mathbf{z} \\neq \\mathbf{0}$, which implies that the first term is greater than zero. Therefore, $\\mathbf{H}_{k+1}$ is positive definite.\nIn order to obtain quasi-Newton updating formulae that are invariant to changes in the variables, it is necessary that the objectives be also invariant. The choice of the weighting matrices $\\mathbf{W}$ used to define the norms ensures that this condition holds. Many other choices of the weighting matrix $\\mathbf{W}$ are possible, each one of them giving a different update formula. However, despite intensive searches, no formula has been found that is significantly more effective than BFGS.\nThe BFGS method has many interesting properties when applied to quadratic functions. We will discuss these properties later on, in the more general context of the Broyden family of updating formulae, of which BFGS is a special case.\nIt is reasonable to ask whether there are situations in which the updating formula such as \\eqref{eq:bfgs_update} can produce bad results. If at some iteration the matrix $\\mathbf{H}_k$ becomes a very poor approximation, is there any hope of correcting it? If, for example, the inner product $\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k$ is tiny (but positive), then it follows from \\eqref{eq:bfgs_update} that $\\mathbf{H}_{k+1}$ becomes huge. Is this behavior reasonable? A related question concerns the rounding errors that occur in finite-precision implementation of these methods. Can these errors grow to the point of erasing all useful information in the quasi-Newton approximate matrix?\nThese questions have been studied analytically and experimentally, and it is now known that the BFGS formula has very effective self-correcting properties. If the matrix $\\mathbf{H}_k$ incorrectly estimates the curvature in the objective function, and if this bad estimate slows down the iteration, then the Hessian approximation will tend to correct itself within a few steps. It is also known that the DFP method is less effective in correcting bad Hessian approximations; this property is believed to be the reason for its poorer practical performance. The self-correcting properties of BFGS hold only when an adequate line search is performed. In particular, the Wolfe line search conditions ensure that the gradients are sampled at points that allow the model to capture appropriate curvature information.\nIt is interesting to note that the DFP and BFGS updating formulae are duals of each other, in the sense that one can be obtained from the other by the interchanges $\\mathbf{s} \\leftrightarrow \\mathbf{y}$, $\\mathbf{B} \\leftrightarrow \\mathbf{H}$. This symmetry is not surprising, given the manner in which we derived these methods above.\nImplementation # A few details and enhancements need to be added to Algorithm 8.1 to produce an efficient implementation. The line search, which should satisfy either the Wolfe conditions or the strong Wolfe conditions, should always try the step length $\\alpha_k=1$ first, because this step length will eventually always be accepted (under certain conditions), thereby producing superlinear convergence of the overall algorithm. Computational observations strongly suggest that it is more economical, in terms of function evaluations, to perform a fairly inaccurate line search. The values $c_1=10^{-4}$ and $c_2=0.9$ are commonly used.\nAs mentioned earlier, the initial matrix $\\mathbf{H}_0$ often is set to some multiple $\\beta \\mathbf{I}$ of the identity, but there is no good general strategy for choosing $\\beta$. If $\\beta$ is \u0026ldquo;too large,\u0026rdquo; so that the first step $\\mathbf{p}_0=-\\beta \\mathbf{g}_0$ is too long, many function evaluations may be required to find a suitable value for the step length $\\alpha_0$. Some software asks the user to prescribe a value $\\delta$ for the norm of the first step, and then set $\\mathbf{H}_0=\\delta\\|\\mathbf{g}_0\\|^{-1} \\mathbf{I}$ to achieve this norm.\nA heuristic that is often quite effective is to scale the starting matrix after the first step has been computed but before the first BFGS update is performed. We change the provisional value $\\mathbf{H}_0=\\mathbf{I}$ by setting\n$$ \\mathbf{H}_0 \\leftarrow \\frac{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{y}_k} \\mathbf{I}, $$\nbefore applying the update \\eqref{eq:bfgs_update} to obtain $\\mathbf{H}_1$. This formula attempts to make the size of $\\mathbf{H}_0$ similar to that of $\\left[\\nabla^{2} f\\left(\\mathbf{x}_0\\right)\\right]^{-1}$, in the following sense. Assuming that the average Hessian is positive definite, there exists a square root $\\overline{\\mathbf{G}}_k^{1 / 2}$ satisfying $\\overline{\\mathbf{G}}_k=\\overline{\\mathbf{G}}_k^{1 / 2} \\overline{\\mathbf{G}}_k^{1 / 2}$. Therefore, by defining $\\mathbf{z}_k=\\overline{\\mathbf{G}}_k^{1 / 2} \\mathbf{s}_k$ and using the relation $\\mathbf{y}_k=\\overline{\\mathbf{G}}_k \\mathbf{s}_k$, we have\n$$ \\frac{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{y}_k}=\\frac{\\left(\\overline{\\mathbf{G}}_k^{1 / 2} \\mathbf{s}_k\\right)^{\\mathrm{T}} \\overline{\\mathbf{G}}_k^{1 / 2} \\mathbf{s}_k}{\\left(\\overline{\\mathbf{G}}_k^{1 / 2} \\mathbf{s}_k\\right)^{\\mathrm{T}} \\overline{\\mathbf{G}}_k \\overline{\\mathbf{G}}_k^{1 / 2} \\mathbf{s}_k}=\\frac{\\mathbf{z}_k^{\\mathrm{T}} \\mathbf{z}_k}{\\mathbf{z}_k^{\\mathrm{T}} \\overline{\\mathbf{G}}_k \\mathbf{z}_k} $$\nThe reciprocal of this expression is an approximation to one of the eigenvalues of $\\overline{\\mathbf{G}}_k$, which in turn is close to an eigenvalue of $\\nabla^{2} f\\left(\\mathbf{x}_k\\right)$. Hence, the quotient itself approximates an eigenvalue of $\\left[\\nabla^{2} f\\left(\\mathbf{x}_k\\right)\\right]^{-1}$. Other scaling factors can be used, but the one presented here appears to be the most successful in practice.\nWe gave an update formula for a BFGS method that works with the Hessian approximation $\\mathbf{B}_k$ instead of the the inverse Hessian approximation $\\mathbf{H}_k$. An efficient implementation of this approach does not store $\\mathbf{B}_k$ explicitly, but rather the Cholesky factorization $\\mathbf{L}_k \\mathbf{D}_k \\mathbf{L}_k^{\\mathrm{T}}$ of this matrix. A formula that updates the factors $\\mathbf{L}_k$ and $\\mathbf{D}_k$ directly in $O\\left(n^{2}\\right)$ operations can be derived from \\eqref{eq:bfgs_b_update}. Since the linear system $\\mathbf{B}_k \\mathbf{p}_k=-\\nabla f_k$ also can be solved in $O\\left(n^{2}\\right)$ operations (by performing triangular substitutions with $\\mathbf{L}_k$ and $\\mathbf{L}_k^{\\mathrm{T}}$ and a diagonal substitution with $\\mathbf{D}_k$ ), the total cost is quite similar to the variant described in Algorithm 8.1. A potential advantage of this alternative strategy is that it gives us the option of modifying diagonal elements in the $\\mathbf{D}_k$ factor if they are not sufficiently large, to prevent instability when we divide by these elements during the calculation of $\\mathbf{p}_k$. However, computational experience suggests no real advantages for this variant, and we prefer the simpler strategy of Algorithm 8.1.\nThe performance of the BFGS method can degrade if the line search is not based on the Wolfe conditions. For example, some software implements an Armijo backtracking line search: The unit step length $\\alpha_k=1$ is tried first and is successively decreased until the sufficient decrease condition is satisfied. For this strategy, there is no guarantee that the curvature condition $\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k\u0026gt;0$ will be satisfied by the chosen step, since a step length greater than 1 may be required to satisfy this condition. To cope with this shortcoming, some implementations simply skip the BFGS update by setting $\\mathbf{H}_{k+1}=\\mathbf{H}_k$ when $\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k$ is negative or too close to zero. This approach is not recommended, because the updates may be skipped much too often to allow $\\mathbf{H}_k$ to capture important curvature information for the objective function $f$.\nThe SR1 method # In the BFGS and DFP updating formulae, the updated matrix $\\mathbf{B}_{k+1}$ (or $\\mathbf{H}_{k+1}$ ) differs from its predecessor $\\mathbf{B}_k$ (or $\\mathbf{H}_k$ ) by a rank-2 matrix. In fact, as we now show, there is a simpler rank-1 update that maintains symmetry of the matrix and allows it to satisfy the secant equation. Unlike the rank-two update formulae, this symmetric-rank-1, or SR1, update does not guarantee that the updated matrix maintains positive definiteness. Good numerical results have been obtained with algorithms based on SR1, so we derive it here and investigate its properties.\nThe symmetric rank-1 update has the general form\n$$ \\mathbf{B}_{k+1}=\\mathbf{B}_k+\\sigma \\mathbf{v} \\mathbf{v}^{\\mathrm{T}} $$\nwhere $\\sigma$ is either +1 or -1 , and $\\sigma$ and $\\mathbf{v}$ are chosen so that $\\mathbf{B}_{k+1}$ satisfies the secant equation, that is, $\\mathbf{y}_k=\\mathbf{B}_{k+1} \\mathbf{s}_k$. By substituting into this equation, we obtain\n$$ \\mathbf{y}_k=\\mathbf{B}_k \\mathbf{s}_k+\\left[\\sigma \\mathbf{v}^{\\mathrm{T}} \\mathbf{s}_k\\right] \\mathbf{v} $$\nSince the term in brackets is a scalar, we deduce that $\\mathbf{v}$ must be a multiple of $\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k$, that is, $\\mathbf{v}=\\delta\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)$ for some scalar $\\delta$. By substituting this form of $\\mathbf{v}$ into the equation, we obtain\n$$ \\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)=\\sigma \\delta^{2}\\left[\\mathbf{s}_k^{\\mathrm{T}}\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)\\right]\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right) $$\nand it is clear that this equation is satisfied if (and only if) we choose the parameters $\\delta$ and $\\sigma$ to be\n$$ \\sigma=\\operatorname{sign}\\left[\\mathbf{s}_k^{\\mathrm{T}}\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)\\right], \\quad \\delta= \\pm\\left|\\mathbf{s}_k^{\\mathrm{T}}\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)\\right|^{-1 / 2} . $$\nHence, we have shown that the only symmetric rank-1 updating formula that satisfies the secant equation is given by\n\\begin{equation} \\text { (SR1) } \\quad \\mathbf{B}_{k+1}=\\mathbf{B}_k+\\frac{\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}}}{\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}} \\mathbf{s}_k} \\label{eq:sr1_update} \\end{equation}\nBy applying the Sherman-Morrison formula, we obtain the corresponding update formula for the inverse Hessian approximation $\\mathbf{H}_k$ :\n\\begin{equation} \\mathbf{H}_{k+1}=\\mathbf{H}_k+\\frac{\\left(\\mathbf{s}_k-\\mathbf{H}_k \\mathbf{y}_k\\right)\\left(\\mathbf{s}_k-\\mathbf{H}_k \\mathbf{y}_k\\right)^{\\mathrm{T}}}{\\left(\\mathbf{s}_k-\\mathbf{H}_k \\mathbf{y}_k\\right)^{\\mathrm{T}} \\mathbf{y}_k} \\label{eq:sr1_h_update} \\end{equation}\nThis derivation is so simple that the SR1 formula has been rediscovered a number of times. It is easy to see that even if $\\mathbf{B}_k$ is positive definite, $\\mathbf{B}_{k+1}$ may not have this property; the same is, of course, true of $\\mathbf{H}_k$. This observation was considered a major drawback in the early days of nonlinear optimization when only line search iterations were used. However, with the advent of trust-region methods, the SR1 updating formula has proved to be quite useful, and its ability to generate indefinite Hessian approximations can actually be regarded as one of its chief advantages.\nThe main drawback of SR1 updating is that the denominator in \\eqref{eq:sr1_update} or \\eqref{eq:sr1_h_update} can vanish. In fact, even when the objective function is a convex quadratic, there may be steps on which there is no symmetric rank-1 update that satisfies the secant equation. It pays to reexamine the derivation above in the light of this observation.\nBy reasoning in terms of $\\mathbf{B}_k$ (similar arguments can be applied to $\\mathbf{H}_k$ ), we see that there are three cases:\nIf $\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}} \\mathbf{s}_k \\neq 0$, then the arguments above show that there is a unique rank-one updating formula satisfying the secant equation, and that it is given by \\eqref{eq:sr1_update}. If $\\mathbf{y}_k=\\mathbf{B}_k \\mathbf{s}_k$, then the only updating formula satisfying the secant equation is simply $\\mathbf{B}_{k+1}=\\mathbf{B}_k$. If $\\mathbf{y}_k \\neq \\mathbf{B}_k \\mathbf{s}_k$ and $\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)^{\\mathrm{T}} \\mathbf{s}_k=0$, then the derivation shows that there is no symmetric rank-one updating formula satisfying the secant equation. The last case clouds an otherwise simple and elegant derivation, and suggests that numerical instabilities and even breakdown of the method can occur. It suggests that rank-one updating does not provide enough freedom to develop a matrix with all the desired characteristics, and that a rank-two correction is required. This reasoning leads us back to the BFGS method, in which positive definiteness (and thus nonsingularity) of all Hessian approximations is guaranteed.\nNevertheless, we are interested in the SR1 formula for the following reasons. (i) A simple safeguard seems to adequately prevent the breakdown of the method and the occurrence of numerical instabilities. (ii) The matrices generated by the SR1 formula tend to be very good approximations of the Hessian matrix—often better than the BFGS approximations. (iii) In quasi-Newton methods for constrained problems, or in methods for partially separable functions, it may not be possible to impose the curvature condition $\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k\u0026gt;0$, and thus BFGS updating is not recommended. Indeed, in these two settings, indefinite Hessian approximations are desirable insofar as they reflect indefiniteness in the true Hessian.\nWe now introduce a strategy to prevent the SR1 method from breaking down. It has been observed in practice that SR1 performs well simply by skipping the update if the denominator is small. More specifically, the update \\eqref{eq:sr1_update} is applied only if\n\\begin{equation} \\left|\\mathbf{s}_k^{\\mathrm{T}}\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right)\\right| \\geq r\\|\\mathbf{s}_k\\|\\|\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\|, \\label{eq:sr1_safeguard} \\end{equation}\nwhere $r \\in(0,1)$ is a small number, say $r=10^{-8}$. If \\eqref{eq:sr1_safeguard} does not hold, we set $\\mathbf{B}_{k+1}=\\mathbf{B}_k$. Most implementations of the SR1 method use a skipping rule of this kind.\nWhy do we advocate skipping of updates for the SR1 method, when in the previous section we discouraged this strategy in the case of BFGS? The two cases are quite different. The condition $\\mathbf{s}_k^{\\mathrm{T}}\\left(\\mathbf{y}_k-\\mathbf{B}_k \\mathbf{s}_k\\right) \\approx 0$ occurs infrequently, since it requires certain vectors to be aligned in a specific way. When it does occur, skipping the update appears to have no negative effects on the iteration. This is not surprising, since the skipping condition implies that $\\mathbf{s}_k^{\\mathrm{T}} \\overline{\\mathbf{G}} \\mathbf{s}_k \\approx \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k$, where $\\overline{\\mathbf{G}}$ is the average Hessian over the last step—meaning that the curvature of $\\mathbf{B}_k$ along $\\mathbf{s}_k$ is already correct. In contrast, the curvature condition $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k \\geq 0$ required for BFGS updating may easily fail if the line search does not impose the Wolfe conditions (e.g., if the step is not long enough), and therefore skipping the BFGS update can occur often and can degrade the quality of the Hessian approximation.\nWe now give a formal description of an SR1 method using a trust-region framework. We prefer it over a line search framework because it does not require us to modify the Hessian approximations to make them sufficiently positive definite.\nAlgorithm 8.2 (SR1 Trust-Region Method).\nGiven starting point $\\mathbf{x}_0$, initial Hessian approximation $\\mathbf{B}_0$, trust-region radius $\\Delta_0$, convergence tolerance $\\epsilon\u0026gt;0$, parameters $\\eta \\in\\left(0,10^{-3}\\right)$ and $r \\in(0,1)$; $k \\leftarrow 0$; while $\\|\\nabla f_k\\|\u0026gt;\\epsilon$; Compute $\\mathbf{s}_k$ by solving the subproblem\n$$ \\min _{\\mathbf{s}} \\nabla f_k^{\\mathrm{T}} \\mathbf{s}+\\frac{1}{2} \\mathbf{s}^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s} \\quad \\text { subject to }\\|\\mathbf{s}\\| \\leq \\Delta_k . $$\nCompute\n$$ \\begin{aligned} \\mathbf{y}_k \u0026amp; =\\nabla f\\left(\\mathbf{x}_k+\\mathbf{s}_k\\right)-\\nabla f_k \\\\ \\text { ared } \u0026amp; =f_k-f\\left(\\mathbf{x}_k+\\mathbf{s}_k\\right) \\quad \\text { (actual reduction) } \\\\ \\text { pred } \u0026amp; =-\\left(\\nabla f_k^{\\mathrm{T}} \\mathbf{s}_k+\\frac{1}{2} \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k\\right) \\quad \\text { (predicted reduction) } \\end{aligned} $$\nif ared/pred \u0026gt; $\\eta$ $\\mathbf{x}_{k+1}=\\mathbf{x}_k+\\mathbf{s}_k$ else $\\mathbf{x}_{k+1}=\\mathbf{x}_k ;$ end (if) if ared/pred \u0026gt; 0.75\nif $\\|\\mathbf{s}_k\\| \\leq 0.8 \\Delta_k$ $\\Delta_{k+1}=\\Delta_k$ else $\\Delta_{k+1}=2 \\Delta_k ;$ end (if) elseif $0.1 \\leq$ ared/pred $\\leq 0.75$ $\\Delta_{k+1}=\\Delta_k$ else $\\Delta_{k+1}=0.5 \\Delta_k ;$ end (if) if \\eqref{eq:sr1\\_safeguard} holds Use \\eqref{eq:sr1\\_update} to compute $\\mathbf{B}_{k+1}$ (even if $\\mathbf{x}_{k+1}=\\mathbf{x}_k$ ) else $\\mathbf{B}_{k+1} \\leftarrow \\mathbf{B}_k ;$ end (if) $k \\leftarrow k+1$; end (while)\nThis algorithm has the typical form of a trust region method. For concreteness we have specified a particular strategy for updating the trust region radius, but other heuristics can be used instead.\nTo obtain a fast rate of convergence, it is important for the matrix $\\mathbf{B}_k$ to be updated even along a failed direction $\\mathbf{s}_k$. The fact that the step was poor indicates that $\\mathbf{B}_k$ is a poor approximation of the true Hessian in this direction. Unless the quality of the approximation is improved, steps along similar directions could be generated on later iterations, and repeated rejection of such steps could prevent superlinear convergence.\nProperties of SR1 updating # One of the main advantages of SR1 updating is its ability to generate very good Hessian approximations. We demonstrate this property by first examining a quadratic function. For functions of this type, the choice of step length does not affect the update, so to examine the effect of the updates, we can assume for simplicity a uniform step length of 1 , that is,\n$$ \\mathbf{p}_k=-\\mathbf{H}_k \\nabla f_k, \\quad \\mathbf{x}_{k+1}=\\mathbf{x}_k+\\mathbf{p}_k $$\nIt follows that $\\mathbf{p}_k=\\mathbf{s}_k$.\nTheorem 1.1 (SR1 finite termination)\nSuppose that $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ is the strongly convex quadratic function $f(\\mathbf{x})=\\mathbf{b}^{\\mathrm{T}} \\mathbf{x}+\\frac{1}{2} \\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}$, where $\\mathbf{A}$ is symmetric positive definite. Then for any starting point $\\mathbf{x}_0$ and any symmetric starting matrix $\\mathbf{H}_0$, the iterates $\\{\\mathbf{x}_k\\}$ generated by the SR1 method converge to the minimizer in at most $n$ steps, provided that $\\left(\\mathbf{s}_k-\\mathbf{H}_k \\mathbf{y}_k\\right)^{\\mathrm{T}} \\mathbf{y}_k \\neq 0$ for all $k$. Moreover, if $n$ steps are performed, and if the search directions $\\mathbf{p}_i$ are linearly independent, then $\\mathbf{H}_n=\\mathbf{A}^{-1}$. Proof\nBecause of our assumption $\\left(\\mathbf{s}_k-\\mathbf{H}_k \\mathbf{y}_k\\right)^{\\mathrm{T}} \\mathbf{y}_k \\neq 0$, the SR1 update is always well-defined. We start by showing inductively that\n$$ \\mathbf{H}_k \\mathbf{y}_j=\\mathbf{s}_j \\quad \\text { for } \\quad j=0, \\ldots, k-1 $$\nIn other words, we claim that the secant equation is satisfied not only along the most recent search direction, but along all previous directions.\nBy definition, the SR1 update satisfies the secant equation, so we have $\\mathbf{H}_1 \\mathbf{y}_0=\\mathbf{s}_0$. Let us now assume that this relation holds for some value $k\u0026gt;1$ and show that it holds also for $k+1$. From this assumption, we have\n$$ \\left(\\mathbf{s}_k-\\mathbf{H}_k \\mathbf{y}_k\\right)^{\\mathrm{T}} \\mathbf{y}_j=\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_j-\\mathbf{y}_k^{\\mathrm{T}}\\left(\\mathbf{H}_k \\mathbf{y}_j\\right)=\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_j-\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_j=0, \\quad \\text { for all } j\u0026lt;k, $$\nwhere the last equality follows because $\\mathbf{y}_i=\\mathbf{A} \\mathbf{s}_i$ for the quadratic function we are considering here. By using this result and the induction hypothesis in \\eqref{eq:sr1_h_update}, we have\n$$ \\mathbf{H}_{k+1} \\mathbf{y}_j=\\mathbf{H}_k \\mathbf{y}_j=\\mathbf{s}_j, \\quad \\text { for all } j\u0026lt;k $$\nSince $\\mathbf{H}_{k+1} \\mathbf{y}_k=\\mathbf{s}_k$ by the secant equation, we have shown that the relation holds when $k$ is replaced by $k+1$. By induction, then, this relation holds for all $k$.\nIf the algorithm performs $n$ steps and if these steps $\\{\\mathbf{s}_j\\}$ are linearly independent, we have\n$$ \\mathbf{s}_j=\\mathbf{H}_n \\mathbf{y}_j=\\mathbf{H}_n \\mathbf{A} \\mathbf{s}_j, \\quad \\text { for } j=0, \\ldots, n-1 $$\nIt follows that $\\mathbf{H}_n \\mathbf{A}=\\mathbf{I}$, that is, $\\mathbf{H}_n=\\mathbf{A}^{-1}$. Therefore, the step taken at $\\mathbf{x}_n$ is the Newton step, and so the next iterate $\\mathbf{x}_{n+1}$ will be the solution, and the algorithm terminates.\nConsider now the case in which the steps become linearly dependent. Suppose that $\\mathbf{s}_k$ is a linear combination of the previous steps, that is,\n$$ \\mathbf{s}_k=\\xi_0 \\mathbf{s}_0+\\cdots+\\xi_{k-1} \\mathbf{s}_{k-1} $$\nfor some scalars $\\xi_i$. From the relations above we have that\n$$ \\begin{aligned} \\mathbf{H}_k \\mathbf{y}_k \u0026amp; =\\mathbf{H}_k \\mathbf{A} \\mathbf{s}_k \\\\ \u0026amp; =\\xi_0 \\mathbf{H}_k \\mathbf{A} \\mathbf{s}_0+\\cdots+\\xi_{k-1} \\mathbf{H}_k \\mathbf{A} \\mathbf{s}_{k-1} \\\\ \u0026amp; =\\xi_0 \\mathbf{H}_k \\mathbf{y}_0+\\cdots+\\xi_{k-1} \\mathbf{H}_k \\mathbf{y}_{k-1} \\\\ \u0026amp; =\\xi_0 \\mathbf{s}_0+\\cdots+\\xi_{k-1} \\mathbf{s}_{k-1} \\\\ \u0026amp; =\\mathbf{s}_k \\end{aligned} $$\nSince $\\mathbf{y}_k=\\nabla f_{k+1}-\\nabla f_k$ and since $\\mathbf{s}_k=\\mathbf{p}_k=-\\mathbf{H}_k \\nabla f_k$, we have that\n$$ \\mathbf{H}_k\\left(\\nabla f_{k+1}-\\nabla f_k\\right)=-\\mathbf{H}_k \\nabla f_k $$\nwhich, by the nonsingularity of $\\mathbf{H}_k$, implies that $\\nabla f_{k+1}=\\mathbf{0}$. Therefore, $\\mathbf{x}_{k+1}$ is the solution point.\n■ The relation from the theorem shows that when $f$ is quadratic, the secant equation is satisfied along all previous search directions, regardless of how the line search is performed. A result like this can be established for BFGS updating only under the restrictive assumption that the line search is exact.\nFor general nonlinear functions, the SR1 update continues to generate good Hessian approximations under certain conditions.\nTheorem 1.2 (SR1 superlinear convergence)\nSuppose that $f$ is twice continuously differentiable, and that its Hessian is bounded and Lipschitz continuous in a neighborhood of a point $\\mathbf{x}^\\star$. Let $\\{\\mathbf{x}_k\\}$ be any sequence of iterates such that $\\mathbf{x}_k \\rightarrow \\mathbf{x}^\\star$ for some $\\mathbf{x}^\\star \\in \\mathbb{R}^{n}$. Suppose in addition that the inequality \\eqref{eq:sr1_safeguard} holds for all $k$, for some $r \\in(0,1)$, and that the steps $\\mathbf{s}_k$ are uniformly linearly independent. Then the matrices $\\mathbf{B}_k$ generated by the SR1 updating formula satisfy\n$$ \\lim _{k \\rightarrow \\infty}\\|\\mathbf{B}_k-\\nabla^{2} f\\left(\\mathbf{x}^\\star\\right)\\|=0 $$\nThe term \u0026ldquo;uniformly linearly independent steps\u0026rdquo; means, roughly speaking, that the steps do not tend to fall in a subspace of dimension less than $n$. This assumption is usually, but not always, satisfied in practice.\nThe Broyden class # So far, we have described the BFGS, DFP, and SR1 quasi-Newton updating formulae, but there are many others. Of particular interest is the Broyden class, a family of updates specified by the following general formula:\n\\begin{equation} \\mathbf{B}_{k+1}=\\mathbf{B}_k-\\frac{\\mathbf{B}_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k}+\\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k}+\\phi_k\\left(\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k\\right) \\mathbf{v}_k \\mathbf{v}_k^{\\mathrm{T}}, \\label{eq:broyden_class} \\end{equation}\nwhere $\\phi_k$ is a scalar parameter and\n$$ \\mathbf{v}_k=\\left[\\frac{\\mathbf{y}_k}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k}-\\frac{\\mathbf{B}_k \\mathbf{s}_k}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k}\\right] . $$\nThe BFGS and DFP methods are members of the Broyden class—we recover BFGS by setting $\\phi_k=0$ and DFP by setting $\\phi_k=1$ in \\eqref{eq:broyden_class}. We can therefore rewrite \\eqref{eq:broyden_class} as a \u0026ldquo;linear combination\u0026rdquo; of these two methods, that is,\n$$ \\mathbf{B}_{k+1}=\\left(1-\\phi_k\\right) \\mathbf{B}_{k+1}^{\\mathrm{BFGS}}+\\phi_k \\mathbf{B}_{k+1}^{\\mathrm{DFP}} . $$\nThis relationship indicates that all members of the Broyden class satisfy the secant equation, since the BFGS and DFP matrices themselves satisfy this equation. Also, since BFGS and DFP updating preserve positive definiteness of the Hessian approximations when $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k\u0026gt;0$, this relation implies that the same property will hold for the Broyden family if $0 \\leq \\phi_k \\leq 1$.\nMuch attention has been given to the so-called restricted Broyden class, which is obtained by restricting $\\phi_k$ to the interval $[0,1]$. It enjoys the following property when applied to quadratic functions. Since the analysis is independent of the step length, we assume for simplicity that each iteration has the form\n$$ \\mathbf{p}_k=-\\mathbf{B}_k^{-1} \\nabla f_k, \\quad \\mathbf{x}_{k+1}=\\mathbf{x}_k+\\mathbf{p}_k $$\nTheorem 1.3 (Broyden class monotonicity)\nSuppose that $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ is the strongly convex quadratic function $f(\\mathbf{x})=\\mathbf{b}^{\\mathrm{T}} \\mathbf{x}+\\frac{1}{2} \\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}$, where $\\mathbf{A}$ is symmetric and positive definite. Let $\\mathbf{x}_0$ be any starting point for the iteration above and $\\mathbf{B}_0$ be any symmetric positive definite starting matrix, and suppose that the matrices $\\mathbf{B}_k$ are updated by the Broyden formula \\eqref{eq:broyden_class} with $\\phi_k \\in[0,1]$. Define $\\lambda_1^k \\leq \\lambda_2^k \\leq \\cdots \\leq \\lambda_n^k$ to be the eigenvalues of the matrix\n$$ \\mathbf{A}^{\\frac{1}{2}} \\mathbf{B}_k^{-1} \\mathbf{A}^{\\frac{1}{2}} $$\nThen for all $k$, we have\n$$ \\min \\left\\{\\lambda_i^k, 1\\right\\} \\leq \\lambda_i^{k+1} \\leq \\max \\left\\{\\lambda_i^k, 1\\right\\}, \\quad i=1, \\ldots, n . $$\nMoreover, the property above does not hold if the Broyden parameter $\\phi_k$ is chosen outside the interval $[0,1]$.\nLet us discuss the significance of this result. If the eigenvalues $\\lambda_i^k$ of the matrix are all 1, then the quasi-Newton approximation $\\mathbf{B}_k$ is identical to the Hessian $\\mathbf{A}$ of the quadratic objective function. This situation is the ideal one, so we should be hoping for these eigenvalues to be as close to 1 as possible. In fact, the relation tells us that the eigenvalues $\\{\\lambda_i^k\\}$ converge monotonically (but not strictly monotonically) to 1. Suppose, for example, that at iteration $k$ the smallest eigenvalue is $\\lambda_1^k=0.7$. Then the theorem tells us that at the next iteration $\\lambda_1^{k+1} \\in[0.7,1]$. We cannot be sure that this eigenvalue has actually gotten closer to 1 , but it is reasonable to expect that it has. In contrast, the first eigenvalue can become smaller than 0.7 if we allow $\\phi_k$ to be outside $[0,1]$. Significantly, the result holds even if the linear searches are not exact.\nAlthough Theorem 1.3 seems to suggest that the best update formulas belong to the restricted Broyden class, the situation is not at all clear. Some analysis and computational testing suggest that algorithms that allow $\\phi_k$ to be negative (in a strictly controlled manner) may in fact be superior to the BFGS method. The SR1 formula is a case in point: It is a member of the Broyden class, obtained by setting\n$$ \\phi_k=\\frac{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k-\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k}, $$\nbut it does not belong to the restricted Broyden class, because this value of $\\phi_k$ may fall outside the interval $[0,1]$.\nWe complete our discussion of the Broyden class by informally stating some of its main properties.\nProperties of the Broyden class # We have noted already that if $\\mathbf{B}_k$ is positive definite, $\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k\u0026gt;0$, and $\\phi_k \\geq 0$, then $\\mathbf{B}_{k+1}$ is also positive definite if a restricted Broyden class update, with $\\phi_k \\in[0,1]$, is used. We would like to determine more precisely the range of values of $\\phi_k$ that preserve positive definiteness.\nThe last term in \\eqref{eq:broyden_class} is a rank-one correction, which by the interlacing eigenvalue theorem decreases the eigenvalues of the matrix when $\\phi_k$ is negative. As we decrease $\\phi_k$, this matrix eventually becomes singular and then indefinite. A little computation shows that $\\mathbf{B}_{k+1}$ is singular when $\\phi_k$ has the value\n$$ \\phi_k^c=\\frac{1}{1-\\mu_k} $$\nwhere\n$$ \\mu_k=\\frac{\\left(\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{B}_k^{-1} \\mathbf{y}_k\\right)\\left(\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k\\right)}{\\left(\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k\\right)^{2}} $$\nBy applying the Cauchy-Schwarz inequality we see that $\\mu_k \\geq 1$ and therefore $\\phi_k^c \\leq 0$. Hence, if the initial Hessian approximation $\\mathbf{B}_0$ is symmetric and positive definite, and if $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k\u0026gt;0$ and $\\phi_k\u0026gt;\\phi_k^c$ for each $k$, then all the matrices $\\mathbf{B}_k$ generated by Broyden\u0026rsquo;s formula \\eqref{eq:broyden_class} remain symmetric and positive definite.\nWhen the line search is exact, all methods in the Broyden class with $\\phi_k \\geq \\phi_k^c$ generate the same sequence of iterates. This result applies to general nonlinear functions and is based on the observation that when all the line searches are exact, the directions generated by Broyden-class methods differ only in their lengths. The line searches identify the same minima along the chosen search direction, though the values of the line search parameter may differ because of the different scaling.\nThe Broyden class has several remarkable properties when applied with exact line searches to quadratic functions. We state some of these properties in the next theorem, whose proof is omitted.\nTheorem 1.4 (Broyden class quadratic properties)\nSuppose that a method in the Broyden class is applied to a strongly convex quadratic function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, where $\\mathbf{x}_0$ is the starting point and $\\mathbf{B}_0$ is any symmetric and positive definite matrix. Assume that $\\alpha_k$ is the exact step length and that $\\phi_k \\geq \\phi_k^c$ for all $k$. Then the following statements are true. (i) The iterates converge to the solution in at most $n$ iterations. (ii) The secant equation is satisfied for all previous search directions, that is,\n$$ \\mathbf{B}_k \\mathbf{s}_j=\\mathbf{y}_j, \\quad j=k-1, \\ldots, 1 $$\n(iii) If the starting matrix is $\\mathbf{B}_0=\\mathbf{I}$, then the iterates are identical to those generated by the conjugate gradient method. In particular, the search directions are conjugate, that is,\n$$ \\mathbf{s}_i^{\\mathrm{T}} \\mathbf{A} \\mathbf{s}_j=0 \\quad \\text { for } i \\neq j $$\nwhere $\\mathbf{A}$ is the Hessian of the quadratic function. (iv) If $n$ iterations are performed, we have $\\mathbf{B}_{n+1}=\\mathbf{A}$.\nNote that parts (i), (ii), and (iv) of this result echo the statement and proof of Theorem 1.1, where similar results were derived for the SR1 update formula.\nIn fact, we can generalize Theorem 1.4 slightly: It continues to hold if the Hessian approximations remain nonsingular but not necessarily positive definite. (Hence, we could allow $\\phi_k$ to be smaller than $\\phi_k^c$, provided that the chosen value did not produce a singular updated matrix.) We also can generalize point (iii) as follows: If the starting matrix $\\mathbf{B}_0$ is not the identity matrix, then the Broyden-class method is identical to the preconditioned conjugate gradient method that uses $\\mathbf{B}_0$ as preconditioner.\nWe conclude by commenting that results like Theorem 1.4 would appear to be mainly of theoretical interest, since the inexact line searches used in practical implementations of Broyden-class methods (and all other quasi-Newton methods) cause their performance to differ markedly. Nevertheless, this type of analysis guided most of the development of quasi-Newton methods.\n"},{"id":3,"href":"/numerical_optimization/docs/lectures/","title":"Lectures","section":"Docs","content":" Lectures # This section regroups the theory : main results, proofs and exercices behind optimization algorithms.\nContent Introduction I - Fundamentals II - Advanced problems III - Machine Learning problems Reminders "},{"id":4,"href":"/numerical_optimization/docs/lectures/reminders/linear_algebra/","title":"Linear Algebra","section":"Reminders","content":" Fundamentals of Linear Algebra # 1 - Introduction # Linear algebra is one of the foundational branches of mathematics, with applications spanning from engineering and computer science to economics and physics. This document is extracted from the textbook Matrix Differential Calculus with Applications in Statistics and Econometrics from Jan R. Magnus; Heinz Neudecker, adapting the notations to the ones used in the lectures.\nIn this chapter, we summarize some of the well-known definitions and theorems of matrix algebra. Most of the theorems will be proved.\n2 - Sets # Definition 0.1 (Set)\nA set is a collection of objects, called the elements (or members) of the set. We write $x \\in S$ to mean \u0026lsquo;$x$ is an element of $S$\u0026rsquo; or \u0026lsquo;$x$ belongs to $S$\u0026rsquo;. If $x$ does not belong to $S$, we write $x \\notin S$. The set that contains no elements is called the empty set, denoted by $\\emptyset$. Sometimes a set can be defined by displaying the elements in braces. For example, $A={0,1}$ or\n$$ \\mathbb{N}={1,2,3, \\ldots} $$\nNotice that $A$ is a finite set (contains a finite number of elements), whereas $\\mathbb{N}$ is an infinite set. If $P$ is a property that any element of $S$ has or does not have, then\n$$ {x: x \\in S, x \\text { satisfies } P} $$\ndenotes the set of all the elements of $S$ that have property $P$.\nDefinition 0.2 (Subset)\nA set $A$ is called a subset of $B$, written $A \\subset B$, whenever every element of $A$ also belongs to $B$. The notation $A \\subset B$ does not rule out the possibility that $A=B$. If $A \\subset B$ and $A \\neq B$, then we say that $A$ is a proper subset of $B$. If $A$ and $B$ are two subsets of $S$, we define\n$$ A \\cup B, $$\nthe union of $A$ and $B$, as the set of elements of $S$ that belong to $A$ or to $B$ or to both, and\n$$ A \\cap B, $$\nthe intersection of $A$ and $B$, as the set of elements of $S$ that belong to both $A$ and $B$. We say that $A$ and $B$ are (mutually) disjoint if they have no common elements, that is, if\n$$ A \\cap B=\\emptyset . $$\nThe complement of $A$ relative to $B$, denoted by $B-A$, is the set ${x: x \\in B$, but $x \\notin A}$. The complement of $A$ (relative to $S$) is sometimes denoted by $A^{c}$.\nDefinition 0.3 (Cartesian Product)\nThe Cartesian product of two sets $A$ and $B$, written $A \\times B$, is the set of all ordered pairs $(a, b)$ such that $a \\in A$ and $b \\in B$. More generally, the Cartesian product of $n$ sets $A_{1}, A_{2}, \\ldots, A_{n}$, written\n$$ \\prod_{i=1}^{n} A_{i} $$\nis the set of all ordered $n$-tuples $\\left(a_{1}, a_{2}, \\ldots, a_{n}\\right)$ such that $a_{i} \\in A_{i}(i=1, \\ldots, n)$.\nThe set of (finite) real numbers (the one-dimensional Euclidean space) is denoted by $\\mathbb{R}$. The $n$-dimensional Euclidean space $\\mathbb{R}^{n}$ is the Cartesian product of $n$ sets equal to $\\mathbb{R}$:\n$$ \\mathbb{R}^{n}=\\mathbb{R} \\times \\mathbb{R} \\times \\cdots \\times \\mathbb{R} \\quad (n \\text { times }) $$\nThe elements of $\\mathbb{R}^{n}$ are thus the ordered $n$-tuples $\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$ of real numbers $x_{1}, x_{2}, \\ldots, x_{n}$.\nDefinition 0.4 (Bounded Set)\nA set $S$ of real numbers is said to be bounded if there exists a number $M$ such that $|x| \\leq M$ for all $x \\in S$. 3 - Matrices: Addition and Multiplication # Definition 0.5 (Real Matrix)\nA real $m \\times n$ matrix $\\mathbf{A}$ is a rectangular array of real numbers\n$$ \\mathbf{A}=\\left(\\begin{array}{cccc} a_{11} \u0026amp; a_{12} \u0026amp; \\ldots \u0026amp; a_{1 n} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; \\ldots \u0026amp; a_{2 n} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ a_{m 1} \u0026amp; a_{m 2} \u0026amp; \\ldots \u0026amp; a_{m n} \\end{array}\\right) $$\nWe sometimes write $\\mathbf{A}=\\left(a_{i j}\\right)$.\nIf one or more of the elements of $\\mathbf{A}$ is complex, we say that $\\mathbf{A}$ is a complex matrix. Almost all matrices in this book are real and the word \u0026lsquo;matrix\u0026rsquo; is assumed to be a real matrix, unless explicitly stated otherwise.\nAn $m \\times n$ matrix can be regarded as a point in $\\mathbb{R}^{m \\times n}$. The real numbers $a_{i j}$ are called the elements of $\\mathbf{A}$. An $m \\times 1$ matrix is a point in $\\mathbb{R}^{m \\times 1}$ (that is, in $\\mathbb{R}^{m}$) and is called a (column) vector of order $m \\times 1$. A $1 \\times n$ matrix is called a row vector (of order $1 \\times n$). The elements of a vector are usually called its components. Matrices are always denoted by capital letters and vectors by lower-case letters.\nDefinition 0.6 (Matrix Addition)\nThe sum of two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of the same order is defined as\n$$ \\mathbf{A}+\\mathbf{B}=\\left(a_{i j}\\right)+\\left(b_{i j}\\right)=\\left(a_{i j}+b_{i j}\\right) $$\nDefinition 0.7 (Scalar Multiplication)\nThe product of a matrix by a scalar $\\lambda$ is\n$$ \\lambda \\mathbf{A}=\\mathbf{A} \\lambda=\\left(\\lambda a_{i j}\\right) $$\nThe following properties are now easily proved for matrices $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$ of the same order and scalars $\\lambda$ and $\\mu$:\n\\begin{equation} \\begin{aligned} \\mathbf{A}+\\mathbf{B} \u0026amp; =\\mathbf{B}+\\mathbf{A}, \\\\ (\\mathbf{A}+\\mathbf{B})+\\mathbf{C} \u0026amp; =\\mathbf{A}+(\\mathbf{B}+\\mathbf{C}), \\\\ (\\lambda+\\mu) \\mathbf{A} \u0026amp; =\\lambda \\mathbf{A}+\\mu \\mathbf{A}, \\\\ \\lambda(\\mathbf{A}+\\mathbf{B}) \u0026amp; =\\lambda \\mathbf{A}+\\lambda \\mathbf{B}, \\\\ \\lambda(\\mu \\mathbf{A}) \u0026amp; =(\\lambda \\mu) \\mathbf{A} . \\end{aligned} \\end{equation}\nA matrix whose elements are all zero is called a null matrix and denoted by $\\mathbf{0}$. We have, of course,\n$$ \\mathbf{A}+(-1) \\mathbf{A}=\\mathbf{0} $$\nDefinition 0.8 (Matrix Multiplication)\nIf $\\mathbf{A}$ is an $m \\times n$ matrix and $\\mathbf{B}$ an $n \\times p$ matrix (so that $\\mathbf{A}$ has the same number of columns as $\\mathbf{B}$ has rows), then we define the product of $\\mathbf{A}$ and $\\mathbf{B}$ as\n$$ \\mathbf{A} \\mathbf{B}=\\left(\\sum_{j=1}^{n} a_{i j} b_{j k}\\right) $$\nThus, $\\mathbf{A} \\mathbf{B}$ is an $m \\times p$ matrix and its $ik$th element is $\\sum_{j=1}^{n} a_{i j} b_{j k}$.\nThe following properties of the matrix product can be established:\n\\begin{equation} \\begin{aligned} (\\mathbf{A} \\mathbf{B}) \\mathbf{C} \u0026amp; =\\mathbf{A}(\\mathbf{B} \\mathbf{C}) \\\\ \\mathbf{A}(\\mathbf{B}+\\mathbf{C}) \u0026amp; =\\mathbf{A} \\mathbf{B}+\\mathbf{A} \\mathbf{C} \\\\ (\\mathbf{A}+\\mathbf{B}) \\mathbf{C} \u0026amp; =\\mathbf{A} \\mathbf{C}+\\mathbf{B} \\mathbf{C} \\end{aligned} \\end{equation}\nThese relations hold provided the matrix products exist.\nWe note that the existence of $\\mathbf{A} \\mathbf{B}$ does not imply the existence of $\\mathbf{B} \\mathbf{A}$, and even when both products exist, they are not generally equal. (Two matrices $\\mathbf{A}$ and $\\mathbf{B}$ for which\n$$ \\mathbf{A} \\mathbf{B}=\\mathbf{B} \\mathbf{A} $$\nare said to commute.) We therefore distinguish between premultiplication and postmultiplication: a given $m \\times n$ matrix $\\mathbf{A}$ can be premultiplied by a $p \\times m$ matrix $\\mathbf{B}$ to form the product $\\mathbf{B} \\mathbf{A}$; it can also be postmultiplied by an $n \\times q$ matrix $\\mathbf{C}$ to form $\\mathbf{A} \\mathbf{C}$.\n4 - The transpose of a matrix # Definition 0.9 (Transpose)\nThe transpose of an $m \\times n$ matrix $\\mathbf{A}=\\left(a_{i j}\\right)$ is the $n \\times m$ matrix, denoted by $\\mathbf{A}^{\\mathrm{T}}$, whose $ij$th element is $a_{j i}$. We have\n\\begin{equation} \\begin{aligned} \\left(\\mathbf{A}^{\\mathrm{T}}\\right)^{\\mathrm{T}} \u0026amp; =\\mathbf{A} \\\\ (\\mathbf{A}+\\mathbf{B})^{\\mathrm{T}} \u0026amp; =\\mathbf{A}^{\\mathrm{T}}+\\mathbf{B}^{\\mathrm{T}} \\\\ (\\mathbf{A} \\mathbf{B})^{\\mathrm{T}} \u0026amp; =\\mathbf{B}^{\\mathrm{T}} \\mathbf{A}^{\\mathrm{T}} \\end{aligned} \\end{equation}\nIf $\\mathbf{x}$ is an $n \\times 1$ vector, then $\\mathbf{x}^{\\mathrm{T}}$ is a $1 \\times n$ row vector and\n$$ \\mathbf{x}^{\\mathrm{T}} \\mathbf{x}=\\sum_{i=1}^{n} x_{i}^{2} $$\nDefinition 0.10 (Euclidean Norm)\nThe (Euclidean) norm of $\\mathbf{x}$ is defined as\n$$ |\\mathbf{x}|=\\left(\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}\\right)^{1 / 2} $$\n5 - Square matrices # Definition 0.11 (Square Matrix)\nA matrix is said to be square if it has as many rows as it has columns. A square matrix $\\mathbf{A}=\\left(a_{i j}\\right)$, real or complex, is said to be\ntype if lower triangular if $a_{i j}=0 \\quad(i\u0026lt;j)$, strictly lower triangular if $a_{i j}=0 \\quad(i \\leq j)$, unit lower triangular if $a_{i j}=0 \\quad(i\u0026lt;j)$ and $a_{i i}=1$ (all $i$), upper triangular if $a_{i j}=0 \\quad(i\u0026gt;j)$, strictly upper triangular if $a_{i j}=0 \\quad(i \\geq j)$, unit upper triangular if $a_{i j}=0 \\quad(i\u0026gt;j)$ and $a_{i i}=1$ (all $i$), idempotent if $\\mathbf{A}^{2}=\\mathbf{A}$. A square matrix $\\mathbf{A}$ is triangular if it is either lower triangular or upper triangular (or both).\nA real square matrix $\\mathbf{A}=\\left(a_{i j}\\right)$ is said to be\ntype if symmetric if $\\mathbf{A}^{\\mathrm{T}} = \\mathbf{A}$, skew-symmetric if $\\mathbf{A}^{\\mathrm{T}} = -\\mathbf{A}$. For any square $n \\times n$ matrix $\\mathbf{A}=\\left(a_{i j}\\right)$, we define $\\operatorname{dg} \\mathbf{A}$ or $\\operatorname{dg}(\\mathbf{A})$ as\n$$ \\operatorname{dg} \\mathbf{A}=\\left(\\begin{array}{cccc} a_{11} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; a_{22} \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; a_{n n} \\end{array}\\right) $$\nor, alternatively,\n$$ \\operatorname{dg} \\mathbf{A}=\\operatorname{diag}\\left(a_{11}, a_{22}, \\ldots, a_{n n}\\right) $$\nDefinition 0.12 (Diagonal Matrix)\nIf $\\mathbf{A}=\\operatorname{dg} \\mathbf{A}$, we say that $\\mathbf{A}$ is diagonal. Definition 0.13 (Identity Matrix)\nA particular diagonal matrix is the identity matrix $\\mathbf{I}_n$ (of order $n \\times n$),\n$$ \\left (\\begin{array}{cccc} 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 1 \\end{array}\\right )=\\left(\\delta_{i j}\\right) $$\nwhere $\\delta_{i j}=1$ if $i=j$ and $\\delta_{i j}=0$ if $i \\neq j$ ($\\delta_{i j}$ is called the Kronecker delta).\nWe sometimes write $\\mathbf{I}$ instead of $\\mathbf{I}_{n}$ when the order is obvious or irrelevant. We have\n$$ \\mathbf{I} \\mathbf{A}=\\mathbf{A} \\mathbf{I}=\\mathbf{A}, $$\nif $\\mathbf{A}$ and $\\mathbf{I}$ have the same order.\nDefinition 0.14 (Orthogonal Matrix)\nA real square matrix $\\mathbf{A}$ is said to be orthogonal if\n$$ \\mathbf{A} \\mathbf{A}^{\\mathrm{T}}=\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I} $$\nand its columns are said to be orthonormal.\nA rectangular (not square) matrix can still have the property that $\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}=\\mathbf{I}$ or $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}=\\mathbf{I}$, but not both. Such a matrix is called semi-orthogonal.\nNote carefully that the concepts of symmetry, skew-symmetry, and orthogonality are defined only for real square matrices. Hence, a complex matrix $\\mathbf{Z}$ satisfying $\\mathbf{Z}^{\\mathrm{T}}=\\mathbf{Z}$ is not called symmetric (in spite of what some textbooks do). This is important because complex matrices can be Hermitian, skew-Hermitian, or unitary, and there are many important results about these classes of matrices. These results should specialize to matrices that are symmetric, skew-symmetric, or orthogonal in the special case that the matrices are real. Thus, a symmetric matrix is just a real Hermitian matrix, a skew-symmetric matrix is a real skew-Hermitian matrix, and an orthogonal matrix is a real unitary matrix; see also Section 1.12.\n6 - Linear forms and quadratic forms # Let $\\mathbf{a}$ be an $n \\times 1$ vector, $\\mathbf{A}$ an $n \\times n$ matrix, and $\\mathbf{B}$ an $n \\times m$ matrix. The expression $\\mathbf{a}^{\\mathrm{T}} \\mathbf{x}$ is called a linear form in $\\mathbf{x}$, the expression $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}$ is a quadratic form in $\\mathbf{x}$, and the expression $\\mathbf{x}^{\\mathrm{T}} \\mathbf{B} \\mathbf{y}$ a bilinear form in $\\mathbf{x}$ and $\\mathbf{y}$. In quadratic forms we may, without loss of generality, assume that $\\mathbf{A}$ is symmetric, because if not then we can replace $\\mathbf{A}$ by $\\left(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}\\right) / 2$, since\n$$ \\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{x}^{\\mathrm{T}}\\left(\\frac{\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}}}{2}\\right) \\mathbf{x} . $$\nThus, let $\\mathbf{A}$ be a symmetric matrix. We say that $\\mathbf{A}$ is\npositive definite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026gt;0$ for all $\\mathbf{x} \\neq \\mathbf{0}$, positive semidefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x} \\geq 0$ for all $\\mathbf{x}$, negative definite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026lt;0$ for all $\\mathbf{x} \\neq \\mathbf{0}$, negative semidefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x} \\leq 0$ for all $\\mathbf{x}$, indefinite if $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026gt;0$ for some $\\mathbf{x}$ and $\\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}\u0026lt;0$ for some $\\mathbf{x}$. It is clear that the matrices $\\mathbf{B} \\mathbf{B}^{\\mathrm{T}}$ and $\\mathbf{B}^{\\mathrm{T}} \\mathbf{B}$ are positive semidefinite, and that $\\mathbf{A}$ is negative (semi)definite if and only if $-\\mathbf{A}$ is positive (semi)definite. A square null matrix is both positive and negative semidefinite.\nDefinition 0.15 (Square Root Matrix)\nIf $\\mathbf{A}$ is positive semidefinite, then there are many matrices $\\mathbf{B}$ satisfying\n$$ \\mathbf{B}^{2}=\\mathbf{A} . $$\nBut there is only one positive semidefinite matrix $\\mathbf{B}$ satisfying $\\mathbf{B}^{2}=\\mathbf{A}$. This matrix is called the square root of $\\mathbf{A}$, denoted by $\\mathbf{A}^{1 / 2}$.\nThe following two theorems are often useful.\nTheorem 0.1 (Matrix Equality Conditions)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix, $\\mathbf{B}$ and $\\mathbf{C}$ $n \\times p$ matrices, and let $\\mathbf{x}$ be an $n \\times 1$ vector. Then, (a) $\\mathbf{A} \\mathbf{x}=\\mathbf{0} \\Longleftrightarrow \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$, (b) $\\mathbf{A} \\mathbf{B}=\\mathbf{0} \\Longleftrightarrow \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{B}=\\mathbf{0}$, (c) $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{B}=\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{C} \\Longleftrightarrow \\mathbf{A} \\mathbf{B}=\\mathbf{A} \\mathbf{C}$. Proof\n(a) Clearly $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ implies $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$. Conversely, if $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=\\mathbf{0}$, then $(\\mathbf{A} \\mathbf{x})^{\\mathrm{T}}(\\mathbf{A} \\mathbf{x})=\\mathbf{x}^{\\mathrm{T}} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}=0$ and hence $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$. (b) follows from (a), and (c) follows from (b) by substituting $\\mathbf{B}-\\mathbf{C}$ for $\\mathbf{B}$ in (b). ■ Theorem 0.2 (Zero Matrix Conditions)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix, $\\mathbf{B}$ and $\\mathbf{C}$ $n \\times n$ matrices, $\\mathbf{B}$ symmetric. Then, (a) $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{A}=\\mathbf{0}$, (b) $\\mathbf{x}^{\\mathrm{T}} \\mathbf{B} \\mathbf{x}=0$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{B}=\\mathbf{0}$, (c) $\\mathbf{x}^{\\mathrm{T}} \\mathbf{C} \\mathbf{x}=0$ for all $n \\times 1$ vectors $\\mathbf{x}$ if and only if $\\mathbf{C}^{\\mathrm{T}}=-\\mathbf{C}$. Proof\nThe proof is easy and is left to the reader. ■ 7 - The rank of a matrix # Definition 0.16 (Linear Independence)\nA set of vectors $\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}$ is said to be linearly independent if $\\sum_{i} \\alpha_{i} \\mathbf{x}_{i}=\\mathbf{0}$ implies that all $\\alpha_{i}=0$. If $\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}$ are not linearly independent, they are said to be linearly dependent. Definition 0.17 (Matrix Rank)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix. The column rank of $\\mathbf{A}$ is the maximum number of linearly independent columns it contains. The row rank of $\\mathbf{A}$ is the maximum number of linearly independent rows it contains. It may be shown that the column rank of $\\mathbf{A}$ is equal to its row rank. Hence, the concept of rank is unambiguous. We denote the rank of $\\mathbf{A}$ by\n$$ r(\\mathbf{A}) . $$\nIt is clear that\n$$ r(\\mathbf{A}) \\leq \\min (m, n) $$\nIf $r(\\mathbf{A})=m$, we say that $\\mathbf{A}$ has full row rank. If $r(\\mathbf{A})=n$, we say that $\\mathbf{A}$ has full column rank. If $r(\\mathbf{A})=0$, then $\\mathbf{A}$ is the null matrix, and conversely, if $\\mathbf{A}$ is the null matrix, then $r(\\mathbf{A})=0$.\nWe have the following important results concerning ranks:\n\\begin{equation} \\begin{gathered} r(\\mathbf{A})=r\\left(\\mathbf{A}^{\\mathrm{T}}\\right)=r\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)=r\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right) \\\\ r(\\mathbf{A} \\mathbf{B}) \\leq \\min (r(\\mathbf{A}), r(\\mathbf{B})) \\\\ r(\\mathbf{A} \\mathbf{B})=r(\\mathbf{A}) \\quad \\text { if } \\mathbf{B} \\text { is square and of full rank, } \\\\ r(\\mathbf{A}+\\mathbf{B}) \\leq r(\\mathbf{A})+r(\\mathbf{B}) \\end{gathered} \\end{equation}\nand finally, if $\\mathbf{A}$ is an $m \\times n$ matrix and $\\mathbf{A} \\mathbf{x}=\\mathbf{0}$ for some $\\mathbf{x} \\neq \\mathbf{0}$, then\n$$ r(\\mathbf{A}) \\leq n-1 $$\nDefinition 0.18 (Column Space)\nThe column space of $\\mathbf{A}(m \\times n)$, denoted by $\\mathcal{M}(\\mathbf{A})$, is the set of vectors\n$$ \\mathcal{M}(\\mathbf{A})=\\left\\{\\mathbf{y}: \\mathbf{y}=\\mathbf{A} \\mathbf{x} \\text { for some } \\mathbf{x} \\text { in } \\mathbb{R}^{n}\\right\\} $$\nThus, $\\mathcal{M}(\\mathbf{A})$ is the vector space generated by the columns of $\\mathbf{A}$.\nThe dimension of this vector space is $r(\\mathbf{A})$. We have\n$ \\mathcal{M}(\\mathbf{A})=\\mathcal{M}\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right) $\nfor any matrix $\\mathbf{A}$.\nExercises:\nIf $\\mathbf{A}$ has full column rank and $\\mathbf{C}$ has full row rank, then $r(\\mathbf{A} \\mathbf{B} \\mathbf{C})=r(\\mathbf{B})$. Let $\\mathbf{A}$ be partitioned as $\\mathbf{A}=\\left(\\mathbf{A}_{1}: \\mathbf{A}_{2}\\right)$. Then $r(\\mathbf{A})=r\\left(\\mathbf{A}_{1}\\right)$ if and only if $\\mathcal{M}\\left(\\mathbf{A}_{2}\\right) \\subset \\mathcal{M}\\left(\\mathbf{A}_{1}\\right)$. 8 - The Inverse # Definition 0.19 (Nonsingular Matrix)\nLet $\\mathbf{A}$ be a square matrix of order $n \\times n$. We say that $\\mathbf{A}$ is nonsingular if $r(\\mathbf{A})=n$, and that $\\mathbf{A}$ is singular if $r(\\mathbf{A})\u0026lt;n$. Definition 0.20 (Matrix Inverse)\nIf $\\mathbf{A}$ is nonsingular, then there exists a nonsingular matrix $\\mathbf{B}$ such that\n$ \\mathbf{A} \\mathbf{B}=\\mathbf{B} \\mathbf{A}=\\mathbf{I}_{n} . $\nThe matrix $\\mathbf{B}$, denoted by $\\mathbf{A}^{-1}$, is unique and is called the inverse of $\\mathbf{A}$.\nWe have\n\\begin{equation} \\begin{aligned} \\left(\\mathbf{A}^{-1}\\right)^{\\mathrm{T}} \u0026amp; =\\left(\\mathbf{A}^{\\mathrm{T}}\\right)^{-1}, \\\\ (\\mathbf{A} \\mathbf{B})^{-1} \u0026amp; =\\mathbf{B}^{-1} \\mathbf{A}^{-1}, \\end{aligned} \\end{equation}\nif the inverses exist.\nDefinition 0.21 (Permutation Matrix)\nA square matrix $\\mathbf{P}$ is said to be a permutation matrix if each row and each column of $\\mathbf{P}$ contain a single element one, and the remaining elements are zero. An $n \\times n$ permutation matrix thus contains $n$ ones and $n(n-1)$ zeros. It can be proved that any permutation matrix is nonsingular. In fact, it is even true that $\\mathbf{P}$ is orthogonal, that is,\n$ \\mathbf{P}^{-1}=\\mathbf{P}^{\\mathrm{T}} $\nfor any permutation matrix $\\mathbf{P}$.\n9 - The Determinant # Definition 0.22 (Determinant)\nAssociated with any $n \\times n$ matrix $\\mathbf{A}$ is the determinant $|\\mathbf{A}|$ defined by\n$ |\\mathbf{A}|=\\sum(-1)^{\\phi\\left(j_{1}, \\ldots, j_{n}\\right)} \\prod_{i=1}^{n} a_{i j_{i}} $\nwhere the summation is taken over all permutations $\\left(j_{1}, \\ldots, j_{n}\\right)$ of the set of integers $(1, \\ldots, n)$, and $\\phi\\left(j_{1}, \\ldots, j_{n}\\right)$ is the number of transpositions required to change $(1, \\ldots, n)$ into $\\left(j_{1}, \\ldots, j_{n}\\right)$.\nWe have\n\\begin{equation} \\begin{aligned} |\\mathbf{A} \\mathbf{B}| \u0026amp; =|\\mathbf{A}||\\mathbf{B}| \\\\ \\left|\\mathbf{A}^{\\mathrm{T}}\\right| \u0026amp; =|\\mathbf{A}| \\\\ |\\alpha \\mathbf{A}| \u0026amp; =\\alpha^{n}|\\mathbf{A}| \\quad \\text { for any scalar } \\alpha \\\\ \\left|\\mathbf{A}^{-1}\\right| \u0026amp; =|\\mathbf{A}|^{-1} \\quad \\text { if } \\mathbf{A} \\text { is nonsingular, } \\\\ \\left|\\mathbf{I}_{n}\\right| \u0026amp; =1 \\end{aligned} \\end{equation}\nDefinition 0.23 (Minor and Cofactor)\nA submatrix of $\\mathbf{A}$ is the rectangular array obtained from $\\mathbf{A}$ by deleting some of its rows and/or some of its columns. A minor is the determinant of a square submatrix of $\\mathbf{A}$. The minor of an element $a_{i j}$ is the determinant of the submatrix of $\\mathbf{A}$ obtained by deleting the $i$th row and $j$th column. The cofactor of $a_{i j}$, say $c_{i j}$, is $(-1)^{i+j}$ times the minor of $a_{i j}$. The matrix $\\mathbf{C}=\\left(c_{i j}\\right)$ is called the cofactor matrix of $\\mathbf{A}$. The transpose of $\\mathbf{C}$ is called the adjoint of $\\mathbf{A}$ and will be denoted by $\\mathbf{A}^{\\#}$.\nWe have\n\\begin{equation} \\begin{aligned} |\\mathbf{A}|=\\sum_{j=1}^{n} a_{i j} c_{i j} \u0026amp; =\\sum_{j=1}^{n} a_{j k} c_{j k} \\quad(i, k=1, \\ldots, n), \\\\ \\mathbf{A} \\mathbf{A}^{\\#} \u0026amp; =\\mathbf{A}^{\\#} \\mathbf{A}=|\\mathbf{A}| \\mathbf{I}, \\\\ (\\mathbf{A} \\mathbf{B})^{\\#} \u0026amp; =\\mathbf{B}^{\\#} \\mathbf{A}^{\\#} . \\end{aligned} \\end{equation}\nDefinition 0.24 (Principal Minor)\nFor any square matrix $\\mathbf{A}$, a principal submatrix of $\\mathbf{A}$ is obtained by deleting corresponding rows and columns. The determinant of a principal submatrix is called a principal minor. Exercises:\nIf $\\mathbf{A}$ is nonsingular, show that $\\mathbf{A}^{\\#}=|\\mathbf{A}| \\mathbf{A}^{-1}$. Prove that the determinant of a triangular matrix is the product of its diagonal elements. 10 - The trace # Definition 0.25 (Trace)\nThe trace of a square $n \\times n$ matrix $\\mathbf{A}$, denoted by $\\operatorname{tr} \\mathbf{A}$ or $\\operatorname{tr}(\\mathbf{A})$, is the sum of its diagonal elements:\n$ \\operatorname{tr} \\mathbf{A}=\\sum_{i=1}^{n} a_{i i} . $\nWe have\n\\begin{equation} \\begin{aligned} \\operatorname{tr}(\\mathbf{A}+\\mathbf{B}) \u0026amp; =\\operatorname{tr} \\mathbf{A}+\\operatorname{tr} \\mathbf{B} \\\\ \\operatorname{tr}(\\lambda \\mathbf{A}) \u0026amp; =\\lambda \\operatorname{tr} \\mathbf{A} \\quad \\text { if } \\lambda \\text { is a scalar } \\\\ \\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \u0026amp; =\\operatorname{tr} \\mathbf{A} \\\\ \\operatorname{tr} \\mathbf{A} \\mathbf{B} \u0026amp; =\\operatorname{tr} \\mathbf{B} \\mathbf{A} \\end{aligned} \\end{equation}\nWe note in (25) that $\\mathbf{A} \\mathbf{B}$ and $\\mathbf{B} \\mathbf{A}$, though both square, need not be of the same order.\nCorresponding to the vector (Euclidean) norm\n$ |\\mathbf{x}|=\\left(\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}\\right)^{1 / 2}, $\ngiven in (4), we now define the matrix (Euclidean) norm as\nDefinition 0.26 (Matrix Norm)\n$ |\\mathbf{A}|=\\left(\\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{1 / 2} $ We have\n$ \\operatorname{tr} \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\geq 0 $\nwith equality if and only if $\\mathbf{A}=\\mathbf{0}$.\n11 - Partitioned matrices # Definition 0.27 (Partitioned Matrix)\nLet $\\mathbf{A}$ be an $m \\times n$ matrix. We can partition $\\mathbf{A}$ as\n$$ \\mathbf{A}=\\left(\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right), $$\nwhere $\\mathbf{A}_{11}$ is $m_1 \\times n_1$, $\\mathbf{A}_{12}$ is $m_1 \\times n_2$, $\\mathbf{A}_{21}$ is $m_2 \\times n_1$, $\\mathbf{A}_{22}$ is $m_2 \\times n_2$, and $m_1+m_2=m$ and $n_1+n_2=n$.\nLet $\\mathbf{B}(m \\times n)$ be similarly partitioned into submatrices $\\mathbf{B}_{ij}(i, j=1,2)$. Then,\n$$ \\mathbf{A}+\\mathbf{B}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}+\\mathbf{B}_{11} \u0026amp; \\mathbf{A}_{12}+\\mathbf{B}_{12} \\\\ \\mathbf{A}_{21}+\\mathbf{B}_{21} \u0026amp; \\mathbf{A}_{22}+\\mathbf{B}_{22} \\end{array}\\right) $$\nNow let $\\mathbf{C}(n \\times p)$ be partitioned into submatrices $\\mathbf{C}_{ij}(i, j=1,2)$ such that $\\mathbf{C}_{11}$ has $n_1$ rows (and hence $\\mathbf{C}_{12}$ also has $n_1$ rows and $\\mathbf{C}_{21}$ and $\\mathbf{C}_{22}$ have $n_2$ rows). Then we may postmultiply $\\mathbf{A}$ by $\\mathbf{C}$ yielding\n$$ \\mathbf{A} \\mathbf{C}=\\left(\\begin{array}{cc} \\mathbf{A}_{11} \\mathbf{C}_{11}+\\mathbf{A}_{12} \\mathbf{C}_{21} \u0026amp; \\mathbf{A}_{11} \\mathbf{C}_{12}+\\mathbf{A}_{12} \\mathbf{C}_{22} \\\\ \\mathbf{A}_{21} \\mathbf{C}_{11}+\\mathbf{A}_{22} \\mathbf{C}_{21} \u0026amp; \\mathbf{A}_{21} \\mathbf{C}_{12}+\\mathbf{A}_{22} \\mathbf{C}_{22} \\end{array}\\right) $$\nThe transpose of the matrix $\\mathbf{A}$ given in (28) is\n$$ \\mathbf{A}^{\\mathrm{T}}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{\\mathrm{T}} \u0026amp; \\mathbf{A}_{21}^{\\mathrm{T}} \\\\ \\mathbf{A}_{12}^{\\mathrm{T}} \u0026amp; \\mathbf{A}_{22}^{\\mathrm{T}} \\end{array}\\right) $$\nIf the off-diagonal blocks $\\mathbf{A}_{12}$ and $\\mathbf{A}_{21}$ are both zero, and $\\mathbf{A}_{11}$ and $\\mathbf{A}_{22}$ are square and nonsingular, then $\\mathbf{A}$ is also nonsingular and its inverse is\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{-1} \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; \\mathbf{A}_{22}^{-1} \\end{array}\\right) $$\nMore generally, if $\\mathbf{A}$ as given in (28) is nonsingular and $\\mathbf{D}=\\mathbf{A}_{22}-\\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}$ is also nonsingular, then\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{A}_{11}^{-1}+\\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{D}^{-1} \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \u0026amp; -\\mathbf{A}_{11}^{-1} \\mathbf{A}_{12} \\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1} \\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \u0026amp; \\mathbf{D}^{-1} \\end{array}\\right) $$\nAlternatively, if $\\mathbf{A}$ is nonsingular and $\\mathbf{E}=\\mathbf{A}_{11}-\\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21}$ is also nonsingular, then\n$$ \\mathbf{A}^{-1}=\\left(\\begin{array}{cc} \\mathbf{E}^{-1} \u0026amp; -\\mathbf{E}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\\\ -\\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\mathbf{E}^{-1} \u0026amp; \\mathbf{A}_{22}^{-1}+\\mathbf{A}_{22}^{-1} \\mathbf{A}_{21} \\mathbf{E}^{-1} \\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\end{array}\\right) . $$\nOf course, if both $\\mathbf{D}$ and $\\mathbf{E}$ are nonsingular, blocks in (29) and (30) can be interchanged. The results (29) and (30) can be easily extended to a $3 \\times 3$ matrix partition. We only consider the following symmetric case where two of the off-diagonal blocks are null matrices.\nTheorem 0.3 (3x3 Symmetric Partitioned Matrix Inverse)\nIf the matrix\n$$ \\left(\\begin{array}{lll} \\mathbf{A} \u0026amp; \\mathbf{B} \u0026amp; \\mathbf{C} \\\\ \\mathbf{B}^{\\mathrm{T}} \u0026amp; \\mathbf{D} \u0026amp; \\mathbf{0} \\\\ \\mathbf{C}^{\\mathrm{T}} \u0026amp; \\mathbf{0} \u0026amp; \\mathbf{E} \\end{array}\\right) $$\nis symmetric and nonsingular, its inverse is given by\n$$ \\left(\\begin{array}{ccc} \\mathbf{Q}^{-1} \u0026amp; -\\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; -\\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\\\ -\\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \u0026amp; \\mathbf{D}^{-1}+\\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; \\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\\\ -\\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \u0026amp; \\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{B} \\mathbf{D}^{-1} \u0026amp; \\mathbf{E}^{-1}+\\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} \\mathbf{Q}^{-1} \\mathbf{C} \\mathbf{E}^{-1} \\end{array}\\right) $$\nwhere\n$$ \\mathbf{Q}=\\mathbf{A}-\\mathbf{B} \\mathbf{D}^{-1} \\mathbf{B}^{\\mathrm{T}}-\\mathbf{C} \\mathbf{E}^{-1} \\mathbf{C}^{\\mathrm{T}} . $$\nProof\nThe proof is left to the reader. ■ As to the determinants of partitioned matrices, we note that\n$$ \\left|\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{0} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right|=\\left|\\mathbf{A}_{11}\\right|\\left|\\mathbf{A}_{22}\\right|=\\left|\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{0} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right| $$\nif both $\\mathbf{A}_{11}$ and $\\mathbf{A}_{22}$ are square matrices.\nExercises:\nFind the determinant and inverse (if it exists) of $$ \\mathbf{B}=\\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{0} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; 1 \\end{array}\\right) . $$\nIf $|\\mathbf{A}| \\neq 0$, prove that $$ \\left|\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right|=\\left(\\alpha-\\mathbf{a}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\\right)|\\mathbf{A}| . $$\nIf $\\alpha \\neq 0$, prove that $$ \\left|\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{a}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right|=\\alpha\\left|\\mathbf{A}-(1 / \\alpha) \\mathbf{b} \\mathbf{a}^{\\mathrm{T}}\\right| . $$\n12 - Complex Matrices # If $\\mathbf{A}$ and $\\mathbf{B}$ are real matrices of the same order, then a complex matrix $\\mathbf{Z}$ can be defined as\n$$ \\mathbf{Z}=\\mathbf{A}+i \\mathbf{B} $$\nwhere $i$ denotes the imaginary unit with the property $i^2=-1$. The complex conjugate of $\\mathbf{Z}$, denoted by $\\mathbf{Z}^mathrm{H}$, is defined as\n$$ \\mathbf{Z}^\\mathrm{H}=\\mathbf{A}^{\\mathrm{T}}-i \\mathbf{B}^{\\mathrm{T}} $$\nIf $\\mathbf{Z}$ is real, then $\\mathbf{Z}^\\mathrm{H}=\\mathbf{Z}^{\\mathrm{T}}$. If $\\mathbf{Z}$ is a scalar, say $\\zeta$, we usually write $\\bar{\\zeta}$ instead of $\\zeta^mathrm{H}$.\nA square complex matrix $\\mathbf{Z}$ is said to be Hermitian if $\\mathbf{Z}^{\\mathrm{H}}=\\mathbf{Z}$ (the complex equivalent to a symmetric matrix), skew-Hermitian if $\\mathbf{Z}^{\\mathrm{H}}=-\\mathbf{Z}$ (the complex equivalent to a skew-symmetric matrix), and unitary if $\\mathbf{Z}^{\\mathrm{H}} \\mathbf{Z}=\\mathbf{I}$ (the complex equivalent to an orthogonal matrix).\nWe shall see in this theorem that the eigenvalues of a symmetric matrix are real. In general, however, eigenvalues (and hence eigenvectors) are complex. In this book, complex numbers appear only in connection with eigenvalues and eigenvectors of matrices that are not symmetric (Chapter 8). A detailed treatment is therefore omitted. Matrices and vectors are assumed to be real, unless it is explicitly specified that they are complex.\n13 - Eigenvalues and Eigenvectors # Definition 0.28 (Eigenvalues and Eigenvectors)\nLet $\\mathbf{A}$ be a square matrix, say $n \\times n$. The eigenvalues of $\\mathbf{A}$ are defined as the roots of the characteristic equation\n$$ \\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right|=0 $$\nThe characteristic equation (31) has $n$ roots, in general complex. Let $\\lambda$ be an eigenvalue of $\\mathbf{A}$. Then there exist vectors $\\mathbf{x}$ and $\\mathbf{y}(\\mathbf{x} \\neq \\mathbf{0}, \\mathbf{y} \\neq \\mathbf{0})$ such that\n$$ (\\lambda \\mathbf{I}-\\mathbf{A}) \\mathbf{x}=\\mathbf{0}, \\quad \\mathbf{y}^{\\mathrm{T}}(\\lambda \\mathbf{I}-\\mathbf{A})=\\mathbf{0} $$\nThat is,\n$$ \\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}, \\quad \\mathbf{y}^{\\mathrm{T}} \\mathbf{A}=\\lambda \\mathbf{y}^{\\mathrm{T}} $$\nThe vectors $\\mathbf{x}$ and $\\mathbf{y}$ are called a (column) eigenvector and row eigenvector of $\\mathbf{A}$ associated with the eigenvalue $\\lambda$.\nEigenvectors are usually normalized in some way to make them unique, for example, by $\\mathbf{x}^{\\mathrm{T}} \\mathbf{x}=\\mathbf{y}^{\\mathrm{T}} \\mathbf{y}=1$ (when $\\mathbf{x}$ and $\\mathbf{y}$ are real).\nNot all roots of the characteristic equation need to be different. Each root is counted a number of times equal to its multiplicity. When a root (eigenvalue) appears more than once it is called a multiple eigenvalue; if it appears only once it is called a simple eigenvalue.\nAlthough eigenvalues are in general complex, the eigenvalues of a symmetric matrix are always real.\nTheorem 0.4 (Symmetric Matrix Eigenvalues)\nA symmetric matrix has only real eigenvalues. Proof\nLet $\\lambda$ be an eigenvalue of a symmetric matrix $\\mathbf{A}$ and let $\\mathbf{x}=\\mathbf{u}+i \\mathbf{v}$ be an associated eigenvector. Then,\n$$ \\mathbf{A}(\\mathbf{u}+i \\mathbf{v})=\\lambda(\\mathbf{u}+i \\mathbf{v}) $$\nand hence\n$$ (\\mathbf{u}-i \\mathbf{v})^{\\mathrm{T}} \\mathbf{A}(\\mathbf{u}+i \\mathbf{v})=\\lambda(\\mathbf{u}-i \\mathbf{v})^{\\mathrm{T}}(\\mathbf{u}+i \\mathbf{v}) $$\nwhich leads to\n$$ \\mathbf{u}^{\\mathrm{T}} \\mathbf{A} \\mathbf{u}+\\mathbf{v}^{\\mathrm{T}} \\mathbf{A} \\mathbf{v}=\\lambda\\left(\\mathbf{u}^{\\mathrm{T}} \\mathbf{u}+\\mathbf{v}^{\\mathrm{T}} \\mathbf{v}\\right) $$\nbecause of the symmetry of $\\mathbf{A}$. This implies that $\\lambda$ is real.\n■ Let us prove the following three results, which will be useful to us later.\nTheorem 0.5 (Similar Matrices Eigenvalues)\nIf $\\mathbf{A}$ is an $n \\times n$ matrix and $\\mathbf{G}$ is a nonsingular $n \\times n$ matrix, then $\\mathbf{A}$ and $\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}$ have the same set of eigenvalues (with the same multiplicities). Proof\nFrom\n$$ \\lambda \\mathbf{I}_n-\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}=\\mathbf{G}^{-1}\\left(\\lambda \\mathbf{I}_n-\\mathbf{A}\\right) \\mathbf{G} $$\nwe obtain\n$$ \\left|\\lambda \\mathbf{I}_n-\\mathbf{G}^{-1} \\mathbf{A} \\mathbf{G}\\right|=\\left|\\mathbf{G}^{-1}\\right|\\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right||\\mathbf{G}|=\\left|\\lambda \\mathbf{I}_n-\\mathbf{A}\\right| $$\nand the result follows.\n■ Theorem 0.6 (Singular Matrix Zero Eigenvalue)\nA singular matrix has at least one zero eigenvalue. Proof\nIf $\\mathbf{A}$ is singular, then $|\\mathbf{A}|=0$ and hence $|\\lambda \\mathbf{I}-\\mathbf{A}|=0$ for $\\lambda=0$. ■ Theorem 0.7 (Special Matrix Eigenvalues)\nAn idempotent matrix has only eigenvalues 0 or 1. All eigenvalues of a unitary matrix have unit modulus. Proof\nLet $\\mathbf{A}$ be idempotent. Then $\\mathbf{A}^2=\\mathbf{A}$. Thus, if $\\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}$, then\n$$ \\lambda \\mathbf{x}=\\mathbf{A} \\mathbf{x}=\\mathbf{A}(\\mathbf{A} \\mathbf{x})=\\mathbf{A}(\\lambda \\mathbf{x})=\\lambda(\\mathbf{A} \\mathbf{x})=\\lambda^2 \\mathbf{x} $$\nand hence $\\lambda=\\lambda^2$, which implies $\\lambda=0$ or $\\lambda=1$.\nIf $\\mathbf{A}$ is unitary, then $\\mathbf{A}^{\\mathrm{H}} \\mathbf{A}=\\mathbf{I}$. Thus, if $\\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}$, then\n$$ \\mathbf{x}^{\\mathrm{H}} \\mathbf{A}^{\\mathrm{H}}=\\bar{\\lambda} \\mathbf{x}^{\\mathrm{H}} $$\nusing the notation of Section 1.12. Hence,\n$$ \\mathbf{x}^{\\mathrm{H}} \\mathbf{x}=\\mathbf{x}^{\\mathrm{H}} \\mathbf{A}^{\\mathrm{H}} \\mathbf{A} \\mathbf{x}=\\bar{\\lambda} \\lambda \\mathbf{x}^{\\mathrm{H}} \\mathbf{x} $$\nSince $\\mathbf{x}^{\\mathrm{H}} \\mathbf{x} \\neq 0$, we obtain $\\bar{\\lambda} \\lambda=1$ and hence $|\\lambda|=1$.\n■ An important theorem regarding positive definite matrices is stated below.\nTheorem 0.8 (Positive Definite Eigenvalues)\nA symmetric matrix is positive definite if and only if all its eigenvalues are positive. Proof\nIf $\\mathbf{A}$ is positive definite and $\\mathbf{A}\\mathbf{x}=\\lambda \\mathbf{x}$, then $\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}=\\lambda \\mathbf{x}^\\mathrm{T} \\mathbf{x}$. Now, $\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}\u0026gt;0$ and $\\mathbf{x}^\\mathrm{T} \\mathbf{x}\u0026gt;0$ imply $\\lambda\u0026gt;0$. The converse will not be proved here. (It follows from this theorem.) ■ Next, let us prove this theorem.\nTheorem 0.9 (Eigenvalue Identity)\nLet $\\mathbf{A}$ be $m \\times n$ and let $\\mathbf{B}$ be $n \\times m(n \\geq m)$. Then the nonzero eigenvalues of $\\mathbf{B}\\mathbf{A}$ and $\\mathbf{A}\\mathbf{B}$ are identical, and $\\left|I_{m}-\\mathbf{A}\\mathbf{B}\\right|=\\left|I_{n}-\\mathbf{B}\\mathbf{A}\\right|$. Proof\nTaking determinants on both sides of the equality\n\\begin{equation} \\left(\\begin{array}{cc} I_{m}-\\mathbf{A}\\mathbf{B} \u0026amp; \\mathbf{A} \\\\ 0 \u0026amp; I_{n} \\end{array}\\right)\\left(\\begin{array}{cc} I_{m} \u0026amp; 0 \\\\ \\mathbf{B} \u0026amp; I_{n} \\end{array}\\right)=\\left(\\begin{array}{cc} I_{m} \u0026amp; 0 \\\\ \\mathbf{B} \u0026amp; I_{n} \\end{array}\\right)\\left(\\begin{array}{cc} I_{m} \u0026amp; \\mathbf{A} \\\\ 0 \u0026amp; I_{n}-\\mathbf{B}\\mathbf{A} \\end{array}\\right), \\end{equation}\nwe obtain\n\\begin{equation} \\left|I_{m}-\\mathbf{A}\\mathbf{B}\\right|=\\left|I_{n}-\\mathbf{B}\\mathbf{A}\\right| . \\end{equation}\nNow let $\\lambda \\neq 0$. Then,\n\\begin{equation} \\begin{aligned} \\left|\\lambda I_{n}-\\mathbf{B}\\mathbf{A}\\right| \u0026amp; =\\lambda^{n}\\left|I_{n}-\\mathbf{B}\\left(\\lambda^{-1} \\mathbf{A}\\right)\\right| \\\\ \u0026amp; =\\lambda^{n}\\left|I_{m}-\\left(\\lambda^{-1} \\mathbf{A}\\right) \\mathbf{B}\\right|=\\lambda^{n-m}\\left|\\lambda I_{m}-\\mathbf{A}\\mathbf{B}\\right| . \\end{aligned} \\end{equation}\nHence, the nonzero eigenvalues of $\\mathbf{B}\\mathbf{A}$ are the same as the nonzero eigenvalues of $\\mathbf{A}\\mathbf{B}$, and this is equivalent to the statement in the theorem.\n■ Without proof we state the following famous result.\nTheorem 0.10 (Cayley-Hamilton)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$. Then,\n\\begin{equation} \\prod_{i=1}^{n}\\left(\\lambda_{i} I_{n}-\\mathbf{A}\\right)=0 . \\end{equation}\nFinally, we present the following result on eigenvectors.\nTheorem 0.11 (Linear Independence of Eigenvectors)\nEigenvectors associated with distinct eigenvalues are linearly independent. Proof\nLet $\\mathbf{A}\\mathbf{x}_{1}=\\lambda_{1} \\mathbf{x}_{1}$, $\\mathbf{A}\\mathbf{x}_{2}=\\lambda_{2} \\mathbf{x}_{2}$, and $\\lambda_{1} \\neq \\lambda_{2}$. Assume that $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ are linearly dependent. Then there is an $\\alpha \\neq 0$ such that $\\mathbf{x}_{2}=\\alpha \\mathbf{x}_{1}$, and hence\n\\begin{equation} \\alpha \\lambda_{1} \\mathbf{x}_{1}=\\alpha \\mathbf{A} \\mathbf{x}_{1}=\\mathbf{A} \\mathbf{x}_{2}=\\lambda_{2} \\mathbf{x}_{2}=\\alpha \\lambda_{2} \\mathbf{x}_{1} \\end{equation}\nthat is\n\\begin{equation} \\alpha\\left(\\lambda_{1}-\\lambda_{2}\\right) \\mathbf{x}_{1}=0 \\end{equation}\nSince $\\alpha \\neq 0$ and $\\lambda_{1} \\neq \\lambda_{2}$, this implies that $\\mathbf{x}_{1}=0$, a contradiction.\n■ Exercices\nShow that \\begin{equation} \\left|\\begin{array}{ll} 0 \u0026amp; I_{m} \\\\ I_{m} \u0026amp; 0 \\end{array}\\right|=(-1)^{m} \\end{equation}\nShow that, for $n=2$, \\begin{equation} |I+\\epsilon \\mathbf{A}|=1+\\epsilon \\operatorname{tr} \\mathbf{A}+\\epsilon^{2}|\\mathbf{A}| \\end{equation}\nShow that, for $n=3$, \\begin{equation} |I+\\epsilon \\mathbf{A}|=1+\\epsilon \\operatorname{tr} \\mathbf{A}+\\frac{\\epsilon^{2}}{2}\\left((\\operatorname{tr} \\mathbf{A})^{2}-\\operatorname{tr} \\mathbf{A}^{2}\\right)+\\epsilon^{3}|\\mathbf{A}| \\end{equation}\n14 - Schur\u0026rsquo;s decomposition theorem # In the next few sections, we present three decomposition theorems: Schur\u0026rsquo;s theorem, Jordan\u0026rsquo;s theorem, and the singular-value decomposition. Each of these theorems will prove useful later in this book. We first state Schur\u0026rsquo;s theorem.\nTheorem 0.12 (Schur decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix, possibly complex. Then there exist a unitary $n \\times n$ matrix $\\mathbf{S}$ (that is, $\\mathbf{S}^\\mathrm{H} \\mathbf{S}=I_{n}$ ) and an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M} \\end{equation}\nThe most important special case of Schur\u0026rsquo;s decomposition theorem is the case where $\\mathbf{A}$ is symmetric.\nTheorem 0.13 (Symmetric Matrix Decomposition)\nLet $\\mathbf{A}$ be a symmetric $n \\times n$ matrix. Then there exist an orthogonal $n \\times n$ matrix $\\mathbf{S}$ (that is, $\\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{n}$ ) whose columns are eigenvectors of $\\mathbf{A}$ and a diagonal matrix $\\boldsymbol{\\Lambda}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{S}^\\mathrm{T} \\mathbf{A} \\mathbf{S}=\\boldsymbol{\\Lambda} \\end{equation}\nProof\nUsing Theorem 0.12, there exists a unitary matrix $\\mathbf{S}=\\mathbf{R}+i \\mathbf{T}$ with real $\\mathbf{R}$ and $\\mathbf{T}$ and an upper triangular matrix $\\mathbf{M}$ such that $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. Then,\n\\begin{equation} \\begin{aligned} \\mathbf{M} \u0026amp; =\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=(\\mathbf{R}-i \\mathbf{T})^\\mathrm{T} \\mathbf{A}(\\mathbf{R}+i \\mathbf{T}) \\\\ \u0026amp; =\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T}\\right)+i\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{T}-\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{R}\\right) \\end{aligned} \\end{equation}\nand hence, using the symmetry of $\\mathbf{A}$,\n\\begin{equation} \\mathbf{M}+\\mathbf{M}^\\mathrm{T}=2\\left(\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T}\\right) . \\end{equation}\nIt follows that $\\mathbf{M}+\\mathbf{M}^\\mathrm{T}$ is a real matrix and hence, since $\\mathbf{M}$ is triangular, that $\\mathbf{M}$ is a real matrix. We thus obtain, from (32),\n\\begin{equation} \\mathbf{M}=\\mathbf{R}^\\mathrm{T} \\mathbf{A} \\mathbf{R}+\\mathbf{T}^\\mathrm{T} \\mathbf{A} \\mathbf{T} . \\end{equation}\nSince $\\mathbf{A}$ is symmetric, $\\mathbf{M}$ is symmetric. But, since $\\mathbf{M}$ is also triangular, $\\mathbf{M}$ must be diagonal. The columns of $\\mathbf{S}$ are then eigenvectors of $\\mathbf{A}$, and since the diagonal elements of $\\mathbf{M}$ are real, $\\mathbf{S}$ can be chosen to be real as well.\n■ Exercices\nLet $\\mathbf{A}$ be a symmetric $n \\times n$ matrix with eigenvalues $\\lambda_{1} \\leq \\lambda_{2} \\leq \\cdots \\leq \\lambda_{n}$. Use Theorem 0.13 to prove that \\begin{equation} \\lambda_{1} \\leq \\frac{\\mathbf{x}^\\mathrm{T} \\mathbf{A} \\mathbf{x}}{\\mathbf{x}^\\mathrm{T} \\mathbf{x}} \\leq \\lambda_{n} . \\end{equation}\nHence show that, for any $m \\times n$ matrix $\\mathbf{A}$, \\begin{equation} |\\mathbf{A} \\mathbf{x}| \\leq \\mu|\\mathbf{x}|, \\end{equation}\nwhere $\\mu^{2}$ denotes the largest eigenvalue of $\\mathbf{A}^\\mathrm{T} \\mathbf{A}$.\nLet $\\mathbf{A}$ be an $m \\times n$ matrix of rank $r$. Show that there exists an $n \\times(n-r)$ matrix $\\mathbf{S}$ such that \\begin{equation} \\mathbf{A} \\mathbf{S}=0, \\quad \\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{n-r} \\end{equation}\nLet $\\mathbf{A}$ be an $m \\times n$ matrix of rank $r$. Let $\\mathbf{S}$ be a matrix such that $\\mathbf{A} \\mathbf{S}=0$. Show that $r(\\mathbf{S}) \\leq n-r$. 15 - The Jordan decomposition # Schur\u0026rsquo;s theorem tells us that there exists, for every square matrix $\\mathbf{A}$, a unitary (possibly orthogonal) matrix $\\mathbf{S}$ which \u0026rsquo;transforms\u0026rsquo; $\\mathbf{A}$ into an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$.\nJordan\u0026rsquo;s theorem similarly states that there exists a nonsingular matrix, say $\\mathbf{T}$, which transforms $\\mathbf{A}$ into an upper triangular matrix $\\mathbf{M}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$. The difference between the two decomposition theorems is that in Jordan\u0026rsquo;s theorem less structure is put on the matrix $\\mathbf{T}$ (nonsingular, but not necessarily unitary) and more structure on the matrix $\\mathbf{M}$.\nTheorem 0.14 (Jordan decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix and denote by $\\mathbf{J}_{k}(\\lambda)$ a $k \\times k$ matrix of the form\n\\begin{equation} \\mathbf{J}_{k}(\\lambda)=\\left(\\begin{array}{cccccc} \\lambda \u0026amp; 1 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda \u0026amp; 1 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\lambda \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \u0026amp; \\lambda \\end{array}\\right) \\end{equation}\n(a so-called Jordan block), where $\\mathbf{J}_{1}(\\lambda)=\\lambda$. Then there exists a nonsingular $n \\times n$ matrix $\\mathbf{T}$ such that\n\\begin{equation} \\mathbf{T}^{-1} \\mathbf{A} \\mathbf{T}=\\left(\\begin{array}{cccc} \\mathbf{J}_{k_{1}}\\left(\\lambda_{1}\\right) \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; \\mathbf{J}_{k_{2}}\\left(\\lambda_{2}\\right) \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\mathbf{J}_{k_{r}}\\left(\\lambda_{r}\\right) \\end{array}\\right) \\end{equation}\nwith $k_{1}+k_{2}+\\cdots+k_{r}=n$. The $\\lambda_{i}$ are the eigenvalues of $\\mathbf{A}$, not necessarily distinct.\nThe most important special case of Theorem 0.14 is this theorem.\nTheorem 0.15 (Distinct Eigenvalues Decomposition)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix with distinct eigenvalues. Then there exist a nonsingular $n \\times n$ matrix $\\mathbf{T}$ and a diagonal $n \\times n$ matrix $\\boldsymbol{\\Lambda}$ whose diagonal elements are the eigenvalues of $\\mathbf{A}$, such that\n\\begin{equation} \\mathbf{T}^{-1} \\mathbf{A} \\mathbf{T}=\\boldsymbol{\\Lambda} \\end{equation}\nProof\nImmediate from Theorem 0.14 (or Theorem 0.11). ■ Exercices\nShow that $\\left(\\lambda I_{k}-\\mathbf{J}_{k}(\\lambda)\\right)^{k}=0$ and use this fact to prove Theorem 0.10. Show that Theorem 0.15 remains valid when $\\mathbf{A}$ is complex. 16 - The singular value decomposition # The third important decomposition theorem is the singular-value decomposition.\nTheorem 0.16 (Singular-value decomposition)\nLet $\\mathbf{A}$ be a real $m \\times n$ matrix with $r(\\mathbf{A})=r\u0026gt;0$. Then there exist an $m \\times r$ matrix $\\mathbf{S}$ such that $\\mathbf{S}^\\mathrm{T} \\mathbf{S}=I_{r}$, an $n \\times r$ matrix $\\mathbf{T}$ such that $\\mathbf{T}^\\mathrm{T} \\mathbf{T}=I_{r}$ and an $r \\times r$ diagonal matrix $\\boldsymbol{\\Lambda}$ with positive diagonal elements, such that\n\\begin{equation} \\mathbf{A}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{T}^\\mathrm{T} \\end{equation}\nProof\nSince $\\mathbf{A} \\mathbf{A}^\\mathrm{T}$ is an $m \\times m$ positive semidefinite matrix of rank $r$ (by (6)), its nonzero eigenvalues are all positive (this theorem). From Theorem 0.13 we know that there exists an orthogonal $m \\times m$ matrix $( \\mathbf{S}: \\mathbf{S}_{2})$ such that\n\\begin{equation} \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}, \\quad \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}_{2}=0, \\quad \\mathbf{S} \\mathbf{S}^\\mathrm{T}+\\mathbf{S}_{2} \\mathbf{S}_{2}^\\mathrm{T}=I_{m} \\end{equation}\nwhere $\\boldsymbol{\\Lambda}$ is an $r \\times r$ diagonal matrix having these $r$ positive eigenvalues as its diagonal elements. Define $\\mathbf{T}=\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}$. Then we see that\n\\begin{equation} \\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda}, \\quad \\mathbf{T}^\\mathrm{T} \\mathbf{T}=I_{r} \\label{eq:33} \\end{equation}\nThus, since $\\mathbf{A}^\\mathrm{T} \\mathbf{S}_{2}=0$, we have\n\\begin{equation} \\mathbf{A}=\\left(\\mathbf{S} \\mathbf{S}^\\mathrm{T}+\\mathbf{S}_{2} \\mathbf{S}_{2}^\\mathrm{T}\\right) \\mathbf{A}=\\mathbf{S} \\mathbf{S}^\\mathrm{T} \\mathbf{A}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2}\\left(\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}\\right)^\\mathrm{T}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{T}^\\mathrm{T} \\label{eq:34} \\end{equation}\nwhich concludes the proof.\n■ We see from \\eqref{eq:33} and \\eqref{eq:34} that the semi-orthogonal matrices $\\mathbf{S}$ and $\\mathbf{T}$ satisfy\n\\begin{equation} \\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}, \\quad \\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda} \\end{equation}\nHence, $\\boldsymbol{\\Lambda}$ contains the $r$ nonzero eigenvalues of $\\mathbf{A} \\mathbf{A}^\\mathrm{T}$ (and of $\\mathbf{A}^\\mathrm{T} \\mathbf{A}$ ) and $\\mathbf{S}$ (by construction) and $\\mathbf{T}$ contain corresponding eigenvectors. A common mistake in applying the singular-value decomposition is to find $\\mathbf{S}$, $\\mathbf{T}$, and $\\boldsymbol{\\Lambda}$ from (35). This is incorrect because, given $\\mathbf{S}$, $\\mathbf{T}$ is not unique. The correct procedure is to find $\\mathbf{S}$ and $\\boldsymbol{\\Lambda}$ from $\\mathbf{A} \\mathbf{A}^\\mathrm{T} \\mathbf{S}=\\mathbf{S} \\boldsymbol{\\Lambda}$ and then define $\\mathbf{T}=\\mathbf{A}^\\mathrm{T} \\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2}$. Alternatively, we can find $\\mathbf{T}$ and $\\boldsymbol{\\Lambda}$ from $\\mathbf{A}^\\mathrm{T} \\mathbf{A} \\mathbf{T}=\\mathbf{T} \\boldsymbol{\\Lambda}$ and define $\\mathbf{S}=\\mathbf{A} \\mathbf{T} \\boldsymbol{\\Lambda}^{-1 / 2}$.\n17 - Further results concerning eigenvalues # Let us now prove the following five theorems, all of which concern eigenvalues. this theorem deals with the sum and the product of the eigenvalues. this theorem and this theorem discuss the relationship between the rank and the number of nonzero eigenvalues, and this theorem concerns idempotent matrices.\nTheorem 0.17 (Trace and Determinant)\nLet $\\mathbf{A}$ be a square, possibly complex, $n \\times n$ matrix with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{n}$. Then,\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\sum_{i=1}^{n} \\lambda_{i}, \\quad |\\mathbf{A}|=\\prod_{i=1}^{n} \\lambda_{i} \\end{equation}\nProof\nWe write, using Theorem 0.12, $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. Then,\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\operatorname{tr} \\mathbf{S} \\mathbf{M} \\mathbf{S}^\\mathrm{H}=\\operatorname{tr} \\mathbf{M} \\mathbf{S}^\\mathrm{H} \\mathbf{S}=\\operatorname{tr} \\mathbf{M}=\\sum_{i} \\lambda_{i} \\end{equation}\nand\n\\begin{equation} |\\mathbf{A}|=\\left|\\mathbf{S} \\mathbf{M} \\mathbf{S}^\\mathrm{H}\\right|=|\\mathbf{S}||\\mathbf{M}|\\left|\\mathbf{S}^\\mathrm{H}\\right|=|\\mathbf{M}|=\\prod_{i} \\lambda_{i} \\end{equation}\nand the result follows.\n■ Theorem 0.18 (Rank and Nonzero Eigenvalues)\nIf $\\mathbf{A}$ has $r$ nonzero eigenvalues, then $r(\\mathbf{A}) \\geq r$. Proof\nWe write again, using Theorem 0.12, $\\mathbf{S}^\\mathrm{H} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$. We partition\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) \\end{equation}\nwhere $\\mathbf{M}_{1}$ is a nonsingular upper triangular $r \\times r$ matrix and $\\mathbf{M}_{3}$ is strictly upper triangular. Since $r(\\mathbf{A})=r(\\mathbf{M}) \\geq r\\left(\\mathbf{M}_{1}\\right)=r$, the result follows.\n■ The following example shows that it is indeed possible that $r(\\mathbf{A})\u0026gt;r$. Let\n\\begin{equation} \\mathbf{A}=\\left(\\begin{array}{ll} 1 \u0026amp; -1 \\\\ 1 \u0026amp; -1 \\end{array}\\right) \\end{equation}\nThen $r(\\mathbf{A})=1$ and both eigenvalues of $\\mathbf{A}$ are zero.\nTheorem 0.19 (Simple Eigenvalue Rank)\nLet $\\mathbf{A}$ be an $n \\times n$ matrix. If $\\lambda$ is a simple eigenvalue of $\\mathbf{A}$, then $r(\\lambda I-\\mathbf{A})=n-1$. Conversely, if $r(\\lambda I-\\mathbf{A})=n-1$, then $\\lambda$ is an eigenvalue of $\\mathbf{A}$, but not necessarily a simple eigenvalue. Proof\nLet $\\lambda_{1}, \\ldots, \\lambda_{n}$ be the eigenvalues of $\\mathbf{A}$. Then $\\mathbf{B}=\\lambda I-\\mathbf{A}$ has eigenvalues $\\lambda-\\lambda_{i}(i=1, \\ldots, n)$, and since $\\lambda$ is a simple eigenvalue of $\\mathbf{A}$, $\\mathbf{B}$ has a simple eigenvalue zero. Hence, $r(\\mathbf{B}) \\leq n-1$. Also, since $\\mathbf{B}$ has $n-1$ nonzero eigenvalues, $r(\\mathbf{B}) \\geq n-1$ (Theorem 0.18). Hence $r(\\mathbf{B})=n-1$. Conversely, if $r(\\mathbf{B})=n-1$, then $\\mathbf{B}$ has at least one zero eigenvalue and hence $\\lambda=\\lambda_{i}$ for at least one $i$. ■ Definition 0.29 (Simple Zero Eigenvalue Corollary)\nAn $n \\times n$ matrix with a simple zero eigenvalue has rank $n-1$. Theorem 0.20 (Symmetric Matrix Rank and Eigenvalues)\nIf $\\mathbf{A}$ is a symmetric matrix with $r$ nonzero eigenvalues, then $r(\\mathbf{A})=r$. Proof\nUsing Theorem 0.13, we have $\\mathbf{S}^\\mathrm{T} \\mathbf{A} \\mathbf{S}=\\boldsymbol{\\Lambda}$ and hence\n\\begin{equation} r(\\mathbf{A})=r\\left(\\mathbf{S} \\boldsymbol{\\Lambda} \\mathbf{S}^\\mathrm{T}\\right)=r(\\boldsymbol{\\Lambda})=r, \\end{equation}\nand the result follows.\n■ Theorem 0.21 (Idempotent Matrix Properties)\nIf $\\mathbf{A}$ is an idempotent matrix, possibly complex, with $r$ eigenvalues equal to one, then $r(\\mathbf{A})=\\operatorname{tr} \\mathbf{A}=r$. Proof\nBy this theorem, $\\mathbf{S}^{*} \\mathbf{A} \\mathbf{S}=\\mathbf{M}$ (upper triangular), where\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) \\end{equation}\nwith $\\mathbf{M}_{1}$ a unit upper triangular $r \\times r$ matrix and $\\mathbf{M}_{3}$ a strictly upper triangular matrix. Since $\\mathbf{A}$ is idempotent, so is $\\mathbf{M}$ and hence\n\\begin{equation} \\left(\\begin{array}{cc} \\mathbf{M}_{1}^{2} \u0026amp; \\mathbf{M}_{1} \\mathbf{M}_{2}+\\mathbf{M}_{2} \\mathbf{M}_{3} \\\\ 0 \u0026amp; \\mathbf{M}_{3}^{2} \\end{array}\\right)=\\left(\\begin{array}{cc} \\mathbf{M}_{1} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; \\mathbf{M}_{3} \\end{array}\\right) . \\end{equation}\nThis implies that $\\mathbf{M}_{1}$ is idempotent; it is nonsingular, hence $\\mathbf{M}_{1}=\\mathbf{I}_{r}$ (see Exercise 1 below). Also, $\\mathbf{M}_{3}$ is idempotent and all its eigenvalues are zero, hence $\\mathbf{M}_{3}=0$ (see Exercise 2 below), so that\n\\begin{equation} \\mathbf{M}=\\left(\\begin{array}{cc} \\mathbf{I}_{r} \u0026amp; \\mathbf{M}_{2} \\\\ 0 \u0026amp; 0 \\end{array}\\right) \\end{equation}\nHence,\n\\begin{equation} r(\\mathbf{A})=r(\\mathbf{M})=r \\end{equation}\nAlso, by:\n\\begin{equation} \\operatorname{tr} \\mathbf{A}=\\text { sum of eigenvalues of } \\mathbf{A}=r, \\end{equation}\nthus completing the proof.\n■ We note that in, the matrix $\\mathbf{A}$ is not required to be symmetric. If $\\mathbf{A}$ is idempotent and symmetric, then it is positive semidefinite. Since its eigenvalues are only 0 and 1 and its rank equals $r$, it that $\\mathbf{A}$ can be written as\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{P}^{\\mathrm{T}} \\mathbf{P}=\\mathbf{I}_{r} \\end{equation}\nExercises\nThe only nonsingular idempotent matrix is the identity matrix. The only idempotent matrix whose eigenvalues are all zero is the null matrix. If $\\mathbf{A}$ is a positive semidefinite $n \\times n$ matrix with $r(\\mathbf{A})=r$, then there exists an $n \\times r$ matrix $\\mathbf{P}$ such that \\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{P}^{\\mathrm{T}} \\mathbf{P}=\\mathbf{\\Lambda} \\end{equation}\nwhere $\\mathbf{\\Lambda}$ is an $r \\times r$ diagonal matrix containing the positive eigenvalues of $\\mathbf{A}$.\nPositive (semi)definite matrices # Positive (semi)definite matrices were introduced in Section 1.6. We have already seen that $\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}$ and $\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}$ are both positive semidefinite and that the eigenvalues of a positive (semi)definite matrix are all positive (nonnegative). We now present some more properties of positive (semi)definite matrices.\nTheorem 0.22 (Determinant inequality for positive definite matrices)\nLet $\\mathbf{A}$ be positive definite and $\\mathbf{B}$ positive semidefinite. Then,\n\\begin{equation} |\\mathbf{A}+\\mathbf{B}| \\geq|\\mathbf{A}| \\end{equation}\nwith equality if and only if $\\mathbf{B}=0$.\nProof\nLet $\\mathbf{\\Lambda}$ be a positive definite diagonal matrix such that\n\\begin{equation} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A} \\mathbf{S}=\\mathbf{\\Lambda}, \\quad \\mathbf{S}^{\\mathrm{T}} \\mathbf{S}=\\mathbf{I} . \\end{equation}\nThen, $\\mathbf{S} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{I}$ and\n\\begin{equation} \\mathbf{A}+\\mathbf{B}=\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2}\\left(\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right) \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}} . \\end{equation}\nHence, using determinant results,\n\\begin{equation} \\begin{aligned} |\\mathbf{A}+\\mathbf{B}| \u0026amp; =\\left|\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2}\\right|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right|\\left|\\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right| \\\\ \u0026amp; =\\left|\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right| \\\\ \u0026amp; =|\\mathbf{A}|\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right| \\end{aligned} \\end{equation}\nIf $\\mathbf{B}=0$ then $|\\mathbf{A}+\\mathbf{B}|=|\\mathbf{A}|$. If $\\mathbf{B} \\neq 0$, then the matrix $\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}$ will be positive semidefinite with at least one positive eigenvalue. Hence we have $\\left|\\mathbf{I}+\\mathbf{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{B} \\mathbf{S} \\mathbf{\\Lambda}^{-1 / 2}\\right|\u0026gt;1$ and $|\\mathbf{A}+\\mathbf{B}|\u0026gt;|\\mathbf{A}|$.\n■ Theorem 0.23 (Simultaneous diagonalization)\nLet $\\mathbf{A}$ be positive definite and $\\mathbf{B}$ symmetric of the same order. Then there exist a nonsingular matrix $\\mathbf{P}$ and a diagonal matrix $\\mathbf{\\Lambda}$ such that\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}=\\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}} \\end{equation}\nIf $\\mathbf{B}$ is positive semidefinite, then so is $\\mathbf{\\Lambda}$.\nProof\nLet $\\mathbf{C}=\\mathbf{A}^{-1 / 2} \\mathbf{B} \\mathbf{A}^{-1 / 2}$. Since $\\mathbf{C}$ is symmetric, there exist an orthogonal matrix $\\mathbf{S}$ and a diagonal matrix $\\mathbf{\\Lambda}$ such that\n\\begin{equation} \\mathbf{S}^{\\mathrm{T}} \\mathbf{C} \\mathbf{S}=\\mathbf{\\Lambda}, \\quad \\mathbf{S}^{\\mathrm{T}} \\mathbf{S}=\\mathbf{I} \\end{equation}\nNow define $\\mathbf{P}=\\mathbf{A}^{1 / 2} \\mathbf{S}$. Then,\n\\begin{equation} \\mathbf{P} \\mathbf{P}^{\\mathrm{T}}=\\mathbf{A}^{1 / 2} \\mathbf{S} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{A} \\end{equation}\nand\n\\begin{equation} \\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}}=\\mathbf{A}^{1 / 2} \\mathbf{S} \\mathbf{\\Lambda} \\mathbf{S}^{\\mathrm{T}} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{C} \\mathbf{A}^{1 / 2}=\\mathbf{A}^{1 / 2} \\mathbf{A}^{-1 / 2} \\mathbf{B} \\mathbf{A}^{-1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{B} . \\end{equation}\nIf $\\mathbf{B}$ is positive semidefinite, then so is $\\mathbf{C}$ and so is $\\mathbf{\\Lambda}$.\n■ For two symmetric matrices $\\mathbf{A}$ and $\\mathbf{B}$, we shall write $\\mathbf{A} \\geq \\mathbf{B}$ (or $\\mathbf{B} \\leq \\mathbf{A}$ ) if $\\mathbf{A}-\\mathbf{B}$ is positive semidefinite, and $\\mathbf{A}\u0026gt;\\mathbf{B}$ (or $\\mathbf{B}\u0026lt;\\mathbf{A}$ ) if $\\mathbf{A}-\\mathbf{B}$ is positive definite.\nTheorem 0.24 (Inverse order for positive definite matrices)\nLet $\\mathbf{A}$ and $\\mathbf{B}$ be positive definite $n \\times n$ matrices. Then $\\mathbf{A}\u0026gt;\\mathbf{B}$ if and only if $\\mathbf{B}^{-1}\u0026gt;\\mathbf{A}^{-1}$. Proof\nBy Theorem 0.23, there exist a nonsingular matrix $\\mathbf{P}$ and a positive definite diagonal matrix $\\mathbf{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{n}\\right)$ such that\n\\begin{equation} \\mathbf{A}=\\mathbf{P} \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}=\\mathbf{P} \\mathbf{\\Lambda} \\mathbf{P}^{\\mathrm{T}} \\end{equation}\nThen,\n\\begin{equation} \\mathbf{A}-\\mathbf{B}=\\mathbf{P}(\\mathbf{I}-\\mathbf{\\Lambda}) \\mathbf{P}^{\\mathrm{T}}, \\quad \\mathbf{B}^{-1}-\\mathbf{A}^{-1}=\\mathbf{P}^{\\mathrm{T}-1}\\left(\\mathbf{\\Lambda}^{-1}-\\mathbf{I}\\right) \\mathbf{P}^{-1} . \\end{equation}\nIf $\\mathbf{A}-\\mathbf{B}$ is positive definite, then $\\mathbf{I}-\\mathbf{\\Lambda}$ is positive definite and hence $0\u0026lt;\\lambda_{i}\u0026lt;$ $1(i=1, \\ldots, n)$. This implies that $\\mathbf{\\Lambda}^{-1}-\\mathbf{I}$ is positive definite and hence that $\\mathbf{B}^{-1}-\\mathbf{A}^{-1}$ is positive definite.\n■ Theorem 0.25 (Determinant monotonicity)\nLet $\\mathbf{A}$ and $\\mathbf{B}$ be positive definite matrices such that $\\mathbf{A}-\\mathbf{B}$ is positive semidefinite. Then, $|\\mathbf{A}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{A}=\\mathbf{B}$. Proof\nLet $\\mathbf{C}=\\mathbf{A}-\\mathbf{B}$. Then $\\mathbf{A}=\\mathbf{B}+\\mathbf{C}$, where $\\mathbf{B}$ is positive definite and $\\mathbf{C}$ is positive semidefinite. Thus, by Theorem 0.22, $|\\mathbf{B}+\\mathbf{C}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{C}=0$, that is, $|\\mathbf{A}| \\geq|\\mathbf{B}|$ with equality if and only if $\\mathbf{A}=\\mathbf{B}$. ■ A useful special case of Theorem 0.25 is this theorem.\nTheorem 0.26 (Identity characterization)\nLet $\\mathbf{A}$ be positive definite with $|\\mathbf{A}|=1$. If $\\mathbf{I}-\\mathbf{A}$ is also positive semidefinite, then $\\mathbf{A}=\\mathbf{I}$. Proof\nThis follows immediately from Theorem 0.25. ■ Three further results for positive definite matrices # Let us now prove this theorem.\nTheorem 0.27 (Block matrix determinant and positive definiteness)\nLet $\\mathbf{A}$ be a positive definite $n \\times n$ matrix, and let $\\mathbf{B}$ be the $(n+1) \\times(n+1)$ matrix\n\\begin{equation} \\mathbf{B}=\\left(\\begin{array}{ll} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{b}^{\\mathrm{T}} \u0026amp; \\alpha \\end{array}\\right) \\end{equation}\nThen, (i) $|\\mathbf{B}| \\leq \\alpha|\\mathbf{A}|$ with equality if and only if $\\mathbf{b}=0$; and (ii) $\\mathbf{B}$ is positive definite if and only if $|\\mathbf{B}|\u0026gt;0$.\nProof\nDefine the $(n+1) \\times(n+1)$ matrix\n\\begin{equation} \\mathbf{P}=\\left(\\begin{array}{cc} \\mathbf{I}_{n} \u0026amp; -\\mathbf{A}^{-1} \\mathbf{b} \\\\ \\mathbf{0}^{\\mathrm{T}} \u0026amp; 1 \\end{array}\\right) \\end{equation}\nThen,\n\\begin{equation} \\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}=\\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; 0 \\\\ \\mathbf{0}^{\\mathrm{T}} \u0026amp; \\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{array}\\right) \\end{equation}\nso that\n\\begin{equation} |\\mathbf{B}|=\\left|\\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}\\right|=|\\mathbf{A}|\\left(\\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\\right) . \\label{eq:det_block} \\end{equation}\n(Compare Exercise 2 in Section 1.11.) Then (i) is an immediate consequence of \\eqref{eq:det_block}. To prove (ii) we note that $|\\mathbf{B}|\u0026gt;0$ if and only if $\\alpha-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b}\u0026gt;0$ (from \\eqref{eq:det_block}), which is the case if and only if $\\mathbf{P}^{\\mathrm{T}} \\mathbf{B} \\mathbf{P}$ is positive definite (from the previous equation). This in turn is true if and only if $\\mathbf{B}$ is positive definite.\n■ An immediate consequence of Theorem 0.27, proved by induction, is the following.\nTheorem 0.28 (Hadamard\u0026#39;s inequality)\nIf $\\mathbf{A}=\\left(a_{ij}\\right)$ is a positive definite $n \\times n$ matrix, then\n\\begin{equation} |\\mathbf{A}| \\leq \\prod_{i=1}^{n} a_{ii} \\end{equation}\nwith equality if and only if $\\mathbf{A}$ is diagonal.\nAnother consequence of Theorem 0.27 is this theorem.\nTheorem 0.29 (Principal minor test)\nA symmetric $n \\times n$ matrix $\\mathbf{A}$ is positive definite if and only if all principal minors $\\left|\\mathbf{A}_{k}\\right|(k=1, \\ldots, n)$ are positive. Note. The $k \\times k$ matrix $\\mathbf{A}_{k}$ is obtained from $\\mathbf{A}$ by deleting the last $n-k$ rows and columns of $\\mathbf{A}$. Notice that $\\mathbf{A}_{n}=\\mathbf{A}$.\nProof\nLet $\\mathbf{E}_{k}=\\left(\\mathbf{I}_{k}: 0\\right)$ be a $k \\times n$ matrix, so that $\\mathbf{A}_{k}=\\mathbf{E}_{k} \\mathbf{A} \\mathbf{E}_{k}^{\\mathrm{T}}$. Let $\\mathbf{y}$ be an arbitrary $k \\times 1$ vector, $\\mathbf{y} \\neq 0$. Then,\n\\begin{equation} \\mathbf{y}^{\\mathrm{T}} \\mathbf{A}_{k} \\mathbf{y}=\\left(\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y}\\right)^{\\mathrm{T}} \\mathbf{A}\\left(\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y}\\right)\u0026gt;0 \\end{equation}\nsince $\\mathbf{E}_{k}^{\\mathrm{T}} \\mathbf{y} \\neq 0$ and $\\mathbf{A}$ is positive definite. Hence, $\\mathbf{A}_{k}$ is positive definite and, in particular, $\\left|\\mathbf{A}_{k}\\right|\u0026gt;0$. The converse follows by repeated application of Theorem 0.27(ii).\n■ Exercises\nIf $\\mathbf{A}$ is positive definite show that the matrix \\begin{equation} \\left(\\begin{array}{cc} \\mathbf{A} \u0026amp; \\mathbf{b} \\\\ \\mathbf{b}^{\\mathrm{T}} \u0026amp; \\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{array}\\right) \\end{equation}\nis positive semidefinite and singular, and find the eigenvector associated with the zero eigenvalue.\nHence show that, for positive definite $\\mathbf{A}$, \\begin{equation} \\mathbf{x}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}-2 \\mathbf{b}^{\\mathrm{T}} \\mathbf{x} \\geq-\\mathbf{b}^{\\mathrm{T}} \\mathbf{A}^{-1} \\mathbf{b} \\end{equation}\nfor every $\\mathbf{x}$, with equality if and only if $\\mathbf{x}=\\mathbf{A}^{-1} \\mathbf{b}$.\nA useful result # If $\\mathbf{A}$ is a positive definite $n \\times n$ matrix, then, in accordance with Theorem 0.28,\n\\begin{equation} |\\mathbf{A}|=\\prod_{i=1}^{n} a_{ii} \\label{eq:diagonal_det} \\end{equation}\nif and only if $\\mathbf{A}$ is diagonal. If $\\mathbf{A}$ is merely symmetric, then \\eqref{eq:diagonal_det}, while obviously necessary, is no longer sufficient for the diagonality of $\\mathbf{A}$. For example, the matrix\n\\begin{equation} \\mathbf{A}=\\left(\\begin{array}{lll} 2 \u0026amp; 3 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 3 \u0026amp; 2 \\end{array}\\right) \\end{equation}\nhas determinant $|\\mathbf{A}|=8$ (its eigenvalues are $-1,-1$, and 8 ), thus satisfying \\eqref{eq:diagonal_det}, but $\\mathbf{A}$ is not diagonal.\nthis theorem gives a necessary and sufficient condition for the diagonality of a symmetric matrix.\nTheorem 0.30 (Diagonal matrix characterization)\nA symmetric matrix is diagonal if and only if its eigenvalues and its diagonal elements coincide. Proof\nLet $\\mathbf{A}=\\left(a_{ij}\\right)$ be a symmetric $n \\times n$ matrix. The \u0026lsquo;only if\u0026rsquo; part of the theorem is trivial. To prove the \u0026lsquo;if\u0026rsquo; part, assume that $\\lambda_{i}(\\mathbf{A})=a_{ii}, i=1, \\ldots, n$, and consider the matrix\n\\begin{equation} \\mathbf{B}=\\mathbf{A}+k \\mathbf{I}, \\end{equation}\nwhere $k\u0026gt;0$ is such that $\\mathbf{B}$ is positive definite. Then,\n\\begin{equation} \\lambda_{i}(\\mathbf{B})=\\lambda_{i}(\\mathbf{A})+k=a_{ii}+k=b_{ii} \\quad(i=1, \\ldots, n), \\end{equation}\nand hence\n\\begin{equation} |\\mathbf{B}|=\\prod_{1}^{n} \\lambda_{i}(\\mathbf{B})=\\prod_{i=1}^{n} b_{ii} . \\end{equation}\nIt then follows from Theorem 0.28 that $\\mathbf{B}$ is diagonal, and hence that $\\mathbf{A}$ is diagonal.\n■ Symmetric matrix functions # Let $\\mathbf{A}$ be a square matrix of order $n \\times n$. The $\\operatorname{trace} \\operatorname{tr} \\mathbf{A}$ and the determinant $|\\mathbf{A}|$ are examples of scalar functions of $\\mathbf{A}$. We can also consider matrix functions, for example, the inverse $\\mathbf{A}^{-1}$. The general definition of a matrix function is somewhat complicated, but for symmetric matrices it is easy. So, let us assume that $\\mathbf{A}$ is symmetric.\nWe known from that any symmetric $n \\times n$ matrix $\\mathbf{A}$ can be diagonalized, which means that there exists an orthogonal matrix $\\mathbf{S}$ and a diagonal matrix $\\mathbf{\\Lambda}$ (containing the eigenvalues of $\\mathbf{A}$ ) such that $\\mathbf{S}^{\\mathrm{T}} \\mathbf{A} \\mathbf{S}=\\mathbf{\\Lambda}$. Let $\\lambda_{i}$ denote the $i$ th diagonal element of $\\mathbf{\\Lambda}$ and let $\\phi$ be a function so that $\\phi(\\lambda)$ is defined, for example, $\\phi(\\lambda)=\\sqrt{\\lambda}$ or $1 / \\lambda$ or $\\log \\lambda$ or $e^{\\lambda}$.\nWe now define the matrix function $F$ as\n\\begin{equation} F(\\mathbf{\\Lambda})=\\left(\\begin{array}{cccc} \\phi\\left(\\lambda_{1}\\right) \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; \\phi\\left(\\lambda_{2}\\right) \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\phi\\left(\\lambda_{n}\\right) \\end{array}\\right), \\end{equation}\nand then\n\\begin{equation} F(\\mathbf{A})=\\mathbf{S} F(\\mathbf{\\Lambda}) \\mathbf{S}^{\\mathrm{T}} \\end{equation}\nFor example, if $\\mathbf{A}$ is nonsingular then all $\\lambda_{i}$ are nonzero, and letting $\\phi(\\lambda)=$ $1 / \\lambda$, we have\n\\begin{equation} F(\\mathbf{\\Lambda})=\\mathbf{\\Lambda}^{-1}=\\left(\\begin{array}{cccc} 1 / \\lambda_{1} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\ 0 \u0026amp; 1 / \\lambda_{2} \u0026amp; \\ldots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 1 / \\lambda_{n} \\end{array}\\right) \\end{equation}\nand hence $\\mathbf{A}^{-1}=\\mathbf{S} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}$. To check, we have\n\\begin{equation} \\mathbf{A} \\mathbf{A}^{-1}=\\mathbf{S} \\mathbf{\\Lambda} \\mathbf{S}^{\\mathrm{T}} \\mathbf{S} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\mathbf{\\Lambda} \\mathbf{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{I}_{n} \\end{equation}\nas we should. Similarly, if $\\mathbf{A}$ is positive semidefinite, then $\\mathbf{A}^{1 / 2}=\\mathbf{S} \\mathbf{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}$ and\n$$ \\mathbf{A}^{1 / 2} \\mathbf{A}^{1 / 2}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}} \\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S} \\boldsymbol{\\Lambda} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{A}, $$\nagain as it should be. Also, when $\\mathbf{A}$ is positive definite (hence nonsingular), then $\\mathbf{A}^{-1 / 2}=\\mathbf{S} \\boldsymbol{\\Lambda}^{-1 / 2} \\mathbf{S}^{\\mathrm{T}}$ and\n$$ \\begin{aligned} \\left(\\mathbf{A}^{1 / 2}\\right)^{-1} \u0026amp; =\\left(\\mathbf{S} \\boldsymbol{\\Lambda}^{1 / 2} \\mathbf{S}^{\\mathrm{T}}\\right)^{-1}=\\mathbf{S}\\left(\\boldsymbol{\\Lambda}^{1 / 2}\\right)^{-1} \\mathbf{S}^{\\mathrm{T}}=\\mathbf{S}\\left(\\boldsymbol{\\Lambda}^{-1}\\right)^{1 / 2} \\mathbf{S}^{\\mathrm{T}} \\\\ \u0026amp; =\\left(\\mathbf{S} \\boldsymbol{\\Lambda}^{-1} \\mathbf{S}^{\\mathrm{T}}\\right)^{1 / 2}=\\left(\\mathbf{A}^{-1}\\right)^{1 / 2} \\end{aligned} $$\nso that this expression is unambiguously defined. Symmetric matrix functions are thus always defined through their eigenvalues. For example, the logarithm or exponential of $\\mathbf{A}$ is not the matrix with elements $\\log a_{i j}$ or $e^{a_{i j}}$, but rather a matrix whose eigenvalues are $\\log \\lambda_{i}$ or $e^{\\lambda_{i}}$ and whose eigenvectors are the same as the eigenvectors of $\\mathbf{A}$. This is similar to the definition of a positive definite matrix, which is not a matrix all whose elements are positive, but rather a matrix all whose eigenvalues are positive.\nMiscellaneous exercises # Exercises\nIf $\\mathbf{A}$ and $\\mathbf{B}$ are square matrices such that $\\mathbf{A}\\mathbf{B}=0, \\mathbf{A} \\neq 0, \\mathbf{B} \\neq 0$, then prove that $|\\mathbf{A}|=|\\mathbf{B}|=0$. If $\\mathbf{x}$ and $\\mathbf{y}$ are vectors of the same order, prove that $\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}=\\operatorname{tr} \\mathbf{y} \\mathbf{x}^{\\mathrm{T}}$. Let $$ \\mathbf{A}=\\left(\\begin{array}{ll} \\mathbf{A}_{11} \u0026amp; \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} \u0026amp; \\mathbf{A}_{22} \\end{array}\\right) $$\nShow that\n$$ |\\mathbf{A}|=\\left|\\mathbf{A}_{11}\\right|\\left|\\mathbf{A}_{22}-\\mathbf{A}_{21} \\mathbf{A}_{11}^{-1} \\mathbf{A}_{12}\\right| $$\nif $\\mathbf{A}_{11}$ is nonsingular, and\n$$ |\\mathbf{A}|=\\left|\\mathbf{A}_{22}\\right|\\left|\\mathbf{A}_{11}-\\mathbf{A}_{12} \\mathbf{A}_{22}^{-1} \\mathbf{A}_{21}\\right| $$\nif $\\mathbf{A}_{22}$ is nonsingular. 4. Show that $(\\mathbf{I}-\\mathbf{A}\\mathbf{B})^{-1}=\\mathbf{I}+\\mathbf{A}(\\mathbf{I}-\\mathbf{B}\\mathbf{A})^{-1}\\mathbf{B}$, if the inverses exist. 5. Show that\n$$ (\\alpha \\mathbf{I}-\\mathbf{A})^{-1}-(\\beta \\mathbf{I}-\\mathbf{A})^{-1}=(\\beta-\\alpha)(\\beta \\mathbf{I}-\\mathbf{A})^{-1}(\\alpha \\mathbf{I}-\\mathbf{A})^{-1} . $$\nIf $\\mathbf{A}$ is positive definite, show that $\\mathbf{A}+\\mathbf{A}^{-1}-2 \\mathbf{I}$ is positive semidefinite. For any symmetric matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that $\\mathbf{A}\\mathbf{B}-\\mathbf{B}\\mathbf{A}$ is skewsymmetric. Let $\\mathbf{A}$ and $\\mathbf{B}$ be two $m \\times n$ matrices of rank $r$. If $\\mathbf{A}\\mathbf{A}^{\\mathrm{T}}=\\mathbf{B}\\mathbf{B}^{\\mathrm{T}}$ then $\\mathbf{A}=\\mathbf{B}\\mathbf{Q}$, where $\\mathbf{Q}\\mathbf{Q}^{\\mathrm{T}}$ (and hence $\\mathbf{Q}^{\\mathrm{T}} \\mathbf{Q}$ ) is idempotent of rank $k \\geq r$ (Neudecker and van de Velden 2000). Let $\\mathbf{A}$ be an $m \\times n$ matrix partitioned as $\\mathbf{A}=\\left(\\mathbf{A}_{1}: \\mathbf{A}_{2}\\right)$ and satisfying $\\mathbf{A}_{1}^{\\mathrm{T}} \\mathbf{A}_{2}=0$ and $r\\left(\\mathbf{A}_{1}\\right)+r\\left(\\mathbf{A}_{2}\\right)=m$. Then, for any positive semidefinite matrix $\\mathbf{V}$, we have $$ r(\\mathbf{V})=r\\left(\\mathbf{A}_{1}\\right)+r\\left(\\mathbf{A}_{2}^{\\mathrm{T}} \\mathbf{V} \\mathbf{A}_{2}\\right) \\Longleftrightarrow r(\\mathbf{V})=r\\left(\\mathbf{V}: \\mathbf{A}_{1}\\right) $$\nProve that the eigenvalues $\\lambda_{i}$ of $(\\mathbf{A}+\\mathbf{B})^{-1} \\mathbf{A}$, where $\\mathbf{A}$ is positive semidefinite and $\\mathbf{B}$ is positive definite, satisfy $0 \\leq \\lambda_{i}\u0026lt;1$. Let $\\mathbf{x}$ and $\\mathbf{y}$ be $n \\times 1$ vectors. Prove that $\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}$ has $n-1$ zero eigenvalues and one eigenvalue $\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. Show that $\\left|\\mathbf{I}+\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}\\right|=1+\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. Let $\\mu=1+\\mathbf{x}^{\\mathrm{T}} \\mathbf{y}$. If $\\mu \\neq 0$, show that $\\left(\\mathbf{I}+\\mathbf{x} \\mathbf{y}^{\\mathrm{T}}\\right)^{-1}=\\mathbf{I}-(1 / \\mu) \\mathbf{x} \\mathbf{y}^{\\mathrm{T}}$. Show that $\\left(\\mathbf{I}+\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right)^{-1} \\mathbf{A}=\\mathbf{A}\\left(\\mathbf{I}+\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{-1}$. Show that $\\mathbf{A}\\left(\\mathbf{A}^{\\mathrm{T}} \\mathbf{A}\\right)^{1 / 2}=\\left(\\mathbf{A} \\mathbf{A}^{\\mathrm{T}}\\right)^{1 / 2} \\mathbf{A}$. (Monotonicity of the entropic complexity.) Let $\\mathbf{A}_{n}$ be a positive definite $n \\times n$ matrix and define $$ \\phi(n)=\\frac{n}{2} \\log \\operatorname{tr}\\left(\\mathbf{A}_{n} / n\\right)-\\frac{1}{2} \\log \\left|\\mathbf{A}_{n}\\right| . $$\nLet $\\mathbf{A}_{n+1}$ be a positive definite $(n+1) \\times(n+1)$ matrix such that\n$$ \\mathbf{A}_{n+1}=\\left(\\begin{array}{cc} \\mathbf{A}_{n} \u0026amp; \\mathbf{a}_{n} \\\\ \\mathbf{a}_{n}^{\\mathrm{T}} \u0026amp; \\alpha_{n} \\end{array}\\right) $$\nThen,\n$$ \\phi(n+1) \\geq \\phi(n) $$\nwith equality if and only if\n$$ \\mathbf{a}_{n}=0, \\quad \\alpha_{n}=\\operatorname{tr} \\mathbf{A}_{n} / n $$\n"},{"id":5,"href":"/numerical_optimization/docs/lectures/advanced/proximal_methods/","title":"2. Proximal methods","section":"II - Advanced problems","content":" Proximal methods # We refer the reader to the monograph by Parikh and Boyd (2014) for a comprehensive overview of proximal methods.\n"},{"id":6,"href":"/numerical_optimization/docs/lectures/machine_learning/svm/","title":"2. Support Vector Machine","section":"III - Machine Learning problems","content":" Support Vector Machine # Soon to be added.\n"},{"id":7,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/","title":"2. Unconstrained optimization : basics","section":"I - Fundamentals","content":" Unconstrained optimization - basics # We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve: $$ \\underset{\\mathbf{x}\\in\\mathbb{R}^d}{\\operatorname{argmin}} f(\\mathbf{x}). $$\nLet us try to characterizes the nature of the solutions under this setup.\nWhat is a solution ? # Figure 2.1: Local and global minimum can coexist.\nGenerally, we would be happiest if we found a global minimizer of $f$ , a point where the function attains its least value. A formal definition is :\nDefinition 2.1 (Global minimizer)\nA point $\\mathbf{x}^\\star$ is a global minimizer if $f(\\mathbf{x}^\\star)\\leq f(\\mathbf{x})$, where $\\mathbf{x}$ ranges over all of $\\mathbb{R}^d$ (or at least over the domain of interest to the modeler). The global minimizer can be difficult to find, because our knowledge of $f$ is usually only local. Since our algorithm does not visit many points (we hope!), we usually do not have a good picture of the overall shape of $f$ , and we can never be sure that the function does not take a sharp dip in some region that has not been sampled by the algorithm. Most algorithms are able to find only a local minimizer, which is a point that achieves the smallest value of f in its neighborhood. Formally, we say:\nDefinition 2.2 (Local minimizer)\nA point $\\mathbf{x}^\\star$ is a local minimizer if $\\exists r\u0026gt;0,\\, f(\\mathbf{x}^\\star)\\leq f(\\mathbf{x})$, $\\forall \\mathbf{x}\\in\\mathcal{B}(\\mathbf{x}^\\star, r)$. A point that satisfies this definition is sometimes called a weak local minimizer. Alternatively, when $f(\\mathbf{x}^\\star)\u0026lt;f(\\mathbf{x})$, we say that the minimum is a strict local minimizer.\nTaylor\u0026rsquo;s theorem # From the definitions given above, it might seem that the only way to find out whether a point $\\mathbf{x}^\\star$ is a local minimum is to examine all the points in its immediate vicinity, to make sure that none of them has a smaller function value. When the function $f$ is smooth, however, there are much more efficient and practical ways to identify local minima. In particular, if $f$ is twice continuously differentiable, we may be able to tell that $\\mathbf{x}^\\star$ is a local minimizer (and possibly a strict local minimizer) by examining just the gradient $\\nabla f (\\mathbf{x}^\\star)$ and the Hessian $\\nabla^2 f (\\mathbf{x}^\\star)$. The mathematical tool used to study minimizers of smooth functions is Taylor’s the- orem. Because this theorem is central to our analysis we state it now.\nTheorem 2.1 (Taylor\u0026#39;s theorem)\nSuppose that $f:\\mathbb{R}^d\\mapsto\\mathbb{R}$ is continuously differentiable and that we have $\\mathbf{p}\\in\\mathbb{R}^d $. The we have : \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x}+t\\mathbf{p})^\\mathrm{T}\\mathbf{p}, \\end{equation} for some $t\\in [0,1]$.\nMoreover, if $f$ is twice continuously differentiable, we have : \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\mathrm{T}\\nabla^2 f(\\mathbf{x}+t\\mathbf{p})\\mathbf{p}), \\end{equation} for some $t\\in [0,1]$.\nProof\nSee any calculus book ■ Note that in this formulation, the definition is exact and the $t$ scalar is usually unknown. The interest lies in skeching proofs. In practical matters, we rather use the following approximation: Theorem 2.2 (Taylor\u0026#39;s approximation)\nFirst order approximation: \\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + o(\\lVert\\mathbf{p}\\rVert), \\end{equation}\nSecond-order approximation:\n\\begin{equation} f(\\mathbf{x}+\\mathbf{p}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\mathrm{T}\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\mathrm{T}\\nabla^2 f(\\mathbf{x})\\mathbf{p} + o(\\lVert\\mathbf{p}\\rVert^2), \\end{equation}\nwhere $o(\\lVert\\mathbf{p}\\rVert)$ and $o(\\lVert\\mathbf{p}\\rVert^2)$ represent terms that grow slower than $\\lVert\\mathbf{p}\\rVert$ and $\\lVert\\mathbf{p}\\rVert^2$ respectively as $\\lVert\\mathbf{p}\\rVert \\to 0$.\nSufficient and necessary conditions for local minima # Let us consider a local minimum and see how they can be characterized to later design appropriate solution finding methods. The first well-known result is as follows: Theorem 2.3 (First-order necessary conditions)\nif $\\mathbf{x}^\\star$ is a local minimize, and $f$ is continuously differentiable in a neighborhood of $\\mathbf{x}^\\star$, then $\\nabla f(\\mathbf{x}^\\star) = \\mathbf{0}$. Proof\nSuppose for contradiction that $\\nabla f(\\mathbf{x}^\\star) \\neq 0$, and define vector $\\mathbf{p}=-\\nabla f(\\mathbf{x}^\\star)$ such that by construction $\\mathbf{p}^\\mathrm{T}\\nabla f(\\mathbf{x}^\\star) = - \\lVert f(\\mathbf{x}^\\star) \\rVert^2 \u0026lt; 0$.\nSince $f$ is a continuous function, we can define a scalar $T\u0026gt;0$ such that $\\forall t\\in [0,T[$, we still have: $$ \\mathbf{p}^\\mathrm{T}f(\\mathbf{x}+t\\mathbf{p}) \u0026lt; 0. $$\nUsing Theorem 2.1 first-order result, we can write: $$ f(\\mathbf{x}^\\star+t\\mathbf{p}) = f(\\mathbf{x}^\\star) + t\\mathbf{p}^\\mathrm{T}\\nabla f(\\mathbf{x}^\\star+\\overline{t}\\mathbf{p}), $$ for some $\\overline{t}\\in[0,T[$ and any $t\\in[0,T[$. Given previous inequality, we obtain: $$ f(\\mathbf{x}^\\star+t\\mathbf{p}) \u0026lt; f(\\mathbf{x}^\\star), $$ which contradicts the fact that $\\mathbf{x}^\\star$ is a local minimizer.\n■ Henceforth, we will call stationary point, any $\\mathbf{x}$ such that $\\nabla f(\\mathbf{x}) = 0$.\nFor the next result we recall that a matrix $\\mathbf{B}$ is positive definite if $\\mathbf{p}^\\mathrm{T} \\mathbf{B} \\mathbf{p}\u0026gt;0$ for all $\\mathbf{p} \\neq \\mathbf{0}$, and positive semidefinite if $\\mathbf{p}^\\mathrm{T} \\mathbf{B} \\mathbf{p} \\geq 0$ for all $\\mathbf{p}$.\nTheorem 2.4 (Second-order necessary conditions)\nIf $\\mathbf{x}^\\star$ is a local minimizer of $f$ and $\\nabla^2 f$ is continuous in an open neighborhood of $\\mathbf{x}^\\star$, then $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$ and $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is positive semidefinite. Proof\nProof. We know from Theorem 2.3 that $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$. For contradiction, assume that $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is not positive semidefinite. Then we can choose a vector $\\mathbf{p}$ such that $\\mathbf{p}^T \\nabla^2 f\\left(\\mathbf{x}^\\star\\right) \\mathbf{p}\u0026lt;0$, and because $\\nabla^2 f$ is continuous near $\\mathbf{x}^\\star$, there is a scalar $T\u0026gt;0$ such that $\\mathbf{p}^\\mathrm{T} \\nabla^2 f\\left(\\mathbf{x}^*+\\overline{t} \\mathbf{p}\\right) \\mathbf{p}\u0026lt;0$ for all $t \\in[0, T[$.\nBy doing a Taylor series expansion around $\\mathbf{x}^\\star$, we have for all $\\bar{t} \\in[0, T[$ and some $t \\in[0, \\bar{t}]$ that\n$$ f\\left(\\mathbf{x}^\\star+\\bar{t} \\mathbf{p}\\right) = f\\left(\\mathbf{x}^\\star\\right)+\\bar{t} \\mathbf{p}^\\mathrm{T} \\nabla f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\bar{t}^2 \\mathbf{p}^\\mathrm{T} \\nabla^2 f\\left(\\mathbf{x}^\\star+t \\mathbf{p}\\right) \\mathbf{p}\u0026lt;f\\left(\\mathbf{x}^\\star\\right) . $$\nAs in Theorem 2.3, we have found a direction from $\\mathbf{x}^\\star$ along which $f$ is decreasing, and so again, $\\mathbf{x}^\\star$ is not a local minimizer.\n■ We now describe sufficient conditions, which are conditions on the derivatives of $f$ at the point $\\mathbf{z}^\\star$ that guarantee that $\\mathbf{x}^\\star$ is a local minimizer.\nTheorem 2.5 (Second-Order Sufficient Conditions)\nSuppose that $\\nabla^2 f$ is continuous in an open neighborhood of $\\mathbf{x}^\\star$ and that $\\nabla f\\left(\\mathbf{x}^\\star\\right)=0$ and $\\nabla^2 f\\left(\\mathbf{x}^\\star\\right)$ is positive definite. Then $\\mathbf{x}^\\star$ is a strict local minimizer of $f$. Proof\nBecause the Hessian is continuous and positive definite at $\\mathbf{x}^\\star$, we can choose a radius $r\u0026gt;0$ so that $\\nabla^2 f(x)$ remains positive definite for all $x$ in the open ball $\\mathcal{D}=\\left\\{\\mathbf{z} \\mid\\left\\lVert\\mathbf{z}-\\mathbf{x}^\\star\\right\\rVert\u0026lt;\\right.$ $r\\}$. Taking any nonzero vector $p$ with $\\lVert\\mathbf{p}\\rVert\u0026lt;r$, we have $\\mathbf{x}^\\star+\\mathbf{p} \\in \\mathcal{D}$ and so\n$$ \\begin{aligned} f\\left(\\mathbf{x}^\\star+p\\right) \u0026amp; =f\\left(\\mathbf{x}^\\star\\right)+\\mathbf{p}^\\mathrm{T} \\nabla f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\mathbf{p}^\\mathrm{T} \\nabla^2 f(\\mathbf{z}) \\mathbf{p} \\ \u0026amp; =f\\left(\\mathbf{x}^\\star\\right)+\\frac{1}{2} \\mathbf{p}^\\mathrm{T} \\nabla^2 f(\\mathbf{z}) \\mathbf{p} \\end{aligned} $$\nwhere $\\mathbf{z}=\\mathbf{x}^\\star+t \\mathbf{p}$ for some $t \\in(0,1)$. Since $\\mathbf{z} \\in \\mathcal{D}$, we have $\\mathbf{p}^{\\mathrm{T}} \\nabla^2 f(\\mathbf{z}) \\mathbf{p}\u0026gt;0$, and therefore $f\\left(\\mathbf{x}^\\star+\\mathbf{p}\\right)\u0026gt;f\\left(\\mathbf{x}^\\star\\right)$, giving the result.\n■ Note that the second-order sufficient conditions of Theorem 2.5 guarantee something stronger than the necessary conditions discussed earlier; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: A point $\\mathbf{x}^\\star$ may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function $f(x)=x^4$, for which the point $x^\\star=0$ is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite). These results, which are based on elementary calculus, provide the foundations for unconstrained optimization algorithms. In one way or another, all algorithms seek a point where $\\nabla f(\\cdot)$ vanishes.\nThe need for algorithms # On question we might ask here is why do we need algorithms to find local minima? After all, we have just shown that if $\\nabla f(\\mathbf{x}^\\star)=0$, then $\\mathbf{x}^\\star$ is a local minimizer. The answer is that in practice, we do not always have the luxury to know the exact solution to $\\nabla f(\\mathbf{x})=0$. Moreover, we can\u0026rsquo;t always compute the Hessian matrix to check the second-order conditions.\nThus, to circumvent the need to solve analytically the equations $\\nabla f(\\mathbf{x})=0$, we will design algorithms that iteratively update a point $\\mathbf{x}$ until it converges to a local minimizer. The algorithms will be based on the properties of the gradient and Hessian, and will use the information they provide to guide the search for a local minimum. When hessian is not computable or too expensive to compute, we will use the gradient only, and the algorithms will be called gradient-based methods. When the Hessian is available, we will use it to accelerate convergence, and the algorithms will be called Newton methods. As a between between these methods lie quasi-Newton methods, which use an approximation of the Hessian to guide the search for a local minimum.\nBut before we dive in more complicated algorithms, let us consider the most obvious approaches and try to understand their limitations.\nSteepest-descent approach # The first algorithm we consider is the so-called Steepest-descent algorithm which involves choosing an initial point $\\mathbf{x}_0$ and compute a series of subsequent points with the following formula:\n\\begin{equation} \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k), \\label{eq: steepest descent} \\end{equation}\nwhere $\\alpha_k$ are a series of scalar values called step-size (or learning rate in machine learning context).\nThe intuition behind this approach is beautifully simple yet profound. Imagine yourself standing on a mountainside in thick fog, trying to find the bottom of the valley. Since you can\u0026rsquo;t see the overall landscape, the most sensible strategy is to feel the slope beneath your feet and take a step in the direction that descends most steeply. This is precisely what the steepest-descent algorithm does mathematically.\nThe negative gradient $-\\nabla f(\\mathbf{x}_k)$ points in the direction of steepest decrease of the function at point $\\mathbf{x}_k$. This isn\u0026rsquo;t just a convenient mathematical fact—it\u0026rsquo;s the fundamental geometric property that makes gradient-based optimization possible. By moving in this direction, we ensure that we\u0026rsquo;re making the most aggressive local progress toward reducing the function value.\nFigure 2.2: Optimization with steepest descent\nUnderstanding the algorithm step by step # Let\u0026rsquo;s walk through what happens in each iteration of steepest descent. Starting from point $\\mathbf{x}_k$, we compute the gradient $\\nabla f(\\mathbf{x}_k)$. This vector tells us which direction the function increases most rapidly. Since we want to minimize, we go in the opposite direction: $-\\nabla f(\\mathbf{x}_k)$.\nThe step size $\\alpha_k$ determines how far we travel in this direction. Think of it as the length of your stride as you walk down the mountain. The choice of step size involves a fundamental trade-off: too small and you make painfully slow progress; too large and you might overshoot the valley bottom or even start climbing uphill again.\nWhy steepest descent can struggle # Figure 2.3: Problem of stepsize\nHere\u0026rsquo;s where the method reveals its first major limitation. Consider a function that looks like a long, narrow valley—mathematically, this corresponds to a function with a large condition number. Steepest descent exhibits what we call \u0026ldquo;zigzag behavior\u0026rdquo; in such cases as illustrated in Figure 2.3 .\nPicture this scenario: you\u0026rsquo;re in a narrow canyon, and the steepest direction points toward one wall rather than down the canyon. You take steps toward that wall, then the gradient changes direction and points toward the other wall. Instead of walking efficiently down the canyon, you find yourself bouncing back and forth between the walls, making very slow progress toward your destination.\nThis zigzag pattern occurs because steepest descent is fundamentally myopic. At each step, it only considers the immediate local slope and ignores the broader geometric structure of the function. The algorithm doesn\u0026rsquo;t \u0026ldquo;remember\u0026rdquo; where it came from or \u0026ldquo;anticipate\u0026rdquo; where the function is heading.\nConvergence properties # Despite these limitations, steepest descent does have reliable convergence properties. Under reasonable mathematical conditions—essentially requiring that the function is well-behaved and doesn\u0026rsquo;t have any pathological features—the algorithm will eventually reach a stationary point where the gradient vanishes.\nThe convergence is what we call \u0026ldquo;linear,\u0026rdquo; meaning that the error decreases by a constant factor at each iteration. While this sounds reasonable, it can be frustratingly slow in practice, especially for poorly conditioned problems where that constant factor is very close to one.\nNewton method # If steepest descent is like navigating with only your immediate sense of slope, Newton\u0026rsquo;s method is like having a detailed topographic map of your local surroundings. This method incorporates not just information about which way is downhill, but also how the slope itself is changing—what we call the curvature of the function.\nThe mathematical foundation # Newton\u0026rsquo;s method emerges from a clever idea: instead of trying to minimize the original function directly, let\u0026rsquo;s create a simpler approximation and minimize that instead. We use the second-order Taylor approximation around our current point $\\mathbf{x}_k$:\n$$f(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T\\mathbf{p} + \\frac{1}{2}\\mathbf{p}^T\\nabla^2 f(\\mathbf{x}_k)\\mathbf{p}$$\nThis quadratic approximation captures both the slope (first-order term) and the curvature (second-order term) at our current location. The brilliant insight is that quadratic functions are easy to minimize—we simply set the gradient of the approximation equal to zero and solve for the optimal step $\\mathbf{p}$.\nTaking the gradient of our quadratic model and setting it to zero gives us: $$\\nabla f(\\mathbf{x}_k) + \\nabla^2 f(\\mathbf{x}_k)\\mathbf{p} = \\mathbf{0}$$\nSolving for the Newton step: $$\\mathbf{p}_k = -[\\nabla^2 f(\\mathbf{x}_k)]^{-1}\\nabla f(\\mathbf{x}_k)$$\nThe complete Newton iteration becomes: $$\\mathbf{x}_{k+1} = \\mathbf{x}_k - [\\nabla^2 f(\\mathbf{x}_k)]^{-1}\\nabla f(\\mathbf{x}_k)$$\nFigure 2.4: Newton optimization step\nThe geometric insight # What makes Newton\u0026rsquo;s method so powerful becomes clear when we think geometrically. The Hessian matrix $\\nabla^2 f(\\mathbf{x}_k)$ encodes information about how the gradient changes in different directions. If the function curves sharply in one direction and gently in another, the Hessian \u0026ldquo;knows\u0026rdquo; this and adjusts the step accordingly.\nConsider our narrow valley example again. While steepest descent keeps pointing toward the valley walls, Newton\u0026rsquo;s method recognizes the elongated shape of the valley and naturally takes larger steps along the valley floor and smaller steps perpendicular to it. This geometric awareness eliminates the zigzag behavior that plagues steepest descent.\nFor quadratic functions, this geometric understanding leads to a remarkable property: Newton\u0026rsquo;s method finds the exact minimum in a single step, regardless of how poorly conditioned the function might be. This happens because our second-order approximation is exact for quadratic functions.\nThe power of quadratic convergence # Near a solution that satisfies our second-order sufficient conditions, Newton\u0026rsquo;s method exhibits quadratic convergence. This technical term describes an almost magical property: the number of correct digits in your answer roughly doubles with each iteration.\nTo appreciate this, consider what linear convergence means: if you have one correct digit, you need about three more iterations to get two correct digits. But with quadratic convergence, if you have one correct digit, the next iteration gives you two, then four, then eight. The improvement accelerates dramatically as you approach the solution.\nThis rapid convergence makes Newton\u0026rsquo;s method incredibly efficient for high-precision optimization, which is why it forms the backbone of many sophisticated algorithms.\nThe computational cost # Newton\u0026rsquo;s method\u0026rsquo;s power comes with a price. At each iteration, we must compute the Hessian matrix, which requires evaluating all second partial derivatives of our function. For a function of $d$ variables, this means computing and storing $d(d+1)/2$ distinct second derivatives.\nEven more expensive is solving the linear system $\\nabla^2 f(\\mathbf{x}_k)\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ at each iteration. For general matrices, this requires roughly $d^3/3$ arithmetic operations, which becomes prohibitive as the dimension grows.\nWhen Newton\u0026rsquo;s method can fail # Pure Newton\u0026rsquo;s method isn\u0026rsquo;t foolproof. The Hessian matrix might not be positive definite away from the minimum, which means our quadratic model might not have a minimum—it could have a maximum or a saddle point instead. In such cases, the Newton step might point in completely the wrong direction.\nAdditionally, if we start too far from a minimum, the quadratic approximation might be a poor representation of the true function, leading to steps that actually increase the function value.\nLooking ahead: the bridge between methods # The contrasting strengths and weaknesses of steepest descent and Newton\u0026rsquo;s method naturally lead to interesting questions. Can we capture some of Newton\u0026rsquo;s geometric insight without the full computational burden? Can we ensure the global reliability of gradient methods while achieving faster local convergence?\nThese questions motivate more sophisticated approaches. Quasi-Newton methods, which we\u0026rsquo;ll explore in later chapters, build approximations to the Hessian using only gradient information. Methods like BFGS achieve superlinear convergence—faster than linear but not quite quadratic—while requiring much less computation than full Newton steps.\nSimilarly, trust region methods and linesearch strategies, which we\u0026rsquo;ll study later, provide systematic ways to ensure that our algorithms make reliable progress even when our local approximations aren\u0026rsquo;t perfect.\n"},{"id":8,"href":"/numerical_optimization/docs/lectures/reminders/differentiation/","title":"Differentiation","section":"Reminders","content":" Differentiation in Multiple Dimensions # 1 - Introduction # Differentiation provides the mathematical framework for understanding how functions change locally. While single-variable calculus introduces derivatives, most applications require working with functions of multiple variables. This chapter extends differentiation concepts to multivariate and matrix-valued functions, building the tools needed for optimization and analysis in higher dimensions.\n2 - Monovariate Reminders # Derivative of a Function # Definition 0.1 (Derivative)\nThe derivative of a function $f:\\mathbb{R}\\to\\mathbb{R}$ at a point $x_0$ is defined as: $$f\u0026rsquo;(x_0) = \\lim_{h\\to 0} \\frac{f(x_0 + h) - f(x_0)}{h}$$ provided this limit exists. The derivative represents the instantaneous rate of change of the function at a specific point. Geometrically, $f\u0026rsquo;(x_0)$ gives the slope of the tangent line to the curve $y = f(x)$ at the point $(x_0, f(x_0))$. This tangent line provides the best linear approximation to the function near $x_0$.\nFor practical computation, we use two fundamental rules:\nProduct rule: $(uv)\u0026rsquo; = u\u0026rsquo;v + uv'$ Chain rule: $(f(g(x)))\u0026rsquo; = f\u0026rsquo;(g(x))g\u0026rsquo;(x)$ These rules allow us to differentiate complex expressions by breaking them down into simpler components.\n3 - Extension to Multivariate Setup: $f:\\mathbb{R}^d \\to \\mathbb{R}$ # Limits and Continuity # Definition 0.2 (Open Disk)\nAn open disk of radius $\\epsilon \u0026gt; 0$ centered at a point $\\mathbf{x}_0 \\in \\mathbb{R}^d$ is defined as: $$\\mathcal{B}(\\mathbf{x}_0, \\epsilon) = {\\mathbf{x} \\in \\mathbb{R}^d : |\\mathbf{x} - \\mathbf{x}_0|_2 \u0026lt; \\epsilon}$$ Definition 0.3 (Limit)\nThe limit of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ at a point $\\mathbf{x}_0$ is defined as: $$\\lim_{\\mathbf{x} \\to \\mathbf{x}_0} f(\\mathbf{x}) = L$$ if for every $\\epsilon \u0026gt; 0$, there exists $\\delta \u0026gt; 0$ such that whenever $|\\mathbf{x} - \\mathbf{x}_0|_2 \u0026lt; \\delta$, we have $|f(\\mathbf{x}) - L| \u0026lt; \\epsilon$. A function is continuous at a point $\\mathbf{x}_0$ if $\\lim_{\\mathbf{x} \\to \\mathbf{x}_0} f(\\mathbf{x}) = f(\\mathbf{x}_0)$. These definitions generalize the single-variable concepts using the Euclidean norm to measure distances in $\\mathbb{R}^d$.\nDirectional Derivative # Definition 0.4 (Directional Derivative)\nThe directional derivative of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ at a point $\\mathbf{x}_0$ in the direction of a vector $\\mathbf{v} \\in \\mathbb{R}^d$ is defined as: $$Df(\\mathbf{x}_0)[\\mathbf{v}] = \\lim_{h \\to 0} \\frac{f(\\mathbf{x}_0 + h\\mathbf{v}) - f(\\mathbf{x}_0)}{h}$$ When $|\\mathbf{v}|_2 = 1$, the directional derivative $Df(\\mathbf{x}_0)[\\mathbf{v}]$ represents the rate of change of $f$ in the direction of $\\mathbf{v}$ at the point $\\mathbf{x}_0$. This generalizes the concept of derivative to any direction in the input space.\nWe also use the notation $\\nabla_{\\mathbf{v}}f(\\mathbf{x}_0)$ for the directional derivative.\nGradient # Definition 0.5 (Gradient)\nThe gradient of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ at a point $\\mathbf{x}_0$ is defined as the vector of all directional derivatives in the standard basis directions: $$\\nabla f(\\mathbf{x}_0) = \\left( Df(\\mathbf{x}_0)[\\mathbf{e}_1], Df(\\mathbf{x}_0)[\\mathbf{e}_2], \\ldots, Df(\\mathbf{x}_0)[\\mathbf{e}_d] \\right)^\\mathrm{T}$$ where ${\\mathbf{e}_1, \\ldots, \\mathbf{e}_d}$ is the standard basis of $\\mathbb{R}^d$. The gradient points in the direction of steepest ascent of the function $f$ at the point $\\mathbf{x}_0$. It encodes all the first-order information about how the function changes locally.\nFor any vector $\\mathbf{v} \\in \\mathbb{R}^d$, the directional derivative can be expressed as: $$Df(\\mathbf{x}_0)[\\mathbf{v}] = \\nabla f(\\mathbf{x}_0)^\\mathrm{T} \\mathbf{v}$$\nThis shows that the gradient contains all the information needed to compute directional derivatives in any direction.\nGradient and Partial Derivatives # Definition 0.6 (Partial Derivative)\nThe partial derivative of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ with respect to the $i$-th variable is defined as: $$\\frac{\\partial f}{\\partial x_i}(\\mathbf{x}_0) = \\lim_{h \\to 0} \\frac{f(\\mathbf{x}_0 + h\\mathbf{e}_i) - f(\\mathbf{x}_0)}{h}$$ where $\\mathbf{e}_i$ is the $i$-th standard basis vector. The gradient can be expressed in terms of partial derivatives as: $$\\nabla f(\\mathbf{x}_0) = \\left( \\frac{\\partial f}{\\partial x_1}(\\mathbf{x}_0), \\frac{\\partial f}{\\partial x_2}(\\mathbf{x}_0), \\ldots, \\frac{\\partial f}{\\partial x_d}(\\mathbf{x}_0) \\right)^\\mathrm{T}$$\nThis representation makes it clear that the gradient is a vector containing all the partial derivatives of the function at the point $\\mathbf{x}_0$.\nGradient Properties and Practical Computation # When computing gradients in practice, we use the following rules:\nTheorem 0.1 (Product Rule for Gradients)\nLet $g:\\mathbb{R}^d \\to \\mathbb{R}$ and $h:\\mathbb{R}^d \\to \\mathbb{R}$ be two functions. Then the gradient of their product $f(\\mathbf{x}) = g(\\mathbf{x})h(\\mathbf{x})$ is: $$\\nabla f(\\mathbf{x}) = g(\\mathbf{x})\\nabla h(\\mathbf{x}) + h(\\mathbf{x})\\nabla g(\\mathbf{x})$$ Theorem 0.2 (Chain Rule for Gradients)\nFor composition of functions, we have two main cases:\nIf $f=h\\circ g$ where $h:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}^d\\to\\mathbb{R}$, then: $$\\nabla f(\\mathbf{x}) = h\u0026rsquo;(g(\\mathbf{x}))\\nabla g(\\mathbf{x})$$ where $h\u0026rsquo;$ is the derivative of $h$. If $f=h\\circ g$ where $h:\\mathbb{R}^d\\to\\mathbb{R}$ and $g:\\mathbb{R}^{d\u0026rsquo;}\\to\\mathbb{R}^d$, we need the more general chain rule discussed later. Hessian Matrix # Definition 0.7 (Hessian Matrix)\nThe Hessian matrix of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ at a point $\\mathbf{x}_0$ is defined as the square matrix of second-order partial derivatives: $$\\mathbf{H}(\\mathbf{x}_0) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}(\\mathbf{x}_0) \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}(\\mathbf{x}_0) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_d}(\\mathbf{x}_0) \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}(\\mathbf{x}_0) \u0026amp; \\frac{\\partial^2 f}{\\partial x_2^2}(\\mathbf{x}_0) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_d}(\\mathbf{x}_0) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_d \\partial x_1}(\\mathbf{x}_0) \u0026amp; \\frac{\\partial^2 f}{\\partial x_d \\partial x_2}(\\mathbf{x}_0) \u0026amp; \\cdots \u0026amp; \\frac{\\partial^2 f}{\\partial x_d^2}(\\mathbf{x}_0) \\end{pmatrix}$$ The Hessian matrix captures the second-order behavior of the function, providing information about its curvature at the point $\\mathbf{x}_0$.\nExercise 1: Compute the gradient and Hessian matrix of the function $f(x,y) = x^2 + 3xy + y^2$ at the point $(1,2)$.\nExercise 2: Using the chain rule, compute the gradient of $f(\\mathbf{x}) = \\left(\\sum_{i=1}^{d}x_i^2\\right)^{1/2}$.\nHessian Matrix Properties # The Hessian matrix has several important properties:\nSymmetry: If $f$ is twice continuously differentiable, then $\\mathbf{H}(\\mathbf{x}_0) = \\mathbf{H}(\\mathbf{x}_0)^\\mathrm{T}$ because mixed partial derivatives are equal: $\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}$.\nCurvature information: The eigenvalues of the Hessian determine the local curvature:\nAll eigenvalues positive: $f$ is locally convex at $\\mathbf{x}_0$ All eigenvalues negative: $f$ is locally concave at $\\mathbf{x}_0$ Mixed positive and negative eigenvalues: $f$ has a saddle point at $\\mathbf{x}_0$ Exercise 3 (Rosenbrock function): The Rosenbrock function is defined as: $$f(x,y) = (a - x)^2 + b(y - x^2)^2$$ where $a$ and $b$ are constants (commonly $a=1$ and $b=100$).\nCompute the gradient $\\nabla f(x,y)$ and find stationary points. Compute the Hessian matrix $\\mathbf{H}(x,y)$ and analyze local curvature at the stationary points. 4 - Multivariate Case: $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ # Multivariate Functions # Definition 0.8 (Vector-Valued Function)\nA vector-valued function $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ maps a vector $\\mathbf{x} \\in \\mathbb{R}^d$ to a vector $\\mathbf{y} \\in \\mathbb{R}^p$. We can write: $$f(\\mathbf{x}) = \\begin{pmatrix} f_1(\\mathbf{x}) \\\\ f_2(\\mathbf{x}) \\\\ \\vdots \\\\ f_p(\\mathbf{x}) \\end{pmatrix}$$ where each component $f_i:\\mathbb{R}^d \\to \\mathbb{R}$ is a scalar function. Gradient and Jacobian # For scalar-valued functions, we defined the gradient. For vector-valued functions, we need the Jacobian matrix.\nDefinition 0.9 (Jacobian Matrix)\nThe Jacobian matrix of a function $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ is defined as: $$\\mathbf{J}_f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\frac{\\partial f_1}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_1}{\\partial x_d} \\\\ \\frac{\\partial f_2}{\\partial x_1} \u0026amp; \\frac{\\partial f_2}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_2}{\\partial x_d} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_p}{\\partial x_1} \u0026amp; \\frac{\\partial f_p}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_p}{\\partial x_d} \\end{pmatrix} \\in \\mathbb{R}^{p \\times d}$$ The Jacobian matrix generalizes the gradient to vector-valued functions. Each row is the gradient of one component function.\nJacobian and Directional Derivative # The directional derivative of a vector-valued function $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ in the direction of a vector $\\mathbf{v} \\in \\mathbb{R}^d$ is: $$Df(\\mathbf{x})[\\mathbf{v}] = \\mathbf{J}_f(\\mathbf{x})\\mathbf{v} = \\begin{pmatrix} \\nabla f_1(\\mathbf{x})^T \\mathbf{v} \\\\ \\nabla f_2(\\mathbf{x})^T \\mathbf{v} \\\\ \\vdots \\\\ \\nabla f_p(\\mathbf{x})^T \\mathbf{v} \\end{pmatrix} \\in \\mathbb{R}^p$$\nThis shows how the Jacobian matrix encodes all directional derivative information.\nChain Rule for Composition of Functions # Theorem 0.3 (General Chain Rule)\nIf $f:\\mathbb{R}^d \\to \\mathbb{R}^p$ and $g:\\mathbb{R}^m \\to \\mathbb{R}^d$, then the composition $h = f \\circ g : \\mathbb{R}^m \\to \\mathbb{R}^p$ is defined as: $$h(\\mathbf{y}) = f(g(\\mathbf{y}))$$ The Jacobian of $h$ can be computed using the chain rule: $$\\mathbf{J}_h(\\mathbf{y}) = \\mathbf{J}_f(g(\\mathbf{y})) \\mathbf{J}_g(\\mathbf{y})$$ where $\\mathbf{J}_h(\\mathbf{y}) \\in \\mathbb{R}^{p \\times m}$, $\\mathbf{J}_f(g(\\mathbf{y})) \\in \\mathbb{R}^{p \\times d}$, and $\\mathbf{J}_g(\\mathbf{y}) \\in \\mathbb{R}^{d \\times m}$. Chain Rule: Special Cases # Case 1: If $f:\\mathbb{R}^d \\to \\mathbb{R}$ and $g:\\mathbb{R}^m \\to \\mathbb{R}^d$, then for $h = f \\circ g : \\mathbb{R}^m \\to \\mathbb{R}$: $$\\nabla h(\\mathbf{y}) = \\mathbf{J}_g(\\mathbf{y})^T \\nabla f(g(\\mathbf{y}))$$\nCase 2: If $f:\\mathbb{R} \\to \\mathbb{R}$ and $g:\\mathbb{R}^m \\to \\mathbb{R}$, then for $h = f \\circ g : \\mathbb{R}^m \\to \\mathbb{R}$: $$\\nabla h(\\mathbf{y}) = f\u0026rsquo;(g(\\mathbf{y})) \\nabla g(\\mathbf{y})$$\nWorked Examples # Example 1: Given:\n$f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{x}$ where $f: \\mathbb{R}^2 \\to \\mathbb{R}$ $g(\\mathbf{y}) = \\begin{pmatrix} y_1 + y_2 \\\\ y_1 - y_2 \\end{pmatrix}$ where $g: \\mathbb{R}^2 \\to \\mathbb{R}^2$ $h = f \\circ g$ Find $\\nabla h(\\mathbf{y})$ using the chain rule.\nSolution:\nFirst, $\\nabla f(\\mathbf{x}) = 2\\mathbf{x}$ The Jacobian is $\\mathbf{J}_g(\\mathbf{y}) = \\begin{pmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix}$ Applying the chain rule: $$\\nabla h(\\mathbf{y}) = \\mathbf{J}_g(\\mathbf{y})^T \\nabla f(g(\\mathbf{y})) = \\begin{pmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix} \\cdot 2g(\\mathbf{y})$$ $$= 2\\begin{pmatrix} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{pmatrix}\\begin{pmatrix} y_1 + y_2 \\ y_1 - y_2 \\end{pmatrix} = \\begin{pmatrix} 4y_1 \\ 4y_2 \\end{pmatrix}$$ Example 2 (General Quadratic Forms): Given:\n$f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{A}\\mathbf{x} + \\mathbf{b}^T\\mathbf{x}$ where $\\mathbf{A}$ is symmetric $g(\\mathbf{y}) = \\mathbf{C}\\mathbf{y}$ (linear transformation) Find $\\nabla h(\\mathbf{y})$ for $h = f \\circ g$.\nSolution:\n$\\nabla f(\\mathbf{x}) = 2\\mathbf{A}\\mathbf{x} + \\mathbf{b}$ $\\mathbf{J}_g(\\mathbf{y}) = \\mathbf{C}$ Therefore: $$\\nabla h(\\mathbf{y}) = \\mathbf{C}^T [2\\mathbf{A}(\\mathbf{C}\\mathbf{y}) + \\mathbf{b}] = 2\\mathbf{C}^T\\mathbf{A}\\mathbf{C}\\mathbf{y} + \\mathbf{C}^T\\mathbf{b}$$ 5 - Matrix Functions: $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}$ # Fréchet Derivative # Definition 0.10 (Fréchet Differentiability)\nA function $f:\\mathbb{R}^{m \\times n}\\to\\mathbb{R}^{p \\times q}$ is Fréchet differentiable at $\\mathbf{X}$ if there exists a linear mapping $Df(\\mathbf{X}):\\mathbb{R}^{m \\times n}\\to\\mathbb{R}^{p \\times q}$ such that $$\\lim_{|\\mathbf{V}|_F\\to 0} \\frac{|f(\\mathbf{X}+\\mathbf{V}) - f(\\mathbf{X}) - Df(\\mathbf{X})[\\mathbf{V}]|_F}{|\\mathbf{V}|_F} = 0$$ The Fréchet derivative can also be characterized using the Gateaux derivative: $$Df(\\mathbf{X})[\\mathbf{V}] = \\left.\\frac{d}{dt}\\right|_{t=0} f(\\mathbf{X}+t\\mathbf{V}) = \\lim_{t\\to 0} \\frac{f(\\mathbf{X}+t\\mathbf{V}) - f(\\mathbf{X})}{t}$$\nIf this limit is not linear in $\\mathbf{V}$, then $f$ is not Fréchet differentiable.\nOften it is useful to see this derivative as a linear operator such that: $$ \\mathbf{D} f(\\mathbf{X})[\\boldsymbol{\\xi}] = f(\\mathbf{X}+\\mathbf{\\xi}) - f(\\mathbf{X}) + o(\\lVert\\boldsymbol{\\xi}\\rVert)$$\nMatrix-to-Scalar Functions # For a function $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}$, the directional derivative at $\\mathbf{X}$ in direction $\\mathbf{V}$ is: $$Df(\\mathbf{X})[\\mathbf{V}] = \\lim_{h \\to 0} \\frac{f(\\mathbf{X} + h\\mathbf{V}) - f(\\mathbf{X})}{h}$$\nDefinition 0.11 (Matrix Gradient)\nFor $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}$, the gradient $\\nabla f(\\mathbf{X}) \\in \\mathbb{R}^{m \\times n}$ satisfies: $$Df(\\mathbf{X})[\\mathbf{V}] = \\mathrm{Tr}(\\nabla f(\\mathbf{X})^\\mathrm{T} \\mathbf{V})$$ where $\\mathrm{Tr}(\\cdot)$ denotes the trace of a matrix. The gradient can be computed element-wise as: $$\\nabla f(\\mathbf{X}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial X_{11}} \u0026amp; \\frac{\\partial f}{\\partial X_{12}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial X_{1n}} \\\\ \\frac{\\partial f}{\\partial X_{21}} \u0026amp; \\frac{\\partial f}{\\partial X_{22}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial X_{2n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f}{\\partial X_{m1}} \u0026amp; \\frac{\\partial f}{\\partial X_{m2}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial X_{mn}} \\end{pmatrix}$$\nExamples of Matrix-to-Scalar Functions # Example 1: $f(\\mathbf{X}) = |\\mathbf{X}|_F^2 = \\mathrm{Tr}(\\mathbf{X}^\\mathrm{T}\\mathbf{X})$\nUsing the Gateaux derivative: $$Df(\\mathbf{X})[\\mathbf{V}] = \\left.\\frac{d}{dt}\\right|_{t=0} \\mathrm{Tr}((\\mathbf{X}+t\\mathbf{V})^\\mathrm{T}(\\mathbf{X}+t\\mathbf{V}))$$\nExpanding and differentiating: $$= \\left.\\frac{d}{dt}\\right|_{t=0} [\\mathrm{Tr}(\\mathbf{X}^\\mathrm{T}\\mathbf{X}) + 2t\\mathrm{Tr}(\\mathbf{X}^\\mathrm{T}\\mathbf{V}) + t^2\\mathrm{Tr}(\\mathbf{V}^\\mathrm{T}\\mathbf{V})]$$ $$= 2\\mathrm{Tr}(\\mathbf{X}^\\mathrm{T}\\mathbf{V})$$\nTherefore: $\\nabla f(\\mathbf{X}) = 2\\mathbf{X}$\nExample 2: $f(\\mathbf{X}) = \\log\\det(\\mathbf{X})$ (for invertible $\\mathbf{X}$)\nFor this function: $$Df(\\mathbf{X})[\\mathbf{V}] = \\left.\\frac{d}{dt}\\right|_{t=0} \\log\\det(\\mathbf{X}+t\\mathbf{V}) = \\mathrm{Tr}(\\mathbf{X}^{-1}\\mathbf{V})$$\nTherefore: $\\nabla f(\\mathbf{X}) = \\mathbf{X}^{-\\mathrm{T}}$\n6 - Matrix Functions: $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{p \\times q}$ # Matrix-to-Matrix Functions # For a function $f:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{p \\times q}$, the directional derivative $Df(\\mathbf{X})[\\mathbf{V}]$ is a linear mapping from $\\mathbb{R}^{m \\times n}$ to $\\mathbb{R}^{p \\times q}$.\nSince $Df(\\mathbf{X})$ is linear, there exists a matrix $\\mathbf{M}_{\\mathbf{X}} \\in \\mathbb{R}^{pq \\times mn}$ such that: $$\\mathrm{vec}(Df(\\mathbf{X})[\\mathbf{V}]) = \\mathbf{M}_{\\mathbf{X}} \\mathrm{vec}(\\mathbf{V})$$ where $\\mathrm{vec}(\\cdot)$ stacks matrix columns into a vector.\nThis representation transforms the problem of computing matrix derivatives into standard matrix-vector multiplication. The matrix $\\mathbf{M}_{\\mathbf{X}}$ is sometimes called the derivative matrix or Jacobian matrix of the vectorized function.\nThe power of this representation becomes clear when combined with the Kronecker product identity:\nTheorem 0.4 (Kronecker Product Identity)\nFor matrices $\\mathbf{A} \\in \\mathbb{R}^{p \\times m}$, $\\mathbf{B} \\in \\mathbb{R}^{n \\times q}$, and $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$: $$\\mathrm{vec}(\\mathbf{A}\\mathbf{X}\\mathbf{B}) = (\\mathbf{B}^\\mathrm{T} \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{X})$$ Example: Consider $f(\\mathbf{X}) = \\mathbf{A}\\mathbf{X}\\mathbf{B}$ where $\\mathbf{A} \\in \\mathbb{R}^{p \\times m}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times q}$ are fixed matrices.\nTo find the derivative, we compute: $$Df(\\mathbf{X})[\\mathbf{V}] = f(\\mathbf{X} + \\mathbf{V}) - f(\\mathbf{X}) = \\mathbf{A}(\\mathbf{X} + \\mathbf{V})\\mathbf{B} - \\mathbf{A}\\mathbf{X}\\mathbf{B} = \\mathbf{A}\\mathbf{V}\\mathbf{B}$$\nUsing the Kronecker product identity: $$\\mathrm{vec}(Df(\\mathbf{X})[\\mathbf{V}]) = \\mathrm{vec}(\\mathbf{A}\\mathbf{V}\\mathbf{B}) = (\\mathbf{B}^\\mathrm{T} \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{V})$$\nTherefore, $\\mathbf{M}_{\\mathbf{X}} = \\mathbf{B}^\\mathrm{T} \\otimes \\mathbf{A}$, which is independent of $\\mathbf{X}$ since $f$ is linear.\nVectorization Identities # Key identities for working with matrix derivatives:\n$\\mathrm{vec}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) = (\\mathbf{C}^\\mathrm{T} \\otimes \\mathbf{A}) \\mathrm{vec}(\\mathbf{B})$ $\\mathrm{Tr}(\\mathbf{A}\\mathbf{B}) = \\mathrm{vec}(\\mathbf{A})^\\mathrm{T}\\mathrm{vec}(\\mathbf{B})$ $\\mathrm{Tr}(\\mathbf{A}^\\mathrm{T}\\mathbf{B}) = \\mathrm{vec}(\\mathbf{A})^\\mathrm{T}\\mathrm{vec}(\\mathbf{B})$ where $\\otimes$ denotes the Kronecker product.\nExamples of Matrix-to-Matrix Functions # Example 1: $f(\\mathbf{X}) = \\mathbf{X}^2$\nUsing the Gateaux derivative: $$Df(\\mathbf{X})[\\mathbf{V}] = \\left.\\frac{d}{dt}\\right|_{t=0} (\\mathbf{X}+t\\mathbf{V})^2 = \\mathbf{X}\\mathbf{V} + \\mathbf{V}\\mathbf{X}$$\nExample 2: $f(\\mathbf{X}) = \\mathbf{X}^{-1}$ (for invertible $\\mathbf{X}$)\nFrom the identity $\\mathbf{X}\\mathbf{X}^{-1} = \\mathbf{I}$ and differentiating: $$Df(\\mathbf{X})[\\mathbf{V}] = -\\mathbf{X}^{-1}\\mathbf{V}\\mathbf{X}^{-1}$$\nProperties of Matrix Function Derivatives # The derivatives of matrix functions follow familiar rules:\nLinearity: For $f = \\alpha g + \\beta h$: $$Df(\\mathbf{X})[\\mathbf{V}] = \\alpha , Dg(\\mathbf{X})[\\mathbf{V}] + \\beta , Dh(\\mathbf{X})[\\mathbf{V}]$$\nProduct rule: For $f(\\mathbf{X}) = g(\\mathbf{X}) \\cdot h(\\mathbf{X})$: $$Df(\\mathbf{X})[\\mathbf{V}] = Dg(\\mathbf{X})[\\mathbf{V}] \\cdot h(\\mathbf{X}) + g(\\mathbf{X}) \\cdot Dh(\\mathbf{X})[\\mathbf{V}]$$\nChain rule: For $f(\\mathbf{X}) = g(h(\\mathbf{X}))$: $$Df(\\mathbf{X})[\\mathbf{V}] = Dg(h(\\mathbf{X}))[Dh(\\mathbf{X})[\\mathbf{V}]]$$\n"},{"id":9,"href":"/numerical_optimization/docs/lectures/1_introduction/","title":"Introduction","section":"Lectures","content":" Introduction # Notations # Let us start by defining the notation used troughout all the lectures and practical labs.\nBasic Notation # Scalars are represented by italic letters (e.g., $x$, $y$, $\\lambda$). Vectors are denoted by bold lowercase letters (e.g., $\\mathbf{v}$, $\\mathbf{x}$), while matrices are represented by bold uppercase letters (e.g., $\\mathbf{A}$, $\\mathbf{B}$). The dimensionality of a vector $\\mathbf{v} \\in \\mathbb{R}^n$ indicates it contains $n$ elements, and similarly, a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ has $m$ rows and $n$ columns.\nMatrix Operations # The transpose of a matrix $\\mathbf{A}$ is denoted as $\\mathbf{A}^\\mathrm{T}$, which reflects the matrix across its diagonal. The trace of a square matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, written as $\\mathrm{tr}(\\mathbf{A})$, is the sum of its diagonal elements, i.e., $\\mathrm{tr}(\\mathbf{A}) = \\sum_{i=1}^{n} a_{ii}$. The determinant of $\\mathbf{A}$ is represented as $\\det(\\mathbf{A})$ or $|\\mathbf{A}|$. A matrix $\\mathbf{A}$ is invertible if and only if $\\det(\\mathbf{A}) \\neq 0$, and its inverse is denoted as $\\mathbf{A}^{-1}$, satisfying $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix.\nVector Operations # The dot product between two vectors $\\mathbf{a}$ and $\\mathbf{b}$ of the same dimension is written as $\\mathbf{a} \\cdot \\mathbf{b}$ or $\\mathbf{a}^\\mathrm{T}\\mathbf{b}$, resulting in a scalar value.\nThe p-norm of a vector $\\mathbf{v}$ is denoted as $\\lVert\\mathbf{v}\\rVert_p$ and defined as $\\lVert\\mathbf{v}\\rVert_p = \\left(\\sum_{i=1}^{n} |v_i|^p\\right)^{1/p}$ for $p \\geq 1$, with common choices being $p=1$ (Manhattan norm), $p=2$ (Euclidean norm), and $p=\\infty$ (maximum norm, defined as $\\lVert\\mathbf{v}\\rVert_{\\infty} = \\max_i |v_i|$); when the subscript $p$ is omitted, as in $\\lVert\\mathbf{v}\\rVert$, it is conventionally understood to refer to the Euclidean (L2) norm. The Euclidean norm (or length) of a vector $\\mathbf{v}$ is represented as $\\lVert\\mathbf{v}\\rVert$ or $\\lVert\\mathbf{v}\\rVert_2$, defined as $\\lVert\\mathbf{v}\\rVert = \\sqrt{\\mathbf{v}^\\mathrm{T}\\mathbf{v}} = \\sqrt{\\sum_{i=1}^{n} v_i^2}$. A unit vector in the direction of $\\mathbf{v}$ is given by $\\hat{\\mathbf{v}} = \\mathbf{v}/\\lVert\\mathbf{v}\\rVert$, having a norm of 1.\nEigenvalues and Eigenvectors # For a square matrix $\\mathbf{A}$, a scalar $\\lambda$ is an eigenvalue if there exists a non-zero vector $\\mathbf{v}$ such that $\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$. The vector $\\mathbf{v}$ is called an eigenvector corresponding to the eigenvalue $\\lambda$. The characteristic polynomial of $\\mathbf{A}$ is defined as $p(\\lambda) = \\det(\\lambda\\mathbf{I} - \\mathbf{A})$, and its roots are the eigenvalues of $\\mathbf{A}$. The spectrum of $\\mathbf{A}$, denoted by $\\sigma(\\mathbf{A})$, is the set of all eigenvalues of $\\mathbf{A}$.\nMatrix Decompositions # The singular value decomposition (SVD) of a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is expressed as $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\mathrm{T}$, where $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ and $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values of $\\mathbf{A}$. The eigendecomposition of a diagonalizable matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is given by $\\mathbf{A} = \\mathbf{P}\\mathbf{\\Lambda}\\mathbf{P}^{-1}$, where $\\mathbf{P}$ is a matrix whose columns are the eigenvectors of $\\mathbf{A}$, and $\\mathbf{\\Lambda}$ is a diagonal matrix containing the corresponding eigenvalues.\nMultivariate Calculus # The gradient of a scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is denoted as $\\nabla f$ or $\\mathrm{grad}(f)$, resulting in a vector of partial derivatives $\\nabla f = [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}]^\\mathrm{T}$. The Jacobian matrix of a vector-valued function $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is represented as $\\mathbf{J}_\\mathbf{f}$ or $\\nabla \\mathbf{f}^\\mathrm{T}$, where $ (\\mathbf{J}_\\mathbf{f})_{ij} = \\frac{\\partial f_i}{\\partial x_j} $.\nThe Hessian matrix of a twice-differentiable scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is denoted as $\\mathbf{H}_f$ or $\\nabla^2 f$, where $(\\mathbf{H}_f)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$.\nSpecial Matrices and Properties # A symmetric matrix satisfies $\\mathbf{A} = \\mathbf{A}^\\mathrm{T}$, while a skew-symmetric matrix has $\\mathbf{A} = -\\mathbf{A}^\\mathrm{T}$. An orthogonal matrix $\\mathbf{Q}$ satisfies $\\mathbf{Q}^\\mathrm{T}\\mathbf{Q} = \\mathbf{Q}\\mathbf{Q}^\\mathrm{T} = \\mathbf{I}$, meaning its inverse equals its transpose: $\\mathbf{Q}^{-1} = \\mathbf{Q}^\\mathrm{T}$. A matrix $\\mathbf{A}$ is positive definite if $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} \u0026gt; 0$ for all non-zero vectors $\\mathbf{x}$, and positive semidefinite if $\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x} \\geq 0$.\nDerivatives of Matrix Expressions # The derivative of a scalar function with respect to a vector $\\mathbf{x}$ is denoted as $\\frac{\\partial f}{\\partial \\mathbf{x}}$, resulting in a vector of the same dimension as $\\mathbf{x}$. For matrix functions, the derivative with respect to a matrix $\\mathbf{X}$ is written as $\\frac{\\partial f}{\\partial \\mathbf{X}}$, producing a matrix of the same dimensions as $\\mathbf{X}$. Common matrix derivatives include $\\frac{\\partial}{\\partial \\mathbf{X}}\\mathrm{tr}(\\mathbf{AX}) = \\mathbf{A}^\\mathrm{T}$ and $\\frac{\\partial}{\\partial \\mathbf{x}}(\\mathbf{x}^\\mathrm{T}\\mathbf{A}\\mathbf{x}) = \\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathrm{T}\\mathbf{x}$ (with $\\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathrm{T}\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}$ when $\\mathbf{A}$ is symmetric).\n"},{"id":10,"href":"/numerical_optimization/docs/lectures/fundamentals/convexity/","title":"3. Convexity theory","section":"I - Fundamentals","content":" Convexity theory # Convexity is a powerful property of functions and sets that simplifies the analysis of optimization problems and the characterization of global minimizers. In this chapter, we will explore the concepts of convex sets, convex functions, and their implications for unconstrained optimization.\nConvex sets # Let us first start by defining the convexity of a given set $\\mathcal{S}\\subset\\mathbb{R}^d$:\nDefinition 3.1 (Convex set)\nLet $\\mathcal{S}\\subset\\mathbb{R}^d$ be a set. The set $\\mathcal{S}$ is convex if, for any two points $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{S}$, the line segment that connects them is also contained in $\\mathcal{S}$, that is, \\begin{equation} \\mathbf{x}, \\mathbf{y} \\in \\mathcal{S} \\implies \\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y} \\in \\mathcal{S}, \\quad \\forall \\lambda \\in [0, 1]. \\label{eq:convex_set} \\end{equation} Figure 3.1: Convex set\nThis is illustrated by Figure 3.1 , where the set $\\mathcal{S}$ is convex, as the line segment between any two points $\\mathbf{x}$ and $\\mathbf{y}$ lies entirely within $\\mathcal{S}$. If this property does not hold, then the set is called non-convex.\nWhile we will not do deeper now, this property is desirable for the constraints of an optimization problem, .because it means that for a given algorithm, any subsequent step is feasible by staying true to the given constraints for the problem.\nConvex functions # A function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is convex if its domain is a convex set and it satisfies the following property:\nDefinition 3.2 (Convex function)\nA function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is convex if, for any two points $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ and for all $\\lambda \\in [0, 1]$, the following inequality holds: \\begin{equation} f(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y}). \\label{eq:convex_function} \\end{equation} Figure 3.2: Convex function\nThis means that the line segment connecting the points $(\\mathbf{x}, f(\\mathbf{x}))$ and $(\\mathbf{y}, f(\\mathbf{y}))$ lies above the graph of the function $f$. In other words, the function is \u0026ldquo;bowl-shaped\u0026rdquo; or \u0026ldquo;curves upwards\u0026rdquo;. Such an illustration is given for a 1-dimensional function in Figure 3.2 , where the function $f$ is convex, as the line segment between any two points $(\\mathbf{x}, f(\\mathbf{x}))$ and $(\\mathbf{y}, f(\\mathbf{y}))$ lies above the graph of $f$.\nIn practice to show that a function is convex, we can make use of the following properties, given convex functions $f$ and $g$:\nlet $\\alpha, \\beta\u0026gt;0$, then $\\alpha f + \\beta g$ is convex $f \\circ g$ is convex Convexity and unconstrained optimization # When the objective function is convex, local and global minimizers are simple to characterize.\nTheorem 3.1 When $f$ is convex, any local minimizer $\\mathbf{x}^\\star$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $\\mathbf{x}^\\star$ is a global minimizer of $f$. Proof\nSuppose that $\\mathbf{x}^\\star$ is a local but not a global minimizer. Then we can find a point $\\mathbf{z} \\in \\mathbb{R}^n$ with $f(\\mathbf{z})\u0026lt;f\\left(\\mathbf{x}^\\star\\right)$. Consider the line segment that joins $\\mathbf{x}^\\star$ to $\\mathbf{z}$, that is, \\begin{equation} \\mathbf{x}=\\lambda \\mathbf{z}+(1-\\lambda) \\mathbf{x}^\\star, \\quad \\text { for some } \\lambda \\in(0,1] \\label{eq:line_segment} \\end{equation} By the convexity property for $f$, we have \\begin{equation} f(\\mathbf{x}) \\leq \\lambda f(\\mathbf{z})+(1-\\lambda) f\\left(\\mathbf{x}^\\star\\right)\u0026lt;f\\left(\\mathbf{x}^\\star\\right) \\label{eq:convexity} \\end{equation}\nAny neighborhood $\\mathcal{N}$ of $\\mathbf{x}^\\star$ contains a piece of the line segment \\eqref{eq:line_segment}, so there will always be points $\\mathbf{x} \\in \\mathcal{N}$ at which \\eqref{eq:convexity} is satisfied. Hence, $\\mathbf{x}^\\star$ is not a local minimizer. For the second part of the theorem, suppose that $\\mathbf{x}^\\star$ is not a global minimizer and choose $\\mathbf{z}$ as above. Then, from convexity, we have\n\\begin{equation} \\begin{aligned} \\nabla f\\left(\\mathbf{x}^\\star\\right)^{\\mathrm{T}}\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right) \u0026amp; =\\left.\\frac{d}{d \\lambda} f\\left(\\mathbf{x}^\\star+\\lambda\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right)\\right)\\right|_{\\lambda=0} \\\\ \u0026amp; =\\lim _{\\lambda \\downarrow 0} \\frac{f\\left(\\mathbf{x}^\\star+\\lambda\\left(\\mathbf{z}-\\mathbf{x}^\\star\\right)\\right)-f\\left(\\mathbf{x}^\\star\\right)}{\\lambda} \\\\ \u0026amp; \\leq \\lim _{\\lambda \\downarrow 0} \\frac{\\lambda f(\\mathbf{z})+(1-\\lambda) f\\left(\\mathbf{x}^\\star\\right)-f\\left(\\mathbf{x}^\\star\\right)}{\\lambda} \\\\ \u0026amp; =f(\\mathbf{z})-f\\left(\\mathbf{x}^\\star\\right)\u0026lt;0 \\end{aligned} \\end{equation}\nTherefore, $\\nabla f\\left(\\mathbf{x}^\\star\\right) \\neq 0$, and so $\\mathbf{x}^\\star$ is not a stationary point.\n■ This result is fundamental in optimization, as it guarantees that if we find a local minimizer of a convex function, we can be sure that it is also the global minimizer. This property greatly simplifies the search for optimal solutions. As such, finding that the function we minimize is convex often means that the problem is easier to solve, as we can use algorithms that are guaranteed to converge to the global minimum.\nConversely, in the design stage, we might prefer to design a convex function, or try to find a convex approximation of a non-convex function, to ensure that the optimization problem is well-behaved and that we can find the global minimum efficiently.\n"},{"id":11,"href":"/numerical_optimization/docs/lectures/machine_learning/neural_networks/","title":"3. Neural Networks","section":"III - Machine Learning problems","content":" Neural Networks # Soon to be added.\n"},{"id":12,"href":"/numerical_optimization/docs/lectures/machine_learning/modern/","title":"4. Modern trends","section":"III - Machine Learning problems","content":" Modern trends # Soon to be added.\n"},{"id":13,"href":"/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/","title":"4. Unconstrained optimization : linesearch","section":"I - Fundamentals","content":" Unconstrained optimization - Linesearch methods # Note : This is in part the content of the book \u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, with some modifications to the notations used in this lecture.\nAll algorithms for unconstrained minimization require the user to supply a starting point, which we usually denote by $\\mathbf{x}_0$. The user with knowledge about the application and the data set may be in a good position to choose $\\mathbf{x}_0$ to be a reasonable estimate of the solution. Otherwise, the starting point must be chosen in some arbitrary manner.\nBeginning at $\\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\\left\\{\\mathbf{x}_k\\right\\}_{k=0}^{\\infty}$ that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy. In deciding how to move from one iterate $\\mathbf{x}_k$ to the next, the algorithms use information about the function $f$ at $\\mathbf{x}_k$, and possibly also information from earlier iterates $\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{k-1}$. They use this information to find a new iterate $\\mathbf{x}_{k+1}$ with a lower function value than $\\mathbf{x}_k$. (There exist nonmonotone algorithms that do not insist on a decrease in $f$ at every step, but even these algorithms require $f$ to be decreased after some prescribed number $m$ of iterations. That is, they enforce $f\\left(\\mathbf{x}_k\\right)\u0026lt;f\\left(\\mathbf{x}_{k-m}\\right)$.)\nThere are two fundamental strategies for moving from the current point $\\mathbf{x}_k$ to a new iterate $\\mathbf{x}_{k+1}$. Most of the algorithms described in this book follow one of these approaches.\nTwo strategies: line search and trust region # In the line search strategy, the algorithm chooses a direction $\\mathbf{p}_k$ and searches along this direction from the current iterate $\\mathbf{x}_k$ for a new iterate with a lower function value. The distance to move along $\\mathbf{p}_k$ can be found by approximately solving the following one-dimensional minimization problem to find a step length $\\alpha$ :\n\\begin{equation} \\min _{\\alpha\u0026gt;0} f\\left(\\mathbf{x}_k+\\alpha \\mathbf{p}_k\\right) \\label{eq:line_search_min} \\end{equation}\nBy solving \\eqref{eq:line_search_min} exactly, we would derive the maximum benefit from the direction $\\mathbf{p}_k$, but an exact minimization is expensive and unnecessary. Instead, the line search algorithm generates a limited number of trial step lengths until it finds one that loosely approximates the minimum of \\eqref{eq:line_search_min}. At the new point a new search direction and step length are computed, and the process is repeated.\nIn the second algorithmic strategy, known as trust region, the information gathered about $f$ is used to construct a model function $m_k$ whose behavior near the current point $\\mathbf{x}_k$ is similar to that of the actual objective function $f$. Because the model $m_k$ may not be a good approximation of $f$ when $\\mathbf{x}$ is far from $\\mathbf{x}_k$, we restrict the search for a minimizer of $m_k$ to some region around $\\mathbf{x}_k$. In other words, we find the candidate step $\\mathbf{p}$ by approximately solving the following subproblem:\n\\begin{equation} \\min _{\\mathbf{p}} m_k\\left(\\mathbf{x}_k+\\mathbf{p}\\right), \\quad \\text { where } \\mathbf{x}_k+\\mathbf{p} \\text { lies inside the trust region. } \\label{eq:trust_region_subproblem} \\end{equation}\nIf the candidate solution does not produce a sufficient decrease in $f$, we conclude that the trust region is too large, and we shrink it and re-solve \\eqref{eq:trust_region_subproblem}. Usually, the trust region is a ball defined by $\\lVert\\mathbf{p}\\rVert_2 \\leq \\Delta$, where the scalar $\\Delta\u0026gt;0$ is called the trust-region radius. Elliptical and box-shaped trust regions may also be used.\nThe model $m_k$ in \\eqref{eq:trust_region_subproblem} is usually defined to be a quadratic function of the form\n\\begin{equation} m_k\\left(\\mathbf{x}_k+\\mathbf{p}\\right)=f_k+\\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\mathbf{p}^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{p} \\label{eq:quadratic_model} \\end{equation}\nwhere $f_k$, $\\nabla f_k$, and $\\mathbf{B}_k$ are a scalar, vector, and matrix, respectively. As the notation indicates, $f_k$ and $\\nabla f_k$ are chosen to be the function and gradient values at the point $\\mathbf{x}_k$, so that $m_k$ and $f$ are in agreement to first order at the current iterate $\\mathbf{x}_k$. The matrix $\\mathbf{B}_k$ is either the Hessian $\\nabla^2 f_k$ or some approximation to it.\nSuppose that the objective function is given by $f(\\mathbf{x})=10\\left(x_2-x_1^2\\right)^2+\\left(1-x_1\\right)^2$. At the point $\\mathbf{x}_k=(0,1)$ its gradient and Hessian are\n$$ \\nabla f_k=\\left[\\begin{array}{c} -2 \\\\ 20 \\end{array}\\right], \\quad \\nabla^2 f_k=\\left[\\begin{array}{cc} -38 \u0026amp; 0 \\\\ 0 \u0026amp; 20 \\end{array}\\right] $$\nNote that each time we decrease the size of the trust region after failure of a candidate iterate, the step from $\\mathbf{x}_k$ to the new candidate will be shorter, and it usually points in a different direction from the previous candidate. The trust-region strategy differs in this respect from line search, which stays with a single search direction.\nIn a sense, the line search and trust-region approaches differ in the order in which they choose the direction and distance of the move to the next iterate. Line search starts by fixing the direction $\\mathbf{p}_k$ and then identifying an appropriate distance, namely the step length $\\alpha_k$. In trust region, we first choose a maximum distance-the trust-region radius $\\Delta_k$-and then seek a direction and step that attain the best improvement possible subject to this distance constraint. If this step proves to be unsatisfactory, we reduce the distance measure $\\Delta_k$ and try again.\nThe line search approach is discussed in more detail in this lecture while the trust-region strategy, is left to the reader to study.\nSearch directions for line search methods # The steepest-descent direction $-\\nabla f_k$ is the most obvious choice for search direction for a line search method. It is intuitive; among all the directions we could move from $\\mathbf{x}_k$, it is the one along which $f$ decreases most rapidly. To verify this claim, we appeal again to Taylor\u0026rsquo;s theorem, which tells us that for any search direction $\\mathbf{p}$ and step-length parameter $\\alpha$, we have\n\\begin{equation} f\\left(\\mathbf{x}_k+\\alpha \\mathbf{p}\\right)=f\\left(\\mathbf{x}_k\\right)+\\alpha \\mathbf{p}^{\\mathrm{T}} \\nabla f_k+\\frac{1}{2} \\alpha^2 \\mathbf{p}^{\\mathrm{T}} \\nabla^2 f\\left(\\mathbf{x}_k+t \\mathbf{p}\\right) \\mathbf{p}, \\quad \\text { for some } t \\in(0, \\alpha) \\label{eq:taylor_expansion} \\end{equation}\nThe rate of change in $f$ along the direction $\\mathbf{p}$ at $\\mathbf{x}_k$ is simply the coefficient of $\\alpha$, namely, $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k$. Hence, the unit direction $\\mathbf{p}$ of most rapid decrease is the solution to the problem\n\\begin{equation} \\min _{\\mathbf{p}} \\mathbf{p}^{\\mathrm{T}} \\nabla f_k, \\quad \\text { subject to }\\lVert\\mathbf{p}\\rVert=1 \\label{eq:steepest_descent_problem} \\end{equation}\nSince $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k=\\lVert\\mathbf{p}\\rVert\\lVert\\nabla f_k\\rVert \\cos \\theta$, where $\\theta$ is the angle between $\\mathbf{p}$ and $\\nabla f_k$, we have from $\\lVert\\mathbf{p}\\rVert=1$ that $\\mathbf{p}^{\\mathrm{T}} \\nabla f_k=\\lVert\\nabla f_k\\rVert \\cos \\theta$, so the objective in \\eqref{eq:steepest_descent_problem} is minimized when $\\cos \\theta$ takes on its minimum value of -1 at $\\theta=\\pi$ radians. In other words, the solution to \\eqref{eq:steepest_descent_problem} is\n$$ \\mathbf{p}=-\\nabla f_k /\\lVert\\nabla f_k\\rVert $$\nas claimed. This direction is orthogonal to the contours of the function.\nThe steepest descent method is a line search method that moves along $\\mathbf{p}_k=-\\nabla f_k$ at every step. It can choose the step length $\\alpha_k$ in a variety of ways, as we will see in next chapter. One advantage of the steepest descent direction is that it requires calculation of the gradient $\\nabla f_k$ but not of second derivatives. However, it can be excruciatingly slow on difficult problems.\nLine search methods may use search directions other than the steepest descent direction. In general, any descent direction-one that makes an angle of strictly less than $\\pi / 2$ radians with $-\\nabla f_k$-is guaranteed to produce a decrease in $f$, provided that the step length is sufficiently small. We can verify this claim by using Taylor\u0026rsquo;s theorem. From \\eqref{eq:taylor_expansion}, we have that\n$$ f\\left(\\mathbf{x}_k+\\epsilon \\mathbf{p}_k\\right)=f\\left(\\mathbf{x}_k\\right)+\\epsilon \\mathbf{p}_k^{\\mathrm{T}} \\nabla f_k+O\\left(\\epsilon^2\\right) $$\nWhen $\\mathbf{p}_k$ is a downhill direction, the angle $\\theta_k$ between $\\mathbf{p}_k$ and $\\nabla f_k$ has $\\cos \\theta_k\u0026lt;0$, so that\n$$ \\mathbf{p}_k^{\\mathrm{T}} \\nabla f_k=\\lVert\\mathbf{p}_k\\rVert\\lVert\\nabla f_k\\rVert \\cos \\theta_k\u0026lt;0 $$\nIt follows that $f\\left(\\mathbf{x}_k+\\epsilon \\mathbf{p}_k\\right)\u0026lt;f\\left(\\mathbf{x}_k\\right)$ for all positive but sufficiently small values of $\\epsilon$.\nAll of the search directions discussed so far can be used directly in a line search framework. They give rise to the steepest descent, Newton, quasi-Newton, and conjugate gradient line search methods. For Newton and quasi-Newton methods, see the next chapter.\nStep-length conditions # In computing the step length $\\alpha_{k}$, we face a tradeoff. We would like to choose $\\alpha_{k}$ to give a substantial reduction of $f$, but at the same time, we do not want to spend too much time making the choice. The ideal choice would be the global minimizer of the univariate function $\\phi(\\cdot)$ defined by\n\\begin{equation} \\phi(\\alpha)=f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right), \\quad \\alpha\u0026gt;0 \\label{eq:phi_def} \\end{equation}\nbut in general, it is too expensive to identify this value. To find even a local minimizer of $\\phi$ to moderate precision generally requires too many evaluations of the objective function $f$ and possibly the gradient $\\nabla f$. More practical strategies perform an inexact line search to identify a step length that achieves adequate reductions in $f$ at minimal cost.\nTypical line search algorithms try out a sequence of candidate values for $\\alpha$, stopping to accept one of these values when certain conditions are satisfied. The line search is done in two stages: A bracketing phase finds an interval containing desirable step lengths, and a bisection or interpolation phase computes a good step length within this interval. Sophisticated line search algorithms can be quite complicated, so we defer a full description until the end of this chapter. We now discuss various termination conditions for the line search algorithm and show that effective step lengths need not lie near minimizers of the univariate function $\\phi(\\alpha)$ defined in \\eqref{eq:phi_def}.\nA simple condition we could impose on $\\alpha_{k}$ is that it provide a reduction in $f$, i.e., $f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)\u0026lt;f\\left(\\mathbf{x}_{k}\\right)$. The difficulty is that we do not have sufficient reduction in $f$, a concept we discuss next.\nThe Wolfe conditions # A popular inexact line search condition stipulates that $\\alpha_{k}$ should first of all give sufficient decrease in the objective function $f$, as measured by the following inequality:\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:armijo} \\end{equation}\nfor some constant $c_{1} \\in(0,1)$. In other words, the reduction in $f$ should be proportional to both the step length $\\alpha_{k}$ and the directional derivative $\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$. Inequality \\eqref{eq:armijo} is sometimes called the Armijo condition.\nThe right-hand-side of \\eqref{eq:armijo}, which is a linear function, can be denoted by $l(\\alpha)$. The function $l(\\cdot)$ has negative slope $c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$, but because $c_{1} \\in(0,1)$, it lies above the graph of $\\phi$ for small positive values of $\\alpha$. The sufficient decrease condition states that $\\alpha$ is acceptable only if $\\phi(\\alpha) \\leq l(\\alpha)$. In practice, $c_{1}$ is chosen to be quite small, say $c_{1}=10^{-4}$.\nThe sufficient decrease condition is not enough by itself to ensure that the algorithm makes reasonable progress, because it is satisfied for all sufficiently small values of $\\alpha$. To rule out unacceptably short steps we introduce a second requirement, called the curvature condition, which requires $\\alpha_{k}$ to satisfy\n\\begin{equation} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \\geq c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:curvature} \\end{equation}\nfor some constant $c_{2} \\in\\left(c_{1}, 1\\right)$, where $c_{1}$ is the constant from \\eqref{eq:armijo}. Note that the left-handside is simply the derivative $\\phi^{\\prime}\\left(\\alpha_{k}\\right)$, so the curvature condition ensures that the slope of $\\phi\\left(\\alpha_{k}\\right)$ is greater than $c_{2}$ times the gradient $\\phi^{\\prime}(0)$. This makes sense because if the slope $\\phi^{\\prime}(\\alpha)$ is strongly negative, we have an indication that we can reduce $f$ significantly by moving further along the chosen direction. On the other hand, if the slope is only slightly negative or even positive, it is a sign that we cannot expect much more decrease in $f$ in this direction, so it might make sense to terminate the line search. Typical values of $c_{2}$ are 0.9 when the search direction $\\mathbf{p}_{k}$ is chosen by a Newton or quasi-Newton method, and 0.1 when $\\mathbf{p}_{k}$ is obtained from a nonlinear conjugate gradient method.\nThe sufficient decrease and curvature conditions are known collectively as the Wolfe conditions. We restate them here for future reference:\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \u0026amp; \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\\\ \\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \u0026amp; \\geq c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\end{aligned} \\label{eq:wolfe} \\end{equation}\nwith $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$.\nA step length may satisfy the Wolfe conditions without being particularly close to a minimizer of $\\phi$. We can, however, modify the curvature condition to force $\\alpha_{k}$ to lie in at least a broad neighborhood of a local minimizer or stationary point of $\\phi$. The strong Wolfe conditions require $\\alpha_{k}$ to satisfy\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \u0026amp; \\leq f\\left(\\mathbf{x}_{k}\\right)+c_{1} \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\\\ \\left|\\nabla f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k}\\right| \u0026amp; \\leq c_{2}\\left|\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\\right|, \\end{aligned} \\label{eq:strong_wolfe} \\end{equation}\nwith $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$. The only difference with the Wolfe conditions is that we no longer allow the derivative $\\phi^{\\prime}\\left(\\alpha_{k}\\right)$ to be too positive. Hence, we exclude points that are far from stationary points of $\\phi$.\nIt is not difficult to prove that there exist step lengths that satisfy the Wolfe conditions for every function $f$ that is smooth and bounded below.\nLemma 4.1 (Existence of step lengths)\nSuppose that $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ is continuously differentiable. Let $\\mathbf{p}_{k}$ be a descent direction at $\\mathbf{x}_{k}$, and assume that $f$ is bounded below along the ray $\\left\\{\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k} \\mid \\alpha\u0026gt;0\\right\\}$. Then if $0\u0026lt;c_{1}\u0026lt;c_{2}\u0026lt;1$, there exist intervals of step lengths satisfying the Wolfe conditions \\eqref{eq:wolfe} and the strong Wolfe conditions \\eqref{eq:strong_wolfe}. Proof\nSince $\\phi(\\alpha)=f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right)$ is bounded below for all $\\alpha\u0026gt;0$ and since $0\u0026lt;c_{1}\u0026lt;1$, the line $l(\\alpha)=f\\left(\\mathbf{x}_{k}\\right)+\\alpha c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$ must intersect the graph of $\\phi$ at least once. Let $\\alpha^{\\prime}\u0026gt;0$ be the smallest intersecting value of $\\alpha$, that is,\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime} \\mathbf{p}_{k}\\right)=f\\left(\\mathbf{x}_{k}\\right)+\\alpha^{\\prime} c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:intersection} \\end{equation}\nThe sufficient decrease condition \\eqref{eq:armijo} clearly holds for all step lengths less than $\\alpha^{\\prime}$.\nBy the mean value theorem, there exists $\\alpha^{\\prime \\prime} \\in\\left(0, \\alpha^{\\prime}\\right)$ such that\n\\begin{equation} f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime} \\mathbf{p}_{k}\\right)-f\\left(\\mathbf{x}_{k}\\right)=\\alpha^{\\prime} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime \\prime} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k} \\label{eq:mean_value} \\end{equation}\nBy combining \\eqref{eq:intersection} and \\eqref{eq:mean_value}, we obtain\n\\begin{equation} \\nabla f\\left(\\mathbf{x}_{k}+\\alpha^{\\prime \\prime} \\mathbf{p}_{k}\\right)^{\\mathrm{T}} \\mathbf{p}_{k}=c_{1} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\u0026gt;c_{2} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:inequality_proof} \\end{equation}\nsince $c_{1}\u0026lt;c_{2}$ and $\\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}\u0026lt;0$. Therefore, $\\alpha^{\\prime \\prime}$ satisfies the Wolfe conditions \\eqref{eq:wolfe}, and the inequalities hold strictly in both conditions. Hence, by our smoothness assumption on $f$, there is an interval around $\\alpha^{\\prime \\prime}$ for which the Wolfe conditions hold. Moreover, since the term in the left-hand side of \\eqref{eq:inequality_proof} is negative, the strong Wolfe conditions \\eqref{eq:strong_wolfe} hold in the same interval.\n■ The Wolfe conditions are scale-invariant in a broad sense: Multiplying the objective function by a constant or making an affine change of variables does not alter them. They can be used in most line search methods, and are particularly important in the implementation of quasi-Newton methods.\nTo summarize, see the following interactive visualisation of the Wolfe conditions, which illustrates the sufficient decrease and curvature conditions in action:\nThe Goldstein conditions # Like the Wolfe conditions, the Goldstein conditions also ensure that the step length $\\alpha$ achieves sufficient decrease while preventing $\\alpha$ from being too small. The Goldstein conditions can also be stated as a pair of inequalities, in the following way:\n\\begin{equation} f\\left(\\mathbf{x}_{k}\\right)+(1-c) \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k} \\leq f\\left(\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c \\alpha_{k} \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}, \\label{eq:goldstein} \\end{equation}\nwith $0\u0026lt;c\u0026lt;\\frac{1}{2}$. The second inequality is the sufficient decrease condition \\eqref{eq:armijo}, whereas the first inequality is introduced to control the step length from below.\nA disadvantage of the Goldstein conditions vis-à-vis the Wolfe conditions is that the first inequality in \\eqref{eq:goldstein} may exclude all minimizers of $\\phi$. However, the Goldstein and Wolfe conditions have much in common, and their convergence theories are quite similar. The Goldstein conditions are often used in Newton-type methods but are not well suited for quasi-Newton methods that maintain a positive definite Hessian approximation.\nAn illustration of the Goldstein conditions is shown in the following interactive visualisation:\nSufficient decrease and backtracking # We have mentioned that the sufficient decrease condition \\eqref{eq:armijo} alone is not sufficient to ensure that the algorithm makes reasonable progress along the given search direction. However, if the line search algorithm chooses its candidate step lengths appropriately, by using a so-called backtracking approach, we can dispense with the extra condition \\eqref{eq:curvature} and use just the sufficient decrease condition to terminate the line search procedure. In its most basic form, backtracking proceeds as follows.\nProcedure (Backtracking Line Search).\nChoose $\\bar{\\alpha}\u0026gt;0, \\rho, c \\in(0,1)$;\nset $\\alpha \\leftarrow \\bar{\\alpha}$;\nrepeat until $f\\left(\\mathbf{x}_{k}+\\alpha \\mathbf{p}_{k}\\right) \\leq f\\left(\\mathbf{x}_{k}\\right)+c \\alpha \\nabla f_{k}^{\\mathrm{T}} \\mathbf{p}_{k}$\n$\\alpha \\leftarrow\\rho\\alpha$;\nend (repeat)\nterminate with $\\alpha_{k}=\\alpha$.\nIn this procedure, the initial step length $\\bar{\\alpha}$ is chosen to be 1 in Newton and quasi-Newton methods, but can have different values in other algorithms such as steepest descent or conjugate gradient. An acceptable step length will be found after a finite number of trials because $\\alpha_{k}$ will eventually become small enough that the sufficient decrease condition holds. In practice, the contraction factor $\\rho$ is often allowed to vary at each iteration of the line search. For example, it can be chosen by safeguarded interpolation, as we describe later. We need ensure only that at each iteration we have $\\rho \\in\\left[\\rho_{\\mathrm{lo}}, \\rho_{\\mathrm{hi}}\\right]$, for some fixed constants $0\u0026lt;\\rho_{\\text {lo }}\u0026lt;\\rho_{\\text {hi }}\u0026lt;1$.\nThe backtracking approach ensures either that the selected step length $\\alpha_{k}$ is some fixed value (the initial choice $\\bar{\\alpha}$ ), or else that it is short enough to satisfy the sufficient decrease condition but not too short. The latter claim holds because the accepted value $\\alpha_{k}$ is within striking distance of the previous trial value, $\\alpha_{k} / \\rho$, which was rejected for violating the sufficient decrease condition, that is, for being too long.\nConvergence of line search methods # To obtain global convergence, we must not only have well-chosen step lengths but also well-chosen search directions $\\mathbf{p}_k$. We discuss requirements on the search direction in this section, focusing on one key property: the angle $\\theta_k$ between $\\mathbf{p}_k$ and the steepest descent direction $-\\nabla f_k$, defined by\n$$ \\cos \\theta_k=\\frac{-\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k}{\\|\\nabla f_k\\|\\|\\mathbf{p}_k\\|} $$\nThe following theorem, due to Zoutendijk, has far-reaching consequences. It shows, for example, that the steepest descent method is globally convergent. For other algorithms it describes how far $\\mathbf{p}_k$ can deviate from the steepest descent direction and still give rise to a globally convergent iteration. Various line search termination conditions can be used to establish this result, but for concreteness we will consider only the Wolfe conditions. Though Zoutendijk\u0026rsquo;s result appears, at first, to be technical and obscure, its power will soon become evident.\nTheorem 4.1 (Zoutendijk\u0026#39;s theorem)\nConsider any iteration of the form $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, where $\\mathbf{p}_k$ is a descent direction and $\\alpha_k$ satisfies the Wolfe conditions. Suppose that $f$ is bounded below in $\\mathbb{R}^n$ and that $f$ is continuously differentiable in an open set $\\mathcal{N}$ containing the level set $\\mathcal{L} = \\{\\mathbf{x}: f(\\mathbf{x}) \\leq f(\\mathbf{x}_0)\\}$, where $\\mathbf{x}_0$ is the starting point of the iteration. Assume also that the gradient $\\nabla f$ is Lipschitz continuous on $\\mathcal{N}$, that is, there exists a constant $L\u0026gt;0$ such that\n\\begin{equation} \\|\\nabla f(\\mathbf{x})-\\nabla f(\\tilde{\\mathbf{x}})\\| \\leq L\\|\\mathbf{x}-\\tilde{\\mathbf{x}}\\|, \\quad \\text { for all } \\mathbf{x}, \\tilde{\\mathbf{x}} \\in \\mathcal{N} . \\label{eq:lipschitz} \\end{equation}\nThen\n\\begin{equation} \\sum_{k \\geq 0} \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2}\u0026lt;\\infty \\label{eq:zoutendijk_condition} \\end{equation}\nProof\nFrom the second Wolfe condition and the iteration formula we have that\n$$ (\\nabla f_{k+1}-\\nabla f_k)^{\\mathrm{T}} \\mathbf{p}_k \\geq(c_2-1) \\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k $$\nwhile the Lipschitz condition \\eqref{eq:lipschitz} implies that\n$$ (\\nabla f_{k+1}-\\nabla f_k)^{\\mathrm{T}} \\mathbf{p}_k \\leq \\alpha_k L\\|\\mathbf{p}_k\\|^{2} $$\nBy combining these two relations, we obtain\n$$ \\alpha_k \\geq \\frac{c_2-1}{L} \\frac{\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k}{\\|\\mathbf{p}_k\\|^{2}} $$\nBy substituting this inequality into the first Wolfe condition, we obtain\n$$ f_{k+1} \\leq f_k-c_1 \\frac{1-c_2}{L} \\frac{(\\nabla f_k^{\\mathrm{T}} \\mathbf{p}_k)^{2}}{\\|\\mathbf{p}_k\\|^{2}} $$\nFrom the definition of $\\cos \\theta_k$, we can write this relation as\n$$ f_{k+1} \\leq f_k-c \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2} $$\nwhere $c=c_1(1-c_2) / L$. By summing this expression over all indices less than or equal to $k$, we obtain\n$$ f_{k+1} \\leq f_0-c \\sum_{j=0}^{k} \\cos ^{2} \\theta_j\\|\\nabla f_j\\|^{2} $$\nSince $f$ is bounded below, we have that $f_0-f_{k+1}$ is less than some positive constant, for all $k$. Hence by taking limits, we obtain\n$$ \\sum_{k=0}^{\\infty} \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2}\u0026lt;\\infty $$\nwhich concludes the proof.\n■ Similar results to this theorem hold when the Goldstein conditions or strong Wolfe conditions are used in place of the Wolfe conditions.\nNote that the assumptions of Theorem 4.1 are not too restrictive. If the function $f$ were not bounded below, the optimization problem would not be well-defined. The smoothness assumption—Lipschitz continuity of the gradient—is implied by many of the smoothness conditions that are used in local convergence theorems and are often satisfied in practice.\nInequality \\eqref{eq:zoutendijk_condition}, which we call the Zoutendijk condition, implies that\n$$ \\cos ^{2} \\theta_k\\|\\nabla f_k\\|^{2} \\rightarrow 0 $$\nThis limit can be used in turn to derive global convergence results for line search algorithms. If our method for choosing the search direction $\\mathbf{p}_k$ in the iteration ensures that the angle $\\theta_k$ is bounded away from $90^{\\circ}$, there is a positive constant $\\delta$ such that\n\\begin{equation} \\cos \\theta_k \\geq \\delta\u0026gt;0, \\quad \\text { for all } k \\label{eq:angle_bound} \\end{equation}\nIt follows immediately from \\eqref{eq:zoutendijk_condition} that\n\\begin{equation} \\lim _{k \\rightarrow \\infty}\\|\\nabla f_k\\|=0 \\label{eq:global_convergence} \\end{equation}\nIn other words, we can be sure that the gradient norms $\\|\\nabla f_k\\|$ converge to zero, provided that the search directions are never too close to orthogonality with the gradient. In particular, the method of steepest descent (for which the search direction $\\mathbf{p}_k$ makes an angle of zero degrees with the negative gradient) produces a gradient sequence that converges to zero, provided that it uses a line search satisfying the Wolfe or Goldstein conditions.\nWe use the term globally convergent to refer to algorithms for which the property \\eqref{eq:global_convergence} is satisfied, but note that this term is sometimes used in other contexts to mean different things. For line search methods of the general form $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, the limit \\eqref{eq:global_convergence} is the strongest global convergence result that can be obtained: We cannot guarantee that the method converges to a minimizer, but only that it is attracted by stationary points. Only by making additional requirements on the search direction $\\mathbf{p}_k$—by introducing negative curvature information from the Hessian $\\nabla^{2} f(\\mathbf{x}_k)$, for example—can we strengthen these results to include convergence to a local minimum.\nNote that throughout this section we have used only the fact that Zoutendijk\u0026rsquo;s condition implies the limit \\eqref{eq:zoutendijk_condition}.\nRate of convergence # We refer the reader to the textbook\n\u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, 2nd edition, Springer, 2006, pages 47-51,\nfor a detailed discussion of the rate of convergence of line search methods.\nPeculiarly, see pages 47-51. In general, the rate of convergence depends on the choice of search direction and the step length conditions used.\n"},{"id":14,"href":"/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/","title":"5. Constrained optimization - Introduction","section":"I - Fundamentals","content":" Constrained optimization methods # Note : This is in part the content of the book \u0026ldquo;Numerical Optimization\u0026rdquo; by Nocedal and Wright, with some modifications to the notations used in this lecture.\nThe second part of this lecture is about minimizing functions subject to constraints on the variables. A general formulation for these problems is\n$$ \\min_{\\mathbf{x} \\in \\mathrm{R}^{n}} f(\\mathbf{x}) \\quad \\text { subject to } \\quad \\begin{cases}c_{i}(\\mathbf{x})=0, \u0026amp; i \\in \\mathcal{E}, \\\\ c_{i}(\\mathbf{x}) \\geq 0, \u0026amp; i \\in \\mathcal{I},\\end{cases} $$\nwhere $f$ and the functions $c_{i}$ are all smooth, real-valued functions on a subset of $\\mathbb{R}^{n}$, and $\\mathcal{I}$ and $\\mathcal{E}$ are two finite sets of indices. As before, we call $f$ the objective function, while $c_{i}$, $i \\in \\mathcal{E}$ are the equality constraints and $c_{i}, i \\in \\mathcal{I}$ are the inequality constraints. We define the feasible set $\\Omega$ to be the set of points $\\mathbf{x}$ that satisfy the constraints; that is,\n$$ \\Omega=\\left\\{\\mathbf{x} \\mid c_{i}(\\mathbf{x})=0, \\quad i \\in \\mathcal{E} ; \\quad c_{i}(\\mathbf{x}) \\geq 0, \\quad i \\in \\mathcal{I}\\right\\} $$\nso that we can rewrite the problem more compactly as\n\\begin{equation} \\min_{\\mathbf{x} \\in \\Omega} f(\\mathbf{x}). \\label{eq:constrained_problem} \\end{equation}\nIn this chapter we derive mathematical characterizations of the solutions of \\eqref{eq:constrained_problem}. Recall that for the unconstrained optimization problem, we characterized solution points $\\mathbf{x}^{\\star}$ in the following way:\nNecessary conditions: Local minima of unconstrained problems have $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$ and $\\nabla^{2} f\\left(\\mathbf{x}^{\\star}\\right)$ positive semidefinite.\nSufficient conditions: Any point $\\mathbf{x}^{\\star}$ at which $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$ and $\\nabla^{2} f\\left(\\mathbf{x}^{\\star}\\right)$ is positive definite is a strong local minimizer of $f$.\nOur aim in this chapter is to derive similar conditions to characterize the solutions of constrained optimization problems.\nLocal and global solutions # We have seen already that global solutions are difficult to find even when there are no constraints. The situation may be improved when we add constraints, since the feasible set might exclude many of the local minima and it may be comparatively easy to pick the global minimum from those that remain. However, constraints can also make things much more difficult. As an example, consider the problem\n$$ \\min_{\\mathbf{x} \\in \\mathrm{R}^{n}}\\|\\mathbf{x}\\|_{2}^{2}, \\quad \\text { subject to }\\|\\mathbf{x}\\|_{2}^{2} \\geq 1 $$\nWithout the constraint, this is a convex quadratic problem with unique minimizer $\\mathbf{x}=\\mathbf{0}$. When the constraint is added, any vector $\\mathbf{x}$ with $\\|\\mathbf{x}\\|_{2}=1$ solves the problem. There are infinitely many such vectors (hence, infinitely many local minima) whenever $n \\geq 2$.\nA second example shows how addition of a constraint produces a large number of local solutions that do not form a connected set. Consider\n$$ \\min \\left(x_{2}+100\\right)^{2}+0.01 x_{1}^{2}, \\quad \\text { subject to } x_{2}-\\cos x_{1} \\geq 0 $$\nWithout the constraint, the problem has the unique solution $(-100,0)$. With the constraint there are local solutions near the points\n$$ \\left(x_{1}, x_{2}\\right)=(k \\pi,-1), \\quad \\text { for } \\quad k= \\pm 1, \\pm 3, \\pm 5, \\ldots $$\nDefinition 5.1 (Local solution)\nA vector $\\mathbf{x}^{\\star}$ is a local solution of the problem \\eqref{eq:constrained_problem} if $\\mathbf{x}^{\\star} \\in \\Omega$ and there is a neighborhood $\\mathcal{N}$ of $\\mathbf{x}^{\\star}$ such that $f(\\mathbf{x}) \\geq f\\left(\\mathbf{x}^{\\star}\\right)$ for $\\mathbf{x} \\in \\mathcal{N} \\cap \\Omega$. Similarly, we can make the following definitions:\nDefinition 5.2 (Strict local solution)\nA vector $\\mathbf{x}^{\\star}$ is a strict local solution (also called a strong local solution) if $\\mathbf{x}^{\\star} \\in \\Omega$ and there is a neighborhood $\\mathcal{N}$ of $\\mathbf{x}^{\\star}$ such that $f(\\mathbf{x})\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $\\mathbf{x} \\in \\mathcal{N} \\cap \\Omega$ with $\\mathbf{x} \\neq \\mathbf{x}^{\\star}$. Definition 5.3 (Isolated local solution)\nA point $\\mathbf{x}^{\\star}$ is an isolated local solution if $\\mathbf{x}^{\\star} \\in \\Omega$ and there is a neighborhood $\\mathcal{N}$ of $\\mathbf{x}^{\\star}$ such that $\\mathbf{x}^{\\star}$ is the only local minimizer in $\\mathcal{N} \\cap \\Omega$. At times, we replace the word \u0026ldquo;solution\u0026rdquo; by \u0026ldquo;minimizer\u0026rdquo; in our discussion. This alternative is frequently used in the literature, but it is slightly less satisfying because it does not account for the role of the constraints in defining the point in question.\nSmoothness # Smoothness of objective functions and constraints is an important issue in characterizing solutions, just as in the unconstrained case. It ensures that the objective function and the constraints all behave in a reasonably predictable way and therefore allows algorithms to make good choices for search directions.\nWe saw earlier that graphs of nonsmooth functions contain \u0026ldquo;kinks\u0026rdquo; or \u0026ldquo;jumps\u0026rdquo; where the smoothness breaks down. If we plot the feasible region for any given constrained optimization problem, we usually observe many kinks and sharp edges. Does this mean that the constraint functions that describe these regions are nonsmooth? The answer is often no, because the nonsmooth boundaries can often be described by a collection of smooth constraint functions. A diamond-shaped feasible region in $\\mathbb{R}^{2}$ could be described by the single nonsmooth constraint\n$$ \\|\\mathbf{x}\\|_{1}=\\left|x_{1}\\right|+\\left|x_{2}\\right| \\leq 1 . $$\nFigure 5.1: Nonsmooth constraints can be described by smooth constraints\nIt can also be described by the following set of smooth (in fact, linear) constraints:\n$$ x_{1}+x_{2} \\leq 1, \\quad x_{1}-x_{2} \\leq 1, \\quad -x_{1}+x_{2} \\leq 1, \\quad -x_{1}-x_{2} \\leq 1 $$\nEach of the four constraints represents one edge of the feasible polytope. In general, the constraint functions are chosen so that each one represents a smooth piece of the boundary of $\\Omega$.\nNonsmooth, unconstrained optimization problems can sometimes be reformulated as smooth constrained problems. An example is given by the unconstrained scalar problem of minimizing a nonsmooth function $f(x)$ defined by\n$$ f(x)=\\max \\left(x^{2}, x\\right), $$\nwhich has kinks at $x=0$ and $x=1$, and the solution at $x^{\\star}=0$. We obtain a smooth, constrained formulation of this problem by adding an artificial variable $t$ and writing\n$$ \\min t \\quad \\text { s.t. } \\quad t \\geq x, \\quad t \\geq x^{2} . $$\nReformulation techniques such as these are used often in cases where $f$ is a maximum of a collection of functions or when $f$ is a 1 -norm or $\\infty$-norm of a vector function.\nIn the examples above we expressed inequality constraints in a slightly different way from the form $c_{i}(\\mathbf{x}) \\geq 0$ that appears in the definition. However, any collection of inequality constraints with $\\geq$ and $\\leq$ and nonzero right-hand-sides can be expressed in the form $c_{i}(\\mathbf{x}) \\geq 0$ by simple rearrangement of the inequality. In general, it is good practice to state the constraint in a way that is intuitive and easy to understand.\nExamples # To introduce the basic principles behind the characterization of solutions of constrained optimization problems, we work through three simple examples. The ideas discussed here will be made rigorous in the sections that follow.\nWe start by noting one item of terminology that recurs throughout the rest of the lecture: At a feasible point $\\mathbf{x}$, the inequality constraint $i \\in \\mathcal{I}$ is said to be active if $c_{i}(\\mathbf{x})=0$ and inactive if the strict inequality $c_{i}(\\mathbf{x})\u0026gt;0$ is satisfied.\nA single equality constraint # Example 1\nOur first example is a two-variable problem with a single equality constraint:\n\\begin{equation} \\min x_{1}+x_{2} \\quad \\text { s.t. } \\quad x_{1}^{2}+x_{2}^{2}-2=0. \\label{eq:equality_example} \\end{equation}\nFigure 5.2: Constraints and gradient of function\nIn the general form, we have $f(\\mathbf{x})=x_{1}+x_{2}, \\mathcal{I}=\\emptyset, \\mathcal{E}=\\{1\\}$, and $c_{1}(\\mathbf{x})=x_{1}^{2}+x_{2}^{2}-2$. We can see by inspection that the feasible set for this problem is the circle of radius $\\sqrt{2}$ centered at the origin-just the boundary of this circle, not its interior. The solution $\\mathbf{x}^{\\star}$ is obviously $(-1,-1)^{\\mathrm{T}}$. From any other point on the circle, it is easy to find a way to move that stays feasible (that is, remains on the circle) while decreasing $f$. For instance, from the point $\\mathbf{x}=(\\sqrt{2}, 0)^{\\mathrm{T}}$ any move in the clockwise direction around the circle has the desired effect.\nWe also see that at the solution $\\mathbf{x}^{\\star}$, the constraint normal $\\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right)$ is parallel to $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)$. That is, there is a scalar $\\lambda_{1}^{\\star}$ such that\n\\begin{equation} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\lambda_{1}^{\\star} \\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right). \\label{eq:parallel_gradients} \\end{equation}\n(In this particular case, we have $\\lambda_{1}^{\\star}=-\\frac{1}{2}$.)\nWe can derive \\eqref{eq:parallel_gradients} by examining first-order Taylor series approximations to the objective and constraint functions. To retain feasibility with respect to the function $c_{1}(\\mathbf{x})=0$, we require that $c_{1}(\\mathbf{x}+\\mathbf{d})=0$; that is,\n$$ 0=c_{1}(\\mathbf{x}+\\mathbf{d}) \\approx c_{1}(\\mathbf{x})+\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}=\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} $$\nHence, the direction $\\mathbf{d}$ retains feasibility with respect to $c_{1}$, to first order, when it satisfies\n$$ \\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}=0 $$\nSimilarly, a direction of improvement must produce a decrease in $f$, so that\n$$ 0\u0026gt;f(\\mathbf{x}+\\mathbf{d})-f(\\mathbf{x}) \\approx \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} $$\nor, to first order,\n$$ \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0 $$\nIf there exists a direction $\\mathbf{d}$ that satisfies both conditions, we conclude that improvement on our current point $\\mathbf{x}$ is possible. It follows that a necessary condition for optimality for the problem \\eqref{eq:equality_example} is that there exist no direction $\\mathbf{d}$ satisfying both conditions.\nBy drawing a picture (see visualization below), the reader can check that the only way that such a direction cannot exist is if $\\nabla f(\\mathbf{x})$ and $\\nabla c_{1}(\\mathbf{x})$ are parallel, that is, if the condition $\\nabla f(\\mathbf{x})=\\lambda_{1} \\nabla c_{1}(\\mathbf{x})$ holds at $\\mathbf{x}$, for some scalar $\\lambda_{1}$. If this condition is not satisfied, the direction defined by\n$$ \\mathbf{d}=-\\left(\\mathbf{I}-\\frac{\\nabla c_{1}(\\mathbf{x}) \\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}}}{\\|\\nabla c_{1}(\\mathbf{x})\\|^{2}}\\right) \\nabla f(\\mathbf{x}) $$\nsatisfies both conditions.\nBy introducing the Lagrangian function\n$$ \\mathcal{L}\\left(\\mathbf{x}, \\lambda_{1}\\right)=f(\\mathbf{x})-\\lambda_{1} c_{1}(\\mathbf{x}), $$\nand noting that $\\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}, \\lambda_{1}\\right)=\\nabla f(\\mathbf{x})-\\lambda_{1} \\nabla c_{1}(\\mathbf{x})$, we can state the condition \\eqref{eq:parallel_gradients} equivalently as follows: At the solution $\\mathbf{x}^{\\star}$, there is a scalar $\\lambda_{1}^{\\star}$ such that\n\\begin{equation} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\lambda_{1}^{\\star}\\right)=0. \\label{eq:lagrangian_gradient_zero} \\end{equation}\nThis observation suggests that we can search for solutions of the equality-constrained problem \\eqref{eq:equality_example} by searching for stationary points of the Lagrangian function. The scalar quantity $\\lambda_{1}$ is called a Lagrange multiplier for the constraint $c_{1}(\\mathbf{x})=0$.\nThough the condition \\eqref{eq:parallel_gradients} (equivalently, \\eqref{eq:lagrangian_gradient_zero}) appears to be necessary for an optimal solution of the problem \\eqref{eq:equality_example}, it is clearly not sufficient. For instance, in this example, \\eqref{eq:parallel_gradients} is satisfied at the point $\\mathbf{x}=(1,1)$ (with $\\lambda_{1}=\\frac{1}{2}$ ), but this point is obviously not a solution-in fact, it maximizes the function $f$ on the circle. Moreover, in the case of equality-constrained problems, we cannot turn the condition \\eqref{eq:parallel_gradients} into a sufficient condition simply by placing some restriction on the sign of $\\lambda_{1}$. To see this, consider replacing the constraint $x_{1}^{2}+x_{2}^{2}-2=0$ by its negative $2-x_{1}^{2}-x_{2}^{2}=0$. The solution of the problem is not affected, but the value of $\\lambda_{1}^{\\star}$ that satisfies the condition \\eqref{eq:parallel_gradients} changes from $\\lambda_{1}^{\\star}=-\\frac{1}{2}$ to $\\lambda_{1}^{\\star}=\\frac{1}{2}$.\nThis situation is illustrated in following visualization:\nA single inequality constraint # Example 2\nThis is a slight modification of Example 1, in which the equality constraint is replaced by an inequality. Consider\n\\begin{equation} \\min x_{1}+x_{2} \\quad \\text { s.t. } \\quad 2-x_{1}^{2}-x_{2}^{2} \\geq 0, \\label{eq:inequality_example} \\end{equation}\nfor which the feasible region consists of the circle of problem \\eqref{eq:equality_example} and its interior. Note that the constraint normal $\\nabla c_{1}$ points toward the interior of the feasible region at each point on the boundary of the circle. By inspection, we see that the solution is still $(-1,-1)$ and that the condition \\eqref{eq:parallel_gradients} holds for the value $\\lambda_{1}^{\\star}=\\frac{1}{2}$. However, this inequality-constrained problem differs from the equality-constrained problem \\eqref{eq:equality_example} in that the sign of the Lagrange multiplier plays a significant role, as we now argue.\nAs before, we conjecture that a given feasible point $\\mathbf{x}$ is not optimal if we can find a step $\\mathbf{d}$ that both retains feasibility and decreases the objective function $f$ to first order. The main difference between problems \\eqref{eq:equality_example} and \\eqref{eq:inequality_example} comes in the handling of the feasibility condition. The direction $\\mathbf{d}$ improves the objective function, to first order, if $\\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0$. Meanwhile, the direction $\\mathbf{d}$ retains feasibility if\n$$ 0 \\leq c_{1}(\\mathbf{x}+\\mathbf{d}) \\approx c_{1}(\\mathbf{x})+\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} $$\nso, to first order, feasibility is retained if\n$$ c_{1}(\\mathbf{x})+\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0 $$\nIn determining whether a direction $\\mathbf{d}$ exists that satisfies both conditions, we consider the following two cases:\nCase I: Consider first the case in which $\\mathbf{x}$ lies strictly inside the circle, so that the strict inequality $c_{1}(\\mathbf{x})\u0026gt;0$ holds. In this case, any vector $\\mathbf{d}$ satisfies the feasibility condition, provided only that its length is sufficiently small. In particular, whenever $\\nabla f\\left(\\mathbf{x}^{\\star}\\right) \\neq \\mathbf{0}$, we can obtain a direction $\\mathbf{d}$ that satisfies both conditions by setting\n$$ \\mathbf{d}=-c_{1}(\\mathbf{x}) \\frac{\\nabla f(\\mathbf{x})}{\\|\\nabla f(\\mathbf{x})\\|} $$\nThe only situation in which such a direction fails to exist is when\n$$ \\nabla f(\\mathbf{x})=\\mathbf{0} . $$\nThis situation is summarized through the following interactive visualization:\nCase II: Consider now the case in which $\\mathbf{x}$ lies on the boundary of the circle, so that $c_{1}(\\mathbf{x})=0$. The conditions therefore become\n$$ \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0, \\quad \\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0 $$\nThe first of these conditions defines an open half-space, while the second defines a closed half-space. It is clear that the two regions fail to intersect only when $\\nabla f(\\mathbf{x})$ and $\\nabla c_{1}(\\mathbf{x})$ point in the same direction, that is, when\n\\begin{equation} \\nabla f(\\mathbf{x})=\\lambda_{1} \\nabla c_{1}(\\mathbf{x}), \\quad \\text { for some } \\lambda_{1} \\geq 0. \\label{eq:inequality_optimality} \\end{equation}\nNote that the sign of the multiplier is significant here. If \\eqref{eq:parallel_gradients} were satisfied with a negative value of $\\lambda_{1}$, then $\\nabla f(\\mathbf{x})$ and $\\nabla c_{1}(\\mathbf{x})$ would point in opposite directions, and we see that the set of directions that satisfy both conditions would make up an entire open half-plane.\nThe optimality conditions for both cases I and II can again be summarized neatly with reference to the Lagrangian function. When no first-order feasible descent direction exists at some point $\\mathbf{x}^{\\star}$, we have that\n\\begin{equation} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\lambda_{1}^{\\star}\\right)=\\mathbf{0}, \\quad \\text { for some } \\lambda_{1}^{\\star} \\geq 0, \\label{eq:kkt_gradient} \\end{equation}\nwhere we also require that\n\\begin{equation} \\lambda_{1}^{\\star} c_{1}\\left(\\mathbf{x}^{\\star}\\right)=0. \\label{eq:complementarity} \\end{equation}\nThis condition is known as a complementarity condition; it implies that the Lagrange multiplier $\\lambda_{1}$ can be strictly positive only when the corresponding constraint $c_{1}$ is active. Conditions of this type play a central role in constrained optimization, as we see in the sections that follow. In case I, we have that $c_{1}\\left(\\mathbf{x}^{\\star}\\right)\u0026gt;0$, so \\eqref{eq:complementarity} requires that $\\lambda_{1}^{\\star}=0$. Hence, \\eqref{eq:kkt_gradient} reduces to $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\mathbf{0}$, as required. In case II, \\eqref{eq:complementarity} allows $\\lambda_{1}^{\\star}$ to take on a nonnegative value, so \\eqref{eq:kkt_gradient} becomes equivalent to \\eqref{eq:inequality_optimality}.\nThis situation is summarized through the following interactive visualization:\nThis situation is also well visualized for quadratic functions:\nTwo inequality constraints # Example 3\nSuppose we add an extra constraint to the problem \\eqref{eq:inequality_example} to obtain\n\\begin{equation} \\min x_{1}+x_{2} \\quad \\text { s.t. } \\quad 2-x_{1}^{2}-x_{2}^{2} \\geq 0, \\quad x_{2} \\geq 0, \\label{eq:two_inequality_example} \\end{equation}\nfor which the feasible region is the half-disk. It is easy to see that the solution lies at $(-\\sqrt{2}, 0)^{\\mathrm{T}}$, a point at which both constraints are active. By repeating the arguments for the previous examples, we conclude that a direction $\\mathbf{d}$ is a feasible descent direction, to first order, if it satisfies the following conditions:\n$$ \\nabla c_{i}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, \\quad i \\in \\mathcal{I}=\\{1,2\\}, \\quad \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0 $$\nHowever, it is clear that no such direction can exist when $\\mathbf{x}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$. The conditions $\\nabla c_{i}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, i=1,2$, are both satisfied only if $\\mathbf{d}$ lies in the quadrant defined by $\\nabla c_{1}(\\mathbf{x})$ and $\\nabla c_{2}(\\mathbf{x})$, but it is clear by inspection that all vectors $\\mathbf{d}$ in this quadrant satisfy $\\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0$.\nLet us see how the Lagrangian and its derivatives behave for the problem \\eqref{eq:two_inequality_example} and the solution point $(-\\sqrt{2}, 0)^{\\mathrm{T}}$. First, we include an additional term $\\lambda_{i} c_{i}(\\mathbf{x})$ in the Lagrangian for each additional constraint, so we have\n\\begin{equation} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=f(\\mathbf{x})-\\lambda_{1} c_{1}(\\mathbf{x})-\\lambda_{2} c_{2}(\\mathbf{x}), \\label{eq:two_constraint_lagrangian} \\end{equation}\nwhere $\\boldsymbol{\\lambda}=\\left(\\lambda_{1}, \\lambda_{2}\\right)^{\\mathrm{T}}$ is the vector of Lagrange multipliers. The extension of condition \\eqref{eq:kkt_gradient} to this case is\n\\begin{equation} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)=\\mathbf{0}, \\quad \\text { for some } \\boldsymbol{\\lambda}^{\\star} \\geq \\mathbf{0}, \\label{eq:two_constraint_kkt} \\end{equation}\nwhere the inequality $\\boldsymbol{\\lambda}^{\\star} \\geq \\mathbf{0}$ means that all components of $\\boldsymbol{\\lambda}^{\\star}$ are required to be nonnegative. By applying the complementarity condition \\eqref{eq:complementarity} to both inequality constraints, we obtain\n\\begin{equation} \\lambda_{1}^{\\star} c_{1}\\left(\\mathbf{x}^{\\star}\\right)=0, \\quad \\lambda_{2}^{\\star} c_{2}\\left(\\mathbf{x}^{\\star}\\right)=0. \\label{eq:two_constraint_complementarity} \\end{equation}\nWhen $\\mathbf{x}^{\\star}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$, we have\n$$ \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad \\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} 2 \\sqrt{2} \\\\ 0 \\end{bmatrix}, \\quad \\nabla c_{2}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, $$\nso that it is easy to verify that $\\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)=\\mathbf{0}$ when we select $\\boldsymbol{\\lambda}^{\\star}$ as follows:\n$$ \\boldsymbol{\\lambda}^{\\star}=\\begin{bmatrix} 1 /(2 \\sqrt{2}) \\\\ 1 \\end{bmatrix} $$\nNote that both components of $\\boldsymbol{\\lambda}^{\\star}$ are positive.\nWe consider now some other feasible points that are not solutions of \\eqref{eq:two_inequality_example}, and examine the properties of the Lagrangian and its gradient at these points.\nFor the point $\\mathbf{x}=(\\sqrt{2}, 0)^{\\mathrm{T}}$, we again have that both constraints are active. However, the objective gradient $\\nabla f(\\mathbf{x})$ no longer lies in the quadrant defined by the conditions $\\nabla c_{i}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, i=1,2$. One first-order feasible descent direction from this point-a vector $\\mathbf{d}$ that satisfies the required conditions-is simply $\\mathbf{d}=(-1,0)^{\\mathrm{T}}$; there are many others. For this value of $\\mathbf{x}$ it is easy to verify that the condition $\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=\\mathbf{0}$ is satisfied only when $\\boldsymbol{\\lambda}=(-1 /(2 \\sqrt{2}), 1)$. Note that the first component $\\lambda_{1}$ is negative, so that the conditions \\eqref{eq:two_constraint_kkt} are not satisfied at this point.\nFinally, let us consider the point $\\mathbf{x}=(1,0)^{\\mathrm{T}}$, at which only the second constraint $c_{2}$ is active. At this point, linearization of $f$ and $c$ gives the following conditions, which must be satisfied for $\\mathbf{d}$ to be a feasible descent direction, to first order:\n$$ 1+\\nabla c_{1}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, \\quad \\nabla c_{2}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d} \\geq 0, \\quad \\nabla f(\\mathbf{x})^{\\mathrm{T}} \\mathbf{d}\u0026lt;0 . $$\nIn fact, we need worry only about satisfying the second and third conditions, since we can always satisfy the first condition by multiplying $\\mathbf{d}$ by a sufficiently small positive quantity. By noting that\n$ \\nabla f(\\mathbf{x})=\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad \\nabla c_{2}(\\mathbf{x})=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $\nit is easy to verify that the vector $\\mathbf{d}=\\left(-\\frac{1}{2}, \\frac{1}{4}\\right)$ satisfies the required conditions and is therefore a descent direction.\nTo show that optimality conditions \\eqref{eq:two_constraint_kkt} and \\eqref{eq:two_constraint_complementarity} fail, we note first from \\eqref{eq:two_constraint_complementarity} that since $c_{1}(\\mathbf{x})\u0026gt;0$, we must have $\\lambda_{1}=0$. Therefore, in trying to satisfy $\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=\\mathbf{0}$, we are left to search for a value $\\lambda_{2}$ such that $\\nabla f(\\mathbf{x})-\\lambda_{2} \\nabla c_{2}(\\mathbf{x})=\\mathbf{0}$. No such $\\lambda_{2}$ exists, and thus this point fails to satisfy the optimality conditions.\nThe following visualization summarizes the discussion of this example on the active constraints:\nFirst-order optimality conditions # Statement of first-order necessary conditions # The three examples above suggest that a number of conditions are important in the characterization of solutions for the general problem. These include the relation $\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=\\mathbf{0}$, the nonnegativity of $\\lambda_{i}$ for all inequality constraints $c_{i}(\\mathbf{x})$, and the complementarity condition $\\lambda_{i} c_{i}(\\mathbf{x})=0$ that is required for all the inequality constraints. We now generalize the observations made in these examples and state the first-order optimality conditions in a rigorous fashion.\nIn general, the Lagrangian for the constrained optimization problem is defined as\n\\begin{equation} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})=f(\\mathbf{x})-\\sum_{i \\in \\mathcal{E} \\cup \\mathcal{I}} \\lambda_{i} c_{i}(\\mathbf{x}). \\label{eq:general_lagrangian} \\end{equation}\nThe active set $\\mathcal{A}(\\mathbf{x})$ at any feasible $\\mathbf{x}$ is the union of the set $\\mathcal{E}$ with the indices of the active inequality constraints; that is,\n\\begin{equation} \\mathcal{A}(\\mathbf{x})=\\mathcal{E} \\cup\\left\\{i \\in \\mathcal{I} \\mid c_{i}(\\mathbf{x})=0\\right\\}. \\label{eq:active_set} \\end{equation}\nNext, we need to give more attention to the properties of the constraint gradients. The vector $\\nabla c_{i}(\\mathbf{x})$ is often called the normal to the constraint $c_{i}$ at the point $\\mathbf{x}$, because it is usually a vector that is perpendicular to the contours of the constraint $c_{i}$ at $\\mathbf{x}$, and in the case of an inequality constraint, it points toward the feasible side of this constraint. It is possible, however, that $\\nabla c_{i}(\\mathbf{x})$ vanishes due to the algebraic representation of $c_{i}$, so that the term $\\lambda_{i} \\nabla c_{i}(\\mathbf{x})$ vanishes for all values of $\\lambda_{i}$ and does not play a role in the Lagrangian gradient $\\nabla_{\\mathbf{x}} \\mathcal{L}$. For instance, if we replaced the constraint in \\eqref{eq:equality_example} by the equivalent condition\n$ c_{1}(\\mathbf{x})=\\left(x_{1}^{2}+x_{2}^{2}-2\\right)^{2}=0 $\nwe would have that $\\nabla c_{1}(\\mathbf{x})=\\mathbf{0}$ for all feasible points $\\mathbf{x}$, and in particular that the condition $\\nabla f(\\mathbf{x})=\\lambda_{1} \\nabla c_{1}(\\mathbf{x})$ no longer holds at the optimal point $(-1,-1)^{\\mathrm{T}}$. We usually make an assumption called a constraint qualification to ensure that such degenerate behavior does not occur at the value of $\\mathbf{x}$ in question. One such constraint qualification-probably the one most often used in the design of algorithms-is the one defined as follows:\nDefinition 5.4 (Linear independence constraint qualification (LICQ))\nGiven the point $\\mathbf{x}^{\\star}$ and the active set $\\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ defined by \\eqref{eq:active_set}, we say that the linear independence constraint qualification (LICQ) holds if the set of active constraint gradients $\\left\\{\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right), i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)\\right\\}$ is linearly independent. Note that if this condition holds, none of the active constraint gradients can be zero.\nThis condition allows us to state the following optimality conditions for a general nonlinear programming problem. These conditions provide the foundation for many of the algorithms described in the remaining chapters of the lecture. They are called first-order conditions because they concern themselves with properties of the gradients (first-derivative vectors) of the objective and constraint functions.\nTheorem 5.1 (First-order necessary conditions)\nSuppose that $\\mathbf{x}^{\\star}$ is a local solution and that the LICQ holds at $\\mathbf{x}^{\\star}$. Then there is a Lagrange multiplier vector $\\boldsymbol{\\lambda}^{\\star}$, with components $\\lambda_{i}^{\\star}, i \\in \\mathcal{E} \\cup \\mathcal{I}$, such that the following conditions are satisfied at $\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)$:\n\\begin{align} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\u0026amp;=\\mathbf{0}, \\label{eq:kkt_gradient_zero} \\\\ c_{i}\\left(\\mathbf{x}^{\\star}\\right)\u0026amp;=0, \\quad \u0026amp;\u0026amp; \\text { for all } i \\in \\mathcal{E}, \\label{eq:kkt_equality} \\\\ c_{i}\\left(\\mathbf{x}^{\\star}\\right)\u0026amp;\\geq 0, \\quad \u0026amp;\u0026amp; \\text { for all } i \\in \\mathcal{I}, \\label{eq:kkt_inequality} \\\\ \\lambda_{i}^{\\star} \u0026amp;\\geq 0, \\quad \u0026amp;\u0026amp; \\text { for all } i \\in \\mathcal{I}, \\label{eq:kkt_multiplier_sign} \\\\ \\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{x}^{\\star}\\right)\u0026amp;=0, \\quad \u0026amp;\u0026amp; \\text { for all } i \\in \\mathcal{E} \\cup \\mathcal{I}. \\label{eq:kkt_complementarity} \\end{align}\nThe conditions \\eqref{eq:kkt_gradient_zero}-\\eqref{eq:kkt_complementarity} are often known as the Karush-Kuhn-Tucker conditions, or KKT conditions for short. Because the complementarity condition implies that the Lagrange multipliers corresponding to inactive inequality constraints are zero, we can omit the terms for indices $i \\notin \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ from \\eqref{eq:kkt_gradient_zero} and rewrite this condition as\n\\begin{equation} \\mathbf{0}=\\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)=\\nabla f\\left(\\mathbf{x}^{\\star}\\right)-\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i}^{\\star} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right). \\label{eq:kkt_active_gradients} \\end{equation}\nA special case of complementarity is important and deserves its own definition:\nDefinition 5.5 (Strict complementarity)\nGiven a local solution $\\mathbf{x}^{\\star}$ and a vector $\\boldsymbol{\\lambda}^{\\star}$ satisfying \\eqref{eq:kkt_gradient_zero}-\\eqref{eq:kkt_complementarity}, we say that the strict complementarity condition holds if exactly one of $\\lambda_{i}^{\\star}$ and $c_{i}\\left(\\mathbf{x}^{\\star}\\right)$ is zero for each index $i \\in \\mathcal{I}$. In other words, we have that $\\lambda_{i}^{\\star}\u0026gt;0$ for each $i \\in \\mathcal{I} \\cap \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$. For a given problem and solution point $\\mathbf{x}^{\\star}$, there may be many vectors $\\boldsymbol{\\lambda}^{\\star}$ for which the conditions \\eqref{eq:kkt_gradient_zero}-\\eqref{eq:kkt_complementarity} are satisfied. When the LICQ holds, however, the optimal $\\boldsymbol{\\lambda}^{\\star}$ is unique.\nExample 4\nConsider the feasible region described by the four constraints:\n\\begin{equation} \\min_{\\mathbf{x}}\\left(x_{1}-\\frac{3}{2}\\right)^{2}+\\left(x_{2}-\\frac{1}{8}\\right)^{4} \\quad \\text { s.t. } \\quad\\begin{bmatrix} 1-x_{1}-x_{2} \\\\ 1-x_{1}+x_{2} \\\\ 1+x_{1}-x_{2} \\\\ 1+x_{1}+x_{2} \\end{bmatrix} \\geq \\mathbf{0}. \\label{eq:diamond_example} \\end{equation}\nIt is fairly clear that the solution is $\\mathbf{x}^{\\star}=(1,0)$. The first and second constraints are active at this point. Denoting them by $c_{1}$ and $c_{2}$ (and the inactive constraints by $c_{3}$ and $c_{4}$ ), we have\n$ \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} -1 \\\\ -\\frac{1}{2} \\end{bmatrix}, \\quad \\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}, \\quad \\nabla c_{2}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} . $\nTherefore, the KKT conditions \\eqref{eq:kkt_gradient_zero}-\\eqref{eq:kkt_complementarity} are satisfied when we set\n$ \\boldsymbol{\\lambda}^{\\star}=\\left(\\frac{3}{4}, \\frac{1}{4}, 0,0\\right)^{\\mathrm{T}}. $\nSensitivity # The convenience of using Lagrange multipliers should now be clear, but what of their intuitive significance? The value of each Lagrange multiplier $\\lambda_{i}^{\\star}$ tells us something about the sensitivity of the optimal objective value $f\\left(\\mathbf{x}^{\\star}\\right)$ to the presence of constraint $c_{i}$. To put it another way, $\\lambda_{i}^{\\star}$ indicates how hard $f$ is \u0026ldquo;pushing\u0026rdquo; or \u0026ldquo;pulling\u0026rdquo; against the particular constraint $c_{i}$. We illustrate this point with a little analysis. When we choose an inactive constraint $i \\notin \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ such that $c_{i}\\left(\\mathbf{x}^{\\star}\\right)\u0026gt;0$, the solution $\\mathbf{x}^{\\star}$ and function value $f\\left(\\mathbf{x}^{\\star}\\right)$ are quite indifferent to whether this constraint is present or not. If we perturb $c_{i}$ by a tiny amount, it will still be inactive and $\\mathbf{x}^{\\star}$ will still be a local solution of the optimization problem. Since $\\lambda_{i}^{\\star}=0$ from \\eqref{eq:kkt_complementarity}, the Lagrange multiplier indicates accurately that constraint $i$ is not significant.\nSuppose instead that constraint $i$ is active, and let us perturb the right-hand-side of this constraint a little, requiring, say, that $c_{i}(\\mathbf{x}) \\geq-\\epsilon\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$ instead of $c_{i}(\\mathbf{x}) \\geq 0$. Suppose that $\\epsilon$ is sufficiently small that the perturbed solution $\\mathbf{x}^{\\star}(\\epsilon)$ still has the same set of active constraints, and that the Lagrange multipliers are not much affected by the perturbation. (These conditions can be made more rigorous with the help of strict complementarity and second-order conditions, as discussed later in the lecture.) We then find that\n\\begin{equation} \\begin{aligned} -\\epsilon\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\| \u0026amp; =c_{i}\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)-c_{i}\\left(\\mathbf{x}^{\\star}\\right) \\approx\\left(\\mathbf{x}^{\\star}(\\epsilon)-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right), \\\\ 0 \u0026amp; =c_{j}\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)-c_{j}\\left(\\mathbf{x}^{\\star}\\right) \\approx\\left(\\mathbf{x}^{\\star}(\\epsilon)-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right), \\end{aligned} \\label{eq:sensitivity_perturbation} \\end{equation}\nfor all $j \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ with $j \\neq i$.\nThe value of $f\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)$, meanwhile, can be estimated with the help of \\eqref{eq:kkt_gradient_zero}. We have\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)-f\\left(\\mathbf{x}^{\\star}\\right) \u0026amp; \\approx\\left(\\mathbf{x}^{\\star}(\\epsilon)-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right) \\\\ \u0026amp; =\\sum_{j \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{j}^{\\star}\\left(\\mathbf{x}^{\\star}(\\epsilon)-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right) \\\\ \u0026amp; \\approx-\\epsilon\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\| \\lambda_{i}^{\\star} \\end{aligned} \\label{eq:sensitivity_objective} \\end{equation}\nBy taking limits, we see that the family of solutions $\\mathbf{x}^{\\star}(\\epsilon)$ satisfies\n\\begin{equation} \\frac{d f\\left(\\mathbf{x}^{\\star}(\\epsilon)\\right)}{d \\epsilon}=-\\lambda_{i}^{\\star}\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\| \\label{eq:sensitivity_derivative} \\end{equation}\nA sensitivity analysis of this problem would conclude that if $\\lambda_{i}^{\\star}\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$ is large, then the optimal value is sensitive to the placement of the $i$th constraint, while if this quantity is small, the dependence is not too strong. If $\\lambda_{i}^{\\star}$ is exactly zero for some active constraint, small perturbations to $c_{i}$ in some directions will hardly affect the optimal objective value at all; the change is zero, to first order.\nThis discussion motivates the definition below, which classifies constraints according to whether or not their corresponding Lagrange multiplier is zero.\nDefinition 5.6 (Strongly active and weakly active constraints)\nLet $\\mathbf{x}^{\\star}$ be a solution of the optimization problem, and suppose that the KKT conditions are satisfied. We say that an inequality constraint $c_{i}$ is strongly active or binding if $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ and $\\lambda_{i}^{\\star}\u0026gt;0$ for some Lagrange multiplier $\\boldsymbol{\\lambda}^{\\star}$ satisfying the KKT conditions. We say that $c_{i}$ is weakly active if $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ and $\\lambda_{i}^{\\star}=0$ for all $\\boldsymbol{\\lambda}^{\\star}$ satisfying the KKT conditions. Note that the analysis above is independent of scaling of the individual constraints. For instance, we might change the formulation of the problem by replacing some active constraint $c_{i}$ by $10 c_{i}$. The new problem will actually be equivalent (that is, it has the same feasible set and same solution), but the optimal multiplier $\\lambda_{i}^{\\star}$ corresponding to $c_{i}$ will be replaced by $\\lambda_{i}^{\\star} / 10$. However, since $\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$ is replaced by $10\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$, the product $\\lambda_{i}^{\\star}\\left\\|\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)\\right\\|$ does not change. If, on the other hand, we replace the objective function $f$ by $10 f$, the multipliers $\\lambda_{i}^{\\star}$ in the KKT conditions all will need to be replaced by $10 \\lambda_{i}^{\\star}$. Hence in \\eqref{eq:sensitivity_derivative} we see that the sensitivity of $f$ to perturbations has increased by a factor of 10, which is exactly what we would expect.\nDerivation of the first-order conditions # Having studied some motivating examples, observed the characteristics of optimal and nonoptimal points, and stated the KKT conditions, we now describe a complete proof of Theorem 5.1. This analysis is not just of esoteric interest, but is rather the key to understanding all constrained optimization algorithms.\nFeasible sequences # The first concept we introduce is that of a feasible sequence. Given a feasible point $\\mathbf{x}^{\\star}$, a sequence $\\left\\{\\mathbf{z}_{k}\\right\\}_{k=0}^{\\infty}$ with $\\mathbf{z}_{k} \\in \\mathbb{R}^{n}$ is a feasible sequence if the following properties hold: (i) $\\mathbf{z}_{k} \\neq \\mathbf{x}^{\\star}$ for all $k$; (ii) $\\lim_{k \\rightarrow \\infty} \\mathbf{z}_{k}=\\mathbf{x}^{\\star}$; (iii) $\\mathbf{z}_{k}$ is feasible for all sufficiently large values of $k$.\nFor later reference, we denote the set of all possible feasible sequences approaching $\\mathbf{x}$ by $\\mathcal{T}(\\mathbf{x})$.\nWe characterize a local solution as a point $\\mathbf{x}$ at which all feasible sequences have the property that $f\\left(\\mathbf{z}_{k}\\right) \\geq f(\\mathbf{x})$ for all $k$ sufficiently large. We derive practical, verifiable conditions under which this property holds. To do so we will make use of the concept of a limiting direction of a feasible sequence.\nLimiting directions of a feasible sequence are vectors $\\mathbf{d}$ such that we have\n\\begin{equation} \\lim_{\\mathbf{z}_{k} \\in \\mathcal{S}_{\\mathbf{d}}} \\frac{\\mathbf{z}_{k}-\\mathbf{x}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}\\right\\|} \\rightarrow \\mathbf{d} \\label{eq:limiting_direction} \\end{equation}\nwhere $\\mathcal{S}_{\\mathbf{d}}$ is some subsequence of $\\left\\{\\mathbf{z}_{k}\\right\\}_{k=0}^{\\infty}$. In general, a feasible sequence has at least one limiting direction and may have more than one. To see this, note that the sequence of vectors defined by\n\\begin{equation} \\mathbf{d}_{k}=\\frac{\\mathbf{z}_{k}-\\mathbf{x}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}\\right\\|} \\label{eq:normalized_direction} \\end{equation}\nlies on the surface of the unit sphere, which is a compact set, and thus there is at least one limit point $\\mathbf{d}$. Moreover, all such points are limiting directions by the definition \\eqref{eq:limiting_direction}. If we have some sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$ with limiting direction $\\mathbf{d}$ and corresponding subsequence $\\mathcal{S}_{\\mathbf{d}}$, we can construct another feasible sequence $\\left\\{\\overline{\\mathbf{z}}_{k}\\right\\}$ such that\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\frac{\\overline{\\mathbf{z}}_{k}-\\mathbf{x}}{\\left\\|\\overline{\\mathbf{z}}_{k}-\\mathbf{x}\\right\\|}=\\mathbf{d} \\label{eq:unique_limiting_direction} \\end{equation}\n(that is, with a unique limit point) by simply defining each $\\overline{\\mathbf{z}}_{k}$ to be an element from the subsequence $\\mathcal{S}_{\\mathbf{d}}$.\nWe illustrate these concepts by revisiting the equality-constrained example.\nExample 1 (Equality-constrained example, revisited)\nThe figure shows a closeup of the equality-constrained problem in which the feasible set is a circle of radius $\\sqrt{2}$, near the nonoptimal point $\\mathbf{x}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$. The figure also shows a feasible sequence approaching $\\mathbf{x}$. This sequence could be defined analytically by the formula\n\\begin{equation} \\mathbf{z}_{k}=\\begin{bmatrix} -\\sqrt{2-1 / k^{2}} \\\\ -1 / k \\end{bmatrix}. \\label{eq:example_sequence_1} \\end{equation}\nThe vector $\\mathbf{d}=(0,-1)^{\\mathrm{T}}$ is a limiting direction of this feasible sequence. Note that $\\mathbf{d}$ is tangent to the feasible sequence at $\\mathbf{x}$ but points in the opposite direction. The objective function $f(\\mathbf{x})=x_{1}+x_{2}$ increases as we move along the sequence \\eqref{eq:example_sequence_1}; in fact, we have $f\\left(\\mathbf{z}_{k+1}\\right)\u0026gt;f\\left(\\mathbf{z}_{k}\\right)$ for all $k=2,3, \\ldots$. It follows that $f\\left(\\mathbf{z}_{k}\\right)\u0026lt;f(\\mathbf{x})$ for $k=2,3, \\ldots$. Hence, $\\mathbf{x}$ cannot be a solution.\nAnother feasible sequence is one that approaches $\\mathbf{x}^{\\star}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$ from the opposite direction. Its elements are defined by\n\\begin{equation} \\mathbf{z}_{k}=\\begin{bmatrix} -\\sqrt{2-1 / k^{2}} \\\\ 1 / k \\end{bmatrix}. \\label{eq:example_sequence_2} \\end{equation}\nIt is easy to show that $f$ decreases along this sequence and that its limiting direction is $\\mathbf{d}=(0,1)^{\\mathrm{T}}$. Other feasible sequences are obtained by combining elements from the two sequences already discussed, for instance\n\\begin{equation} \\mathbf{z}_{k}= \\begin{cases}\\left(-\\sqrt{2-1 / k^{2}}, 1 / k\\right)^{\\mathrm{T}}, \u0026amp; \\text { when } k \\text { is a multiple of } 3 \\\\ \\left(-\\sqrt{2-1 / k^{2}},-1 / k\\right)^{\\mathrm{T}}, \u0026amp; \\text { otherwise. }\\end{cases} \\label{eq:example_sequence_3} \\end{equation}\nIn general, feasible sequences of points approaching $(-\\sqrt{2}, 0)^{\\mathrm{T}}$ will have two limiting directions, $(0,1)^{\\mathrm{T}}$ and $(0,-1)^{\\mathrm{T}}$.\nWe now consider feasible sequences and limiting directions for an example that involves inequality constraints.\nExample 2 (Inequality-constrained example, revisited)\nWe now reconsider the inequality-constrained problem. The solution $\\mathbf{x}^{\\star}=(-1,-1)^{\\mathrm{T}}$ is the same as in the equality-constrained case, but there is a much more extensive collection of feasible sequences that converge to any given feasible point. From the point $\\mathbf{x}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$, the various feasible sequences defined above for the equality-constrained problem are still feasible for the inequality-constrained problem. There are also infinitely many feasible sequences that converge to $\\mathbf{x}=(-\\sqrt{2}, 0)^{\\mathrm{T}}$ along a straight line from the interior of the circle. These are defined by\n\\begin{equation} \\mathbf{z}_{k}=(-1,0)^{\\mathrm{T}}+(1 / k) \\mathbf{w}, \\label{eq:straight_line_sequence} \\end{equation}\nwhere $\\mathbf{w}$ is any vector whose first component is positive ($w_{1}\u0026gt;0$). Now, $\\mathbf{z}_{k}$ is feasible, provided that $\\left\\|\\mathbf{z}_{k}\\right\\| \\leq 1$, that is,\n\\begin{equation} \\left(-1+w_{1} / k\\right)^{2}+\\left(w_{2} / k\\right)^{2} \\leq 1, \\label{eq:feasibility_condition} \\end{equation}\na condition that is satisfied, provided that $k\u0026gt;\\left(2 w_{1}\\right) /\\left(w_{1}^{2}+w_{2}^{2}\\right)$. In addition to these straight-line feasible sequences, we can also define an infinite variety of sequences that approach $(-\\sqrt{2}, 0)^{\\mathrm{T}}$ along a curve from the interior of the circle or that make the approach in a seemingly random fashion.\nGiven a point $\\mathbf{x}$, if it is possible to choose a feasible sequence from $\\mathcal{T}(\\mathbf{x})$ such that the first-order approximation to the objective function actually increases monotonically along the sequence, then $\\mathbf{x}$ must not be optimal. This condition is the fundamental first-order necessary condition, and we state it formally in the following theorem.\nTheorem 5.2 (First-order necessary condition for feasible sequences)\nIf $\\mathbf{x}^{\\star}$ is a local solution, then all feasible sequences $\\left\\{\\mathbf{z}_{k}\\right\\}$ in $\\mathcal{T}\\left(\\mathbf{x}^{\\star}\\right)$ must satisfy\n\\begin{equation} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d} \\geq 0 \\label{eq:first_order_feasible_condition} \\end{equation}\nwhere $\\mathbf{d}$ is any limiting direction of the feasible sequence.\nProof\nSuppose that there is a feasible sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$ with the property $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}\u0026lt;0$, for some limiting direction $\\mathbf{d}$, and let $\\mathcal{S}_{\\mathbf{d}}$ be the subsequence of $\\left\\{\\mathbf{z}_{k}\\right\\}$ that approaches $\\mathbf{x}^{\\star}$. By Taylor\u0026rsquo;s theorem, we have for any $\\mathbf{z}_{k} \\in \\mathcal{S}_{\\mathbf{d}}$ that\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{z}_{k}\\right) \u0026amp; =f\\left(\\mathbf{x}^{\\star}\\right)+\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\\\ \u0026amp; =f\\left(\\mathbf{x}^{\\star}\\right)+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\mathbf{d}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\end{aligned} \\label{eq:taylor_expansion_proof} \\end{equation}\nSince $\\mathbf{d}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)\u0026lt;0$, we have that the remainder term is eventually dominated by the first-order term, that is,\n\\begin{equation} f\\left(\\mathbf{z}_{k}\\right)\u0026lt;f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\mathbf{d}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right), \\quad \\text { for all } k \\text { sufficiently large. } \\label{eq:contradiction_inequality} \\end{equation}\nHence, given any open neighborhood of $\\mathbf{x}^{\\star}$, we can choose $k$ sufficiently large that $\\mathbf{z}_{k}$ lies within this neighborhood and has a lower value of the objective $f$. Therefore, $\\mathbf{x}^{\\star}$ is not a local solution.\n■ This theorem tells us why we can ignore constraints that are strictly inactive (that is, constraints for which $c_{i}(\\mathbf{x})\u0026gt;0$) in formulating optimality conditions. The theorem does not use the whole range of properties of the feasible sequence, but rather one specific property: the limiting directions of $\\left\\{\\mathbf{z}_{k}\\right\\}$. Because of the way in which the limiting directions are defined, it is clear that only the asymptotic behavior of the sequence is relevant, that is, its behavior for large values of the index $k$. If some constraint $i \\in \\mathcal{I}$ is inactive at $\\mathbf{x}$, then we have $c_{i}\\left(\\mathbf{z}_{k}\\right)\u0026gt;0$ for all $k$ sufficiently large, so that a constraint that is inactive at $\\mathbf{x}$ is also inactive at all sufficiently advanced elements of the feasible sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$.\nCharacterizing limiting directions: constraint qualifications # Theorem 5.2 is quite general, but it is not very useful as stated, because it seems to require knowledge of all possible limiting directions for all feasible sequences $\\mathcal{T}\\left(\\mathbf{x}^{\\star}\\right)$. In this section we show that constraint qualifications allow us to characterize the salient properties of $\\mathcal{T}\\left(\\mathbf{x}^{\\star}\\right)$, and therefore make the condition \\eqref{eq:first_order_feasible_condition} easier to verify.\nOne frequently used constraint qualification is the linear independence constrained qualification (LICQ) given in Definition 5.4 . The following lemma shows that when LICQ holds, there is a neat way to characterize the set of all possible limiting directions $\\mathbf{d}$ in terms of the gradients $\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)$ of the active constraints at $\\mathbf{x}^{\\star}$.\nIn subsequent results we introduce the notation $\\mathbf{A}$ to represent the matrix whose rows are the active constraint gradients at the optimal point, that is,\n\\begin{equation} \\nabla c_{i}^{\\star}=\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right), \\quad \\mathbf{A}^{\\mathrm{T}}=\\left[\\nabla c_{i}^{\\star}\\right]_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)}, \\quad \\nabla f^{\\star}=\\nabla f\\left(\\mathbf{x}^{\\star}\\right), \\label{eq:matrix_notation} \\end{equation}\nwhere the active set $\\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ is defined as in \\eqref{eq:active_set}.\nLemma 5.1 (Characterization of limiting directions)\nThe following two statements are true. (i) If $\\mathbf{d} \\in \\mathbb{R}^{n}$ is a limiting direction of a feasible sequence, then\n\\begin{equation} \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}=0, \\quad \\text { for all } i \\in \\mathcal{E}, \\quad \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0, \\quad \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}. \\label{eq:limiting_direction_conditions} \\end{equation}\n(ii) If \\eqref{eq:limiting_direction_conditions} holds with $\\left\\|\\mathbf{d}\\right\\|=1$ and the LICQ condition is satisfied, then $\\mathbf{d} \\in \\mathbb{R}^{n}$ is a limiting direction of some feasible sequence.\nProof\nWithout loss of generality, let us assume that all the constraints $c_{i}(\\cdot), i=1,2, \\ldots, m$, are active. (We can arrive at this convenient ordering by simply dropping all inactive constraints—which are irrelevant in some neighborhood of $\\mathbf{x}^{\\star}$—and renumbering the active constraints that remain.)\nTo prove (i), let $\\left\\{\\mathbf{z}_{k}\\right\\} \\in \\mathcal{T}\\left(\\mathbf{x}^{\\star}\\right)$ be some feasible sequence for which $\\mathbf{d}$ is a limiting direction, and assume (by taking a subsequence if necessary) that\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\frac{\\mathbf{z}_{k}-\\mathbf{x}^{\\star}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}=\\mathbf{d} \\label{eq:limiting_definition} \\end{equation}\nFrom this definition, we have that\n\\begin{equation} \\mathbf{z}_{k}=\\mathbf{x}^{\\star}+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right). \\label{eq:sequence_expansion} \\end{equation}\nBy taking $i \\in \\mathcal{E}$ and using Taylor\u0026rsquo;s theorem, we have that\n\\begin{equation} \\begin{aligned} 0 \u0026amp; =\\frac{1}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} c_{i}\\left(\\mathbf{z}_{k}\\right) \\\\ \u0026amp; =\\frac{1}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}\\left[c_{i}\\left(\\mathbf{x}^{\\star}\\right)+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)\\right] \\\\ \u0026amp; =\\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}+\\frac{o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} \\end{aligned} \\label{eq:equality_constraint_proof} \\end{equation}\nBy taking the limit as $k \\rightarrow \\infty$, the last term in this expression vanishes, and we have $\\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}=0$, as required. For the active inequality constraints $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$, we have similarly that\n\\begin{equation} \\begin{aligned} 0 \u0026amp; \\leq \\frac{1}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} c_{i}\\left(\\mathbf{z}_{k}\\right) \\\\ \u0026amp; =\\frac{1}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}\\left[c_{i}\\left(\\mathbf{x}^{\\star}\\right)+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)\\right] \\\\ \u0026amp; =\\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}+\\frac{o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} \\end{aligned} \\label{eq:inequality_constraint_proof} \\end{equation}\nHence, by a similar limiting argument, we have that $\\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d} \\geq 0$, as required.\nFor (ii), we use the implicit function theorem. First, since the LICQ holds, we have from\nDefinition 5.4\nthat the $m \\times n$ matrix $\\mathbf{A}$ of active constraint gradients has full row rank $m$. Let $\\mathbf{Z}$ be a matrix whose columns are a basis for the null space of $\\mathbf{A}$; that is,\n\\begin{equation} \\mathbf{Z} \\in \\mathbb{R}^{n \\times(n-m)}, \\quad \\mathbf{Z} \\text{ has full column rank }, \\quad \\mathbf{A} \\mathbf{Z}=\\mathbf{0}. \\label{eq:null_space_basis} \\end{equation}\nLet $\\mathbf{d}$ have the properties \\eqref{eq:limiting_direction_conditions}, and suppose that $\\left\\{t_{k}\\right\\}_{k=0}^{\\infty}$ is any sequence of positive scalars such $\\lim_{k \\rightarrow \\infty} t_{k}=0$. Define the parametrized system of equations $\\mathbf{R}: \\mathbb{R}^{n} \\times \\mathbb{R} \\rightarrow \\mathbb{R}^{n}$ by\n\\begin{equation} \\mathbf{R}(\\mathbf{z}, t)=\\begin{bmatrix} \\mathbf{c}(\\mathbf{z})-t \\mathbf{A} \\mathbf{d} \\\\ \\mathbf{Z}^{\\mathrm{T}}\\left(\\mathbf{z}-\\mathbf{x}^{\\star}-t \\mathbf{d}\\right) \\end{bmatrix}=\\begin{bmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{bmatrix} \\label{eq:parametrized_system} \\end{equation}\nWe claim that for each $t=t_{k}$, the solutions $\\mathbf{z}=\\mathbf{z}_{k}$ of this system for small $t\u0026gt;0$ give a feasible sequence that approaches $\\mathbf{x}^{\\star}$.\nClearly, for $t=0$, the solution of \\eqref{eq:parametrized_system} is $\\mathbf{z}=\\mathbf{x}^{\\star}$, and the Jacobian of $\\mathbf{R}$ at this point is\n\\begin{equation} \\nabla_{\\mathbf{z}} \\mathbf{R}\\left(\\mathbf{x}^{\\star}, 0\\right)=\\begin{bmatrix} \\mathbf{A} \\\\ \\mathbf{Z}^{\\mathrm{T}} \\end{bmatrix}, \\label{eq:jacobian_matrix} \\end{equation}\nwhich is nonsingular by construction of $\\mathbf{Z}$. Hence, according to the implicit function theorem, the system \\eqref{eq:parametrized_system} has a unique solution $\\mathbf{z}_{k}$ for all values of $t_{k}$ sufficiently small. Moreover, we have from \\eqref{eq:parametrized_system} and \\eqref{eq:limiting_direction_conditions} that\n\\begin{equation} \\begin{aligned} i \\in \\mathcal{E} \u0026amp; \\Rightarrow c_{i}\\left(\\mathbf{z}_{k}\\right)=t_{k} \\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d}=0, \\\\ i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \u0026amp; \\Rightarrow c_{i}\\left(\\mathbf{z}_{k}\\right)=t_{k} \\nabla c_{i}^{\\star \\mathrm{T}} \\mathbf{d} \\geq 0, \\end{aligned} \\label{eq:feasibility_proof} \\end{equation}\nso that $\\mathbf{z}_{k}$ is indeed feasible. Also, for any positive value $t=\\bar{t}\u0026gt;0$, we cannot have $\\mathbf{z}(t)=\\mathbf{x}^{\\star}$, since otherwise by substituting $(\\mathbf{z}, t)=\\left(\\mathbf{x}^{\\star}, \\bar{t}\\right)$ into \\eqref{eq:parametrized_system}, we obtain\n\\begin{equation} \\begin{bmatrix} \\mathbf{c}\\left(\\mathbf{x}^{\\star}\\right)-\\bar{t} \\mathbf{A} \\mathbf{d} \\\\ -\\mathbf{Z}^{\\mathrm{T}}(\\bar{t} \\mathbf{d}) \\end{bmatrix}=\\begin{bmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{bmatrix}. \\label{eq:contradiction_system} \\end{equation}\nSince $\\mathbf{c}\\left(\\mathbf{x}^{\\star}\\right)=\\mathbf{0}$ (we have assumed that all constraints are active) and $\\bar{t}\u0026gt;0$, we have from the full rank of the matrix in \\eqref{eq:jacobian_matrix} that $\\mathbf{d}=\\mathbf{0}$, which contradicts $\\left\\|\\mathbf{d}\\right\\|=1$. It follows that $\\mathbf{z}_{k}=\\mathbf{z}\\left(t_{k}\\right) \\neq \\mathbf{x}^{\\star}$ for all $k$.\nIt remains to show that $\\mathbf{d}$ is a limiting direction of $\\left\\{\\mathbf{z}_{k}\\right\\}$. Using the fact that $\\mathbf{R}\\left(\\mathbf{z}_{k}, t_{k}\\right)=\\mathbf{0}$ for all $k$ together with Taylor\u0026rsquo;s theorem, we find that\n\\begin{equation} \\begin{aligned} \\mathbf{0}=\\mathbf{R}\\left(\\mathbf{z}_{k}, t_{k}\\right) \u0026amp; =\\begin{bmatrix} \\mathbf{c}\\left(\\mathbf{z}_{k}\\right)-t_{k} \\mathbf{A} \\mathbf{d} \\\\ \\mathbf{Z}^{\\mathrm{T}}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}-t_{k} \\mathbf{d}\\right) \\end{bmatrix} \\\\ \u0026amp; =\\begin{bmatrix} \\mathbf{A}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right)-t_{k} \\mathbf{A} \\mathbf{d} \\\\ \\mathbf{Z}^{\\mathrm{T}}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}-t_{k} \\mathbf{d}\\right) \\end{bmatrix} \\\\ \u0026amp; =\\begin{bmatrix} \\mathbf{A} \\\\ \\mathbf{Z}^{\\mathrm{T}} \\end{bmatrix}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}-t_{k} \\mathbf{d}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\end{aligned} \\label{eq:taylor_system} \\end{equation}\nBy dividing this expression by $\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|$ and using nonsingularity of the coefficient matrix in the first term, we obtain\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\mathbf{d}_{k}-\\frac{t_{k}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} \\mathbf{d}=\\mathbf{0}, \\quad \\text { where } \\mathbf{d}_{k}=\\frac{\\mathbf{z}_{k}-\\mathbf{x}^{\\star}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|} \\label{eq:direction_limit} \\end{equation}\nSince $\\left\\|\\mathbf{d}_{k}\\right\\|=1$ for all $k$ and since $\\left\\|\\mathbf{d}\\right\\|=1$, we must have\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\frac{t_{k}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}=1 \\label{eq:scaling_limit} \\end{equation}\n(We leave the simple proof by contradiction of this statement as an exercise.) Hence, from \\eqref{eq:direction_limit}, we have $\\lim_{k \\rightarrow \\infty} \\mathbf{d}_{k}=\\mathbf{d}$, as required.\n■ The set of directions defined by \\eqref{eq:limiting_direction_conditions} plays a central role in the optimality conditions, so for future reference we give this set a name and define it formally.\nDefinition 5.7 (Linearized feasible directions)\nGiven a point $\\mathbf{x}^{\\star}$ and the active constraint set $\\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$ defined by \\eqref{eq:active_set}, the set $F_{1}$ is defined by\n\\begin{equation} F_{1}=\\left\\{\\alpha \\mathbf{d} \\mid \\alpha\u0026gt;0, \\quad \\begin{array}{ll} \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}=0, \u0026amp; \\text { for all } i \\in \\mathcal{E} \\\\ \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0, \u0026amp; \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\end{array}\\right\\} \\label{eq:linearized_feasible_set} \\end{equation}\nNote that $F_{1}$ is a cone. In fact, when a constraint qualification is satisfied, $F_{1}$ is the tangent cone to the feasible set at $\\mathbf{x}^{\\star}$.\nIntroducing Lagrange multipliers # lemma 5.1 tells us that when the LICQ holds, the cone $F_{1}$ is simply the set of all positive multiples of all limiting directions of all possible feasible sequences. Therefore, the condition \\eqref{eq:first_order_feasible_condition} of Theorem 5.2 holds if $\\nabla f\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}\u0026lt;0$ for all $\\mathbf{d} \\in F_{1}$. This condition, too, would appear to be impossible to check, since the set $F_{1}$ contains infinitely many vectors in general. The next lemma gives an alternative, practical way to check this condition that makes use of the Lagrange multipliers, the variables $\\lambda_{i}$ that were introduced in the definition of the Lagrangian $\\mathcal{L}$. Lemma 5.2 (Characterization using Lagrange multipliers)\nThere is no direction $\\mathbf{d} \\in F_{1}$ for which $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}\u0026lt;0$ if and only if there exists a vector $\\boldsymbol{\\lambda} \\in \\mathbb{R}^{m}$ with \\begin{equation} \\nabla f^{\\star}=\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i} \\nabla c_{i}^{\\star}=\\mathbf{A}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\boldsymbol{\\lambda}, \\quad \\lambda_{i} \\geq 0 \\text { for } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\label{eq:lagrange_condition} \\end{equation} Proof\nIf we define the cone $N$ by \\begin{equation} N=\\left\\{\\mathbf{s} \\mid \\mathbf{s}=\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i} \\nabla c_{i}^{\\star}, \\quad \\lambda_{i} \\geq 0 \\text { for } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}\\right\\} \\label{eq:cone_n_definition} \\end{equation} then the condition \\eqref{eq:lagrange_condition} is equivalent to $\\nabla f^{\\star} \\in N$. We note first that the set $N$ is closed—a fact that is intuitively clear but nontrivial to prove rigorously. We prove the forward implication by supposing that \\eqref{eq:lagrange_condition} holds and choosing $\\mathbf{d}$ to be any vector satisfying \\eqref{eq:limiting_direction_conditions}. We then have that \\begin{equation} \\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}=\\sum_{i \\in \\mathcal{E}} \\lambda_{i}\\left(\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}\\right)+\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}} \\lambda_{i}\\left(\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}\\right) \\label{eq:forward_implication} \\end{equation} The first summation is zero because $\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}=0$ for $i \\in \\mathcal{E}$, while the second term is nonnegative because $\\lambda_{i} \\geq 0$ and $\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0$ for $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$. Hence $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star} \\geq 0$. For the reverse implication, we show that if $\\nabla f^{\\star}$ does not satisfy \\eqref{eq:lagrange_condition} (that is, $\\nabla f^{\\star} \\notin N$), then we can find a vector $\\mathbf{d}$ for which $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}\u0026lt;0$ and \\eqref{eq:limiting_direction_conditions} holds. Let $\\hat{\\mathbf{s}}$ be the vector in $N$ that is closest to $\\nabla f^{\\star}$. Because $N$ is closed, $\\hat{\\mathbf{s}}$ is well-defined. In fact, $\\hat{\\mathbf{s}}$ solves the constrained optimization problem \\begin{equation} \\min \\left|\\mathbf{s}-\\nabla f^{\\star}\\right|_{2}^{2} \\quad \\text { subject to } \\mathbf{s} \\in N \\label{eq:projection_problem} \\end{equation} Since $\\hat{\\mathbf{s}} \\in N$, we also have $t \\hat{\\mathbf{s}} \\in N$ for all scalars $t \\geq 0$. Since $\\left|t \\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right|_{2}^{2}$ is minimized at $t=1$, we have \\begin{equation} \\begin{aligned} \\left.\\frac{d}{d t}\\left|t \\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right|{2}^{2}\\right|{t=1}=0 \u0026amp; \\Rightarrow\\left.\\left(-2 \\hat{\\mathbf{s}}^{\\mathrm{T}} \\nabla f^{\\star}+2 t \\hat{\\mathbf{s}}^{\\mathrm{T}} \\hat{\\mathbf{s}}\\right)\\right|_{t=1}=0 \\ \u0026amp; \\Rightarrow \\hat{\\mathbf{s}}^{\\mathrm{T}}\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right)=0 \\end{aligned} \\label{eq:orthogonality_condition} \\end{equation} Now, let $\\mathbf{s}$ be any other vector in $N$. Since $N$ is convex, we have by the minimizing property of $\\hat{\\mathbf{s}}$ that \\begin{equation} \\left|\\hat{\\mathbf{s}}+\\theta(\\mathbf{s}-\\hat{\\mathbf{s}})-\\nabla f^{\\star}\\right|{2}^{2} \\geq\\left|\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right|{2}^{2} \\quad \\text { for all } \\theta \\in[0,1] \\label{eq:convexity_property} \\end{equation} and hence \\begin{equation} 2 \\theta(\\mathbf{s}-\\hat{\\mathbf{s}})^{\\mathrm{T}}\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right)+\\theta^{2}\\left|\\mathbf{s}-\\hat{\\mathbf{s}}\\right|_{2}^{2} \\geq 0 \\label{eq:quadratic_expansion} \\end{equation} By dividing this expression by $\\theta$ and taking the limit as $\\theta \\downarrow 0$, we have $(\\mathbf{s}-\\hat{\\mathbf{s}})^{\\mathrm{T}}\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right) \\geq 0$. Therefore, because of \\eqref{eq:orthogonality_condition}, \\begin{equation} \\mathbf{s}^{\\mathrm{T}}\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right) \\geq 0, \\quad \\text { for all } \\mathbf{s} \\in N \\label{eq:separation_property} \\end{equation} We claim now that the vector \\begin{equation} \\mathbf{d}=\\hat{\\mathbf{s}}-\\nabla f^{\\star} \\label{eq:descent_direction} \\end{equation} satisfies both \\eqref{eq:limiting_direction_conditions} and $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}\u0026lt;0$. Note that $\\mathbf{d} \\neq \\mathbf{0}$ because $\\nabla f^{\\star}$ does not belong to the cone $N$. We have from \\eqref{eq:orthogonality_condition} that \\begin{equation} \\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star}=\\mathbf{d}^{\\mathrm{T}}(\\hat{\\mathbf{s}}-\\mathbf{d})=\\left(\\hat{\\mathbf{s}}-\\nabla f^{\\star}\\right)^{\\mathrm{T}} \\hat{\\mathbf{s}}-\\mathbf{d}^{\\mathrm{T}} \\mathbf{d}=-\\left|\\mathbf{d}\\right|_{2}^{2}\u0026lt;0 \\label{eq:descent_property} \\end{equation} so that $\\mathbf{d}$ satisfies the descent property. By making appropriate choices of coefficients $\\lambda_{i}, i=1,2, \\ldots, m$, it is easy to see that \\begin{equation} \\begin{aligned} i \\in \\mathcal{E} \u0026amp; \\Rightarrow \\nabla c_{i}^{\\star} \\in N \\quad \\text { and } \\quad -\\nabla c_{i}^{\\star} \\in N \\ i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \u0026amp; \\Rightarrow \\nabla c_{i}^{\\star} \\in N \\end{aligned} \\label{eq:gradient_membership} \\end{equation} Hence, from \\eqref{eq:separation_property}, we have by substituting $\\mathbf{d}=\\hat{\\mathbf{s}}-\\nabla f^{\\star}$ and the particular choices $\\mathbf{s}=\\nabla c_{i}^{\\star}$ and $\\mathbf{s}=-\\nabla c_{i}^{\\star}$ that \\begin{equation} \\begin{aligned} i \\in \\mathcal{E} \u0026amp; \\Rightarrow \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0 \\quad \\text { and } \\quad -\\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0 \\Rightarrow \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star}=0 \\\\ i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \u0026amp; \\Rightarrow \\mathbf{d}^{\\mathrm{T}} \\nabla c_{i}^{\\star} \\geq 0 \\end{aligned} \\label{eq:direction_conditions_verified} \\end{equation} Therefore, $\\mathbf{d}$ also satisfies \\eqref{eq:limiting_direction_conditions}, so the reverse implication is proved. ■ Proof of the first-order necessary conditions lemma 5.1 and lemma 5.2 can be combined to give the KKT conditions described in Theorem 5.1. Suppose that $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ is a feasible point at which the LICQ holds. The theorem claims that if $\\mathbf{x}^{\\star}$ is a local solution, then there is a vector $\\boldsymbol{\\lambda}^{\\star} \\in \\mathbb{R}^{m}$ that satisfies the KKT conditions. We show first that there are multipliers $\\lambda_{i}, i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$, such that \\eqref{eq:lagrange_condition} is satisfied. Theorem 5.2 tells us that $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star} \\geq 0$ for all vectors $\\mathbf{d}$ that are limiting directions of feasible sequences. From lemma 5.1 , we know that when LICQ holds, the set of all possible limiting directions is exactly the set of vectors that satisfy the conditions \\eqref{eq:limiting_direction_conditions}. By putting these two statements together, we find that all directions $\\mathbf{d}$ that satisfy \\eqref{eq:limiting_direction_conditions} must also have $\\mathbf{d}^{\\mathrm{T}} \\nabla f^{\\star} \\geq 0$. Hence, from lemma 5.2 , we have that there is a vector $\\boldsymbol{\\lambda}$ for which \\eqref{eq:lagrange_condition} holds, as claimed. We now define the vector $\\boldsymbol{\\lambda}^{\\star}$ by \\begin{equation} \\lambda_{i}^{\\star}= \\begin{cases}\\lambda_{i}, \u0026amp; i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\ 0, \u0026amp; \\text { otherwise }\\end{cases} \\label{eq:multiplier_definition} \\end{equation} and show that this choice of $\\boldsymbol{\\lambda}^{\\star}$, together with our local solution $\\mathbf{x}^{\\star}$, satisfies the KKT conditions. We check these conditions in turn.\nThe condition \\eqref{eq:kkt_gradient_zero} follows immediately from \\eqref{eq:lagrange_condition} and the definitions of the Lagrangian function and \\eqref{eq:multiplier_definition} of $\\boldsymbol{\\lambda}^{\\star}$. Since $\\mathbf{x}^{\\star}$ is feasible, the conditions \\eqref{eq:kkt_equality} and \\eqref{eq:kkt_inequality} are satisfied. We have from \\eqref{eq:lagrange_condition} that $\\lambda_{i}^{\\star} \\geq 0$ for $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$, while from \\eqref{eq:multiplier_definition}, $\\lambda_{i}^{\\star}=0$ for $i \\in \\mathcal{I} \\backslash \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$. Hence, $\\lambda_{i}^{\\star} \\geq 0$ for $i \\in \\mathcal{I}$, so that \\eqref{eq:kkt_multiplier_sign} holds. We have for $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$ that $c_{i}\\left(\\mathbf{x}^{\\star}\\right)=0$, while for $i \\in \\mathcal{I} \\backslash \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$, we have $\\lambda_{i}^{\\star}=0$. Hence $\\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{x}^{\\star}\\right)=0$ for $i \\in \\mathcal{I}$, so that \\eqref{eq:kkt_complementarity} is satisfied as well.\nThis completes the proof.\nSecond-order conditions # So far, we have described the first-order conditions—the KKT conditions—which tell us how the first derivatives of $f$ and the active constraints $c_{i}$ are related at $\\mathbf{x}^{\\star}$. When these conditions are satisfied, a move along any vector $\\mathbf{w}$ from $F_{1}$ either increases the first-order approximation to the objective function (that is, $\\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)\u0026gt;0$), or else keeps this value the same (that is, $\\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$).\nWhat implications does optimality have for the second derivatives of $f$ and the constraints $c_{i}$? We see in this section that these derivatives play a \u0026ldquo;tiebreaking\u0026rdquo; role. For the directions $\\mathbf{w} \\in F_{1}$ for which $\\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$, we cannot determine from first derivative information alone whether a move along this direction will increase or decrease the objective function $f$. Second-order conditions examine the second derivative terms in the Taylor series expansions of $f$ and $c_{i}$, to see whether this extra information resolves the issue of increase or decrease in $f$. Essentially, the second-order conditions concern the curvature of the Lagrangian function in the \u0026ldquo;undecided\u0026rdquo; directions—the directions $\\mathbf{w} \\in F_{1}$ for which $\\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=0$.\nSince we are discussing second derivatives, stronger smoothness assumptions are needed here than in the previous sections. For the purpose of this section, $f$ and $c_{i}, i \\in \\mathcal{E} \\cup \\mathcal{I}$, are all assumed to be twice continuously differentiable.\nGiven $F_{1}$ from Definition 5.7 and some Lagrange multiplier vector $\\boldsymbol{\\lambda}^{\\star}$ satisfying the KKT conditions, we define a subset $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ of $F_{1}$ by\n\\begin{equation} F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)=\\left\\{\\mathbf{w} \\in F_{1} \\mid \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}=0, \\text { all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\text { with } \\lambda_{i}^{\\star}\u0026gt;0\\right\\} \\label{eq:f2_definition} \\end{equation}\nEquivalently,\n\\begin{equation} \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\Leftrightarrow \\begin{cases}\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}=0, \u0026amp; \\text { for all } i \\in \\mathcal{E}, \\\\ \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}=0, \u0026amp; \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\text { with } \\lambda_{i}^{\\star}\u0026gt;0, \\\\ \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w} \\geq 0, \u0026amp; \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I} \\text { with } \\lambda_{i}^{\\star}=0 .\\end{cases} \\label{eq:f2_conditions} \\end{equation}\nThe subset $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ contains the directions $\\mathbf{w}$ that tend to \u0026ldquo;adhere\u0026rdquo; to the active inequality constraints for which the Lagrange multiplier component $\\lambda_{i}^{\\star}$ is positive, as well as to the equality constraints. From the definition \\eqref{eq:f2_definition} and the fact that $\\lambda_{i}^{\\star}=0$ for all inactive components $i \\in \\mathcal{I} \\backslash \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$, it follows immediately that\n\\begin{equation} \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\Rightarrow \\lambda_{i}^{\\star} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}=0 \\text { for all } i \\in \\mathcal{E} \\cup \\mathcal{I}. \\label{eq:f2_property} \\end{equation}\nHence, from the first KKT condition \\eqref{eq:kkt_gradient_zero} and the definition of the Lagrangian function, we have that\n\\begin{equation} \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\Rightarrow \\mathbf{w}^{\\mathrm{T}} \\nabla f\\left(\\mathbf{x}^{\\star}\\right)=\\sum_{i \\in \\mathcal{E} \\cup \\mathcal{I}} \\lambda_{i}^{\\star} \\mathbf{w}^{\\mathrm{T}} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)=0 \\label{eq:f2_gradient_zero} \\end{equation}\nHence the set $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ contains directions from $F_{1}$ for which it is not clear from first derivative information alone whether $f$ will increase or decrease.\nThe first theorem defines a necessary condition involving the second derivatives: If $\\mathbf{x}^{\\star}$ is a local solution, then the curvature of the Lagrangian along directions in $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ must be nonnegative.\nTheorem 5.3 (Second-order necessary conditions)\nSuppose that $\\mathbf{x}^{\\star}$ is a local solution and that the LICQ condition is satisfied. Let $\\boldsymbol{\\lambda}^{\\star}$ be a Lagrange multiplier vector such that the KKT conditions are satisfied, and let $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ be defined as above. Then\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w} \\geq 0, \\quad \\text { for all } \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\label{eq:second_order_necessary} \\end{equation}\nProof\nSince $\\mathbf{x}^{\\star}$ is a local solution, all feasible sequences $\\left\\{\\mathbf{z}_{k}\\right\\}$ approaching $\\mathbf{x}^{\\star}$ must have $f\\left(\\mathbf{z}_{k}\\right) \\geq f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k$ sufficiently large. Our approach in this proof is to construct a feasible sequence whose limiting direction is $\\mathbf{w} /\\left\\|\\mathbf{w}\\right\\|$ and show that the property $f\\left(\\mathbf{z}_{k}\\right) \\geq f\\left(\\mathbf{x}^{\\star}\\right)$ implies that \\eqref{eq:second_order_necessary} holds.\nSince $\\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\subset F_{1}$, we can use the technique in the proof of\nlemma 5.1\nto construct a feasible sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$ such that\n\\begin{equation} \\lim_{k \\rightarrow \\infty} \\frac{\\mathbf{z}_{k}-\\mathbf{x}^{\\star}}{\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|}=\\frac{\\mathbf{w}}{\\left\\|\\mathbf{w}\\right\\|} \\label{eq:sequence_limit} \\end{equation}\nIn particular, we have from the construction that\n\\begin{equation} c_{i}\\left(\\mathbf{z}_{k}\\right)=\\frac{t_{k}}{\\left\\|\\mathbf{w}\\right\\|} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w}, \\quad \\text { for all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\label{eq:constraint_values} \\end{equation}\nwhere $\\left\\{t_{k}\\right\\}$ is some sequence of positive scalars decreasing to zero. Moreover, we have that\n\\begin{equation} \\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|=t_{k}+o\\left(t_{k}\\right) \\label{eq:distance_relation} \\end{equation}\nand so by substitution, we obtain\n\\begin{equation} \\mathbf{z}_{k}-\\mathbf{x}^{\\star}=\\frac{t_{k}}{\\left\\|\\mathbf{w}\\right\\|} \\mathbf{w}+o\\left(t_{k}\\right) \\label{eq:difference_expansion} \\end{equation}\nFrom the definition of the Lagrangian and \\eqref{eq:constraint_values}, we have that\n\\begin{equation} \\begin{aligned} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right) \u0026amp;=f\\left(\\mathbf{z}_{k}\\right)-\\sum_{i \\in \\mathcal{E} \\cup \\mathcal{I}} \\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{z}_{k}\\right) \\\\ \u0026amp;=f\\left(\\mathbf{z}_{k}\\right)-\\frac{t_{k}}{\\left\\|\\mathbf{w}\\right\\|} \\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i}^{\\star} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{w} \\\\ \u0026amp;=f\\left(\\mathbf{z}_{k}\\right) \\end{aligned} \\label{eq:lagrangian_simplification} \\end{equation}\nwhere the last equality follows from the critical property \\eqref{eq:f2_property}. On the other hand, we can perform a Taylor series expansion to obtain an estimate of $\\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)$ near $\\mathbf{x}^{\\star}$. By using Taylor\u0026rsquo;s theorem and continuity of the Hessians $\\nabla^{2} f$ and $\\nabla^{2} c_{i}, i \\in \\mathcal{E} \\cup \\mathcal{I}$, we obtain\n\\begin{equation} \\begin{aligned} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)= \u0026amp; \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)+\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\\\ \u0026amp; +\\frac{1}{2}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\end{aligned} \\label{eq:taylor_expansion} \\end{equation}\nBy the complementarity conditions \\eqref{eq:kkt_complementarity} the first term on the right-hand-side of this expression is equal to $f\\left(\\mathbf{x}^{\\star}\\right)$. From \\eqref{eq:kkt_gradient_zero}, the second term is zero. Hence we can rewrite \\eqref{eq:taylor_expansion} as\n\\begin{equation} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)=f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\label{eq:simplified_taylor} \\end{equation}\nBy using \\eqref{eq:difference_expansion} and \\eqref{eq:distance_relation}, we have for the second-order term and the remainder term that\n\\begin{equation} \\begin{aligned} \u0026amp; \\frac{1}{2}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\\\ \u0026amp; =\\frac{1}{2}\\left(t_{k} /\\left\\|\\mathbf{w}\\right\\|\\right)^{2} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}+o\\left(t_{k}^{2}\\right) \\end{aligned} \\label{eq:second_order_term} \\end{equation}\nHence, by substituting this expression together with \\eqref{eq:lagrangian_simplification} into \\eqref{eq:simplified_taylor}, we obtain\n\\begin{equation} f\\left(\\mathbf{z}_{k}\\right)=f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left(t_{k} /\\left\\|\\mathbf{w}\\right\\|\\right)^{2} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}+o\\left(t_{k}^{2}\\right) \\label{eq:objective_expansion} \\end{equation}\nIf $\\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}\u0026lt;0$, then \\eqref{eq:objective_expansion} would imply that $f\\left(\\mathbf{z}_{k}\\right)\u0026lt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k$ sufficiently large, contradicting the fact that $\\mathbf{x}^{\\star}$ is a local solution. Hence, the condition \\eqref{eq:second_order_necessary} must hold, as claimed.\n■ Sufficient conditions are conditions on $f$ and $c_{i}, i \\in \\mathcal{E} \\cup \\mathcal{I}$, that ensure that $\\mathbf{x}^{\\star}$ is a local solution. (They take the opposite tack to necessary conditions, which assume that $\\mathbf{x}^{\\star}$ is a local solution and deduce properties of $f$ and $c_{i}$.) The second-order sufficient condition stated in the next theorem looks very much like the necessary condition just discussed, but it differs in that the constraint qualification is not required, and the inequality in \\eqref{eq:second_order_necessary} is replaced by a strict inequality.\nTheorem 5.4 (Second-order sufficient conditions)\nSuppose that for some feasible point $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ there is a Lagrange multiplier vector $\\boldsymbol{\\lambda}^{\\star}$ such that the KKT conditions are satisfied. Suppose also that\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}\u0026gt;0, \\quad \\text { for all } \\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right), \\mathbf{w} \\neq \\mathbf{0} \\label{eq:second_order_sufficient} \\end{equation}\nThen $\\mathbf{x}^{\\star}$ is a strict local solution.\nProof\nThe result is proved if we can show that for any feasible sequence $\\left\\{\\mathbf{z}_{k}\\right\\}$ approaching $\\mathbf{x}^{\\star}$, we have that $f\\left(\\mathbf{z}_{k}\\right)\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k$ sufficiently large.\nGiven any feasible sequence, we have from\nlemma 5.1\n(i) and\nDefinition 5.7\nthat all its limiting directions $\\mathbf{d}$ satisfy $\\mathbf{d} \\in F_{1}$. Choose a particular limiting direction $\\mathbf{d}$ whose associated subsequence $\\mathcal{S}_{\\mathbf{d}}$ satisfies \\eqref{eq:limiting_direction}. In other words, we have for all $k \\in \\mathcal{S}_{\\mathbf{d}}$ that\n\\begin{equation} \\mathbf{z}_{k}-\\mathbf{x}^{\\star}=\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\label{eq:limiting_direction_expansion} \\end{equation}\nFrom the definition of the Lagrangian, we have that\n\\begin{equation} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)=f\\left(\\mathbf{z}_{k}\\right)-\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{z}_{k}\\right) \\leq f\\left(\\mathbf{z}_{k}\\right) \\label{eq:lagrangian_bound} \\end{equation}\nwhile the Taylor series approximation \\eqref{eq:simplified_taylor} from the proof of Theorem 5.3 continues to hold.\nWe know that $\\mathbf{d} \\in F_{1}$, but suppose first that it is not in $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$. We can then identify some index $j \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\cap \\mathcal{I}$ such that the strict positivity condition\n\\begin{equation} \\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}\u0026gt;0 \\label{eq:strict_positivity} \\end{equation}\nis satisfied, while for the remaining indices $i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)$, we have\n\\begin{equation} \\lambda_{i}^{\\star} \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d} \\geq 0 \\label{eq:nonnegativity} \\end{equation}\nFrom Taylor\u0026rsquo;s theorem and \\eqref{eq:limiting_direction_expansion}, we have for all $k \\in \\mathcal{S}_{\\mathbf{d}}$ and for this particular value of $j$ that\n\\begin{equation} \\begin{aligned} \\lambda_{j}^{\\star} c_{j}\\left(\\mathbf{z}_{k}\\right) \u0026amp; =\\lambda_{j}^{\\star} c_{j}\\left(\\mathbf{x}^{\\star}\\right)+\\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\\\ \u0026amp; =\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\end{aligned} \\label{eq:constraint_taylor} \\end{equation}\nHence, from \\eqref{eq:lagrangian_bound}, we have for $k \\in \\mathcal{S}_{\\mathbf{d}}$ that\n\\begin{equation} \\begin{aligned} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right) \u0026amp; =f\\left(\\mathbf{z}_{k}\\right)-\\sum_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)} \\lambda_{i}^{\\star} c_{i}\\left(\\mathbf{z}_{k}\\right) \\\\ \u0026amp; \\leq f\\left(\\mathbf{z}_{k}\\right)-\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\end{aligned} \\label{eq:lagrangian_inequality} \\end{equation}\nFrom the Taylor series estimate \\eqref{eq:simplified_taylor}, we have meanwhile that\n\\begin{equation} \\mathcal{L}\\left(\\mathbf{z}_{k}, \\boldsymbol{\\lambda}^{\\star}\\right)=f\\left(\\mathbf{x}^{\\star}\\right)+O\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\label{eq:lagrangian_taylor} \\end{equation}\nand by combining with \\eqref{eq:lagrangian_inequality}, we obtain\n\\begin{equation} f\\left(\\mathbf{z}_{k}\\right) \\geq f\\left(\\mathbf{x}^{\\star}\\right)+\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\| \\lambda_{j}^{\\star} \\nabla c_{j}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|\\right) \\label{eq:objective_lower_bound} \\end{equation}\nTherefore, because of \\eqref{eq:strict_positivity}, we have $f\\left(\\mathbf{z}_{k}\\right)\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k \\in \\mathcal{S}_{\\mathbf{d}}$ sufficiently large.\nFor the other case of $\\mathbf{d} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$, we use \\eqref{eq:limiting_direction_expansion}, \\eqref{eq:lagrangian_bound}, and \\eqref{eq:simplified_taylor} to write\n\\begin{equation} \\begin{aligned} f\\left(\\mathbf{z}_{k}\\right) \u0026amp; \\geq f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)\\left(\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right)+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\\\ \u0026amp; =f\\left(\\mathbf{x}^{\\star}\\right)+\\frac{1}{2}\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2} \\mathbf{d}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{d}+o\\left(\\left\\|\\mathbf{z}_{k}-\\mathbf{x}^{\\star}\\right\\|^{2}\\right) \\end{aligned} \\label{eq:second_order_case} \\end{equation}\nBecause of \\eqref{eq:second_order_sufficient}, we again have $f\\left(\\mathbf{z}_{k}\\right)\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k \\in \\mathcal{S}_{\\mathbf{d}}$ sufficiently large.\nSince this reasoning applies to all limiting directions of $\\left\\{\\mathbf{z}_{k}\\right\\}$, and since each element $\\mathbf{z}_{k}$ of the sequence can be assigned to one of the subsequences $\\mathcal{S}_{\\mathbf{d}}$ that converge to one of these limiting directions, we conclude that $f\\left(\\mathbf{z}_{k}\\right)\u0026gt;f\\left(\\mathbf{x}^{\\star}\\right)$ for all $k$ sufficiently large.\n■ Example 3 (Inequality-constrained example, one more time)\nWe now return to the inequality-constrained example to check the second-order conditions. In this problem we have $f(\\mathbf{x})=x_{1}+x_{2}$, $c_{1}(\\mathbf{x})=2-x_{1}^{2}-x_{2}^{2}$, $\\mathcal{E}=\\emptyset$, and $\\mathcal{I}=\\{1\\}$. The Lagrangian is\n\\begin{equation} \\mathcal{L}(\\mathbf{x}, \\lambda)=\\left(x_{1}+x_{2}\\right)-\\lambda_{1}\\left(2-x_{1}^{2}-x_{2}^{2}\\right) \\label{eq:example_lagrangian} \\end{equation}\nand it is easy to show that the KKT conditions are satisfied by $\\mathbf{x}^{\\star}=(-1,-1)^{\\mathrm{T}}$, with $\\lambda_{1}^{\\star}=\\frac{1}{2}$. The Lagrangian Hessian at this point is\n\\begin{equation} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)=\\begin{bmatrix} 2 \\lambda_{1}^{\\star} \u0026amp; 0 \\\\ 0 \u0026amp; 2 \\lambda_{1}^{\\star} \\end{bmatrix}=\\begin{bmatrix} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{bmatrix} \\label{eq:example_hessian} \\end{equation}\nThis matrix is positive definite, that is, it satisfies $\\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}\u0026gt;0$ for all $\\mathbf{w} \\neq \\mathbf{0}$, so it certainly satisfies the conditions of Theorem 5.4. We conclude that $\\mathbf{x}^{\\star}=(-1,-1)^{\\mathrm{T}}$ is a strict local solution. (In fact, it is the global solution of this problem, since this problem is a convex programming problem.)\nExample 4\nFor an example in which the issues are more complex, consider the problem\n\\begin{equation} \\min -0.1\\left(x_{1}-4\\right)^{2}+x_{2}^{2} \\quad \\text { s.t. } \\quad x_{1}^{2}+x_{2}^{2}-1 \\geq 0, \\label{eq:nonconvex_example} \\end{equation}\nin which we seek to minimize a nonconvex function over the exterior of the unit circle. Obviously, the objective function is not bounded below on the feasible region, since we can take the feasible sequence\n\\begin{equation} \\begin{bmatrix} 10 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 20 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 30 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 40 \\\\ 0 \\end{bmatrix}, \\label{eq:unbounded_sequence} \\end{equation}\nand note that $f(\\mathbf{x})$ approaches $-\\infty$ along this sequence. Therefore, no global solution exists, but it may still be possible to identify a strict local solution on the boundary of the constraint. We search for such a solution by using the KKT conditions and the second-order conditions of Theorem 5.4.\nBy defining the Lagrangian for \\eqref{eq:nonconvex_example} in the usual way, it is easy to verify that\n\\begin{equation} \\begin{aligned} \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\lambda) \u0026amp; =\\begin{bmatrix} -0.2\\left(x_{1}-4\\right)-2 \\lambda x_{1} \\\\ 2 x_{2}-2 \\lambda x_{2} \\end{bmatrix}, \\\\ \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\lambda) \u0026amp; =\\begin{bmatrix} -0.2-2 \\lambda \u0026amp; 0 \\\\ 0 \u0026amp; 2-2 \\lambda \\end{bmatrix}. \\end{aligned} \\label{eq:example_derivatives} \\end{equation}\nThe point $\\mathbf{x}^{\\star}=(1,0)^{\\mathrm{T}}$ satisfies the KKT conditions with $\\lambda_{1}^{\\star}=0.3$ and the active set $\\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)=\\{1\\}$. To check that the second-order sufficient conditions are satisfied at this point, we note that\n\\begin{equation} \\nabla c_{1}\\left(\\mathbf{x}^{\\star}\\right)=\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}, \\label{eq:constraint_gradient} \\end{equation}\nso that the space $F_{2}$ defined in \\eqref{eq:f2_definition} is simply\n\\begin{equation} F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)=\\left\\{\\mathbf{w} \\mid w_{1}=0\\right\\}=\\left\\{\\left(0, w_{2}\\right)^{\\mathrm{T}} \\mid w_{2} \\in \\mathbb{R}\\right\\} \\label{eq:f2_example} \\end{equation}\nNow, by substituting $\\mathbf{x}^{\\star}$ and $\\boldsymbol{\\lambda}^{\\star}$ into \\eqref{eq:example_derivatives}, we have for any $\\mathbf{w} \\in F_{2}$ with $\\mathbf{w} \\neq \\mathbf{0}$ that\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}=\\begin{bmatrix} 0 \\\\ w_{2} \\end{bmatrix}^{\\mathrm{T}}\\begin{bmatrix} -0.4 \u0026amp; 0 \\\\ 0 \u0026amp; 1.4 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ w_{2} \\end{bmatrix}=1.4 w_{2}^{2}\u0026gt;0 \\label{eq:positive_definiteness} \\end{equation}\nHence, the second-order sufficient conditions are satisfied, and we conclude from Theorem 5.4 that $(1,0)^{\\mathrm{T}}$ is a strict local solution for \\eqref{eq:nonconvex_example}.\nSecond-order conditions and projected Hessians # The second-order conditions are sometimes stated in a form that is weaker but easier to verify than \\eqref{eq:second_order_necessary} and \\eqref{eq:second_order_sufficient}. This form uses a two-sided projection of the Lagrangian Hessian $\\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right)$ onto subspaces that are related to $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$.\nThe simplest case is obtained when the multiplier $\\boldsymbol{\\lambda}^{\\star}$ that satisfies the KKT conditions is unique (as happens, for example, when the LICQ condition holds) and strict complementarity holds. In this case, the definition \\eqref{eq:f2_definition} of $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ reduces to\n\\begin{equation} F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)=\\operatorname{Null}\\left[\\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}}\\right]_{i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)}=\\operatorname{Null} \\mathbf{A}, \\label{eq:f2_null_space} \\end{equation}\nwhere $\\mathbf{A}$ is defined as in \\eqref{eq:matrix_notation}. In other words, $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ is the null space of the matrix whose rows are the active constraint gradients at $\\mathbf{x}^{\\star}$. As in \\eqref{eq:null_space_basis}, we can define the matrix $\\mathbf{Z}$ with full column rank whose columns span the space $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$. Any vector $\\mathbf{w} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ can be written as $\\mathbf{w}=\\mathbf{Z} \\mathbf{u}$ for some vector $\\mathbf{u}$, and conversely, we have that $\\mathbf{Z} \\mathbf{u} \\in F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ for all $\\mathbf{u}$. Hence, the condition \\eqref{eq:second_order_necessary} in Theorem 5.3 can be restated as\n\\begin{equation} \\mathbf{u}^{\\mathrm{T}} \\mathbf{Z}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{Z} \\mathbf{u} \\geq 0 \\text { for all } \\mathbf{u} \\label{eq:projected_necessary} \\end{equation}\nor, more succinctly,\n\\begin{equation} \\mathbf{Z}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{Z} \\text { is positive semidefinite.} \\label{eq:projected_psd} \\end{equation}\nSimilarly, the condition \\eqref{eq:second_order_sufficient} in Theorem 5.4 can be restated as\n\\begin{equation} \\mathbf{Z}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{Z} \\text { is positive definite.} \\label{eq:projected_pd} \\end{equation}\nWe see at the end of this section that $\\mathbf{Z}$ can be computed numerically, so that the positive (semi)definiteness conditions can actually be checked by forming these matrices and finding their eigenvalues.\nWhen the optimal multiplier $\\boldsymbol{\\lambda}^{\\star}$ is unique but the strict complementarity condition is not satisfied, $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$ is no longer a subspace. Instead, it is an intersection of planes (defined by the first two conditions in \\eqref{eq:f2_conditions}) and half-spaces (defined by the third condition in \\eqref{eq:f2_conditions}). We can still, however, define two subspaces $\\overline{F}_{2}$ and $\\underline{F}_{2}$ that \u0026ldquo;bound\u0026rdquo; $F_{2}$ above and below, in the sense that $\\overline{F}_{2}$ is the smallest-dimensional subspace that contains $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$, while $\\underline{F}_{2}$ is the largest-dimensional subspace contained in $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$. To be precise, we have\n\\begin{equation} \\underline{F}_{2}=\\left\\{\\mathbf{d} \\in F_{1} \\mid \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}=0, \\text { all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right)\\right\\} \\label{eq:f2_lower} \\end{equation}\n\\begin{equation} \\overline{F}_{2}=\\left\\{\\mathbf{d} \\in F_{1} \\mid \\nabla c_{i}\\left(\\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{d}=0, \\text { all } i \\in \\mathcal{A}\\left(\\mathbf{x}^{\\star}\\right) \\text { with } i \\in \\mathcal{E} \\text { or } \\lambda_{i}^{\\star}\u0026gt;0\\right\\} \\label{eq:f2_upper} \\end{equation}\nso that\n\\begin{equation} \\underline{F}_{2} \\subset F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\subset \\overline{F}_{2}. \\label{eq:f2_containment} \\end{equation}\nAs in the previous case, we can construct matrices $\\underline{\\mathbf{Z}}$ and $\\overline{\\mathbf{Z}}$ whose columns span the subspaces $\\underline{F}_{2}$ and $\\overline{F}_{2}$, respectively. If the condition \\eqref{eq:second_order_necessary} of Theorem 5.3 holds, we can be sure that\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w} \\geq 0, \\quad \\text { for all } \\mathbf{w} \\in \\underline{F}_{2}, \\label{eq:lower_bound_condition} \\end{equation}\nbecause $\\underline{F}_{2} \\subset F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right)$. Therefore, an immediate consequence of \\eqref{eq:second_order_necessary} is that the matrix $\\underline{\\mathbf{Z}}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\underline{\\mathbf{Z}}$ is positive semidefinite.\nAnalogously, we have from $F_{2}\\left(\\boldsymbol{\\lambda}^{\\star}\\right) \\subset \\overline{F}_{2}$ that condition \\eqref{eq:second_order_sufficient} is implied by the condition\n\\begin{equation} \\mathbf{w}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\mathbf{w}\u0026gt;0, \\quad \\text { for all } \\mathbf{w} \\in \\overline{F}_{2} \\label{eq:upper_bound_condition} \\end{equation}\nHence, given that the $\\boldsymbol{\\lambda}^{\\star}$ satisfying the KKT conditions is unique, a sufficient condition for \\eqref{eq:second_order_sufficient} is that the matrix $\\overline{\\mathbf{Z}}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\overline{\\mathbf{Z}}$ be positive definite. Again, this condition provides a practical way to check the second-order sufficient condition.\nThe matrices $\\underline{\\mathbf{Z}}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\underline{\\mathbf{Z}}$ and $\\overline{\\mathbf{Z}}^{\\mathrm{T}} \\nabla_{\\mathbf{x}\\mathbf{x}} \\mathcal{L}\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}^{\\star}\\right) \\overline{\\mathbf{Z}}$ are sometimes called two-sided projected Hessian matrices, or simply projected Hessians for short.\nOne way to compute the matrix $\\mathbf{Z}$ (and its counterparts $\\underline{\\mathbf{Z}}$ and $\\overline{\\mathbf{Z}}$) is to apply a QR factorization to the matrix of active constraint gradients whose null space we seek. In the simplest case above (in which the multiplier $\\boldsymbol{\\lambda}^{\\star}$ is unique and strictly complementary), we define $\\mathbf{A}$ as in \\eqref{eq:matrix_notation} and write the QR factorization of $\\mathbf{A}^{\\mathrm{T}}$ as\n\\begin{equation} \\mathbf{A}^{\\mathrm{T}}=\\mathbf{Q}\\begin{bmatrix} \\mathbf{R} \\\\ \\mathbf{0} \\end{bmatrix}=\\begin{bmatrix} \\mathbf{Q}_{1} \u0026amp; \\mathbf{Q}_{2} \\end{bmatrix}\\begin{bmatrix} \\mathbf{R} \\\\ \\mathbf{0} \\end{bmatrix}=\\mathbf{Q}_{1} \\mathbf{R} \\label{eq:qr_factorization} \\end{equation}\nwhere $\\mathbf{R}$ is a square upper triangular matrix, and $\\mathbf{Q}$ is $n \\times n$ orthogonal. If $\\mathbf{R}$ is nonsingular, we can set $\\mathbf{Z}=\\mathbf{Q}_{2}$. If $\\mathbf{R}$ is singular (indicating that the active constraint gradients are linearly dependent), a slight enhancement of this procedure that makes use of column pivoting during the QR procedure can be used to identify $\\mathbf{Z}$.\n"},{"id":15,"href":"/numerical_optimization/docs/lectures/fundamentals/constrained_projected/","title":"5b. Constrained optimization - Projected Gradient Descent","section":"I - Fundamentals","content":" Understanding Saddle Points in Constrained Optimization: From KKT Conditions to Projected Gradient Methods # When we first encounter the Karush-Kuhn-Tucker (KKT) conditions in constrained optimization, they often appear as a collection of mathematical requirements that characterize optimal solutions. However, these conditions actually emerge from a deeper geometric structure that reveals why constrained optimization problems possess fundamentally different mathematical properties than their unconstrained counterparts. This exploration will guide you through understanding how constraint conflicts create saddle point structures in the Lagrangian, and how this mathematical insight leads naturally to practical algorithms.\nThe foundation: KKT conditions and the Lagrangian # Consider a general constrained optimization problem where we seek to minimize an objective function subject to both equality and inequality constraints. The mathematical framework begins with defining our constraint sets and constructing the Lagrangian function that will encode the relationship between our objective and constraints.\nDefinition 5.1 (Constrained optimization problem)\nWe seek to solve: \\begin{equation} \\begin{aligned} \\text{minimize} \\quad \u0026amp; f(\\mathbf{x}) \\\\ \\text{subject to} \\quad \u0026amp; c_i(\\mathbf{x}) = 0, \\quad i \\in \\mathcal{E} \\\\ \u0026amp; c_i(\\mathbf{x}) \\geq 0, \\quad i \\in \\mathcal{I} \\end{aligned} \\label{eq:constrained_problem} \\end{equation} where $\\mathcal{E}$ represents the set of equality constraint indices and $\\mathcal{I}$ represents the set of inequality constraint indices. The Lagrangian function serves as the mathematical bridge that connects our objective function with the constraint structure. Rather than treating constraints as separate mathematical entities, the Lagrangian weaves them together into a single function that encodes the fundamental trade-offs inherent in constrained optimization.\nDefinition 5.2 (Lagrangian function)\nFor the constrained optimization problem in \\eqref{eq:constrained_problem}, the Lagrangian function is defined as: \\begin{equation} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}) = f(\\mathbf{x}) - \\sum_{i \\in \\mathcal{E} \\cup \\mathcal{I}} \\lambda_i c_i(\\mathbf{x}) \\label{eq:lagrangian} \\end{equation} where $\\boldsymbol{\\lambda} = (\\lambda_1, \\lambda_2, \\ldots, \\lambda_m)$ are the Lagrange multipliers associated with the constraints. The KKT conditions emerge as necessary conditions that any optimal solution must satisfy, provided certain regularity assumptions hold. These conditions capture the essential balance that must exist at an optimal point between the desire to improve the objective function and the requirement to respect the constraints.\nTheorem 5.1 (Karush-Kuhn-Tucker necessary conditions)\nIf $\\mathbf{x}^\\star$ is a local solution to \\eqref{eq:constrained_problem} and the Linear Independence Constraint Qualification (LICQ) holds at $\\mathbf{x}^\\star$, then there exists a vector $\\boldsymbol{\\lambda}^\\star$ such that: \\begin{equation} \\begin{aligned} \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}^\\star, \\boldsymbol{\\lambda}^\\star) \u0026amp;= \\mathbf{0} \u0026amp;\u0026amp; \\text{(Stationarity)} \\\\ c_i(\\mathbf{x}^\\star) \u0026amp;= 0, \\quad i \\in \\mathcal{E} \u0026amp;\u0026amp; \\text{(Equality feasibility)} \\\\ c_i(\\mathbf{x}^\\star) \u0026amp;\\geq 0, \\quad i \\in \\mathcal{I} \u0026amp;\u0026amp; \\text{(Inequality feasibility)} \\\\ \\lambda_i^\\star \u0026amp;\\geq 0, \\quad i \\in \\mathcal{I} \u0026amp;\u0026amp; \\text{(Dual feasibility)} \\\\ \\lambda_i^\\star c_i(\\mathbf{x}^\\star) \u0026amp;= 0, \\quad i \\in \\mathcal{E} \\cup \\mathcal{I} \u0026amp;\u0026amp; \\text{(Complementarity)} \\end{aligned} \\label{eq:kkt_conditions} \\end{equation} While these conditions tell us what an optimal solution must look like, they leave a crucial question unanswered: how should we actually optimize the Lagrangian function to find such a solution? This question leads us to discover one of the most elegant structures in mathematical optimization.\nThe emergence of saddle point structure # The key insight that transforms our understanding comes from recognizing that the Lagrangian possesses a special geometric property called a saddle point structure. This property emerges naturally from the mathematical conflict between objectives and constraints, and it explains why constrained optimization requires fundamentally different approaches than unconstrained problems.\nTo understand why this structure arises, let us examine what happens when we attempt different optimization strategies on the Lagrangian. Consider what would occur if we tried to minimize the Lagrangian with respect to both the primal variables $\\mathbf{x}$ and the dual variables $\\boldsymbol{\\lambda}$ simultaneously.\nFor an inequality constraint $c_i(\\mathbf{x}) \\geq 0$, suppose we have a point where $c_i(\\mathbf{x}) \u0026gt; 0$, meaning the constraint is satisfied with some slack. The Lagrangian contains the term $-\\lambda_i c_i(\\mathbf{x})$, which becomes increasingly negative as $\\lambda_i$ increases. If we were minimizing over $\\lambda_i$, this would drive $\\lambda_i$ toward positive infinity, creating an unbounded minimization problem. This mathematical behavior makes no economic sense and violates the dual feasibility requirement $\\lambda_i \\geq 0$.\nThe resolution to this apparent contradiction reveals the fundamental insight: we must maximize over the dual variables rather than minimize. When $c_i(\\mathbf{x}) \u0026gt; 0$ and we maximize over $\\lambda_i \\geq 0$, the maximization process naturally drives $\\lambda_i$ toward zero, which aligns perfectly with the complementarity condition $\\lambda_i c_i(\\mathbf{x}) = 0$.\nThis mathematical necessity gives rise to the saddle point property, which provides both the theoretical foundation and the algorithmic guidance for solving constrained optimization problems.\nTheorem 5.2 (Saddle point characterization)\nA point $(\\mathbf{x}^\\star, \\boldsymbol{\\lambda}^\\star)$ solves the constrained optimization problem \\eqref{eq:constrained_problem} if and only if it constitutes a saddle point of the Lagrangian function: \\begin{equation} \\mathcal{L}(\\mathbf{x}^\\star, \\boldsymbol{\\lambda}) \\leq \\mathcal{L}(\\mathbf{x}^\\star, \\boldsymbol{\\lambda}^\\star) \\leq \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}^\\star) \\label{eq:saddle_point} \\end{equation} for all feasible $\\mathbf{x}$ and all $\\boldsymbol{\\lambda} \\geq 0$. The saddle point inequality \\eqref{eq:saddle_point} encodes a beautiful mathematical principle. The left inequality tells us that $\\mathcal{L}(\\mathbf{x}^\\star, \\boldsymbol{\\lambda})$ achieves its maximum over $\\boldsymbol{\\lambda}$ at $\\boldsymbol{\\lambda}^\\star$, while the right inequality indicates that $\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}^\\star)$ achieves its minimum over $\\mathbf{x}$ at $\\mathbf{x}^\\star$. This creates the characteristic saddle shape: the surface curves downward (like a valley) in the primal direction and upward (like a ridge) in the dual direction.\nIllustrating the saddle point through a concrete example # To make these abstract concepts concrete, let us examine a specific problem that clearly demonstrates how constraint conflicts create saddle point structures. Consider the problem of minimizing $f(x) = -(x-3)^2$ subject to the constraint $x \\geq 1$.\nThis example creates a compelling mathematical conflict. The objective function $f(x) = -(x-3)^2$ wants to make the expression $-(x-3)^2$ as small as possible. Since $(x-3)^2$ is always non-negative, the term $-(x-3)^2$ is always non-positive, reaching its maximum value of zero when $x = 3$. To minimize $-(x-3)^2$, we need $(x-3)^2$ to be as large as possible, which drives $x$ away from 3 toward negative infinity.\nHowever, the constraint $x \\geq 1$ acts as a mathematical barrier that prevents this natural tendency. The objective desperately wants to push $x$ toward $-\\infty$ where $f(x) \\to -\\infty$, but the constraint forces the solution to occur at the boundary $x^\\star = 1$.\nThe Lagrangian for this problem becomes: \\begin{equation} \\mathcal{L}(x,\\lambda) = -(x-3)^2 - \\lambda(x-1) \\label{eq:example_lagrangian} \\end{equation}\nTo find the optimal point, we apply the stationarity condition: \\begin{equation} \\frac{\\partial \\mathcal{L}}{\\partial x} = -2(x-3) - \\lambda = 0 \\label{eq:stationarity_condition} \\end{equation}\nAt the constrained optimum $x^\\star = 1$, this gives us: \\begin{equation} -2(1-3) - \\lambda = 0 \\Rightarrow 4 - \\lambda = 0 \\Rightarrow \\lambda^\\star = 4 \\label{eq:optimal_multiplier} \\end{equation}\nWe can verify the saddle point property by examining cross-sections of the Lagrangian. When we fix $\\lambda = 4$ and vary $x$, we obtain: \\begin{equation} \\mathcal{L}(x,4) = -(x-3)^2 - 4(x-1) = -(x-3)^2 - 4x + 4 \\label{eq:primal_cross_section} \\end{equation}\nThis function has a unique minimum at $x = 1$, confirming that we should minimize over the primal variable. When we fix $x = 1$ and vary $\\lambda$, we get: \\begin{equation} \\mathcal{L}(1,\\lambda) = -(1-3)^2 - \\lambda(1-1) = -4 \\label{eq:dual_cross_section} \\end{equation}\nThe Lagrangian becomes constant with respect to $\\lambda$ when the constraint is exactly satisfied. This apparent insensitivity to $\\lambda$ actually illustrates a profound principle: the dual variable value is uniquely determined by the stationarity requirement, and it encodes the economic value of constraint relaxation.\nThe Lagrange multiplier $\\lambda^\\star = 4$ represents the shadow price of the constraint. If we could relax the constraint from $x \\geq 1$ to $x \\geq 1 - \\epsilon$ for some small $\\epsilon \u0026gt; 0$, the optimal objective value would improve by approximately $4\\epsilon$. We can verify this directly: with the relaxed constraint, the new optimum would be $x^\\star = 1 - \\epsilon$, giving $f(1-\\epsilon) = -((1-\\epsilon)-3)^2 = -(2+\\epsilon)^2 = -4 - 4\\epsilon - \\epsilon^2 \\approx -4 - 4\\epsilon$ for small $\\epsilon$. The improvement of $4\\epsilon$ confirms that $\\lambda^\\star = 4$ correctly captures the economic value of constraint relaxation.\nFrom theory to computation: the projected gradient method # The saddle point structure provides both theoretical insight and algorithmic guidance. Since we need to minimize over primal variables and maximize over dual variables, the natural computational approach involves alternating between these two types of updates. This leads to the projected gradient method, which implements the saddle point structure through iterative optimization.\nTheorem 5.3 (Projected gradient algorithm)\nGiven the Lagrangian $\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda})$ from \\eqref{eq:lagrangian}, the projected gradient method alternates between primal minimization and dual maximization:\nInitialize: $\\mathbf{x}^0$, $\\boldsymbol{\\lambda}^0 \\geq \\mathbf{0}$\nFor $k = 0, 1, 2, \\ldots$ until convergence: \\begin{equation} \\begin{aligned} \\mathbf{x}^{k+1} \u0026amp;= \\mathbf{x}^k - \\alpha_k \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}^k, \\boldsymbol{\\lambda}^k) \\\\ \\boldsymbol{\\lambda}^{k+1} \u0026amp;= \\max(\\mathbf{0}, \\boldsymbol{\\lambda}^k + \\beta_k \\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}(\\mathbf{x}^{k+1}, \\boldsymbol{\\lambda}^k)) \\end{aligned} \\label{eq:projected_gradient} \\end{equation} where $\\alpha_k \u0026gt; 0$ and $\\beta_k \u0026gt; 0$ are step size parameters.\nThe algorithm embodies the saddle point structure through its alternating updates. The primal step performs gradient descent on the Lagrangian with respect to $\\mathbf{x}$, following the downward-curving direction of the saddle surface. The dual step performs projected gradient ascent on the Lagrangian with respect to $\\boldsymbol{\\lambda}$, following the upward-curving direction while maintaining the constraint $\\boldsymbol{\\lambda} \\geq \\mathbf{0}$ through the projection operation $\\max(\\mathbf{0}, \\cdot)$.\nTo understand the specific update formulas, we need to compute the gradients of the Lagrangian. For our general formulation:\n\\begin{equation} \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\nabla f(\\mathbf{x}) - \\sum_{i} \\lambda_i \\nabla c_i(\\mathbf{x}) \\label{eq:primal_gradient} \\end{equation}\n\\begin{equation} \\frac{\\partial \\mathcal{L}}{\\partial \\lambda_i} = -c_i(\\mathbf{x}) \\label{eq:dual_gradient} \\end{equation}\nThese gradients reveal the intuitive behavior of the algorithm. The primal update balances the objective gradient against the weighted constraint gradients, with the dual variables serving as the weights that encode constraint importance. The dual update increases $\\lambda_i$ when the constraint $c_i(\\mathbf{x}) \u0026lt; 0$ is violated and decreases $\\lambda_i$ when $c_i(\\mathbf{x}) \u0026gt; 0$ provides slack, naturally driving the algorithm toward complementarity.\nExercise: constrained optimization with mixed constraints # Let us apply our understanding to a concrete problem that illustrates both the theoretical principles and the algorithmic implementation. This exercise demonstrates how the projected gradient method handles a mixture of equality and inequality constraints.\nProblem Setup # $$ \\begin{aligned} \\text{minimize} \\quad \u0026amp; f(x,y) = (x-2)^2 + (y-2)^2 \\\\ \\text{subject to:} \\quad \u0026amp; g(x,y) = x + y - 2 = 0 \\\\ \u0026amp; h_1(x,y) = x \\geq 0 \\\\ \u0026amp; h_2(x,y) = y \\geq 0 \\end{aligned} $$\nThis problem seeks the point closest to $(2,2)$ that lies on the line $x + y = 2$ while remaining in the first quadrant. The geometric intuition suggests that since the unconstrained minimizer $(2,2)$ lies on the line $x + y = 4$, and our constraint line is $x + y = 2$, the optimal point should be the point on $x + y = 2$ that is closest to $(2,2)$.\nLagrangian Construction # For a problem with equality constraints $g_j(\\mathbf{x}) = 0$ and inequality constraints $h_i(\\mathbf{x}) \\geq 0$, the standard Lagrangian is:\n$$\\mathcal{L}(\\mathbf{x}, \\lambda, \\mu) = f(\\mathbf{x}) - \\sum_j \\lambda_j g_j(\\mathbf{x}) - \\sum_i \\mu_i h_i(\\mathbf{x})$$\nWith our constraints:\nEquality: $g(x,y) = x + y - 2 = 0$ Inequalities: $h_1(x,y) = x \\geq 0$ and $h_2(x,y) = y \\geq 0$ The Lagrangian becomes: $$\\mathcal{L}(x,y,\\lambda,\\mu_1,\\mu_2) = (x-2)^2 + (y-2)^2 - \\lambda(x + y - 2) - \\mu_1 x - \\mu_2 y$$\nGradient Computations # The gradients required for the projected gradient algorithm are:\n$$ \\begin{aligned} \\frac{\\partial \\mathcal{L}}{\\partial x} \u0026amp;= 2(x-2) - \\lambda - \\mu_1 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial y} \u0026amp;= 2(y-2) - \\lambda - \\mu_2 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \u0026amp;= -(x + y - 2) \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\mu_1} \u0026amp;= -x \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\mu_2} \u0026amp;= -y \\end{aligned} $$\nProjected Gradient Updates # The algorithm seeks to minimize $\\mathcal{L}$ with respect to primal variables $(x,y)$ and maximize with respect to dual variables $(\\lambda, \\mu_1, \\mu_2)$. The updates become:\n$$ \\begin{aligned} x^{k+1} \u0026amp;= x^k - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial x} = x^k - \\alpha(2(x^k-2) - \\lambda^k - \\mu_1^k) \\\\ y^{k+1} \u0026amp;= y^k - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial y} = y^k - \\alpha(2(y^k-2) - \\lambda^k - \\mu_2^k) \\\\ \\lambda^{k+1} \u0026amp;= \\lambda^k + \\beta \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = \\lambda^k + \\beta(x^{k+1} + y^{k+1} - 2) \\\\ \\mu_1^{k+1} \u0026amp;= \\Pi_{[0,\\infty)} \\left[\\mu_1^k + \\beta \\frac{\\partial \\mathcal{L}}{\\partial \\mu_1}\\right] = \\max(0, \\mu_1^k - \\beta x^{k+1}) \\\\ \\mu_2^{k+1} \u0026amp;= \\Pi_{[0,\\infty)} \\left[\\mu_2^k + \\beta \\frac{\\partial \\mathcal{L}}{\\partial \\mu_2}\\right] = \\max(0, \\mu_2^k - \\beta y^{k+1}) \\end{aligned} $$\nNote that:\nFor the equality constraint, we do not project $\\lambda^{k+1}$ since equality constraint multipliers can take any real value For inequality constraints, we project $\\mu_i$ onto $[0,\\infty)$ to maintain dual feasibility Analytical Solution # To understand what the algorithm should converge to, let us solve the problem analytically using the KKT conditions. At the optimal point, we need:\nKKT Conditions: # Stationarity: $\\nabla_x \\mathcal{L} = 0$ and $\\nabla_y \\mathcal{L} = 0$ Primal feasibility: $x + y - 2 = 0$, $x \\geq 0$, $y \\geq 0$ Dual feasibility: $\\mu_1 \\geq 0$, $\\mu_2 \\geq 0$ Complementary slackness: $\\mu_1 x = 0$, $\\mu_2 y = 0$ Since the solution likely lies in the interior of the first quadrant (given the symmetry of the problem), we expect both inequality constraints to be inactive, meaning $\\mu_1^\\star = \\mu_2^\\star = 0$.\nWith $\\mu_1^\\star = \\mu_2^\\star = 0$, the stationarity conditions become: $$ \\begin{aligned} 2(x^-2) - \\lambda^ \u0026amp;= 0 \\\\ 2(y^-2) - \\lambda^ \u0026amp;= 0 \\\\ x^* + y^* - 2 \u0026amp;= 0 \\end{aligned} $$\nFrom the first two equations: $2(x^-2) = 2(y^-2)$, which implies $x^* = y^*$.\nSubstituting into the equality constraint: $$x^* + x^* = 2 \\Rightarrow x^* = y^* = 1$$\nThe Lagrange multiplier for the equality constraint is: $$\\lambda^* = 2(1-2) = -2$$\nThe negative value of $\\lambda^*$ indicates that relaxing the constraint (increasing the right-hand side from 2 to $2 + \\epsilon$) would worsen the objective by approximately $2\\epsilon$.\nVerification # We verify that $(x^\\star, y^\\star) = (1, 1)$ with $\\lambda^\\star = -2$ and $\\mu_1^\\star = \\mu_2^\\star = 0$ satisfies all KKT conditions:\nStationarity: $\\frac{\\partial \\mathcal{L}}{\\partial x} = 2(1-2) - (-2) - 0 = -2 + 2 = 0$ ✓ $\\frac{\\partial \\mathcal{L}}{\\partial y} = 2(1-2) - (-2) - 0 = -2 + 2 = 0$ ✓ Primal feasibility: $1 + 1 - 2 = 0$ ✓ $1 \\geq 0$ and $1 \\geq 0$ ✓ Dual feasibility: $\\mu_1^* = 0 \\geq 0$ and $\\mu_2^* = 0 \\geq 0$ ✓ Complementary slackness: $\\mu_1^\\star \\cdot 1 = 0$ and $\\mu_2^\\star \\cdot 1 = 0$ ✓ Geometric Interpretation # The solution $(1,1)$ is indeed the point on the line $x + y = 2$ that is closest to $(2,2)$. The projected gradient algorithm will converge to this solution, automatically determining that the inequality constraints are inactive through the projection steps that drive $\\mu_1$ and $\\mu_2$ toward zero.\n"},{"id":16,"href":"/numerical_optimization/docs/lectures/fundamentals/constrained_optimization_linear/","title":"6. Constrained optimization - Linear programming","section":"I - Fundamentals","content":" Linear programming # Linear programs have a linear objective function and linear constraints, which may include both equalities and inequalities. The feasible set is a polytope, that is, a convex, connected set with flat, polygonal faces. The contours of the objective function are planar. The solution in this case is unique-a single vertex. A simple reorientation of the polytope or the objective gradient $\\mathbf{c}$ could, however, make the solution nonunique; the optimal value $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$ could be the same on an entire edge. In higher dimensions, the set of optimal points can be a single vertex, an edge or face, or even the entire feasible set!\nLinear programs are usually stated and analyzed in the following standard form:\n\\begin{equation} \\min \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}, \\text { subject to } \\mathbf{A} \\mathbf{x}=\\mathbf{b}, \\mathbf{x} \\geq \\mathbf{0}, \\label{eq:standard_form} \\end{equation}\nwhere $\\mathbf{c}$ and $\\mathbf{x}$ are vectors in $\\mathbb{R}^{n}$, $\\mathbf{b}$ is a vector in $\\mathbb{R}^{m}$, and $\\mathbf{A}$ is an $m \\times n$ matrix. Simple devices can be used to transform any linear program to this form. For instance, given the problem\n$$ \\min \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}, \\text { subject to } \\mathbf{A} \\mathbf{x} \\geq \\mathbf{b} $$\n(without any bounds on $\\mathbf{x}$ ), we can convert the inequality constraints to equalities by introducing a vector of surplus variables $\\mathbf{z}$ and writing\n$$ \\min \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}, \\text { subject to } \\mathbf{A} \\mathbf{x}-\\mathbf{z}=\\mathbf{b}, \\mathbf{z} \\geq \\mathbf{0} $$\nThis form is still not quite standard, since not all the variables are constrained to be nonnegative. We deal with this by splitting $\\mathbf{x}$ into its nonnegative and nonpositive parts, $\\mathbf{x}=\\mathbf{x}^{+}-\\mathbf{x}^{-}$, where $\\mathbf{x}^{+}=\\max (\\mathbf{x}, \\mathbf{0}) \\geq \\mathbf{0}$ and $\\mathbf{x}^{-}=\\max (-\\mathbf{x}, \\mathbf{0}) \\geq \\mathbf{0}$. The problem can now be written as\n$$ \\min \\begin{bmatrix} \\mathbf{c} \\\\\\ -\\mathbf{c} \\\\\\ \\mathbf{0} \\end{bmatrix}^{\\mathrm{T}}\\begin{bmatrix} \\mathbf{x}^{+} \\\\\\ \\mathbf{x}^{-} \\\\\\ \\mathbf{z} \\end{bmatrix}, \\text { s.t. }\\begin{bmatrix}\\mathbf{A} \u0026amp; -\\mathbf{A} \u0026amp; -\\mathbf{I}\\end{bmatrix}\\begin{bmatrix} \\mathbf{x}^{+} \\\\\\ \\mathbf{x}^{-} \\\\\\ \\mathbf{z} \\end{bmatrix}=\\mathbf{b},\\begin{bmatrix} \\mathbf{x}^{+} \\\\\\ \\mathbf{x}^{-} \\\\\\ \\mathbf{z} \\end{bmatrix} \\geq \\mathbf{0} $$\nwhich clearly has the same form as \\eqref{eq:standard_form}. Inequality constraints of the form $\\mathbf{x} \\leq \\mathbf{u}$ or $\\mathbf{A} \\mathbf{x} \\leq \\mathbf{b}$ can be dealt with by adding slack variables to make up the difference between the left- and right-hand-sides. Hence\n\\begin{align} \\mathbf{x} \u0026amp; \\leq \\mathbf{u} \\Leftrightarrow \\mathbf{x}+\\mathbf{w}=\\mathbf{u}, \\mathbf{w} \\geq \\mathbf{0} \\\\\\ \\mathbf{A} \\mathbf{x} \u0026amp; \\leq \\mathbf{b} \\Leftrightarrow \\mathbf{A} \\mathbf{x}+\\mathbf{y}=\\mathbf{b}, \\mathbf{y} \\geq \\mathbf{0} \\end{align}\nWe can also convert a \u0026ldquo;maximize\u0026rdquo; objective $\\max \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$ into the \u0026ldquo;minimize\u0026rdquo; form of \\eqref{eq:standard_form} by simply negating $\\mathbf{c}$ to obtain $\\min (-\\mathbf{c})^{\\mathrm{T}} \\mathbf{x}$.\nMany linear programs arise from models of transshipment and distribution networks. These problems have much additional structure in their constraints; special-purpose simplex algorithms that exploit this structure are highly efficient. We do not discuss these network-flow problems further in this lecture, except to note that the subject is important and complex, and that a number of excellent texts are available (see, for example, Ahuja, Magnanti, and Orlin [1]).\nFor the standard formulation \\eqref{eq:standard_form}, we will assume throughout that $m\u0026lt;n$. Otherwise, the system $\\mathbf{A} \\mathbf{x}=\\mathbf{b}$ contains redundant rows, is infeasible, or defines a unique point. When $m \\geq n$, factorizations such as the QR or LU factorization can be used to transform the system $\\mathbf{A} \\mathbf{x}=\\mathbf{b}$ to one with a coefficient matrix of full row rank and, in some cases, decide that the feasible region is empty or consists of a single point.\nOptimality and duality # Optimality conditions # Optimality conditions for the problem \\eqref{eq:standard_form} can be derived from the theory of the previous chapter. Only the first-order conditions-the Karush-Kuhn-Tucker (KKT) conditions-are needed. Convexity of the problem ensures that these conditions are sufficient for a global minimum, as we show below by a simple argument. (We do not need to refer to the second-order conditions from the previous chapter, which are not informative in any case because the Hessian of the Lagrangian for \\eqref{eq:standard_form} is zero.)\nThe tools we developed in the previous chapter make derivation of optimality and duality theory for linear programming much easier than in other treatments of the subject, where this theory has to be developed more or less from scratch.\nThe KKT conditions follow from this theorem. As stated in the previous chapter, this theorem requires linear independence of the active constraint gradients (LICQ). However, as we showed in the section on constraint qualifications, the result continues to hold for dependent constraints, provided that they are linear, as is the case here.\nWe partition the Lagrange multipliers for the problem \\eqref{eq:standard_form} into two vectors $\\boldsymbol{\\pi}$ and $\\mathbf{s}$, where $\\boldsymbol{\\pi} \\in \\mathbb{R}^{m}$ is the multiplier vector for the equality constraints $\\mathbf{A} \\mathbf{x}=\\mathbf{b}$, while $\\mathbf{s} \\in \\mathbb{R}^{n}$ is the multiplier vector for the bound constraints $\\mathbf{x} \\geq \\mathbf{0}$. Using the definition from the previous chapter, we can write the Lagrangian function for \\eqref{eq:standard_form} as\n\\begin{equation} \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\pi}, \\mathbf{s})=\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}-\\boldsymbol{\\pi}^{\\mathrm{T}}(\\mathbf{A} \\mathbf{x}-\\mathbf{b})-\\mathbf{s}^{\\mathrm{T}} \\mathbf{x} . \\label{eq:lp_lagrangian} \\end{equation}\nApplying this theorem, we find that the first-order necessary conditions for $\\mathbf{x}^{\\star}$ to be a solution of \\eqref{eq:standard_form} are that there exist vectors $\\boldsymbol{\\pi}$ and $\\mathbf{s}$ such that\n\\begin{align} \\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi}+\\mathbf{s} \u0026amp; =\\mathbf{c}, \\label{eq:lp_kkt_gradient} \\\\\\ \\mathbf{A} \\mathbf{x} \u0026amp; =\\mathbf{b}, \\label{eq:lp_kkt_equality} \\\\\\ \\mathbf{x} \u0026amp; \\geq \\mathbf{0}, \\label{eq:lp_kkt_primal_feasible} \\\\\\ \\mathbf{s} \u0026amp; \\geq \\mathbf{0}, \\label{eq:lp_kkt_dual_feasible} \\\\\\ x_{i} s_{i} \u0026amp; =0, \\quad i=1,2, \\ldots, n . \\label{eq:lp_kkt_complementarity} \\end{align}\nThe complementarity condition \\eqref{eq:lp_kkt_complementarity}, which essentially says that at least one of the components $x_{i}$ and $s_{i}$ must be zero for each $i=1,2, \\ldots, n$, is often written in the alternative form $\\mathbf{x}^{\\mathrm{T}} \\mathbf{s}=0$. Because of the nonnegativity conditions \\eqref{eq:lp_kkt_primal_feasible}, \\eqref{eq:lp_kkt_dual_feasible}, the two forms are identical.\nLet $\\left(\\mathbf{x}^{\\star}, \\boldsymbol{\\pi}^{\\star}, \\mathbf{s}^{\\star}\\right)$ denote a vector triple that satisfies \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity}. By combining the three equalities \\eqref{eq:lp_kkt_gradient}, \\eqref{eq:lp_kkt_dual_feasible}, and \\eqref{eq:lp_kkt_complementarity}, we find that\n\\begin{equation} \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{\\star}=\\left(\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi}^{\\star}+\\mathbf{s}^{\\star}\\right)^{\\mathrm{T}} \\mathbf{x}^{\\star}=\\left(\\mathbf{A} \\mathbf{x}^{\\star}\\right)^{\\mathrm{T}} \\boldsymbol{\\pi}^{\\star}=\\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}^{\\star} . \\label{eq:primal_dual_equality} \\end{equation}\nAs we shall see in a moment, $\\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}$ is the objective function for the dual problem to \\eqref{eq:standard_form}, so \\eqref{eq:primal_dual_equality} indicates that the primal and dual objectives are equal for vector triples $(\\mathbf{x}, \\boldsymbol{\\pi}, \\mathbf{s})$ that satisfy \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity}.\nIt is easy to show directly that the conditions \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity} are sufficient for $\\mathbf{x}^{\\star}$ to be a global solution of \\eqref{eq:standard_form}. Let $\\overline{\\mathbf{x}}$ be any other feasible point, so that $\\mathbf{A} \\overline{\\mathbf{x}}=\\mathbf{b}$ and $\\overline{\\mathbf{x}} \\geq \\mathbf{0}$. Then\n\\begin{equation} \\mathbf{c}^{\\mathrm{T}} \\overline{\\mathbf{x}}=\\left(\\mathbf{A} \\boldsymbol{\\pi}^{\\star}+\\mathbf{s}^{\\star}\\right)^{\\mathrm{T}} \\overline{\\mathbf{x}}=\\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}^{\\star}+\\overline{\\mathbf{x}}^{\\mathrm{T}} \\mathbf{s}^{\\star} \\geq \\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}^{\\star}=\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{\\star} \\label{eq:sufficiency_proof} \\end{equation}\nWe have used \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity} and \\eqref{eq:primal_dual_equality} here; the inequality relation follows trivially from $\\overline{\\mathbf{x}} \\geq \\mathbf{0}$ and $\\mathbf{s}^{\\star} \\geq \\mathbf{0}$. The inequality \\eqref{eq:sufficiency_proof} tells us that no other feasible point can have a lower objective value than $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{\\star}$. We can say more: The feasible point $\\overline{\\mathbf{x}}$ is optimal if and only if\n$$ \\overline{\\mathbf{x}}^{\\mathrm{T}} \\mathbf{s}^{\\star}=0 $$\nsince otherwise the inequality in \\eqref{eq:sufficiency_proof} is strict. In other words, when $s_{i}^{\\star}\u0026gt;0$, then we must have $\\overline{x}_{i}=0$ for all solutions $\\overline{\\mathbf{x}}$ of \\eqref{eq:standard_form}.\nThe dual problem # Given the data $\\mathbf{c}$, $\\mathbf{b}$, and $\\mathbf{A}$, which define the problem \\eqref{eq:standard_form}, we can define another, closely related, problem as follows:\n\\begin{equation} \\max \\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}, \\quad \\text { subject to } \\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi} \\leq \\mathbf{c} . \\label{eq:dual_problem} \\end{equation}\nThis problem is called the dual problem for \\eqref{eq:standard_form}. In contrast, \\eqref{eq:standard_form} is often referred to as the primal.\nThe primal and dual problems are two sides of the same coin, as we see when we write down the KKT conditions for \\eqref{eq:dual_problem}. Let us first rewrite \\eqref{eq:dual_problem} in the form\n$$ \\min -\\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi} \\quad \\text { subject to } \\mathbf{c}-\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi} \\geq \\mathbf{0} $$\nto fit the formulation from the previous chapter. By using $\\mathbf{x} \\in \\mathbb{R}^{n}$ to denote the Lagrange multipliers for the constraints $\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi} \\leq \\mathbf{c}$, we write the Lagrangian function as\n$$ \\overline{\\mathcal{L}}(\\boldsymbol{\\pi}, \\mathbf{x})=-\\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}-\\mathbf{x}^{\\mathrm{T}}\\left(\\mathbf{c}-\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi}\\right) . $$\nNoting again that the conclusions of this theorem continue to hold if the linear independence assumption is replaced by linearity of all constraints, we find the first-order necessary condition for $\\boldsymbol{\\pi}$ to be optimal for \\eqref{eq:dual_problem} to be that there exist a vector $\\mathbf{x}$ such that\n\\begin{align} \\mathbf{A} \\mathbf{x} \u0026amp; =\\mathbf{b}, \\label{eq:dual_kkt_a} \\\\\\ \\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi} \u0026amp; \\leq \\mathbf{c}, \\label{eq:dual_kkt_b} \\\\\\ \\mathbf{x} \u0026amp; \\geq \\mathbf{0}, \\label{eq:dual_kkt_c} \\\\\\ x_{i}\\left(\\mathbf{c}-\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi}\\right)_{i} \u0026amp; =0, \\quad i=1,2, \\ldots, n . \\label{eq:dual_kkt_d} \\end{align}\nIf we define $\\mathbf{s}=\\mathbf{c}-\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi}$ and substitute in \\eqref{eq:dual_kkt_a}-\\eqref{eq:dual_kkt_d}, we find that the conditions \\eqref{eq:dual_kkt_a}-\\eqref{eq:dual_kkt_d} and \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity} are identical! The optimal Lagrange multipliers $\\boldsymbol{\\pi}$ in the primal problem are the optimal variables in the dual problem, while the optimal Lagrange multipliers $\\mathbf{x}$ in the dual problem are the optimal variables in the primal problem.\nThe primal-dual relationship is symmetric; by taking the dual of the dual, we recover the primal. To see this, we restate \\eqref{eq:dual_problem} in standard form by introducing the slack vector $\\mathbf{s}$ (so that $\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi}+\\mathbf{s}=\\mathbf{c}$ ) and splitting the unbounded variables $\\boldsymbol{\\pi}$ as $\\boldsymbol{\\pi}=\\boldsymbol{\\pi}^{+}-\\boldsymbol{\\pi}^{-}$, where $\\boldsymbol{\\pi}^{+} \\geq \\mathbf{0}$, and $\\boldsymbol{\\pi}^{-} \\geq \\mathbf{0}$. We can now write the dual as\n$$ \\min \\begin{bmatrix} -\\mathbf{b} \\\\\\ \\mathbf{b} \\\\\\ \\mathbf{0} \\end{bmatrix}^{\\mathrm{T}}\\begin{bmatrix} \\boldsymbol{\\pi}^{+} \\\\\\ \\boldsymbol{\\pi}^{-} \\\\\\ \\mathbf{s} \\end{bmatrix} \\text { s.t. }\\begin{bmatrix}\\mathbf{A}^{\\mathrm{T}} \u0026amp; -\\mathbf{A}^{\\mathrm{T}} \u0026amp; \\mathbf{I}\\end{bmatrix}\\begin{bmatrix} \\boldsymbol{\\pi}^{+} \\\\\\ \\boldsymbol{\\pi}^{-} \\\\\\ \\mathbf{s} \\end{bmatrix}=\\mathbf{c},\\begin{bmatrix} \\boldsymbol{\\pi}^{+} \\\\\\ \\boldsymbol{\\pi}^{-} \\\\\\ \\mathbf{s} \\end{bmatrix} \\geq \\mathbf{0} $$\nwhich clearly has the standard form \\eqref{eq:standard_form}. The dual of this problem is now\n$$ \\max \\mathbf{c}^{\\mathrm{T}} \\mathbf{z} \\text { subject to }\\begin{bmatrix} \\mathbf{A} \\\\\\ -\\mathbf{A} \\\\\\ \\mathbf{I} \\end{bmatrix} \\mathbf{z} \\leq\\begin{bmatrix} -\\mathbf{b} \\\\\\ \\mathbf{b} \\\\\\ \\mathbf{0} \\end{bmatrix} $$\nNow $\\mathbf{A} \\mathbf{z} \\leq-\\mathbf{b}$ and $-\\mathbf{A} \\mathbf{z} \\leq \\mathbf{b}$ together imply that $\\mathbf{A} \\mathbf{z}=-\\mathbf{b}$, so we obtain the equivalent problem\n$$ \\min -\\mathbf{c}^{\\mathrm{T}} \\mathbf{z} \\text { subject to } \\mathbf{A} \\mathbf{z}=-\\mathbf{b}, \\mathbf{z} \\leq \\mathbf{0} $$\nBy making the identification $\\mathbf{z}=-\\mathbf{x}$, we recover \\eqref{eq:standard_form}, as claimed.\nGiven a feasible vector $\\mathbf{x}$ for the primal-that is, $\\mathbf{A} \\mathbf{x}=\\mathbf{b}$ and $\\mathbf{x} \\geq \\mathbf{0}$-and a feasible point $(\\boldsymbol{\\pi}, \\mathbf{s})$ for the dual-that is, $\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi}+\\mathbf{s}=\\mathbf{c}, \\mathbf{s} \\geq \\mathbf{0}$-we have as in \\eqref{eq:sufficiency_proof} that\n\\begin{equation} 0 \\leq \\mathbf{x}^{\\mathrm{T}} \\mathbf{s}=\\mathbf{x}^{\\mathrm{T}}\\left(\\mathbf{c}-\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi}\\right)=\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}-\\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi} \\label{eq:weak_duality} \\end{equation}\nTherefore, we have $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x} \\geq \\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}$ when both the primal and dual variables are feasible-the dual objective function is a lower bound on the primal objective function. At a solution, the gap between primal and dual shrinks to zero, as we show in the following theorem.\nTheorem 6.1 (Duality theorem of linear programming)\n(i) If either problem \\eqref{eq:standard_form} or \\eqref{eq:dual_problem} has a solution with finite optimal objective value, then so does the other, and the objective values are equal. (ii) If either problem \\eqref{eq:standard_form} or \\eqref{eq:dual_problem} has an unbounded objective, then the other problem has no feasible points. Proof\nFor (i), suppose that \\eqref{eq:standard_form} has a finite optimal solution. Then because of this theorem, there are vectors $\\boldsymbol{\\pi}$ and $\\mathbf{s}$ such that $(\\mathbf{x}, \\boldsymbol{\\pi}, \\mathbf{s})$ satisfies \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity}. Since \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity} and \\eqref{eq:dual_kkt_a}-\\eqref{eq:dual_kkt_d} are equivalent, it follows that $\\boldsymbol{\\pi}$ is a solution of the dual problem \\eqref{eq:dual_problem}, since there exists a vector $\\mathbf{x}$ that satisfies \\eqref{eq:dual_kkt_a}-\\eqref{eq:dual_kkt_d}. Because $\\mathbf{x}^{\\mathrm{T}} \\mathbf{s}=0$, it follows from \\eqref{eq:weak_duality} that $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}=\\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}$, so the optimal objective values are equal.\nWe can make a symmetric argument if we start by assuming that the dual problem \\eqref{eq:dual_problem} has a solution.\nFor (ii), suppose that the primal objective value is unbounded below. Then there must exist a direction $\\mathbf{d} \\in \\mathbb{R}^{n}$ along which $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$ decreases without violating feasibility. That is,\n$$ \\mathbf{c}^{\\mathrm{T}} \\mathbf{d}\u0026lt;0, \\quad \\mathbf{A} \\mathbf{d}=\\mathbf{0}, \\quad \\mathbf{d} \\geq \\mathbf{0} $$\nSuppose now that there does exist a feasible point $\\boldsymbol{\\pi}$ for the dual problem \\eqref{eq:dual_problem}, that is $\\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi} \\leq \\mathbf{c}$. Multiplying from the left by $\\mathbf{d}^{\\mathrm{T}}$, using the nonnegativity of $\\mathbf{d}$, we obtain\n$$ 0=\\mathbf{d}^{\\mathrm{T}} \\mathbf{A}^{\\mathrm{T}} \\boldsymbol{\\pi} \\leq \\mathbf{d}^{\\mathrm{T}} \\mathbf{c}\u0026lt;0 $$\ngiving a contradiction.\nAgain, we can make a symmetric argument to prove (ii) if we start by assuming that the dual objective is unbounded below.\n■ As we showed in the discussion following this theorem, the multiplier values $\\boldsymbol{\\pi}$ and $\\mathbf{s}$ for \\eqref{eq:standard_form} tell us how sensitive the optimal objective value is to perturbations in the constraints. In fact, the process of finding $(\\boldsymbol{\\pi}, \\mathbf{s})$ for a given optimal $\\mathbf{x}$ is often called sensitivity analysis. We can make a simple direct argument to illustrate this dependence. If a small change $\\Delta \\mathbf{b}$ is made to the vector $\\mathbf{b}$ (the right-hand-side in \\eqref{eq:standard_form} and objective gradient in \\eqref{eq:dual_problem}), then we would usually expect small perturbations in the primal and dual solutions. If these perturbations $(\\Delta \\mathbf{x}, \\Delta \\boldsymbol{\\pi}, \\Delta \\mathbf{s})$ are small enough, we know that provided the problem is not degenerate (defined below), the vectors $\\Delta \\mathbf{s}$ and $\\Delta \\mathbf{x}$ have zeros in the same locations as $\\mathbf{s}$ and $\\mathbf{x}$, respectively. Since $\\mathbf{x}$ and $\\mathbf{s}$ are complementary (see \\eqref{eq:lp_kkt_complementarity}), it follows that\n$$ \\mathbf{x}^{\\mathrm{T}} \\mathbf{s}=\\mathbf{x}^{\\mathrm{T}} \\Delta \\mathbf{s}=(\\Delta \\mathbf{x})^{\\mathrm{T}} \\mathbf{s}=(\\Delta \\mathbf{x})^{\\mathrm{T}} \\Delta \\mathbf{s}=0 $$\nNow we have from the duality theorem that\n$$ \\mathbf{c}^{\\mathrm{T}}(\\mathbf{x}+\\Delta \\mathbf{x})=(\\mathbf{b}+\\Delta \\mathbf{b})^{\\mathrm{T}}(\\boldsymbol{\\pi}+\\Delta \\boldsymbol{\\pi}) $$\nSince\n$$ \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}=\\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}, \\quad \\mathbf{A}(\\mathbf{x}+\\Delta \\mathbf{x})=\\mathbf{b}+\\Delta \\mathbf{b}, \\quad \\mathbf{A}^{\\mathrm{T}} \\Delta \\boldsymbol{\\pi}=-\\Delta \\mathbf{s} $$\nwe have\n\\begin{align} \\mathbf{c}^{\\mathrm{T}} \\Delta \\mathbf{x} \u0026amp; =(\\mathbf{b}+\\Delta \\mathbf{b})^{\\mathrm{T}} \\Delta \\boldsymbol{\\pi}+\\Delta \\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi} \\\\\\ \u0026amp; =(\\mathbf{x}+\\Delta \\mathbf{x})^{\\mathrm{T}} \\mathbf{A}^{\\mathrm{T}} \\Delta \\boldsymbol{\\pi}+\\Delta \\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi} \\\\\\ \u0026amp; =-(\\mathbf{x}+\\Delta \\mathbf{x})^{\\mathrm{T}} \\Delta \\mathbf{s}+\\Delta \\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi}=\\Delta \\mathbf{b}^{\\mathrm{T}} \\boldsymbol{\\pi} . \\end{align}\nIn particular, if $\\Delta \\mathbf{b}=\\epsilon \\mathbf{e}_{j}$, where $\\mathbf{e}_{j}$ is the $j$ th unit vector in $\\mathbb{R}^{m}$, we have for all $\\epsilon$ sufficiently small that\n$$ \\mathbf{c}^{\\mathrm{T}} \\Delta \\mathbf{x}=\\epsilon \\pi_{j} $$\nThat is, the change in primal objective is linear in the value of $\\pi_{j}$ for small perturbations in the components of the right-hand-side $b_{j}$.\nGeometry of the feasible set # Basic feasible points # We assume for the remainder of the chapter that\n$$ \\text { The matrix } \\mathbf{A} \\text { in } \\eqref{eq:standard_form} \\text{ has full row rank. } $$\nIn practice, a preprocessing phase is applied to the user-supplied data to remove some redundancies from the given constraints and eliminate some of the variables. Reformulation by adding slack, surplus, and artificial variables is also used to force $\\mathbf{A}$ to satisfy this property.\nSuppose that $\\mathbf{x}$ is a feasible point with at most $m$ nonzero components. Suppose, too, that we can identify a subset $\\mathcal{B}(\\mathbf{x})$ of the index set $\\{1,2, \\ldots, n\\}$ such that\n$\\mathcal{B}(\\mathbf{x})$ contains exactly $m$ indices; $i \\notin \\mathcal{B}(\\mathbf{x}) \\Rightarrow x_{i}=0$; the $m \\times m$ matrix $\\mathbf{B}$ defined by $$ \\mathbf{B}=\\left[\\mathbf{A}_{i}\\right]_{i \\in \\mathcal{B}(\\mathbf{x})} $$\nis nonsingular, where $\\mathbf{A}_{i}$ is the $i$ th column of $\\mathbf{A}$.\nDefinition 6.1 (Basic feasible point)\nIf all these conditions are true, we call $\\mathbf{x}$ a basic feasible point for \\eqref{eq:standard_form}. The simplex method generates a sequence of iterates $\\mathbf{x}^{k}$ all of which are basic feasible points. Since we want the iterates to converge to a solution of \\eqref{eq:standard_form}, the simplex strategy will make sense only if (a) the problem has basic feasible points; and (b) at least one such point is a basic optimal point, that is, a solution of \\eqref{eq:standard_form} that is also a basic feasible point.\nHappily, both (a) and (b) are true under minimal assumptions.\nTheorem 6.2 (Fundamental theorem of linear programming)\n(i) If there is a feasible point for \\eqref{eq:standard_form}, then there is a basic feasible point. (ii) If \\eqref{eq:standard_form} has solutions, then at least one such solution is a basic optimal point. (iii) If \\eqref{eq:standard_form} is feasible and bounded, then it has an optimal solution. Proof\nAmong all feasible vectors $\\mathbf{x}$, choose one with the minimal number of nonzero components, and denote this number by $p$. Without loss of generality, assume that the nonzeros are $x_{1}, x_{2}, \\ldots, x_{p}$, so we have\n$$ \\sum_{i=1}^{p} \\mathbf{A}_{i} x_{i}=\\mathbf{b} $$\nSuppose first that the columns $\\mathbf{A}_{1}, \\mathbf{A}_{2}, \\ldots, \\mathbf{A}_{p}$ are linearly dependent. Then we can express one of them ($\\mathbf{A}_{p}$, say) in terms of the others, and write\n$$ \\mathbf{A}_{i}=\\sum_{i=1}^{p-1} \\mathbf{A}_{i} z_{i}, $$\nfor some scalars $z_{1}, z_{2}, \\ldots, z_{p-1}$. It is easy to check that the vector\n$$ \\mathbf{x}(\\epsilon)=\\mathbf{x}+\\epsilon\\left(z_{1}, z_{2}, \\ldots, z_{p-1},-1,0,0, \\ldots, 0\\right)^{\\mathrm{T}}=\\mathbf{x}+\\epsilon \\mathbf{z} $$\nsatisfies $\\mathbf{A} \\mathbf{x}(\\epsilon)=\\mathbf{b}$ for any scalar $\\epsilon$. In addition, since $x_{i}\u0026gt;0$ for $i=1,2, \\ldots, p$, we also have $x_{i}(\\epsilon)\u0026gt;0$ for the same indices $i=1,2, \\ldots, p$ and all $\\epsilon$ sufficiently small in magnitude. However, there is a value $\\bar{\\epsilon} \\in\\left(0, x_{p}\\right]$ such that $x_{i}(\\bar{\\epsilon})=0$ for some $i=1,2, \\ldots, p$. Hence, $\\mathbf{x}(\\bar{\\epsilon})$ is feasible and has at most $p-1$ nonzero components, contradicting our choice of $p$ as the minimal number of nonzeros.\nTherefore, columns $\\mathbf{A}_{1}, \\mathbf{A}_{2}, \\ldots, \\mathbf{A}_{p}$ must be linearly independent, and so $p \\leq m$. If $p=m$, we are done, since then $\\mathbf{x}$ is a basic feasible point and $\\mathcal{B}(\\mathbf{x})$ is simply $\\{1,2, \\ldots, m\\}$. Otherwise, $p\u0026lt;m$, and because $\\mathbf{A}$ has full row rank, we can choose $m-p$ columns from among $\\mathbf{A}_{p+1}, \\mathbf{A}_{p+2}, \\ldots, \\mathbf{A}_{n}$ to build up a set of $m$ linearly independent vectors. We construct $\\mathcal{B}(\\mathbf{x})$ by adding the corresponding indices to $\\{1,2, \\ldots, p\\}$. The proof of (i) is complete.\nThe proof of (ii) is quite similar. Let $\\mathbf{x}^{\\star}$ be a solution with a minimal number of nonzero components $p$, and assume again that $x_{1}^{\\star}, x_{2}^{\\star}, \\ldots, x_{p}^{\\star}$ are the nonzeros. If the columns $\\mathbf{A}_{1}, \\mathbf{A}_{2}, \\ldots, \\mathbf{A}_{p}$ are linearly dependent, we define\n$$ \\mathbf{x}^{\\star}(\\epsilon)=\\mathbf{x}^{\\star}+\\epsilon \\mathbf{z}, $$\nwhere $\\mathbf{z}$ is chosen exactly as above. It is easy to check that $\\mathbf{x}^{\\star}(\\epsilon)$ will be feasible for all $\\epsilon$ sufficiently small, both positive and negative. Hence, since $\\mathbf{x}^{\\star}$ is optimal, we must have\n$$ \\mathbf{c}^{\\mathrm{T}}\\left(\\mathbf{x}^{\\star}+\\epsilon \\mathbf{z}\\right) \\geq \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{\\star} \\Rightarrow \\epsilon \\mathbf{c}^{\\mathrm{T}} \\mathbf{z} \\geq 0 $$\nfor all $|\\epsilon|$ sufficiently small. Therefore, $\\mathbf{c}^{\\mathrm{T}} \\mathbf{z}=0$, and so $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{\\star}(\\epsilon)=\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{\\star}$ for all $\\epsilon$. The same logic as in the proof of (i) can be applied to find $\\bar{\\epsilon}\u0026gt;0$ such that $\\mathbf{x}^{\\star}(\\bar{\\epsilon})$ is feasible and optimal, with at most $p-1$ nonzero components. This contradicts our choice of $p$ as the minimal number of nonzeros, so the columns $\\mathbf{A}_{1}, \\mathbf{A}_{2}, \\ldots, \\mathbf{A}_{p}$ must be linearly independent. We can now apply the same logic as above to conclude that $\\mathbf{x}^{\\star}$ is already a basic feasible point and therefore a basic optimal point.\nThe final statement (iii) is a consequence of finite termination of the simplex method. We comment on the latter property in the next section.\n■ The terminology we use here is not standard, as the following table shows:\nour terminology standard terminology basic feasible point basic feasible solution basic optimal point optimal basic feasible solution The standard terms arose because \u0026ldquo;solution\u0026rdquo; and \u0026ldquo;feasible solution\u0026rdquo; were originally used as synonyms for \u0026ldquo;feasible point.\u0026rdquo; However, as the discipline of optimization developed, the word \u0026ldquo;solution\u0026rdquo; took on a more specific and intuitive meaning (as in \u0026ldquo;solution to the problem . . .\u0026rdquo;). We keep the terminology of this chapter consistent with the rest of the lecture by sticking to this more modern usage.\nVertices of the feasible polytope # The feasible set defined by the linear constraints is a polytope, and the vertices of this polytope are the points that do not lie on a straight line between two other points in the set. Geometrically, they are easily recognizable.\nAlgebraically, the vertices are exactly the basic feasible points that we described above. We therefore have an important relationship between the algebraic and geometric viewpoints and a useful aid to understanding how the simplex method works.\nTheorem 6.3 (Vertices and basic feasible points)\nAll basic feasible points for \\eqref{eq:standard_form} are vertices of the feasible polytope $\\{\\mathbf{x} \\mid \\mathbf{A} \\mathbf{x}=\\mathbf{b}, \\mathbf{x} \\geq \\mathbf{0}\\}$, and vice versa. Proof\nLet $\\mathbf{x}$ be a basic feasible point and assume without loss of generality that $\\mathcal{B}(\\mathbf{x})=\\{1,2, \\ldots, m\\}$. The matrix $\\mathbf{B}=\\left[\\mathbf{A}_{i}\\right]_{i=1,2, \\ldots, m}$ is therefore nonsingular, and\n$$ x_{m+1}=x_{m+2}=\\cdots=x_{n}=0 $$\nSuppose that $\\mathbf{x}$ lies on a straight line between two other feasible points $\\mathbf{y}$ and $\\mathbf{z}$. Then we can find $\\alpha \\in(0,1)$ such that $\\mathbf{x}=\\alpha \\mathbf{y}+(1-\\alpha) \\mathbf{z}$. Because of the above condition and the fact that $\\alpha$ and $1-\\alpha$ are both positive, we must have $y_{i}=z_{i}=0$ for $i=m+1, m+2, \\ldots, n$. Writing $\\mathbf{x}_{\\mathrm{B}}=\\left(x_{1}, x_{2}, \\ldots, x_{m}\\right)^{\\mathrm{T}}$ and defining $\\mathbf{y}_{\\mathrm{B}}$ and $\\mathbf{z}_{\\mathrm{B}}$ likewise, we have from $\\mathbf{A} \\mathbf{x}=\\mathbf{A} \\mathbf{y}=\\mathbf{A} \\mathbf{z}=\\mathbf{b}$ that\n$$ \\mathbf{B} \\mathbf{x}_{\\mathrm{B}}=\\mathbf{B} \\mathbf{y}_{\\mathrm{B}}=\\mathbf{B} \\mathbf{z}_{\\mathrm{B}}=\\mathbf{b}, $$\nand so, by nonsingularity of $\\mathbf{B}$, we have $\\mathbf{x}_{\\mathrm{B}}=\\mathbf{y}_{\\mathrm{B}}=\\mathbf{z}_{\\mathrm{B}}$. Therefore, $\\mathbf{x}=\\mathbf{y}=\\mathbf{z}$, contradicting our assertion that $\\mathbf{y}$ and $\\mathbf{z}$ are two feasible points other than $\\mathbf{x}$. Therefore, $\\mathbf{x}$ is a vertex.\nConversely, let $\\mathbf{x}$ be a vertex of the feasible polytope, and suppose that the nonzero components of $\\mathbf{x}$ are $x_{1}, x_{2}, \\ldots, x_{p}$. If the corresponding columns $\\mathbf{A}_{1}, \\mathbf{A}_{2}, \\ldots, \\mathbf{A}_{p}$ are linearly dependent, then we can construct the vector $\\mathbf{x}(\\epsilon)=\\mathbf{x}+\\epsilon \\mathbf{z}$ as before. Since $\\mathbf{x}(\\epsilon)$ is feasible for all $\\epsilon$ with sufficiently small magnitude, we can define $\\hat{\\epsilon}\u0026gt;0$ such that $\\mathbf{x}(\\hat{\\epsilon})$ and $\\mathbf{x}(-\\hat{\\epsilon})$ are both feasible. Since $\\mathbf{x}=\\mathbf{x}(0)$ obviously lies on a straight line between these two points, it cannot be a vertex. Hence our assertion that $\\mathbf{A}_{1}, \\mathbf{A}_{2}, \\ldots, \\mathbf{A}_{p}$ are linearly dependent must be incorrect, so these columns must be linearly independent and $p \\leq m$. The same arguments as in the proof of Theorem 6.2 can now be used to show that $\\mathbf{x}$ is a basic feasible point, completing our proof.\n■ We conclude this discussion of the geometry of the feasible set with a definition of degeneracy. This term has a variety of meanings in optimization, as we discuss later. For the purposes of this chapter, we use the following definition.\nDefinition 6.2 (Degenerate linear program)\nA linear program \\eqref{eq:standard_form} is said to be degenerate if there exists at least one basic feasible point that has fewer than $m$ nonzero components. Naturally, nondegenerate linear programs are those for which this definition is not satisfied.\nThe simplex method # Outline of the method # As we just described, all iterates of the simplex method are basic feasible points for \\eqref{eq:standard_form} and therefore vertices of the feasible polytope. Most steps consist of a move from one vertex to an adjacent one for which the set of basic indices $\\mathcal{B}(\\mathbf{x})$ differs in exactly one component. On most steps (but not all), the value of the primal objective function $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$ is decreased. Another type of step occurs when the problem is unbounded: The step is an edge along which the objective function is reduced, and along which we can move infinitely far without ever reaching a vertex.\nThe major issue at each simplex iteration is to decide which index to change in the basis set $\\mathcal{B}$. Unless the step is a direction of unboundedness, one index must be removed from $\\mathcal{B}$ and replaced by another from outside $\\mathcal{B}$. We can get some insight into how this decision is made by looking again at the KKT conditions \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity} to see how they relate to the algorithm.\nFrom $\\mathcal{B}$ and \\eqref{eq:lp_kkt_gradient}-\\eqref{eq:lp_kkt_complementarity}, we can derive values for not just the primal variable $\\mathbf{x}$ but also the dual variables $\\boldsymbol{\\pi}$ and $\\mathbf{s}$, as we now show. We define the index set $\\mathcal{N}$ as the complement of $\\mathcal{B}$, that is,\n$$ \\mathcal{N}=\\{1,2, \\ldots, n\\} \\backslash \\mathcal{B} . $$\nJust as $\\mathbf{B}$ is the column submatrix of $\\mathbf{A}$ that corresponds to the indices $i \\in \\mathcal{B}$, we use $\\mathbf{N}$ to denote the submatrix $\\mathbf{N}=\\left[\\mathbf{A}_{i}\\right]_{i \\in \\mathcal{N}}$. We also partition the $n$-element vectors $\\mathbf{x}$, $\\mathbf{s}$, and $\\mathbf{c}$ according to the index sets $\\mathcal{B}$ and $\\mathcal{N}$, using the notation\n$$ \\mathbf{x}_{\\mathrm{B}}=\\left[x_{i}\\right]_{i \\in \\mathcal{B}}, \\quad \\mathbf{x}_{\\mathrm{N}}=\\left[x_{i}\\right]_{i \\in \\mathcal{N}}, \\quad \\mathbf{s}_{\\mathrm{B}}=\\left[s_{i}\\right]_{i \\in \\mathcal{B}}, \\quad \\mathbf{s}_{\\mathrm{N}}=\\left[s_{i}\\right]_{i \\in \\mathcal{N}} . $$\nFrom the KKT condition \\eqref{eq:lp_kkt_equality}, we have that\n$$ \\mathbf{A} \\mathbf{x}=\\mathbf{B} \\mathbf{x}_{\\mathrm{B}}+\\mathbf{N} \\mathbf{x}_{\\mathrm{N}}=\\mathbf{b} . $$\nThe primal variable $\\mathbf{x}$ for this simplex iterate is defined as\n$$ \\mathbf{x}_{\\mathrm{B}}=\\mathbf{B}^{-1} \\mathbf{b}, \\quad \\mathbf{x}_{\\mathrm{N}}=\\mathbf{0} . $$\nSince we are dealing only with basic feasible points, we know that $\\mathbf{B}$ is nonsingular and that $\\mathbf{x}_{\\mathrm{B}} \\geq \\mathbf{0}$, so this choice of $\\mathbf{x}$ satisfies two of the KKT conditions: the equality constraints \\eqref{eq:lp_kkt_equality} and the nonnegativity condition \\eqref{eq:lp_kkt_primal_feasible}.\nWe choose $\\mathbf{s}$ to satisfy the complementarity condition \\eqref{eq:lp_kkt_complementarity} by setting $\\mathbf{s}_{\\mathrm{B}}=\\mathbf{0}$. The remaining components $\\boldsymbol{\\pi}$ and $\\mathbf{s}_{\\mathrm{N}}$ can be found by partitioning this condition into $\\mathcal{B}$ and $\\mathcal{N}$ components and using $\\mathbf{s}_{\\mathrm{B}}=\\mathbf{0}$ to obtain\n$$ \\mathbf{B}^{\\mathrm{T}} \\boldsymbol{\\pi}=\\mathbf{c}_{\\mathrm{B}}, \\quad \\mathbf{N}^{\\mathrm{T}} \\boldsymbol{\\pi}+\\mathbf{s}_{\\mathrm{N}}=\\mathbf{c}_{\\mathrm{N}} . $$\nSince $\\mathbf{B}$ is square and nonsingular, the first equation uniquely defines $\\boldsymbol{\\pi}$ as\n$$ \\boldsymbol{\\pi}=\\mathbf{B}^{-\\mathrm{T}} \\mathbf{c}_{\\mathrm{B}} . $$\nThe second equation implies a value for $\\mathbf{s}_{\\mathrm{N}}$:\n\\begin{equation} \\mathbf{s}_{\\mathrm{N}}=\\mathbf{c}_{\\mathrm{N}}-\\mathbf{N}^{\\mathrm{T}} \\boldsymbol{\\pi}=\\mathbf{c}_{\\mathrm{N}}-\\left(\\mathbf{B}^{-1} \\mathbf{N}\\right)^{\\mathrm{T}} \\mathbf{c}_{\\mathrm{B}} . \\label{eq:reduced_costs} \\end{equation}\nComputation of the vector $\\mathbf{s}_{\\mathrm{N}}$ is often referred to as pricing. The components of $\\mathbf{s}_{\\mathrm{N}}$ are often called the reduced costs of the nonbasic variables $\\mathbf{x}_{\\mathrm{N}}$.\nThe only KKT condition that we have not enforced explicitly is the nonnegativity condition $\\mathbf{s} \\geq \\mathbf{0}$. The basic components $\\mathbf{s}_{\\mathrm{B}}$ certainly satisfy this condition, by our choice $\\mathbf{s}_{\\mathrm{B}}=\\mathbf{0}$. If the vector $\\mathbf{s}_{\\mathrm{N}}$ defined by \\eqref{eq:reduced_costs} also satisfies $\\mathbf{s}_{\\mathrm{N}} \\geq \\mathbf{0}$, we have found an optimal vector triple $(\\mathbf{x}, \\boldsymbol{\\pi}, \\mathbf{s})$, so the algorithm can terminate and declare success. The usual case, however, is that one or more of the components of $\\mathbf{s}_{\\mathrm{N}}$ are negative, so the condition $\\mathbf{s} \\geq \\mathbf{0}$ is violated. The new index to enter the basic index set $\\mathcal{B}$-the entering index-is now chosen to be one of the indices $q \\in \\mathcal{N}$ for which $s_{q}\u0026lt;0$. As we show below, the objective $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$ will decrease when we allow $x_{q}$ to become positive if and only if $q$ has the property that $s_{q}\u0026lt;0$. Our procedure for altering $\\mathcal{B}$ and changing $\\mathbf{x}$ and $\\mathbf{s}$ accordingly is as follows:\nallow $x_{q}$ to increase from zero during the next step; fix all other components of $\\mathbf{x}_{\\mathrm{N}}$ at zero; figure out the effect of increasing $x_{q}$ on the current basic vector $\\mathbf{x}_{\\mathrm{B}}$, given that we want to stay feasible with respect to the equality constraints $\\mathbf{A} \\mathbf{x}=\\mathbf{b}$; keep increasing $x_{q}$ until one of the components of $\\mathbf{x}_{\\mathrm{B}}$ (corresponding to $x_{p}$, say) is driven to zero, or determining that no such component exists (the unbounded case); remove index $p$ (known as the leaving index) from $\\mathcal{B}$ and replace it with the entering index $q$. It is easy to formalize this procedure in algebraic terms. Since we want both the new iterate $\\mathbf{x}^{+}$and the current iterate $\\mathbf{x}$ to satisfy $\\mathbf{A} \\mathbf{x}=\\mathbf{b}$, and since $\\mathbf{x}_{\\mathrm{N}}=\\mathbf{0}$ and $x_{i}^{+}=0$ for $i \\in \\mathcal{N} \\backslash\\{q\\}$, we have\n$$ \\mathbf{A} \\mathbf{x}^{+}=\\mathbf{B} \\mathbf{x}_{\\mathrm{B}}^{+}+\\mathbf{A}_{q} x_{q}^{+}=\\mathbf{B} \\mathbf{x}_{\\mathrm{B}}=\\mathbf{A} \\mathbf{x} . $$\nBy multiplying this expression by $\\mathbf{B}^{-1}$ and rearranging, we obtain\n\\begin{equation} \\mathbf{x}_{\\mathrm{B}}^{+}=\\mathbf{x}_{\\mathrm{B}}-\\mathbf{B}^{-1} \\mathbf{A}_{q} x_{q}^{+} . \\label{eq:simplex_step} \\end{equation}\nWe show in a moment that the direction $-\\mathbf{B}^{-1} \\mathbf{A}_{q}$ is a descent direction for $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$. Geometrically speaking, \\eqref{eq:simplex_step} is a move along an edge of the feasible polytope that decreases $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$. We continue to move along this edge until a new vertex is encountered. We have to stop at this vertex, since by definition we cannot move any further without leaving the feasible region. At the new vertex, a new constraint $x_{i} \\geq 0$ must have become active, that is, one of the components $x_{i}, i \\in \\mathcal{B}$, has decreased to zero. This index $i$ is the one that is removed from the basis.\nFinite termination of the simplex method # Let us now verify that the step defined by \\eqref{eq:simplex_step} leads to a decrease in $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$. By using the definition \\eqref{eq:simplex_step} of $\\mathbf{x}_{\\mathrm{B}}^{+}$together with\n$$ \\mathbf{x}_{\\mathrm{N}}^{+}=\\left(0, \\ldots, 0, x_{q}^{+}, 0, \\ldots, 0\\right)^{\\mathrm{T}} $$\nwe have\n\\begin{align} \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{+} \u0026amp; =\\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{x}_{\\mathrm{B}}^{+}+\\mathbf{c}_{\\mathrm{N}}^{\\mathrm{T}} \\mathbf{x}_{\\mathrm{N}}^{+} \\\\\\ \u0026amp; =\\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{x}_{\\mathrm{B}}^{+}+c_{q} x_{q}^{+} \\\\\\ \u0026amp; =\\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{x}_{\\mathrm{B}}-\\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{B}^{-1} \\mathbf{A}_{q} x_{q}^{+}+c_{q} x_{q}^{+} \\end{align}\nNow, from the definition of $\\boldsymbol{\\pi}$ we have $\\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{B}^{-1}=\\boldsymbol{\\pi}^{\\mathrm{T}}$, while from the second equation above we have $\\mathbf{A}_{q}^{\\mathrm{T}} \\boldsymbol{\\pi}=c_{q}-s_{q}$, since $\\mathbf{A}_{q}$ is a column of $\\mathbf{N}$. Therefore,\n$$ \\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{B}^{-1} \\mathbf{A}_{q} x_{q}^{+}=\\boldsymbol{\\pi}^{\\mathrm{T}} \\mathbf{A}_{q} x_{q}^{+}=\\left(c_{q}-s_{q}\\right) x_{q}^{+} $$\nso by substituting above we obtain\n$$ \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{+}=\\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{x}_{\\mathrm{B}}-\\left(c_{q}-s_{q}\\right) x_{q}^{+}+c_{q} x_{q}^{+}=\\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{x}_{\\mathrm{B}}-s_{q} x_{q}^{+} . $$\nSince $\\mathbf{x}_{\\mathrm{N}}=\\mathbf{0}$, we have $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}=\\mathbf{c}_{\\mathrm{B}}^{\\mathrm{T}} \\mathbf{x}_{\\mathrm{B}}$ and therefore\n\\begin{equation} \\mathbf{c}^{\\mathrm{T}} \\mathbf{x}^{+}=\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}-s_{q} x_{q}^{+} \\label{eq:objective_decrease} \\end{equation}\nSince we chose $q$ such that $s_{q}\u0026lt;0$, and since $x_{q}^{+}\u0026gt;0$ if we are able to move at all along the edge, it follows from \\eqref{eq:objective_decrease} that the step \\eqref{eq:simplex_step} produces a decrease in the primal objective function $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$.\nIf the problem is nondegenerate (see Definition 6.2 ), then we are guaranteed that $x_{q}^{+}\u0026gt;0$, so we can be assured of a strict decrease in the objective function $\\mathbf{c}^{\\mathrm{T}} \\mathbf{x}$ at every simplex step. We can therefore prove the following result concerning termination of the simplex method.\nTheorem 6.4 (Finite termination of simplex method)\nProvided that the linear program \\eqref{eq:standard_form} is nondegenerate and bounded, the simplex method terminates at a basic optimal point. Proof\nThe simplex method cannot visit the same basic feasible point $\\mathbf{x}$ at two different iterations, because it attains a strict decrease at each iteration. Since each subset of $m$ indices drawn from the set $\\{1,2, \\ldots, n\\}$ is associated with at most one basic feasible point, it follows that no basis $\\mathcal{B}$ can be visited at two different simplex iterations. The number of possible bases is at most $\\binom{n}{m}$ (which is the number of ways to choose the $m$ elements of a basis $\\mathcal{B}$ from among the $n$ possible indices), so there can be only a finite number of iterations. Since the method is always able to take a step away from a nonoptimal basic feasible point, the point of termination must be a basic optimal point. ■ Note that this result gives us a proof of Theorem 6.2 (iii) for the nondegenerate case.\nA single step of the method # We have covered most of the mechanics of taking a single step of the simplex method. To make subsequent discussions easier to follow, we summarize our description in a semiformal way.\nProcedure: One step of simplex\nGiven $\\mathcal{B}, \\mathcal{N}, \\mathbf{x}_{\\mathrm{B}}=\\mathbf{B}^{-1} \\mathbf{b} \\geq \\mathbf{0}, \\mathbf{x}_{\\mathrm{N}}=\\mathbf{0}$;\nSolve $\\mathbf{B}^{\\mathrm{T}} \\boldsymbol{\\pi}=\\mathbf{c}_{\\mathrm{B}}$ for $\\boldsymbol{\\pi}$,\nCompute $\\mathbf{s}_{\\mathrm{N}}=\\mathbf{c}_{\\mathrm{N}}-\\mathbf{N}^{\\mathrm{T}} \\boldsymbol{\\pi}$;\nif $\\mathbf{s}_{\\mathrm{N}} \\geq \\mathbf{0}$\nSTOP; (optimal point found)\nSelect $q \\in \\mathcal{N}$ with $s_{q}\u0026lt;0$ as the entering index;\nSolve $\\mathbf{B} \\mathbf{t}=\\mathbf{A}_{q}$ for $\\mathbf{t}$;\nif $\\mathbf{t} \\leq \\mathbf{0}$\nSTOP; (problem is unbounded)\nCalculate $x_{q}^{+}=\\min _{i \\mid t_{i}\u0026gt;0}\\left(\\mathbf{x}_{\\mathrm{B}}\\right)_{i} / t_{i}$, and use $p$ to denote the index of the basic variable for which this minimum is achieved;\nUpdate $\\mathbf{x}_{\\mathrm{B}}^{+}=\\mathbf{x}_{\\mathrm{B}}-\\mathbf{t} x_{q}^{+}, \\mathbf{x}_{\\mathrm{N}}^{+}=\\left(0, \\ldots, 0, x_{q}^{+}, 0, \\ldots, 0\\right)^{\\mathrm{T}}$;\nChange $\\mathcal{B}$ by adding $q$ and removing $p$.\nWe need to flesh out this description with specifics of three important points. These are as follows.\nLinear algebra issues-maintaining an LU factorization of $\\mathbf{B}$ that can be used to solve for $\\boldsymbol{\\pi}$ and $\\mathbf{t}$. Selection of the entering index $q$ from among the negative components of $\\mathbf{s}_{\\mathrm{N}}$. (In general, there are many such components.) Handling of degenerate steps, in which $x_{q}^{+}=0$, so that $\\mathbf{x}$ is not changed. Proper handling of these issues is crucial to the efficiency of a simplex implementation.\n"},{"id":17,"href":"/numerical_optimization/docs/lectures/fundamentals/","title":"I - Fundamentals","section":"Lectures","content":" Fundamentals # Content 1. Optimization problems 2. Unconstrained optimization : basics 3. Convexity theory 4. Unconstrained optimization : linesearch 5. Constrained optimization - Introduction 5b. Constrained optimization - Projected Gradient Descent 6. Constrained optimization - Linear programming "},{"id":18,"href":"/numerical_optimization/docs/practical_labs/linear_regression/","title":"I - Linear Regression models","section":"Practical labs","content":" Linear Regression models # Introduction # In this lab session, we will explore the fundamental concepts of numerical optimization through the lens of linear regression. We\u0026rsquo;ll begin with the simplest case and gradually build up to more complex scenarios, comparing analytical solutions with numerical methods at each step.\nLinear regression is perhaps the most fundamental problem in machine learning and statistics. While it has a closed-form solution, implementing numerical optimization methods for this problem provides excellent intuition for more complex optimization scenarios where analytical solutions don\u0026rsquo;t exist.\nLearning objectives # By the end of this session, you should be able to:\nDerive the analytical solution for simple linear regression Implement gradient descent with various step size strategies Understand the connection between the one-dimensional and multi-dimensional cases Apply line search techniques to improve convergence I - One dimensional case # Let us first start with the form that most people are familiar with, the linear regression model in one dimension. The setup is as follows:\nWe have a set of data points $\\{(x_i, y_i)\\}_{i=1}^n$. Here the $x_i$ are the input features and the $y_i$ are the target values. Assuming there is a linear relationship between and target because of some underlying phenomenon, we model the observations as: \\begin{equation} y_i = \\alpha x_i + \\beta + \\epsilon \\label{eq:linear_model_1d} \\end{equation} where $\\epsilon$ is a random noise term that we assume to be normally distributed with mean 0 and variance $\\sigma^2$. Our goal is then to find the parameters $\\alpha$ and $\\beta$ that \u0026ldquo;best match\u0026rdquo; the data points. Such a program can is illustrated with following interactive plot.\n1. Modeling and solving the problem # Propose a loss function that quantifies the difference between the observed $y_i$ and the predicted values $\\hat{y}_i = \\alpha x_i + \\beta$. Hint The most common loss function for regression problems is the mean squared error (MSE): $$ L(\\alpha, \\beta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - (\\alpha x_i + \\beta))^2 $$ Show that the loss function is convex in the parameters $\\alpha$ and $\\beta$. Hint To show convexity, we need to demonstrate that the Hessian matrix of second derivatives is positive semi-definite. Or that the function is a positive linear combination of convex functions. The loss function is a quadratic function in $\\alpha$ and $\\beta$, which is convex. The Hessian matrix will have positive eigenvalues, confirming convexity. Derive the analytical solution for the parameters $\\alpha$ and $\\beta$ by setting the gradients of the loss function with respect to these parameters to zero. Hint It is often useful to express the gradients in terms of the means and variances of the data points:\nmeans : $$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$ variance: $$s_{xx} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2$$ covariance: $$s_{xy} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$$ Implement the analytical solution in Python and compute the optimal parameters for a given dataset. To generate dataset, you can use following code snippet: import numpy as np import matplotlib.pyplot as plt # Set random seed for reproducibility rng = np.random.default_rng(42) # Generate synthetic data n_samples = 50 x = np.linspace(0, 10, n_samples) # True parameters alpha = 2.5 beta = 1.0 # Add Gaussian noise noise = rng.normal(0, 1, n_samples) y = alpha * x + beta + noise # Visualize the data plt.figure(figsize=(8, 6)) plt.scatter(x, y, alpha=0.7, label=\u0026#39;Data points\u0026#39;) plt.xlabel(\u0026#39;x\u0026#39;) plt.ylabel(\u0026#39;y\u0026#39;) plt.title(\u0026#39;Synthetic linear data with noise\u0026#39;) plt.grid(True, alpha=0.3) plt.legend() plt.show() (Bonus) Show that doing a Maximum Likelihood Estimation (MLE) for the parameters $\\alpha$ and $\\beta$ leads to the same solution as minimizing the loss function derived above. Hint The MLE for the parameters in a linear regression model with Gaussian noise leads to minimizing the negative log-likelihood, which is equivalent to minimizing the mean squared error loss function. The derivation involves taking the logarithm of the Gaussian probability density function and simplifying it, leading to the same equations for $\\alpha$ and $\\beta$ as derived from the loss function. 2. Gradient descent for the one-dimensional case # Now that we have a good understanding of the problem and have implemented the analytical solution, let\u0026rsquo;s explore how we can solve this problem using numerical optimization techniques, specifically steepest gradient descent, i.e always taking the gradient as the direction.\nRecalling the update rule for steepest gradient descent of a point $\\theta_k$ at iteration $k$: $$ \\theta_{k+1} = \\theta_k - \\alpha_k \\nabla L(\\theta_k) $$ where $\\alpha_k$ is the step size at iteration $k$, give the update rule for the parameters $\\alpha$ and $\\beta$ in the context of our linear regression problem.\nHint The update rules for the parameters $\\alpha$ and $\\beta$ can be derived from the gradients of the loss function: $$ \\begin{align*} \\alpha_{k+1} \u0026amp;= \\alpha_k - \\alpha_k \\frac{\\partial L}{\\partial \\alpha}(\\alpha_k, \\beta_k) \\\\ \\beta_{k+1} \u0026amp;= \\beta_k - \\alpha_k \\frac{\\partial L}{\\partial \\beta}(\\alpha_k, \\beta_k) \\end{align*} $$ where the gradients are computed as follows: $$ \\begin{align*} \\frac{\\partial L}{\\partial \\alpha} \u0026amp;= -\\sum_{i=1}^n (y_i - (\\alpha_k x_i + \\beta_k)) x_i \\\\ \\frac{\\partial L}{\\partial \\beta} \u0026amp;= -\\sum_{i=1}^n (y_i - (\\alpha_k x_i + \\beta_k)) \\end{align*} $$ Implement gradient descent with a constant step size $\\alpha_k = \\alpha$ for all iterations. Your function should:\nTake initial parameters $(\\alpha_0, \\beta_0)$, step size $\\alpha$, and number of iterations as inputs. Return the trajectory of parameters and loss values. Include a stopping criterion based on gradient magnitude. Experiment with different step sizes: $\\alpha \\in \\{0.0001, 0.001, 0.01, 0.1\\}$. Plot the loss function over iterations for each case. What do you observe?\nHint The loss function should decrease over iterations, but the rate of decrease will depend on the step size. A very small step size will lead to slow convergence, while a very large step size may cause divergence or oscillations. For a fixed number of iterations (say 1000), plot the final error as a function of step size on a logarithmic scale. What is the optimal range for $\\alpha$? Hint The optimal range for $\\alpha$ is typically small enough to ensure convergence but large enough to allow for reasonable speed of convergence. You may find that values around $0.001$ to $0.01$ work well, but this can depend on the specific dataset and problem. Let\u0026rsquo;s try a first experiment with a decreasing step size. Implement a linear decay strategy: $$ \\alpha_k = \\alpha_0 - k \\cdot \\gamma, $$ where $\\gamma$ is a small constant (e.g., $0.0001$). Experiment with different values of $\\alpha_0$ and $\\gamma$. Compare the convergence behavior with constant step size. Plot the loss function and parameter trajectories over iterations.\nWhy might decreasing step sizes be beneficial? What are the trade-offs between aggressive and conservative decay rates? Hint Decreasing step sizes can help avoid overshooting the minimum and allow for finer adjustments as the algorithm converges.\nIn nonconvex problmes, aggressive decay rates may lead to faster convergence initially but can cause the algorithm to get stuck in local minima, while conservative rates may lead to slower convergence but better exploration of the parameter space.\nTry implementing an exponential decay strategy: $$ \\alpha_k = \\alpha_0 \\cdot \\gamma^k$$ where $\\gamma \\in (0, 1)$ is the decay rate. Experiment with different values of $\\gamma$ (e.g., $0.9$, $0.95$, $0.99$) and compare the convergence behavior with constant and linear decay strategies. Hint Exponential decay is more aggressive, thus it may also cause the step size to become too small too quickly, leading to slow convergence in later iterations. The choice of $\\gamma$ can significantly affect the convergence behavior. II - Multiple variables case # Now that we have a good understanding of the one-dimensional case, let\u0026rsquo;s generalize our approach to multiple dimensions. The setup is similar, but now we have multiple features and parameters:\nWe have a set of data points $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ are the input features and $y_i \\in \\mathbb{R}$ are the target values. We model the observations as: \\begin{equation} y_i = \\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_i + \\beta + \\epsilon \\label{eq:linear_model_d} \\end{equation} where $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector, $\\beta \\in \\mathbb{R}$ is the bias term, and $\\epsilon$ is a random noise term that we assume to be normally distributed with mean 0 and variance $\\sigma^2$. Figure 0.1: 2D linear regression\nAn example of this model is illustrated in Figure 0.1 , where the data points are represented in a two-dimensional space, and the linear regression model fits a plane to the data.\nThis model is more general than the one-dimensional case, as it allows for multiple features to influence the target variable. Notably, we can also augment the feature vectors with a constant term to simplify the notation so that we don\u0026rsquo;t have to deal with the bias term separately. We define: \\begin{equation} \\tilde{\\mathbf{x}}_i = [1, \\mathbf{x}_i^{\\mathrm{T}}]^{\\mathrm{T}} \\in \\mathbb{R}^{d+1} \\end{equation} and \\begin{equation} \\tilde{\\mathbf{w}} = [\\beta, \\mathbf{w}^{\\mathrm{T}}]^{\\mathrm{T}} \\in \\mathbb{R}^{d+1} \\end{equation} so that we can rewrite the model as: \\begin{equation} y_i = \\tilde{\\mathbf{w}}^{\\mathrm{T}} \\tilde{\\mathbf{x}}_i + \\epsilon \\label{eq:linear_model_d_augmented} \\end{equation}\nOur goal is then to find the parameters $\\mathbf{w}$ and $\\beta$ that \u0026ldquo;best match\u0026rdquo; the data points, or in augmented notation, to find $\\tilde{\\mathbf{w}}$ that minimizes the loss function.\n1. Modeling and solving the problem # While the augmented formulation is nice, we can also express the model in matrix form for the observed data. We define the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times (d+1)}$ as: \\begin{equation} \\mathbf{X} = \\begin{bmatrix} 1 \u0026amp; x_{11} \u0026amp; x_{12} \u0026amp; \\ldots \u0026amp; x_{1d} \\\\ 1 \u0026amp; x_{21} \u0026amp; x_{22} \u0026amp; \\ldots \u0026amp; x_{2d} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_{n1} \u0026amp; x_{n2} \u0026amp; \\ldots \u0026amp; x_{nd} \\end{bmatrix} \\end{equation} and the target vector $\\mathbf{y} \\in \\mathbb{R}^n$ as: \\begin{equation} \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\end{equation}\nThen we can express the model as: \\begin{equation} \\mathbf{y} = \\mathbf{X} \\tilde{\\mathbf{w}} + \\boldsymbol{\\epsilon}, \\label{eq:linear_model_matrix} \\end{equation}\nwhere $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^n$ is the noise vector. This more compact formulation is interesting for several reasons:\nIt already encapsulates the observed data in the model and we consider all the $y_i$ as a vector, which allows us to work with the entire dataset at once. the matrix form allow us to obtain solutions that will be expressed as matrix operations, which is more efficient for larger datasets it allows us to use linear algebra techniques to derive the solution Propose a loss function that quantifies the difference between the observed $y_i$ and the predicted values $\\hat{y}_i = \\mathbf{X} \\tilde{\\mathbf{w}}$. Hint The most common loss function for regression problems is the mean squared error (MSE): $$ L(\\tilde{\\mathbf{w}}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}\\|^2_2 $$ Show that the loss function is convex in the parameters $\\tilde{\\mathbf{w}}$. Hint As in the one-dimensional case, the loss function is a quadratic function in $\\tilde{\\mathbf{w}}$, which is convex. The Hessian matrix of second derivatives will be positive semi-definite, confirming convexity. Derive the gradient of the loss function with respect to $\\tilde{\\mathbf{w}}$ in matrix form. To help yourselves, you can use the properties of matrix derivatives from matrix cookbook available here and identity of vector norms: $$ \\lVert \\mathbf{u} - \\mathbf{v} \\rVert^2_2 = \\mathbf{u}^{\\mathrm{T}} \\mathbf{u} - 2 \\mathbf{u}^{\\mathrm{T}} \\mathbf{v} + \\mathbf{v}^{\\mathrm{T}} \\mathbf{v}. $$ and show that optimal solution $\\tilde{\\mathbf{w}}$ satisfies the normal equations: $$ \\mathbf{X}^{\\mathrm{T}} \\mathbf{X} \\tilde{\\mathbf{w}} = \\mathbf{X}^{\\mathrm{T}} \\mathbf{y}. $$\nHint Make use of\n$\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^{\\mathrm{T}} \\mathbf{a} = \\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{a}^{\\mathrm{T}} \\mathbf{x} = \\mathbf{a}$ $\\frac{\\partial}{\\partial \\mathbf{x}} \\|\\mathbf{x}\\|^2_2 = \\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^{\\mathrm{T}} \\mathbf{x} = 2 \\mathbf{x}$ $\\frac{\\partial}{\\partial \\mathbf{x}} \\lVert \\mathbf{A} \\mathbf{x} \\rVert^2_2 = 2 \\mathbf{A}^{\\mathrm{T}} \\mathbf{A} \\mathbf{x}$ Thus, to obtain optimal parameters $\\tilde{\\mathbf{w}}$, we can solve the normal equations: $$ \\tilde{\\mathbf{w}} = (\\mathbf{X}^{\\mathrm{T}} \\mathbf{X})^{-1} \\mathbf{X}^{\\mathrm{T}} \\mathbf{y}. $$\nNote: The matrix $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ is known as the Gram matrix, and it is positive semi-definite. If $\\mathbf{X}$ has full column rank, then $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$ is invertible, and we can compute the unique solution for $\\tilde{\\mathbf{w}}$. Otherwise, the solution is not unique, and we may need to use regularization techniques (e.g., ridge regression) to obtain a stable solution, but we will not cover this in this lab.\nImplement the analytical solution using NumPy\u0026rsquo;s linear algebra functions. Compare your result with np.linalg.lstsq. To generate dataset, you can use following code snippet: import numpy as np # Generate multi-dimensional data d = 5 # number of features n_samples = 100 # Generate random features X = np.random.randn(n_samples, d) # Add intercept term X_augmented = np.column_stack([np.ones(n_samples), X]) # True parameters w_true = np.random.randn(d + 1) # including bias # Generate targets with noise y = X_augmented @ w_true + 0.5 * np.random.randn(n_samples) print(f\u0026#34;Data shape: {X.shape}\u0026#34;) print(f\u0026#34;Augmented data shape: {X_augmented.shape}\u0026#34;) print(f\u0026#34;True parameters: {w_true}\u0026#34;) 2. Gradient descent for the multiple variables case # Rather than inverting the matrix $\\mathbf{X}^{\\mathrm{T}} \\mathbf{X}$, which can be computationally expensive for large datasets, we can use gradient descent to find the optimal parameters $\\tilde{\\mathbf{w}}$.\nGive the update rule for the parameters $\\tilde{\\mathbf{w}}$ in the context of our linear regression problem using steepest gradient descent. Hint The update rule for the parameters $\\tilde{\\mathbf{w}}$ can be derived from the gradient of the loss function: $$ \\tilde{\\mathbf{w}}_{k+1} = \\tilde{\\mathbf{w}}_k - \\alpha_k \\nabla L(\\tilde{\\mathbf{w}}_k) $$ where the gradient is given by: $$ \\nabla L(\\tilde{\\mathbf{w}}) = -\\mathbf{X}^{\\mathrm{T}} (\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}) $$ Implement gradient descent with a constant step size $\\alpha_k = \\alpha$ for all iterations. Your function should: Take initial parameters $\\tilde{\\mathbf{w}}_0$, step size $\\alpha$, and number of iterations as inputs. Return the trajectory of parameters and loss values. Include a stopping criterion based on gradient magnitude. 3. Experimenting with backtracking line search # From implementing gradient descent, we have seen that the choice of step size $\\alpha$ can significantly affect the convergence behavior. A fixed step size may not be optimal for all iterations, leading to slow convergence or oscillations. On the other hand, a decreasing step size is not the best choice as it may lead to very small step sizes in later iterations, causing slow convergence. Let us put in practice the theory we have set in place around line search techniques to adaptively choose the step size at each iteration.\nImplement a backtracking line search algorithm to adaptively choose the step size $\\alpha_k$ at each iteration. For a reminder, check the memo here.\nUse the backtracking line search to find the optimal step size for each iteration of gradient descent. Compare the convergence behavior with constant and decreasing step sizes.\nExperiment with different parameters for the backtracking line search, such as the initial step size $\\alpha_0$, reduction factor $\\rho$, and Armijo parameter $c_1$. How do these parameters affect the convergence behavior?\nHint The backtracking line search will adaptively adjust the step size based on the Armijo condition, allowing for more efficient convergence. The choice of $\\alpha_0$, $\\rho$, and $c_1$ can significantly affect the speed of convergence and stability of the algorithm. 4. Using more complex linesearch techniques using toolboxes # In practice, we often use more sophisticated line search techniques that are not so easy to implement from scratch. One such technique is the line_search function from SciPy\u0026rsquo;s optimization module, which implements interpolation techniques to find an optimal step size.\nUse the line_search function from SciPy\u0026rsquo;s optimization module to find the optimal step size for each iteration of gradient descent. Compare the convergence behavior with the backtracking line search. Documentation is available here. III - (Bonus) The general case # Consider the general case where the target is also a vector, i.e., we have a multi-output linear regression problem. The model can be expressed as: \\begin{equation} \\mathbf{Y} = \\mathbf{X} \\tilde{\\mathbf{W}} + \\boldsymbol{\\epsilon}, \\label{eq:linear_model_multi_output} \\end{equation} where $\\mathbf{Y} \\in \\mathbb{R}^{n \\times m}$ is the target matrix with $m$ outputs, and $\\tilde{\\mathbf{W}} \\in \\mathbb{R}^{(d+1) \\times m}$ is the weight matrix.\nDerive all the necessary tools to solve this problem using the same techniques as in the previous sections.\nIV - (Bonus) Regularization # In practice, we often encounter situations where the model is overfitting the data, especially in high-dimensional settings. Regularization techniques are used to prevent overfitting by adding a penalty term to the loss function.\nImplement L2 regularization (ridge regression) by adding a penalty term to the loss function: \\begin{equation} L(\\tilde{\\mathbf{w}}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{X} \\tilde{\\mathbf{w}}\\|^2_2 + \\frac{\\lambda}{2} \\|\\tilde{\\mathbf{w}}\\|^2_2, \\end{equation} where $\\lambda \u0026gt; 0$ is the regularization parameter.\nV - (Bonus) Nonlinear regression # It\u0026rsquo;s actually possible to extend the linear regression model to nonlinear regression by setting up the design matrix $\\mathbf{X}$ to include nonlinear features of the input data. For example, we can include polynomial features suc as: \\begin{equation} \\mathbf{X} = \\begin{bmatrix} 1 \u0026amp; x_1 \u0026amp; x_1^2 \u0026amp; \\ldots \u0026amp; x_1^d \\\\ 1 \u0026amp; x_2 \u0026amp; x_2^2 \u0026amp; \\ldots \u0026amp; x_2^d \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; x_n \u0026amp; x_n^2 \u0026amp; \\ldots \u0026amp; x_n^d \\end{bmatrix} \\end{equation}\nFrom this information and your own research , implement a nonlinear regression model using polynomial features. You can use the PolynomialFeatures class from sklearn.preprocessing to generate polynomial features.\n"},{"id":19,"href":"/numerical_optimization/docs/lectures/advanced/","title":"II - Advanced problems","section":"Lectures","content":" Advanced problems # Content 1. Unconstrained optimization : Second-order 2. Proximal methods "},{"id":20,"href":"/numerical_optimization/docs/practical_labs/remote_sensing/","title":"II - Remote Sensing Project","section":"Practical labs","content":" Solving Inverse Problems in Remote Sensing # README # To get all the material for the labs, clone the following git repo : https://github.com/y-mhiri/hsi_unmixing_lab. You can follow this lab in multiple level of difficulty The easiest way : Answer only the theoretical question and follow the notebooks in notebooks/ to do the programming Intermediary level : Implement your own code to answer the lab questions using the helper functions you\u0026rsquo;ll find in src/ Expert level : I guess you don\u0026rsquo;t even need to clone the git repo\u0026hellip; If anything don\u0026rsquo;t hesitate to reach me at yassine.mhiri@univ-smb.fr.\nIntroduction # In this lab session, we will explore one of the many applications of numerical optimization, namely solving inverse problems. Inverse problems constitute a sub-field of applied mathematics with many applications in real-world data analysis. In this lab, you will work with hyperspectral images, commonly used in remote sensing.\nWe will begin with an introduction to hyperspectral images and derive a formulation of the hyperspectral unmixing problem. We will then express the inverse problem through an objective function to optimize. In the second part of the lab, we will experiment with multiple algorithms to solve the optimization problem and analyze their performance.\nHyperspectral unmixing is a fundamental problem in remote sensing that involves decomposing mixed pixel spectra into their constituent materials (endmembers) and their respective proportions (abundances). This problem is inherently an inverse problem where we seek to recover the underlying components from observed mixed signals.\nAs a bonus, we will explore blind hyperspectral unmixing where both endmembers and abundances are unknown, making the problem significantly more challenging.\nLearning Objectives # By the end of the session, you should be able to:\nDerive a data model and objective function to solve an inverse problem Implement descent algorithms to solve multi-objective optimization problems Handle constrained optimization problems using Lagrange multipliers and projection methods Benchmark optimization algorithms and measure their performance Manipulate real-world hyperspectral data in Python Understand the relationship between physical constraints and mathematical optimization I - Modelization and Problem Setup # 1. What is a Hyperspectral Image? # A hyperspectral image (HSI) captures information across a wide range of the electromagnetic spectrum. Unlike traditional images that capture data in three bands (red, green, and blue), hyperspectral images can capture data in hundreds of contiguous spectral bands. This allows for detailed spectral analysis and identification of materials in remote sensing based on their spectral signatures.\nEach pixel in a hyperspectral image contains a complete spectrum, which can be thought of as a \u0026ldquo;fingerprint\u0026rdquo; of the materials present in that pixel. The spectral dimension provides rich information about the chemical and physical properties of the observed scene.\nFigure 0.1: A hyperspectral image cube with spatial dimensions (x,y) and spectral dimension (λ)\nIn this lab session, we will work with an open dataset for hyperspectral data analysis: the Pavia University HSI dataset. This dataset is widely used in the remote sensing community and contains agricultural fields with different crop types.\nTasks:\nOpen the PaviaU dataset. You can either use the helper function provided or use the loadmatfunction from scipy.io.\nWhat is the size of the image and how many spectral bands does the image contain?\nHint The Pavia University dataset typically has dimensions of 145×145 pixels with 200 spectral bands after noise removal. You can check the shape using .shape attribute in Python. Use imshow or the provided helper functions to display a few band images of the HSI cube at different wavelengths. Try displaying bands at different spectral regions (e.g., visible, near-infrared, short-wave infrared).\nLoad and display the ground truth classification map. This shows the different crop types present in the scene.\nExtract and plot the mean spectrum of the first three classes from the ground truth. What differences do you observe between the spectral signatures?\nHint Use the ground truth labels to mask the hyperspectral data and compute the mean spectrum for each class. Different materials will have distinct spectral signatures, particularly in the near-infrared region. Save the matrix formed by the spectra of all the classes in a .npy using np.save 2. The Spectral Unmixing Linear Model # In remote sensing, hyperspectral images of the Earth are composed of pixels that represent mixed spectral signatures of various materials. Due to the limited spatial resolution of sensors, each pixel often contains multiple materials. The spectrum observed at each pixel is the result of multiple constituent spectra called endmembers.\nSpectral unmixing consists of estimating the per-pixel abundances of each endmember, giving insight into the various constituents of the observed field. This is particularly important in applications such as:\nAgricultural monitoring (crop type identification) Environmental monitoring (vegetation health assessment) Geological surveys (mineral identification) Urban planning (land cover classification) Figure 0.2: Spectral unmixing: decomposing mixed pixel spectra into endmember spectra and abundances\nIt is commonly accepted to model the pixel spectra by a linear mixing model as follows:\n$$ \\mathbf{y}_p = \\sum_{k=1}^K a_{kp} \\mathbf{s}_k + \\mathbf{n}_p $$\nwhere:\n$\\mathbf{y}_p \\in \\mathbb{R}^m$ is the observed spectrum at pixel $p$ (with $m$ spectral bands) $a_{kp} \\in \\mathbb{R}$ is the abundance (proportion) of the $k$-th endmember at pixel $p$ $\\mathbf{s}_k \\in \\mathbb{R}^m$ is the spectral signature of the $k$-th endmember $\\mathbf{n}_p \\in \\mathbb{R}^m$ represents the noise at pixel $p$ $K$ is the number of endmembers The linear mixing model assumes that (i) the observed spectrum is a linear combination of endmember spectra, (ii) there are no multiple scattering effects and (iii) the endmembers are spectrally distinct.\nTasks:\nDerive a matrix formulation of the linear mixing model in which images are vectorized. Define clearly: The data matrix $\\mathbf{Y} \\in \\mathbb{R}^{m \\times n}$ where $n$ is the number of pixels The endmember matrix $\\mathbf{S} \\in \\mathbb{R}^{m \\times K}$ The abundance matrix $\\mathbf{A} \\in \\mathbb{R}^{K \\times n}$ The noise matrix $\\mathbf{N} \\in \\mathbb{R}^{m \\times n}$ Hint Vectorize the spatial dimension by stacking the pixels column-wise. What are the physical constraints that should be imposed on the abundance matrix $\\mathbf{A}$? Justify your answer. Hint Think about what abundances represent physically. They are proportions of materials in a pixel. Remember that we assume that the endmember matrix includes all the material present in the scene. 3. Formulation of the Inverse Problem # The inverse problem in hyperspectral unmixing consists of estimating the endmember matrix $\\mathbf{S}$ and the abundance matrix $\\mathbf{A}$ from the observed data matrix $\\mathbf{Y}$. This is essentially a matrix factorization problem.\nThe basic objective function for this optimization problem can be written as:\n$$ \\min_{\\mathbf{A}, \\mathbf{S}} \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_F^2 $$\nwhere $\\Vert\\cdot\\Vert_F^2$ is the squared Frobenius norm of the matrix, defined as: $$ \\Vert\\mathbf{X}\\Vert_F^2 = \\sum_{i,j} x_{ij}^2 = \\text{tr}(\\mathbf{X}^T\\mathbf{X}) $$\nTasks:\nExplain why this is called an inverse problem. What makes it challenging compared to a forward problem?\nIs the objective function convex in both $\\mathbf{A}$ and $\\mathbf{S}$ simultaneously? Justify your answer.\nHint Is it convex in $\\mathbf{A}$ when $\\mathbf{S}$ is fixed, and convex in $\\mathbf{S}$ when $\\mathbf{A}$ is fixed ? Is it jointly convex in both variables ? II - Solving the Unconstrained Least Squares Inverse Problem # An inverse problem involves determining the input or parameters of a system from its observed output. In the context of spectral unmixing, the inverse problem is to estimate the endmember spectra and their abundances from the observed hyperspectral data.\nWe will mostly consider the case where the endmembers $\\mathbf{S}$ are known (e.g., from a spectral library or field measurements). In this case, we only need to estimate the abundance matrix $\\mathbf{A}$.\n1. Unconstrained Least Squares Solution # When the endmembers are known, the problem becomes a linear least squares problem for each pixel:\n$$ \\min_{\\mathbf{A}} \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_F^2 $$\nTasks:\nDerive the analytical solution for the unconstrained least squares problem. Show that the optimal abundance matrix is given by: $$ \\mathbf{A}^* = (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{S}^T\\mathbf{Y} $$ Hint This is similar to the multiple linear regression problem from Lab I. Use the fact that the Frobenius norm can be expressed as a sum of vector norms, and solve for each pixel independently. Under what conditions is this solution unique? What happens if $\\mathbf{S}^T\\mathbf{S}$ is not invertible?\nImplement the unconstrained least squares solution as a lambda function or regular function.\n2. Performance Evaluation # To evaluate the quality of spectral unmixing results, we need appropriate metrics that measure both spectral and spatial accuracy.\nTasks:\nImplement a function that evaluates the following evaluation metrics: Spectral Angle Mapper (SAM): Measures the angle between reconstructed and original spectra. Root Mean Square Error (RMSE): Measures the pixel-wise reconstruction error. SSIM : Measure a perceptual similiarity between images. You can either use the helper function implemented in the repo or use implementation for external libraries (scipy, sklearn).\nImplement a reconstruction and visualization function that:\nReconstructs the hyperspectral image from estimated abundances Displays RGB composite images (original vs. reconstructed) Shows abundance maps for each endmember Computes and displays the evaluation metrics Apply your unconstrained least squares solution to the Pavia University dataset:\nUse the mean spectra of the first 3-5 classes as endmembers Compute the abundance maps Evaluate the reconstruction quality Comment on the results. What do you observe about the abundance values? Are they physically meaningful?\nHint You\u0026rsquo;ll likely observe that some abundance values are negative or that abundances for a pixel don\u0026rsquo;t sum to one. This violates the physical constraints and motivates the need for constrained optimization. III - Solving Constrained Least Squares Inverse Problems # Given the physical interpretation of the spectral linear mixing model, a set of constraints should be added to the optimization problem to ensure physically meaningful solutions.\nTasks:\nPropose a set of equality and/or inequality constraints to ensure the interpretability of the solutions. Explain the physical meaning of each constraint. Hint The two main physical constraints are:\nSum-to-one constraint: $\\sum_{k=1}^K a_{kp} = 1$ for all pixels $p$ (abundances are proportions) Non-negativity constraint: $a_{kp} \\geq 0$ for all $k, p$ (negative abundances are not physical) As you must notice, this makes the optimization problem more difficult to solve. In the next parts of the lab, we will derive methods to solve the relaxed version of the fully constrained problem.\n1. Sum-to-One Constrained Least Squares # Let\u0026rsquo;s first consider only the sum-to-one constraint. The optimization problem becomes:\n$$ \\begin{align} \\min_{\\mathbf{A}} \u0026amp;\\quad \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_F^2 \\ \\text{subject to} \u0026amp;\\quad \\mathbf{1}^T\\mathbf{A} = \\mathbf{1}^T \\end{align} $$\nwhere $\\mathbf{1}$ is a vector of ones.\nTasks:\nDerive the Lagrangian of the constrained optimization problem Solution $$ L(\\mathbf{A}, \\lambda) = \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_2^2 + \\sum_{i=1}^n\\lambda_i\\left(1 - \\sum_{k=1}^K a_{ik}\\right) $$ Compute the optimal solution by setting the gradients to zero. Derive the closed-form expression for $\\lambda^*$ and the final solution. Solution Recall that the matrix vector product $$ (\\mathbf{c}\\mathbf{A})_i = \\sum_{k} c_k a_{ki}, (\\mathbf{A}\\mathbf{c})_k = \\sum_{i} a_{ki} c_i $$\nIn the problem below, we note $\\lambda\\in\\mathbb{R}^{n\\times 1}$, $\\mathbf{A}\\in\\mathbb{R}^{K\\times n}$ and $\\mathbf{1}_{d} = [1, \u0026hellip;, 1]^T \\in\\mathbb{R}^{d\\times 1}$.\nThe Lagrangian below can thus be expressed as, $$ \\begin{aligned} L(\\mathbf{A}, \\lambda) \u0026amp;= \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_2^2 + \\sum_{i=1}^n\\lambda_i - \\sum_{i=1}^n \\sum_{k=1}^K \\lambda_i a_{ik} \\\\ \u0026amp;= \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_2^2 + \\mathbf{1}_n^T \\mathbf{\\lambda} - \\sum_{k=1}^K (\\mathbf{A}\\mathbf{\\lambda})_k \\\\ \u0026amp;= \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_2^2 + \\mathbf{1}_n^T \\mathbf{\\lambda} - \\mathbf{1}_{K}^T\\mathbf{A}\\mathbf{\\lambda} \\end{aligned} $$\nThe gradient of $L$ w.r.t to $\\mathbf{A}$ and $\\lambda$ reads, $$ \\begin{aligned} \\nabla_{\\mathbf{A}}f(\\mathbf{A}) \u0026amp;= \\mathbf{S}^T(\\mathbf{S}\\mathbf{A} - \\mathbf{Y}) - \\mathbf{1}_{K}\\mathbf{\\lambda}^T \\\\ \\nabla_{\\mathbf{\\lambda}}f(\\mathbf{\\lambda}) \u0026amp;= -\\mathbf{A}^T\\mathbf{1}_{K} + \\mathbf{1}_{n} \\end{aligned} $$ Note that $\\mathbf{1}_{K}\\mathbf{\\lambda}^T \\in \\mathbf{R}^{K\\times n}$\nWe can now solve the system of linear equation that cancels the gradients, $$ \\begin{aligned} \\mathbf{S}^T(\\mathbf{S}\\mathbf{A} - \\mathbf{Y}) - \\mathbf{1}_{K}\\mathbf{\\lambda}^T \u0026amp;= 0\\\\ -\\mathbf{A}^T\\mathbf{1}_{K} + \\mathbf{1}_{n} =\u0026amp; 0 \\end{aligned} $$\nWithout extensive linear algebra we get\n$$ \\begin{aligned} \\mathbf{A}^* \u0026amp;= (\\mathbf{S}^T\\mathbf{S})^{-1} \\mathbf{S}^T \\mathbf{Y} + (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{1}_{K}\\mathbf{\\lambda}^T \\\\ \\end{aligned} $$ We notice that we can use the unconstrained solution to express this new constrained solution $$ \\mathbf{A}^* = \\mathbf{A}_{\\rm un}^* + (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{1}_{K}\\mathbf{\\lambda}^T $$ Injecting into the second equation we get $$ \\begin{aligned} -(\\mathbf{A}_{\\rm un}^* + (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{1}_{K}\\mathbf{\\lambda}^T)^T\\mathbf{1}_{K} + \\mathbf{1}_{n} \u0026amp;= 0 \\\\ -\\mathbf{A}_{\\rm un}^{*^T}\\mathbf{1}_{K} - \\mathbf{\\lambda}\\mathbf{1}_{K}^T(\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{1}_{K} + \\mathbf{1}_{n} \u0026amp;= 0 \\\\ \\end{aligned} $$ $$ \\mathbf{\\lambda}^{*} = \\frac{\\mathbf{1}_{n} - \\mathbf{A}_{\\rm un}^{*^T}\\mathbf{1}_{K}}{\\mathbf{1}_{K}^T(\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{1}_{K} } $$\nImplement the sum-to-one constrained solution and test it on the Pavia University dataset.\nDisplay the results: RGB composite, abundance maps, and compute evaluation metrics. Comment on the improvements compared to the unconstrained solution.\n2. Non-Negativity Constrained Least Squares # Now consider only the non-negativity constraints:\n$$ \\begin{align} \\min_{\\mathbf{A}} \u0026amp;\\quad \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_F^2 \\ \\text{subject to} \u0026amp;\\quad \\mathbf{A} \\geq 0 \\end{align} $$\nThis is a quadratic programming problem with inequality constraints. The analytical solution is more complex, but we can use iterative methods.\nWhat is a Quadratic Programming Problem? # A Quadratic Programming Problem is an optimization problem where:\nThe objective function is quadratic in the decision variables The constraints are linear (equality and/or inequality constraints) The general form of a QPP is: $$ \\begin{align} \\min_{\\mathbf{x}} \u0026amp;\\quad \\frac{1}{2}\\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{c}^T\\mathbf{x} \\ \\text{subject to} \u0026amp;\\quad \\mathbf{A}_{eq}\\mathbf{x} = \\mathbf{b}_{eq} \\ \u0026amp;\\quad \\mathbf{A}_{ineq}\\mathbf{x} \\leq \\mathbf{b}_{ineq} \\end{align} $$\nIn our case:\n$\\mathbf{Q} = \\mathbf{S}^T\\mathbf{S}$ (positive semi-definite matrix) The constraint $\\mathbf{A} \\geq 0$ represents simple bounds QPPs are convex when $\\mathbf{Q}$ is positive semi-definite, ensuring a unique global minimum Projected Gradient Algorithm # Since analytical solutions for QPPs with inequality constraints can be complex, we use iterative methods. The projected gradient algorithm is particularly effective for problems with simple constraints like non-negativity.\nThe algorithm works as follows:\nGradient Step: Take a standard gradient descent step $$\\tilde{\\mathbf{A}}^{(k+1)} = \\mathbf{A}^{(k)} - \\alpha \\nabla f(\\mathbf{A}^{(k)})$$\nProjection Step: Project the result onto the feasible set $$\\mathbf{A}^{(k+1)} = \\text{Proj}_{\\mathcal{C}}(\\tilde{\\mathbf{A}}^{(k+1)})$$\nwhere $\\mathcal{C}$ is the constraint set and $\\text{Proj}_{\\mathcal{C}}(\\cdot)$ is the orthogonal projection operator.\nFor our problem:\nThe gradient is: $\\nabla f(\\mathbf{A}) = 2\\mathbf{S}^T(\\mathbf{S}\\mathbf{A} - \\mathbf{Y})$ The projection onto the non-negative orthant is: $\\text{Proj}_{\\mathbb{R}_+}(\\mathbf{x})_i = \\max(0, x_i)$ The projection step ensures that all iterates remain feasible while the gradient step minimizes the objective function.\nTasks:\nDerive the gradient of the objective function $f(\\mathbf{A}) = \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_F^2$ with respect to $\\mathbf{A}$.\nImplement a projected gradient descent algorithm to solve the non-negativity constrained problem.\nApply the non-negativity constrained method to the Pavia University dataset. Be carefull on how you choose the descent step size. Compare results with previous methods.\n3. Fully Constrained Least Squares # Finally, let\u0026rsquo;s combine both constraints:\n$$ \\begin{align} \\min_{\\mathbf{A}} \u0026amp;\\quad \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_F^2 \\ \\text{subject to} \u0026amp;\\quad \\mathbf{1}^T\\mathbf{A} = \\mathbf{1}^T \\ \u0026amp;\\quad \\mathbf{A} \\geq 0 \\end{align} $$\nThe Simplex and Simplex Projection # The set defined by the intersection of the sum-to-one constraint and the non-negativity constraint defines the unit simplex.\nDefinition: The unit simplex in $\\mathbb{R}^K$ is defined as: $$\\Delta^{K-1} = \\left\\{\\mathbf{x} \\in \\mathbb{R}^K : \\sum_{i=1}^K x_i = 1, \\forall i \\leq K \\ x_i \\geq 0 \\right\\}$$\nGeometric Interpretation:\nIn 2D ($K=2$): The simplex is a line segment from $(1,0)$ to $(0,1)$ In 3D ($K=3$): The simplex is a triangle with vertices at $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$ In general: The simplex is a $(K-1)$-dimensional polytope Physical Meaning: In our context, each column of $\\mathbf{A}$ (representing abundances for one pixel) must lie on the simplex, ensuring that abundances are non-negative and sum to one.\nProjection onto the Simplex # The projection of a point $\\mathbf{v}$ onto the simplex is the closest point in the simplex to $\\mathbf{v}$ in the Euclidean sense: $$\\text{Proj}_{\\Delta^{K-1}}(\\mathbf{v}) = \\arg\\min_{\\mathbf{x} \\in \\Delta^{K-1}} \\Vert\\mathbf{x} - \\mathbf{v}\\Vert_2^2$$\nAn algorithm for simplex projection (Duchi et al., 2008) Sort the coordinates: $v_1 \\geq v_2 \\geq \\ldots \\geq v_K$ Find the threshold: $\\rho = \\max{j : v_j - \\frac{1}{j}(\\sum_{i=1}^j v_i - 1) \u0026gt; 0}$ Compute the Lagrange multiplier: $\\lambda = \\frac{1}{\\rho}(\\sum_{i=1}^\\rho v_i - 1)$ Project: $[\\text{Proj}_{\\Delta^{K-1}}(\\mathbf{v})]_i = \\max(0, v_i - \\lambda)$ Tasks:\nImplement a function that perform projection on the simplex:\nModify the previous projected gradient descent algorithm to the Fully Constrained Least Square problem.\nApply the fully constrained method to the Pavia University dataset.\nCompare all methods (unconstrained, sum-to-one, non-negativity, fully constrained) in terms of:\nReconstruction quality (SAM, RMSE, SSIM) Physical meaningfulness of abundances Computational efficiency Visual quality of abundance maps Analyze the convergence behavior of the different projected gradient algorithms. Plot the objective function value vs. iteration number.\nIV - Blind Hyperspectral Unmixing (Bonus) # This section is intended for advanced students who have successfully completed the previous sections.\nIn the previous sections, we assumed that the endmembers were known. In practice, this is often not the case, and we need to estimate both the endmembers and abundances simultaneously. This is called blind hyperspectral unmixing or blind source separation.\nThe optimization problem becomes:\n$$ \\begin{align} \\min_{\\mathbf{A}, \\mathbf{S}} \u0026amp;\\quad \\Vert\\mathbf{S}\\mathbf{A} - \\mathbf{Y}\\Vert_F^2 \\ \\text{subject to} \u0026amp;\\quad \\mathbf{1}^T\\mathbf{A} = \\mathbf{1}^T \\ \u0026amp;\\quad \\mathbf{A} \\geq 0 \\ \u0026amp;\\quad \\mathbf{S} \\geq 0 \\end{align} $$\nThis problem is significantly more challenging because:\nIt is non-convex in the joint variables $(\\mathbf{A}, \\mathbf{S})$ Multiple local minima exist The solution is not unique (scaling ambiguity) Block Coordinate Descent Algorithm # Since the problem is not jointly convex, but is convex in each variable when the other is fixed, we can use Block Coordinate Descent (BCD). This approach alternates between optimizing blocks of variables while keeping others fixed.\nAlgorithm Structure:\nInitialize $\\mathbf{S}^{(0)}$ and $\\mathbf{A}^{(0)}$ For $k = 0, 1, 2, \\ldots$ until convergence: Fix $\\mathbf{S} = \\mathbf{S}^{(k)}$ and solve for $\\mathbf{A}^{(k+1)}$: $$\\mathbf{A}^{(k+1)} = \\arg\\min_{\\mathbf{A}} \\Vert\\mathbf{S}^{(k)}\\mathbf{A} - \\mathbf{Y}\\Vert_F^2 \\quad \\text{s.t. } \\mathbf{1}^T\\mathbf{A} = \\mathbf{1}^T, \\mathbf{A} \\geq 0$$\nFix $\\mathbf{A} = \\mathbf{A}^{(k+1)}$ and solve for $\\mathbf{S}^{(k+1)}$: $$\\mathbf{S}^{(k+1)} = \\arg\\min_{\\mathbf{S}} \\Vert\\mathbf{S}\\mathbf{A}^{(k+1)} - \\mathbf{Y}\\Vert_F^2 \\quad \\text{s.t. } \\mathbf{S} \\geq 0$$\nKey Insight: Each subproblem is a constrained least squares problem that we know how to solve from Section III!\nTasks:\nAnalyze the subproblems:\nWhat type of optimization problem is the $\\mathbf{A}$-subproblem? Which method from Section III can you use? What type of optimization problem is the $\\mathbf{S}$-subproblem? How does it differ from the abundance estimation? Implement the Block Coordinate Descent algorithm:\nInitialization strategies: Implement at least two initialization methods:\nRandom initialization: Random positive values with proper normalization Supervised initialization: Use the endmembers provided on section III. Convergence analysis:\nImplement a convergence criterion based on the relative change in objective function Plot the objective function value vs. iteration number Compare convergence behavior with different initializations Algorithm analysis:\nWhat are the main limitations of this approach? How sensitive is the algorithm to initialization? What happens when the number of endmembers $K$ is incorrectly specified? Limitations and Advanced Methods # The Block Coordinate Descent approach presented here provides a good introduction to blind unmixing, but it has several limitations:\nLocal minima: The algorithm may converge to poor local solutions Initialization dependence: Results can vary significantly with different initializations Scaling ambiguity: Solutions are not unique up to scaling factors Slow convergence: Simple alternating optimization can be slow Advanced methods exist that address these limitations and provide better performance:\nNon-negative Matrix Factorization (NMF) with multiplicative updates Independent Component Analysis (ICA) for statistical independence \u0026hellip; These advanced methods often incorporate additional prior knowledge, regularization terms, or sophisticated optimization techniques to achieve more robust and accurate unmixing results.\n"},{"id":21,"href":"/numerical_optimization/docs/practical_labs/mnist/","title":"III - Digit recognition","section":"Practical labs","content":" Digit recognition with multi-layer perceptron # Soon to be added.\n"},{"id":22,"href":"/numerical_optimization/docs/lectures/machine_learning/","title":"III - Machine Learning problems","section":"Lectures","content":" Machine Learning problems # Content 1. From Linear regression to perceptron 2. Support Vector Machine 3. Neural Networks 4. Modern trends "},{"id":23,"href":"/numerical_optimization/docs/lectures/reminders/","title":"Reminders","section":"Lectures","content":" Reminders # Content Linear Algebra Differentiation "},{"id":24,"href":"/numerical_optimization/docs/practical_labs/environment/","title":"Lab environment","section":"Practical labs","content":" Lab Environment Setup # Welcome to the numerical optimization course! This page will guide you through setting up a modern, efficient Python environment using uv.\nPrerequisites # Good news! uv doesn\u0026rsquo;t require Python to be pre-installed - it can manage Python installations for you. However, having Python already installed won\u0026rsquo;t hurt.\nInstalling uv # 🐧 Linux \u0026amp; 🍎 macOS # The fastest way to install uv is using the official installer:\ncurl -LsSf https://astral.sh/uv/install.sh | sh This will:\nDownload and install the latest version of uv Add uv to your PATH automatically Work on both Linux and macOS Alternative installation methods:\nUsing pipx (if you have it): pipx install uv Using pip: pip install uv Using Homebrew (macOS): brew install uv 🪟 Windows # Option 1: PowerShell Installer (Recommended) Open PowerShell and run:\npowershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Option 2: Using pipx or pip If you have Python already installed:\npipx install uv # or pip install uv Option 3: Download from GitHub You can also download the installer or binaries directly from the GitHub releases page.\nVerify Installation # After installation, restart your terminal and verify uv is working:\nuv --version You should see output like uv 0.7.8 or similar (version numbers may vary).\nSetting Up the Lab Project # Now let\u0026rsquo;s create a dedicated project for all your numerical optimization lab sessions.\nStep 1: Create the Project # Navigate to where you want to store your course materials and run:\nuv init --bare numerical-optimization-labs cd numerical-optimization-labs The --bare flag creates a minimal project structure with only essential files: pyproject.toml, .python-version, and README.md (no default main.py file).\nStep 2: Install Required Packages # Now we\u0026rsquo;ll install all the packages you\u0026rsquo;ll need for the course. The uv add command will automatically create a virtual environment, install packages, and update both pyproject.toml and the lock file:\n# Core scientific computing packages uv add numpy scipy scikit-learn # Visualization libraries uv add matplotlib plotly # Machine learning framework uv add torch # Development and interactive tools uv add rich jupyter ipython # Optional: Add some useful development tools uv add pytest black ruff What\u0026rsquo;s happening behind the scenes:\nuv creates a virtual environment at .venv/ in your project directory All packages are installed into this isolated environment A lockfile (uv.lock) is generated containing exact versions of all dependencies for reproducible installations Your pyproject.toml is updated with the new dependencies Step 3: Verify Installation # Check that everything installed correctly:\nuv run python -c \u0026#34;import numpy, scipy, sklearn, matplotlib, plotly, torch, rich, jupyter, IPython; print(\u0026#39;✅ All packages imported successfully!\u0026#39;)\u0026#34; Using Your Lab Environment # Running Python Scripts # To run any Python script in your lab environment:\nuv run python your_script.py The uv run command ensures your script runs in the project\u0026rsquo;s virtual environment with all dependencies available.\nStarting Jupyter Lab/Notebook # To start Jupyter for interactive development:\n# For Jupyter Lab (recommended) uv run jupyter lab # For classic Jupyter Notebook uv run jupyter notebook Interactive Python (IPython) # For an enhanced interactive Python experience:\nuv run ipython Adding More Packages Later # If you need additional packages during the course:\nuv add package-name To remove packages you no longer need:\nuv remove package-name Project Structure # Your lab project will look like this:\nnumerical-optimization-labs/ ├── .venv/ # Virtual environment (auto-created) ├── .python-version # Pinned Python version ├── pyproject.toml # Project configuration and dependencies ├── uv.lock # Exact dependency versions (for reproducibility) ├── README.md # Project documentation └── your_lab_files.py # Your lab work goes here Sharing and Collaboration # Setting up the Environment on Another Machine # If you clone this project or share it with others, they can recreate the exact environment by running:\ncd numerical-optimization-labs uv sync This command reads the lockfile and installs the exact same versions of all dependencies.\nVersion Control # Make sure to commit these files to Git:\n✅ pyproject.toml ✅ uv.lock ✅ .python-version ❌ .venv/ (add this to .gitignore) Troubleshooting # Permission Issues # If you encounter permission errors, use sudo on macOS/Linux or run your command prompt as administrator on Windows.\nPython Version Issues # If you need a specific Python version:\n# Install a specific Python version uv python install 3.11 # Pin it to your project uv python pin 3.11 Environment Issues # If something goes wrong with your environment:\n# Sync environment with lockfile uv sync # Force recreate environment rm -rf .venv uv sync Package Conflicts # uv\u0026rsquo;s dependency resolver is much more robust than pip and should handle conflicts automatically. If you encounter issues, check the error message and try updating conflicting packages.\nQuick Reference Commands # Task Command Create new project uv init project-name Add packages uv add package1 package2 Remove packages uv remove package-name Run Python script uv run python script.py Start Jupyter uv run jupyter lab Install from lockfile uv sync List installed packages uv tree Update a package uv add package-name --upgrade Getting Help # uv Documentation: docs.astral.sh/uv Command Help: uv help or uv command --help Course Forum: [link to your course forum/discussion board] 🎉 You\u0026rsquo;re all set! Your lab environment is ready for numerical optimization adventures. If you encounter any issues during setup, don\u0026rsquo;t hesitate to ask for help during class or by mail.\n"},{"id":25,"href":"/numerical_optimization/docs/practical_labs/backtracking/","title":"Backtracking memo","section":"Practical labs","content":" Backtracking procedure for step size selection # Introduction # The backtracking line search is a fundamental technique in optimization algorithms for determining an appropriate step size that ensures sufficient decrease in the objective function. This procedure is particularly useful in gradient-based methods where choosing an optimal step size analytically is difficult or computationally expensive.\nMathematical setup # Consider the optimization problem: $\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x})$\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a continuously differentiable function. At iteration $k$, we have:\nCurrent point: $\\mathbf{x}_k$ Search direction: $\\mathbf{p}_k$ (typically $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ for steepest descent) Step size: $\\alpha_k \u0026gt; 0$ The Armijo condition # The backtracking procedure is based on the Armijo condition, which requires: $f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\leq f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k$\nwhere $c_1 \\in (0, 1)$ is a constant, typically $c_1 = 10^{-4}$.\nBacktracking algorithm steps # Step 1: Initialize parameters # Choose initial step size $\\alpha_0 \u0026gt; 0$ (e.g., $\\alpha_0 = 1$) Set reduction factor $\\rho \\in (0, 1)$ (typically $\\rho = 0.5$) Set Armijo parameter $c_1 \\in (0, 1)$ (typically $c_1 = 10^{-4}$) Set $\\alpha = \\alpha_0$ Step 2: Check Armijo condition # Evaluate the condition: $f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\leq f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k$\nStep 3: Backtrack if necessary # If the Armijo condition is satisfied:\nAccept $\\alpha_k = \\alpha$ Go to Step 4 Else:\nUpdate $\\alpha \\leftarrow \\rho \\alpha$ Return to Step 2 Step 4: Update iteration # Compute the new iterate: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$\nAlgorithmic description # Algorithm: Backtracking Line Search Input: x_k, p_k, α₀, ρ, c₁ Output: α_k 1. Set α = α₀ 2. While f(x_k + α·p_k) \u0026gt; f(x_k) + c₁·α·∇f(x_k)ᵀ·p_k do 3. α ← ρ·α 4. End while 5. Return α_k = α Theoretical properties # Convergence guarantee # Under mild conditions on $f$ and $\\mathbf{p}_k$, the backtracking procedure terminates in finite steps. Specifically, if:\n$f$ is continuously differentiable $\\mathbf{p}_k$ is a descent direction: $\\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k \u0026lt; 0$ Then there exists a step size $\\alpha \u0026gt; 0$ satisfying the Armijo condition.\nSufficient decrease property # The accepted step size $\\alpha_k$ ensures: $f(\\mathbf{x}_{k+1}) - f(\\mathbf{x}_k) \\leq c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^{\\mathrm{T}} \\mathbf{p}_k \u0026lt; 0$\nThis guarantees that each iteration decreases the objective function value.\nImplementation considerations # Choice of parameters # Initial step size $\\alpha_0$: Common choices are $\\alpha_0 = 1$ for Newton-type methods, or $\\alpha_0 = 1/|\\nabla f(\\mathbf{x}_k)|$ for gradient methods Reduction factor $\\rho$: Typically $\\rho = 0.5$ or $\\rho = 0.8$ Armijo parameter $c_1$: Usually $c_1 = 10^{-4}$ or $c_1 = 10^{-3}$ Computational complexity # Each backtracking iteration requires:\nOne function evaluation: $f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$ One gradient evaluation: $\\nabla f(\\mathbf{x}_k)$ (if not already computed) One vector operation: $\\mathbf{x}_k + \\alpha \\mathbf{p}_k$ Practical modifications # Maximum iterations: Limit the number of backtracking steps to prevent infinite loops:\nmax_backtracks = 50 iter = 0 while (Armijo condition not satisfied) and (iter \u0026lt; max_backtracks): α ← ρ·α iter ← iter + 1 Minimum step size: Set a lower bound $\\alpha_{min}$ to avoid numerical issues:\nif α \u0026lt; α_min: α = α_min break Applications # The backtracking procedure is widely used in:\nGradient descent: $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ Newton\u0026rsquo;s method: $\\mathbf{p}_k = -(\\mathbf{H}_k)^{-1} \\nabla f(\\mathbf{x}_k)$ where $\\mathbf{H}_k$ is the Hessian Quasi-Newton methods: $\\mathbf{p}_k = -\\mathbf{B}_k^{-1} \\nabla f(\\mathbf{x}_k)$ where $\\mathbf{B}_k$ approximates the Hessian Conjugate gradient methods Example implementation # def backtracking_line_search(f, grad_f, x_k, p_k, alpha_0=1.0, rho=0.5, c1=1e-4): \u0026#34;\u0026#34;\u0026#34; Backtracking line search for step size selection Parameters: - f: objective function - grad_f: gradient function - x_k: current point - p_k: search direction - alpha_0: initial step size - rho: reduction factor - c1: Armijo parameter Returns: - alpha_k: accepted step size \u0026#34;\u0026#34;\u0026#34; alpha = alpha_0 f_k = f(x_k) grad_k = grad_f(x_k) # Armijo condition right-hand side armijo_rhs = f_k + c1 * alpha * np.dot(grad_k, p_k) while f(x_k + alpha * p_k) \u0026gt; armijo_rhs: alpha *= rho armijo_rhs = f_k + c1 * alpha * np.dot(grad_k, p_k) return alpha Exercises\nImplement the backtracking line search for the quadratic function $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\mathrm{T}} \\mathbf{Q} \\mathbf{x} - \\mathbf{b}^{\\mathrm{T}} \\mathbf{x}$, where $\\mathbf{Q}$ is positive definite.\nCompare the performance of different values of $\\rho$ and $c_1$ on a test optimization problem.\nAnalyze the number of backtracking steps required as a function of the condition number of the Hessian matrix.\n"},{"id":26,"href":"/numerical_optimization/docs/practical_labs/quasinewton/","title":"Quasi-Newton methods memo","section":"Practical labs","content":" BFGS and SR1 Quasi-Newton Methods # Introduction # Quasi-Newton methods are a class of optimization algorithms that approximate the Newton direction without requiring explicit computation of the Hessian matrix. These methods achieve superlinear convergence while avoiding the computational expense and potential numerical difficulties associated with second derivatives. The two most prominent quasi-Newton methods are the BFGS (Broyden-Fletcher-Goldfarb-Shanno) and SR1 (Symmetric Rank-One) methods.\nMathematical setup # Consider the optimization problem: $\\min_{\\mathbf{x} \\in \\mathbb{R}^n} f(\\mathbf{x})$\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}$ is twice continuously differentiable. At iteration $k$, we have:\nCurrent point: $\\mathbf{x}_k$ Hessian approximation: $\\mathbf{B}_k$ (or inverse approximation $\\mathbf{H}_k = \\mathbf{B}_k^{-1}$) Search direction: $\\mathbf{p}_k = -\\mathbf{B}_k^{-1} \\nabla f_k = -\\mathbf{H}_k \\nabla f_k$ The secant equation # Quasi-Newton methods are based on the secant equation, which approximates the relationship between gradient changes and the Hessian:\n$\\mathbf{B}_{k+1} \\mathbf{s}_k = \\mathbf{y}_k$\nwhere:\n$\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ (step vector) $\\mathbf{y}_k = \\nabla f_{k+1} - \\nabla f_k$ (gradient difference) This equation ensures that the Hessian approximation captures the local curvature information from the most recent step.\nBFGS method # BFGS update formula # The BFGS method uses a rank-two update to maintain positive definiteness:\n$\\mathbf{B}_{k+1} = \\mathbf{B}_k - \\frac{\\mathbf{B}_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k}{\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{B}_k \\mathbf{s}_k} + \\frac{\\mathbf{y}_k \\mathbf{y}_k^{\\mathrm{T}}}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k}$\nInverse BFGS formula # For computational efficiency, the inverse approximation is often updated directly:\n$\\mathbf{H}_{k+1} = \\left(\\mathbf{I} - \\rho_k \\mathbf{s}_k \\mathbf{y}_k^{\\mathrm{T}}\\right) \\mathbf{H}_k \\left(\\mathbf{I} - \\rho_k \\mathbf{y}_k \\mathbf{s}_k^{\\mathrm{T}}\\right) + \\rho_k \\mathbf{s}_k \\mathbf{s}_k^{\\mathrm{T}}$\nwhere $\\rho_k = \\frac{1}{\\mathbf{y}_k^{\\mathrm{T}} \\mathbf{s}_k}$.\nProperties of BFGS # Maintains positive definiteness when $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k \u0026gt; 0$ Guarantees descent direction when positive definite Superlinear convergence rate Most robust and widely used quasi-Newton method SR1 method # SR1 update formula # The Symmetric Rank-One method uses a simpler rank-one update:\n$\\mathbf{B}_{k+1} = \\mathbf{B}_k + \\frac{(\\mathbf{y}_k - \\mathbf{B}_k \\mathbf{s}_k)(\\mathbf{y}_k - \\mathbf{B}_k \\mathbf{s}_k)^{\\mathrm{T}}}{(\\mathbf{y}_k - \\mathbf{B}_k \\mathbf{s}_k)^{\\mathrm{T}} \\mathbf{s}_k}$\nProperties of SR1 # Simpler structure (rank-one vs rank-two) Does not guarantee positive definiteness Can approximate indefinite Hessians better than BFGS May produce non-descent directions Often used in trust region methods Algorithm steps # Step 1: Initialize parameters # Choose initial point $\\mathbf{x}_0$ Set initial Hessian approximation $\\mathbf{H}_0 = \\mathbf{I}$ (or scaled identity) Set convergence tolerance $\\epsilon \u0026gt; 0$ Set $k = 0$ Step 2: Check convergence # If $|\\nabla f_k| \\leq \\epsilon$, stop and return $\\mathbf{x}_k$.\nStep 3: Compute search direction # $\\mathbf{p}_k = -\\mathbf{H}_k \\nabla f_k$\nStep 4: Line search # Find step size $\\alpha_k$ using backtracking line search or other methods.\nStep 5: Update iterate # $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$\nStep 6: Update Hessian approximation # Compute $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ and $\\mathbf{y}_k = \\nabla f_{k+1} - \\nabla f_k$. Update $\\mathbf{H}_{k+1}$ using BFGS or SR1 formula.\nStep 7: Increment and repeat # Set $k \\leftarrow k + 1$ and go to Step 2.\nAlgorithmic description # Algorithm: BFGS Quasi-Newton Method Input: x₀, ε, max_iter Output: x* 1. Set H₀ = I, k = 0 2. While ‖∇f_k‖ \u0026gt; ε and k \u0026lt; max_iter do 3. p_k = -H_k ∇f_k 4. α_k = backtracking_line_search(x_k, p_k) 5. x\\_{k+1} = x_k + α_k p_k 6. s_k = x\\_{k+1} - x_k 7. y_k = ∇f\\_{k+1} - ∇f_k 8. if s_k^T y_k \u0026gt; 0 then 9. ρ_k = 1/(y_k^T s_k) 10. H\\_{k+1} = (I - ρ_k s_k y_k^T) H_k (I - ρ_k y_k s_k^T) + ρ_k s_k s_k^T 11. else 12. H\\_{k+1} = H_k // Skip update 13. k ← k + 1 14. End while 15. Return x_k Theoretical properties # Convergence rate # Under suitable conditions:\nBFGS: Superlinear convergence when started sufficiently close to the solution SR1: Superlinear convergence in trust region frameworks Both methods reduce to Newton\u0026rsquo;s method when the Hessian approximation becomes exact Curvature condition # For BFGS, the condition $\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k \u0026gt; 0$ is crucial for maintaining positive definiteness. This is automatically satisfied for strongly convex functions with exact line search.\nImplementation considerations # Initial Hessian approximation # Common choices for $\\mathbf{H}_0$:\nIdentity matrix: $\\mathbf{H}_0 = \\mathbf{I}$ Scaled identity: $\\mathbf{H}_0 = \\gamma \\mathbf{I}$ where $\\gamma \u0026gt; 0$ Diagonal scaling based on the gradient Skipping updates # When $|\\mathbf{s}_k^{\\mathrm{T}} \\mathbf{y}_k|$ is too small, skip the update to avoid numerical instability:\nif |s_k^T y_k| \u0026lt; epsilon_skip * ‖s_k‖ * ‖y_k‖: H\\_{k+1} = H_k // Skip update Memory considerations # For large-scale problems, consider:\nL-BFGS: Limited memory version storing only recent vector pairs Matrix-free implementations: Avoid storing full matrices Applications # Quasi-Newton methods are widely used in:\nMachine learning: Training neural networks, logistic regression Engineering optimization: Design optimization, parameter estimation Economics: Portfolio optimization, economic modeling Scientific computing: Parameter fitting, inverse problems Example implementation # import numpy as np from scipy.optimize import line_search def bfgs_optimizer(f, grad_f, x0, tol=1e-6, max_iter=100): \u0026#34;\u0026#34;\u0026#34; BFGS quasi-Newton optimization algorithm Parameters: - f: objective function - grad_f: gradient function - x0: initial point - tol: convergence tolerance - max_iter: maximum iterations Returns: - x: optimal point - history: optimization history \u0026#34;\u0026#34;\u0026#34; n = len(x0) x = x0.copy() H = np.eye(n) # Initial inverse Hessian approximation history = {\u0026#39;x\u0026#39;: [x.copy()], \u0026#39;f\u0026#39;: [f(x)], \u0026#39;grad_norm\u0026#39;: [np.linalg.norm(grad_f(x))]} for k in range(max_iter): grad = grad_f(x) # Check convergence if np.linalg.norm(grad) \u0026lt; tol: break # Compute search direction p = -H @ grad # Line search (simplified backtracking) alpha = backtracking_line_search(f, grad_f, x, p) # Update iterate x_new = x + alpha * p grad_new = grad_f(x_new) # Compute vectors for BFGS update s = x_new - x y = grad_new - grad # BFGS update (if curvature condition satisfied) if s @ y \u0026gt; 1e-10: rho = 1.0 / (y @ s) I = np.eye(n) # BFGS inverse Hessian update H = (I - rho * np.outer(s, y)) @ H @ (I - rho * np.outer(y, s)) + rho * np.outer(s, s) # Update for next iteration x = x_new # Store history history[\u0026#39;x\u0026#39;].append(x.copy()) history[\u0026#39;f\u0026#39;].append(f(x)) history[\u0026#39;grad_norm\u0026#39;].append(np.linalg.norm(grad_new)) return x, history def sr1_optimizer(f, grad_f, x0, tol=1e-6, max_iter=100): \u0026#34;\u0026#34;\u0026#34; SR1 quasi-Newton optimization algorithm Parameters similar to BFGS \u0026#34;\u0026#34;\u0026#34; n = len(x0) x = x0.copy() H = np.eye(n) history = {\u0026#39;x\u0026#39;: [x.copy()], \u0026#39;f\u0026#39;: [f(x)], \u0026#39;grad_norm\u0026#39;: [np.linalg.norm(grad_f(x))]} for k in range(max_iter): grad = grad_f(x) if np.linalg.norm(grad) \u0026lt; tol: break p = -H @ grad alpha = backtracking_line_search(f, grad_f, x, p) x_new = x + alpha * p grad_new = grad_f(x_new) s = x_new - x y = grad_new - grad # SR1 update y_minus_Hs = y - H @ s denominator = y_minus_Hs @ s # Skip update if denominator too small if abs(denominator) \u0026gt; 1e-10: H = H + np.outer(y_minus_Hs, y_minus_Hs) / denominator x = x_new history[\u0026#39;x\u0026#39;].append(x.copy()) history[\u0026#39;f\u0026#39;].append(f(x)) history[\u0026#39;grad_norm\u0026#39;].append(np.linalg.norm(grad_new)) return x, history def backtracking_line_search(f, grad_f, x, p, alpha0=1.0, rho=0.5, c1=1e-4): \u0026#34;\u0026#34;\u0026#34;Simple backtracking line search\u0026#34;\u0026#34;\u0026#34; alpha = alpha0 f_x = f(x) grad_x = grad_f(x) while f(x + alpha * p) \u0026gt; f_x + c1 * alpha * (grad_x @ p): alpha *= rho if alpha \u0026lt; 1e-10: # Prevent infinite loop break return alpha Exercises # Exercise 1: Himmelblau\u0026rsquo;s Function # Implement both BFGS and SR1 methods to minimize Himmelblau\u0026rsquo;s function: $$f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2$$\nStarting points to try:\n$(0, 0)$ $(1, 1)$ $(-1, 1)$ Compare convergence rates and final solutions. Himmelblau\u0026rsquo;s function has four global minima.\nExercise 2: Mixed Function # Minimize the function: $$f(x_1, x_2) = \\frac{1}{2}x_1^2 + x_1 \\cos(x_2)$$\nStarting points:\n$(2, 0)$ $(0, \\pi)$ $(1, \\pi/2)$ Analyze how the cosine term affects convergence behavior and compare BFGS vs SR1 performance.\nExercise 3: Comparative Analysis # For both test functions:\nPlot the convergence history (function values and gradient norms) Count the number of function and gradient evaluations Analyze the condition number of the final Hessian approximations Compare with gradient descent and Newton\u0026rsquo;s method (when applicable) Exercise 4: Parameter Sensitivity # Study the effect of:\nDifferent initial Hessian approximations ($\\mathbf{H}_0 = \\gamma \\mathbf{I}$ for various $\\gamma$) Line search parameters in the backtracking procedure Skipping criteria for Hessian updates "},{"id":27,"href":"/numerical_optimization/docs/practical_labs/","title":"Practical labs","section":"Docs","content":" Practical labs # Content I - Linear Regression models II - Remote Sensing Project III - Digit recognition Lab environment Backtracking memo Quasi-Newton methods memo "}]