<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Practical labs on Numerical optimization</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/</link><description>Recent content in Practical labs on Numerical optimization</description><generator>Hugo</generator><language>fr</language><atom:link href="http://ammarmian.github.io/numerical_optimization/docs/practical_labs/index.xml" rel="self" type="application/rss+xml"/><item><title>I - Linear Regression models</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/linear_regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/linear_regression/</guid><description>&lt;h1 id="linear-regression-models">
 Linear Regression models
 &lt;a class="anchor" href="#linear-regression-models">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>In this lab session, we will explore the fundamental concepts of numerical optimization through the lens of linear regression. We&amp;rsquo;ll begin with the simplest case and gradually build up to more complex scenarios, comparing analytical solutions with numerical methods at each step.&lt;/p>
&lt;p>Linear regression is perhaps the most fundamental problem in machine learning and statistics. While it has a closed-form solution, implementing numerical optimization methods for this problem provides excellent intuition for more complex optimization scenarios where analytical solutions don&amp;rsquo;t exist.&lt;/p></description></item><item><title>II - Remote Sensing Project</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/remote_sensing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/remote_sensing/</guid><description>&lt;h1 id="solving-inverse-problems-in-remote-sensing">
 Solving Inverse Problems in Remote Sensing
 &lt;a class="anchor" href="#solving-inverse-problems-in-remote-sensing">#&lt;/a>
&lt;/h1>
&lt;h2 id="readme">
 README
 &lt;a class="anchor" href="#readme">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>To get all the material for the labs, clone the following git repo : &lt;a href="https://github.com/y-mhiri/hsi_unmixing_lab">https://github.com/y-mhiri/hsi_unmixing_lab&lt;/a>.&lt;/li>
&lt;li>You can follow this lab in multiple level of difficulty&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>&lt;strong>The easiest way&lt;/strong> : Answer only the theoretical question and follow the notebooks in &lt;code>notebooks/&lt;/code> to do the programming&lt;/li>
&lt;li>&lt;strong>Intermediary level&lt;/strong> : Implement your own code to answer the lab questions using the helper functions you&amp;rsquo;ll find in &lt;code>src/&lt;/code>&lt;/li>
&lt;li>&lt;strong>Expert level&lt;/strong> : I guess you don&amp;rsquo;t even need to clone the git repo&amp;hellip;&lt;/li>
&lt;/ol>
&lt;p>If anything don&amp;rsquo;t hesitate to reach me at &lt;code>yassine.mhiri@univ-smb.fr&lt;/code>.&lt;/p></description></item><item><title>III - Digit recognition</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/mnist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/mnist/</guid><description>&lt;h1 id="digit-recognition-with-multi-layer-perceptron">
 Digit recognition with multi-layer perceptron
 &lt;a class="anchor" href="#digit-recognition-with-multi-layer-perceptron">#&lt;/a>
&lt;/h1>
&lt;p>Soon to be added.&lt;/p></description></item><item><title>Lab environment</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/environment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/environment/</guid><description>&lt;h1 id="lab-environment-setup">
 Lab Environment Setup
 &lt;a class="anchor" href="#lab-environment-setup">#&lt;/a>
&lt;/h1>
&lt;p>Welcome to the numerical optimization course! This page will guide you through setting up a modern, efficient Python environment using &lt;strong>uv&lt;/strong>.&lt;/p>
&lt;h2 id="prerequisites">
 Prerequisites
 &lt;a class="anchor" href="#prerequisites">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Good news!&lt;/strong> uv doesn&amp;rsquo;t require Python to be pre-installed - it can manage Python installations for you. However, having Python already installed won&amp;rsquo;t hurt.&lt;/p>
&lt;h2 id="installing-uv">
 Installing uv
 &lt;a class="anchor" href="#installing-uv">#&lt;/a>
&lt;/h2>
&lt;h3 id="-linux---macos">
 🐧 Linux &amp;amp; 🍎 macOS
 &lt;a class="anchor" href="#-linux---macos">#&lt;/a>
&lt;/h3>
&lt;p>The fastest way to install uv is using the official installer:&lt;/p></description></item><item><title>Backtracking memo</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/backtracking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/backtracking/</guid><description>&lt;h1 id="backtracking-procedure-for-step-size-selection">
 Backtracking procedure for step size selection
 &lt;a class="anchor" href="#backtracking-procedure-for-step-size-selection">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>The backtracking line search is a fundamental technique in optimization algorithms for determining an appropriate step size that ensures sufficient decrease in the objective function. This procedure is particularly useful in gradient-based methods where choosing an optimal step size analytically is difficult or computationally expensive.&lt;/p>
&lt;h2 id="mathematical-setup">
 Mathematical setup
 &lt;a class="anchor" href="#mathematical-setup">#&lt;/a>
&lt;/h2>
&lt;p>Consider the optimization problem:
$\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$&lt;/p>
&lt;p>where $f: \mathbb{R}^n \to \mathbb{R}$ is a continuously differentiable function. At iteration $k$, we have:&lt;/p></description></item><item><title>Quasi-Newton methods memo</title><link>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/quasinewton/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://ammarmian.github.io/numerical_optimization/docs/practical_labs/quasinewton/</guid><description>&lt;h1 id="bfgs-and-sr1-quasi-newton-methods">
 BFGS and SR1 Quasi-Newton Methods
 &lt;a class="anchor" href="#bfgs-and-sr1-quasi-newton-methods">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>Quasi-Newton methods are a class of optimization algorithms that approximate the Newton direction without requiring explicit computation of the Hessian matrix. These methods achieve superlinear convergence while avoiding the computational expense and potential numerical difficulties associated with second derivatives. The two most prominent quasi-Newton methods are the BFGS (Broyden-Fletcher-Goldfarb-Shanno) and SR1 (Symmetric Rank-One) methods.&lt;/p>
&lt;h2 id="mathematical-setup">
 Mathematical setup
 &lt;a class="anchor" href="#mathematical-setup">#&lt;/a>
&lt;/h2>
&lt;p>Consider the optimization problem:
$\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$&lt;/p></description></item></channel></rss>