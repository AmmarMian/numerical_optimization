<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  BFGS and SR1 Quasi-Newton Methods
  #


  Introduction
  #

Quasi-Newton methods are a class of optimization algorithms that approximate the Newton direction without requiring explicit computation of the Hessian matrix. These methods achieve superlinear convergence while avoiding the computational expense and potential numerical difficulties associated with second derivatives. The two most prominent quasi-Newton methods are the BFGS (Broyden-Fletcher-Goldfarb-Shanno) and SR1 (Symmetric Rank-One) methods.

  Mathematical setup
  #

Consider the optimization problem:
$\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/practical_labs/quasinewton/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="Quasi-Newton methods memo"><meta property="og:description" content="BFGS and SR1 Quasi-Newton Methods # Introduction # Quasi-Newton methods are a class of optimization algorithms that approximate the Newton direction without requiring explicit computation of the Hessian matrix. These methods achieve superlinear convergence while avoiding the computational expense and potential numerical difficulties associated with second derivatives. The two most prominent quasi-Newton methods are the BFGS (Broyden-Fletcher-Goldfarb-Shanno) and SR1 (Symmetric Rank-One) methods.
Mathematical setup # Consider the optimization problem: $\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$"><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Quasi-Newton methods memo | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/practical_labs/quasinewton/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.6698db5ec0b50ada94dfb50e4977481fdc66b0a9d0e104277817f19b95d6da6f.js integrity="sha256-ZpjbXsC1CtqU37UOSXdIH9xmsKnQ4QQneBfxm5XW2m8=" crossorigin=anonymous></script><script>const chapterNum=0;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_projected/>5b. Constrained optimization - Projected Gradient Descent</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimization_linear/>6. Constrained optimization - Linear programming</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/fundamentals/>1. Machine learning fundamentals</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Classification and support vector machines</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - MNIST and Fashion-MNIST Classification</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li><li><a href=/numerical_optimization/docs/practical_labs/quasinewton/ class=active>Quasi-Newton methods memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>Quasi-Newton methods memo</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#mathematical-setup>Mathematical setup</a></li><li><a href=#the-secant-equation>The secant equation</a></li><li><a href=#bfgs-method>BFGS method</a><ul><li><a href=#bfgs-update-formula>BFGS update formula</a></li><li><a href=#inverse-bfgs-formula>Inverse BFGS formula</a></li><li><a href=#properties-of-bfgs>Properties of BFGS</a></li></ul></li><li><a href=#sr1-method>SR1 method</a><ul><li><a href=#sr1-update-formula>SR1 update formula</a></li><li><a href=#properties-of-sr1>Properties of SR1</a></li></ul></li><li><a href=#algorithm-steps>Algorithm steps</a><ul><li><a href=#step-1-initialize-parameters>Step 1: Initialize parameters</a></li><li><a href=#step-2-check-convergence>Step 2: Check convergence</a></li><li><a href=#step-3-compute-search-direction>Step 3: Compute search direction</a></li><li><a href=#step-4-line-search>Step 4: Line search</a></li><li><a href=#step-5-update-iterate>Step 5: Update iterate</a></li><li><a href=#step-6-update-hessian-approximation>Step 6: Update Hessian approximation</a></li><li><a href=#step-7-increment-and-repeat>Step 7: Increment and repeat</a></li></ul></li><li><a href=#algorithmic-description>Algorithmic description</a></li><li><a href=#theoretical-properties>Theoretical properties</a><ul><li><a href=#convergence-rate>Convergence rate</a></li><li><a href=#curvature-condition>Curvature condition</a></li></ul></li><li><a href=#implementation-considerations>Implementation considerations</a><ul><li><a href=#initial-hessian-approximation>Initial Hessian approximation</a></li><li><a href=#skipping-updates>Skipping updates</a></li><li><a href=#memory-considerations>Memory considerations</a></li></ul></li><li><a href=#applications>Applications</a></li><li><a href=#example-implementation>Example implementation</a></li><li><a href=#exercises>Exercises</a><ul><li><a href=#exercise-1-himmelblaus-function>Exercise 1: Himmelblau&rsquo;s Function</a></li><li><a href=#exercise-2-mixed-function>Exercise 2: Mixed Function</a></li><li><a href=#exercise-3-comparative-analysis>Exercise 3: Comparative Analysis</a></li><li><a href=#exercise-4-parameter-sensitivity>Exercise 4: Parameter Sensitivity</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=bfgs-and-sr1-quasi-newton-methods>BFGS and SR1 Quasi-Newton Methods
<a class=anchor href=#bfgs-and-sr1-quasi-newton-methods>#</a></h1><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>Quasi-Newton methods are a class of optimization algorithms that approximate the Newton direction without requiring explicit computation of the Hessian matrix. These methods achieve superlinear convergence while avoiding the computational expense and potential numerical difficulties associated with second derivatives. The two most prominent quasi-Newton methods are the BFGS (Broyden-Fletcher-Goldfarb-Shanno) and SR1 (Symmetric Rank-One) methods.</p><h2 id=mathematical-setup>Mathematical setup
<a class=anchor href=#mathematical-setup>#</a></h2><p>Consider the optimization problem:
$\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$</p><p>where $f: \mathbb{R}^n \to \mathbb{R}$ is twice continuously differentiable. At iteration $k$, we have:</p><ul><li>Current point: $\mathbf{x}_k$</li><li>Hessian approximation: $\mathbf{B}_k$ (or inverse approximation $\mathbf{H}_k = \mathbf{B}_k^{-1}$)</li><li>Search direction: $\mathbf{p}_k = -\mathbf{B}_k^{-1} \nabla f_k = -\mathbf{H}_k \nabla f_k$</li></ul><h2 id=the-secant-equation>The secant equation
<a class=anchor href=#the-secant-equation>#</a></h2><p>Quasi-Newton methods are based on the secant equation, which approximates the relationship between gradient changes and the Hessian:</p><p>$\mathbf{B}_{k+1} \mathbf{s}_k = \mathbf{y}_k$</p><p>where:</p><ul><li>$\mathbf{s}_k = \mathbf{x}_{k+1} - \mathbf{x}_k$ (step vector)</li><li>$\mathbf{y}_k = \nabla f_{k+1} - \nabla f_k$ (gradient difference)</li></ul><p>This equation ensures that the Hessian approximation captures the local curvature information from the most recent step.</p><h2 id=bfgs-method>BFGS method
<a class=anchor href=#bfgs-method>#</a></h2><h3 id=bfgs-update-formula>BFGS update formula
<a class=anchor href=#bfgs-update-formula>#</a></h3><p>The BFGS method uses a rank-two update to maintain positive definiteness:</p><p>$\mathbf{B}_{k+1} = \mathbf{B}_k - \frac{\mathbf{B}_k \mathbf{s}_k \mathbf{s}_k^{\mathrm{T}} \mathbf{B}_k}{\mathbf{s}_k^{\mathrm{T}} \mathbf{B}_k \mathbf{s}_k} + \frac{\mathbf{y}_k \mathbf{y}_k^{\mathrm{T}}}{\mathbf{y}_k^{\mathrm{T}} \mathbf{s}_k}$</p><h3 id=inverse-bfgs-formula>Inverse BFGS formula
<a class=anchor href=#inverse-bfgs-formula>#</a></h3><p>For computational efficiency, the inverse approximation is often updated directly:</p><p>$\mathbf{H}_{k+1} = \left(\mathbf{I} - \rho_k \mathbf{s}_k \mathbf{y}_k^{\mathrm{T}}\right) \mathbf{H}_k \left(\mathbf{I} - \rho_k \mathbf{y}_k \mathbf{s}_k^{\mathrm{T}}\right) + \rho_k \mathbf{s}_k \mathbf{s}_k^{\mathrm{T}}$</p><p>where $\rho_k = \frac{1}{\mathbf{y}_k^{\mathrm{T}} \mathbf{s}_k}$.</p><h3 id=properties-of-bfgs>Properties of BFGS
<a class=anchor href=#properties-of-bfgs>#</a></h3><ul><li>Maintains positive definiteness when $\mathbf{s}_k^{\mathrm{T}} \mathbf{y}_k > 0$</li><li>Guarantees descent direction when positive definite</li><li>Superlinear convergence rate</li><li>Most robust and widely used quasi-Newton method</li></ul><h2 id=sr1-method>SR1 method
<a class=anchor href=#sr1-method>#</a></h2><h3 id=sr1-update-formula>SR1 update formula
<a class=anchor href=#sr1-update-formula>#</a></h3><p>The Symmetric Rank-One method uses a simpler rank-one update:</p><p>$\mathbf{B}_{k+1} = \mathbf{B}_k + \frac{(\mathbf{y}_k - \mathbf{B}_k \mathbf{s}_k)(\mathbf{y}_k - \mathbf{B}_k \mathbf{s}_k)^{\mathrm{T}}}{(\mathbf{y}_k - \mathbf{B}_k \mathbf{s}_k)^{\mathrm{T}} \mathbf{s}_k}$</p><h3 id=properties-of-sr1>Properties of SR1
<a class=anchor href=#properties-of-sr1>#</a></h3><ul><li>Simpler structure (rank-one vs rank-two)</li><li>Does not guarantee positive definiteness</li><li>Can approximate indefinite Hessians better than BFGS</li><li>May produce non-descent directions</li><li>Often used in trust region methods</li></ul><h2 id=algorithm-steps>Algorithm steps
<a class=anchor href=#algorithm-steps>#</a></h2><h3 id=step-1-initialize-parameters>Step 1: Initialize parameters
<a class=anchor href=#step-1-initialize-parameters>#</a></h3><ul><li>Choose initial point $\mathbf{x}_0$</li><li>Set initial Hessian approximation $\mathbf{H}_0 = \mathbf{I}$ (or scaled identity)</li><li>Set convergence tolerance $\epsilon > 0$</li><li>Set $k = 0$</li></ul><h3 id=step-2-check-convergence>Step 2: Check convergence
<a class=anchor href=#step-2-check-convergence>#</a></h3><p>If $|\nabla f_k| \leq \epsilon$, <strong>stop</strong> and return $\mathbf{x}_k$.</p><h3 id=step-3-compute-search-direction>Step 3: Compute search direction
<a class=anchor href=#step-3-compute-search-direction>#</a></h3><p>$\mathbf{p}_k = -\mathbf{H}_k \nabla f_k$</p><h3 id=step-4-line-search>Step 4: Line search
<a class=anchor href=#step-4-line-search>#</a></h3><p>Find step size $\alpha_k$ using backtracking line search or other methods.</p><h3 id=step-5-update-iterate>Step 5: Update iterate
<a class=anchor href=#step-5-update-iterate>#</a></h3><p>$\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k$</p><h3 id=step-6-update-hessian-approximation>Step 6: Update Hessian approximation
<a class=anchor href=#step-6-update-hessian-approximation>#</a></h3><p>Compute $\mathbf{s}_k = \mathbf{x}_{k+1} - \mathbf{x}_k$ and $\mathbf{y}_k = \nabla f_{k+1} - \nabla f_k$.
Update $\mathbf{H}_{k+1}$ using BFGS or SR1 formula.</p><h3 id=step-7-increment-and-repeat>Step 7: Increment and repeat
<a class=anchor href=#step-7-increment-and-repeat>#</a></h3><p>Set $k \leftarrow k + 1$ and <strong>go to</strong> Step 2.</p><h2 id=algorithmic-description>Algorithmic description
<a class=anchor href=#algorithmic-description>#</a></h2><pre tabindex=0><code>Algorithm: BFGS Quasi-Newton Method
Input: x₀, ε, max_iter
Output: x*

1. Set H₀ = I, k = 0
2. While ‖∇f_k‖ &gt; ε and k &lt; max_iter do
3.    p_k = -H_k ∇f_k
4.    α_k = backtracking_line_search(x_k, p_k)
5.    x\_{k+1} = x_k + α_k p_k
6.    s_k = x\_{k+1} - x_k
7.    y_k = ∇f\_{k+1} - ∇f_k
8.    if s_k^T y_k &gt; 0 then
9.       ρ_k = 1/(y_k^T s_k)
10.      H\_{k+1} = (I - ρ_k s_k y_k^T) H_k (I - ρ_k y_k s_k^T) + ρ_k s_k s_k^T
11.   else
12.      H\_{k+1} = H_k  // Skip update
13.   k ← k + 1
14. End while
15. Return x_k
</code></pre><h2 id=theoretical-properties>Theoretical properties
<a class=anchor href=#theoretical-properties>#</a></h2><h3 id=convergence-rate>Convergence rate
<a class=anchor href=#convergence-rate>#</a></h3><p>Under suitable conditions:</p><ul><li><strong>BFGS:</strong> Superlinear convergence when started sufficiently close to the solution</li><li><strong>SR1:</strong> Superlinear convergence in trust region frameworks</li><li>Both methods reduce to Newton&rsquo;s method when the Hessian approximation becomes exact</li></ul><h3 id=curvature-condition>Curvature condition
<a class=anchor href=#curvature-condition>#</a></h3><p>For BFGS, the condition $\mathbf{s}_k^{\mathrm{T}} \mathbf{y}_k > 0$ is crucial for maintaining positive definiteness. This is automatically satisfied for strongly convex functions with exact line search.</p><h2 id=implementation-considerations>Implementation considerations
<a class=anchor href=#implementation-considerations>#</a></h2><h3 id=initial-hessian-approximation>Initial Hessian approximation
<a class=anchor href=#initial-hessian-approximation>#</a></h3><p>Common choices for $\mathbf{H}_0$:</p><ul><li>Identity matrix: $\mathbf{H}_0 = \mathbf{I}$</li><li>Scaled identity: $\mathbf{H}_0 = \gamma \mathbf{I}$ where $\gamma > 0$</li><li>Diagonal scaling based on the gradient</li></ul><h3 id=skipping-updates>Skipping updates
<a class=anchor href=#skipping-updates>#</a></h3><p>When $|\mathbf{s}_k^{\mathrm{T}} \mathbf{y}_k|$ is too small, skip the update to avoid numerical instability:</p><pre tabindex=0><code>if |s_k^T y_k| &lt; epsilon_skip * ‖s_k‖ * ‖y_k‖:
    H\_{k+1} = H_k  // Skip update
</code></pre><h3 id=memory-considerations>Memory considerations
<a class=anchor href=#memory-considerations>#</a></h3><p>For large-scale problems, consider:</p><ul><li><strong>L-BFGS:</strong> Limited memory version storing only recent vector pairs</li><li><strong>Matrix-free implementations:</strong> Avoid storing full matrices</li></ul><h2 id=applications>Applications
<a class=anchor href=#applications>#</a></h2><p>Quasi-Newton methods are widely used in:</p><ul><li><strong>Machine learning:</strong> Training neural networks, logistic regression</li><li><strong>Engineering optimization:</strong> Design optimization, parameter estimation</li><li><strong>Economics:</strong> Portfolio optimization, economic modeling</li><li><strong>Scientific computing:</strong> Parameter fitting, inverse problems</li></ul><h2 id=example-implementation>Example implementation
<a class=anchor href=#example-implementation>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> scipy.optimize <span style=color:#f92672>import</span> line_search
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bfgs_optimizer</span>(f, grad_f, x0, tol<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>, max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    BFGS quasi-Newton optimization algorithm
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - f: objective function
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - grad_f: gradient function
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - x0: initial point
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - tol: convergence tolerance
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - max_iter: maximum iterations
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - x: optimal point
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - history: optimization history
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> len(x0)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x0<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>    H <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(n)  <span style=color:#75715e># Initial inverse Hessian approximation</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    history <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;x&#39;</span>: [x<span style=color:#f92672>.</span>copy()], <span style=color:#e6db74>&#39;f&#39;</span>: [f(x)], <span style=color:#e6db74>&#39;grad_norm&#39;</span>: [np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(grad_f(x))]}
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(max_iter):
</span></span><span style=display:flex><span>        grad <span style=color:#f92672>=</span> grad_f(x)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Check convergence</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(grad) <span style=color:#f92672>&lt;</span> tol:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        <span style=color:#75715e># Compute search direction</span>
</span></span><span style=display:flex><span>        p <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>H <span style=color:#f92672>@</span> grad
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Line search (simplified backtracking)</span>
</span></span><span style=display:flex><span>        alpha <span style=color:#f92672>=</span> backtracking_line_search(f, grad_f, x, p)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Update iterate</span>
</span></span><span style=display:flex><span>        x_new <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> alpha <span style=color:#f92672>*</span> p
</span></span><span style=display:flex><span>        grad_new <span style=color:#f92672>=</span> grad_f(x_new)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Compute vectors for BFGS update</span>
</span></span><span style=display:flex><span>        s <span style=color:#f92672>=</span> x_new <span style=color:#f92672>-</span> x
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> grad_new <span style=color:#f92672>-</span> grad
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># BFGS update (if curvature condition satisfied)</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s <span style=color:#f92672>@</span> y <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1e-10</span>:
</span></span><span style=display:flex><span>            rho <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> (y <span style=color:#f92672>@</span> s)
</span></span><span style=display:flex><span>            I <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(n)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># BFGS inverse Hessian update</span>
</span></span><span style=display:flex><span>            H <span style=color:#f92672>=</span> (I <span style=color:#f92672>-</span> rho <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>outer(s, y)) <span style=color:#f92672>@</span> H <span style=color:#f92672>@</span> (I <span style=color:#f92672>-</span> rho <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>outer(y, s)) <span style=color:#f92672>+</span> rho <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>outer(s, s)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Update for next iteration</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x_new
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Store history</span>
</span></span><span style=display:flex><span>        history[<span style=color:#e6db74>&#39;x&#39;</span>]<span style=color:#f92672>.</span>append(x<span style=color:#f92672>.</span>copy())
</span></span><span style=display:flex><span>        history[<span style=color:#e6db74>&#39;f&#39;</span>]<span style=color:#f92672>.</span>append(f(x))
</span></span><span style=display:flex><span>        history[<span style=color:#e6db74>&#39;grad_norm&#39;</span>]<span style=color:#f92672>.</span>append(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(grad_new))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x, history
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sr1_optimizer</span>(f, grad_f, x0, tol<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>, max_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    SR1 quasi-Newton optimization algorithm
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters similar to BFGS
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> len(x0)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> x0<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>    H <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(n)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    history <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;x&#39;</span>: [x<span style=color:#f92672>.</span>copy()], <span style=color:#e6db74>&#39;f&#39;</span>: [f(x)], <span style=color:#e6db74>&#39;grad_norm&#39;</span>: [np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(grad_f(x))]}
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(max_iter):
</span></span><span style=display:flex><span>        grad <span style=color:#f92672>=</span> grad_f(x)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(grad) <span style=color:#f92672>&lt;</span> tol:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        p <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>H <span style=color:#f92672>@</span> grad
</span></span><span style=display:flex><span>        alpha <span style=color:#f92672>=</span> backtracking_line_search(f, grad_f, x, p)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        x_new <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> alpha <span style=color:#f92672>*</span> p
</span></span><span style=display:flex><span>        grad_new <span style=color:#f92672>=</span> grad_f(x_new)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        s <span style=color:#f92672>=</span> x_new <span style=color:#f92672>-</span> x
</span></span><span style=display:flex><span>        y <span style=color:#f92672>=</span> grad_new <span style=color:#f92672>-</span> grad
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># SR1 update</span>
</span></span><span style=display:flex><span>        y_minus_Hs <span style=color:#f92672>=</span> y <span style=color:#f92672>-</span> H <span style=color:#f92672>@</span> s
</span></span><span style=display:flex><span>        denominator <span style=color:#f92672>=</span> y_minus_Hs <span style=color:#f92672>@</span> s
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Skip update if denominator too small</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> abs(denominator) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1e-10</span>:
</span></span><span style=display:flex><span>            H <span style=color:#f92672>=</span> H <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>outer(y_minus_Hs, y_minus_Hs) <span style=color:#f92672>/</span> denominator
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> x_new
</span></span><span style=display:flex><span>        history[<span style=color:#e6db74>&#39;x&#39;</span>]<span style=color:#f92672>.</span>append(x<span style=color:#f92672>.</span>copy())
</span></span><span style=display:flex><span>        history[<span style=color:#e6db74>&#39;f&#39;</span>]<span style=color:#f92672>.</span>append(f(x))
</span></span><span style=display:flex><span>        history[<span style=color:#e6db74>&#39;grad_norm&#39;</span>]<span style=color:#f92672>.</span>append(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(grad_new))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x, history
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backtracking_line_search</span>(f, grad_f, x, p, alpha0<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>, rho<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, c1<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-4</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Simple backtracking line search&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    alpha <span style=color:#f92672>=</span> alpha0
</span></span><span style=display:flex><span>    f_x <span style=color:#f92672>=</span> f(x)
</span></span><span style=display:flex><span>    grad_x <span style=color:#f92672>=</span> grad_f(x)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> f(x <span style=color:#f92672>+</span> alpha <span style=color:#f92672>*</span> p) <span style=color:#f92672>&gt;</span> f_x <span style=color:#f92672>+</span> c1 <span style=color:#f92672>*</span> alpha <span style=color:#f92672>*</span> (grad_x <span style=color:#f92672>@</span> p):
</span></span><span style=display:flex><span>        alpha <span style=color:#f92672>*=</span> rho
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> alpha <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1e-10</span>:  <span style=color:#75715e># Prevent infinite loop</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> alpha
</span></span></code></pre></div><h2 id=exercises>Exercises
<a class=anchor href=#exercises>#</a></h2><h3 id=exercise-1-himmelblaus-function>Exercise 1: Himmelblau&rsquo;s Function
<a class=anchor href=#exercise-1-himmelblaus-function>#</a></h3><p>Implement both BFGS and SR1 methods to minimize Himmelblau&rsquo;s function:
$$f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2$$</p><p>Starting points to try:</p><ul><li>$(0, 0)$</li><li>$(1, 1)$</li><li>$(-1, 1)$</li></ul><p>Compare convergence rates and final solutions. Himmelblau&rsquo;s function has four global minima.</p><h3 id=exercise-2-mixed-function>Exercise 2: Mixed Function
<a class=anchor href=#exercise-2-mixed-function>#</a></h3><p>Minimize the function:
$$f(x_1, x_2) = \frac{1}{2}x_1^2 + x_1 \cos(x_2)$$</p><p>Starting points:</p><ul><li>$(2, 0)$</li><li>$(0, \pi)$</li><li>$(1, \pi/2)$</li></ul><p>Analyze how the cosine term affects convergence behavior and compare BFGS vs SR1 performance.</p><h3 id=exercise-3-comparative-analysis>Exercise 3: Comparative Analysis
<a class=anchor href=#exercise-3-comparative-analysis>#</a></h3><p>For both test functions:</p><ol><li>Plot the convergence history (function values and gradient norms)</li><li>Count the number of function and gradient evaluations</li><li>Analyze the condition number of the final Hessian approximations</li><li>Compare with gradient descent and Newton&rsquo;s method (when applicable)</li></ol><h3 id=exercise-4-parameter-sensitivity>Exercise 4: Parameter Sensitivity
<a class=anchor href=#exercise-4-parameter-sensitivity>#</a></h3><p>Study the effect of:</p><ul><li>Different initial Hessian approximations ($\mathbf{H}_0 = \gamma \mathbf{I}$ for various $\gamma$)</li><li>Line search parameters in the backtracking procedure</li><li>Skipping criteria for Hessian updates</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#mathematical-setup>Mathematical setup</a></li><li><a href=#the-secant-equation>The secant equation</a></li><li><a href=#bfgs-method>BFGS method</a><ul><li><a href=#bfgs-update-formula>BFGS update formula</a></li><li><a href=#inverse-bfgs-formula>Inverse BFGS formula</a></li><li><a href=#properties-of-bfgs>Properties of BFGS</a></li></ul></li><li><a href=#sr1-method>SR1 method</a><ul><li><a href=#sr1-update-formula>SR1 update formula</a></li><li><a href=#properties-of-sr1>Properties of SR1</a></li></ul></li><li><a href=#algorithm-steps>Algorithm steps</a><ul><li><a href=#step-1-initialize-parameters>Step 1: Initialize parameters</a></li><li><a href=#step-2-check-convergence>Step 2: Check convergence</a></li><li><a href=#step-3-compute-search-direction>Step 3: Compute search direction</a></li><li><a href=#step-4-line-search>Step 4: Line search</a></li><li><a href=#step-5-update-iterate>Step 5: Update iterate</a></li><li><a href=#step-6-update-hessian-approximation>Step 6: Update Hessian approximation</a></li><li><a href=#step-7-increment-and-repeat>Step 7: Increment and repeat</a></li></ul></li><li><a href=#algorithmic-description>Algorithmic description</a></li><li><a href=#theoretical-properties>Theoretical properties</a><ul><li><a href=#convergence-rate>Convergence rate</a></li><li><a href=#curvature-condition>Curvature condition</a></li></ul></li><li><a href=#implementation-considerations>Implementation considerations</a><ul><li><a href=#initial-hessian-approximation>Initial Hessian approximation</a></li><li><a href=#skipping-updates>Skipping updates</a></li><li><a href=#memory-considerations>Memory considerations</a></li></ul></li><li><a href=#applications>Applications</a></li><li><a href=#example-implementation>Example implementation</a></li><li><a href=#exercises>Exercises</a><ul><li><a href=#exercise-1-himmelblaus-function>Exercise 1: Himmelblau&rsquo;s Function</a></li><li><a href=#exercise-2-mixed-function>Exercise 2: Mixed Function</a></li><li><a href=#exercise-3-comparative-analysis>Exercise 3: Comparative Analysis</a></li><li><a href=#exercise-4-parameter-sensitivity>Exercise 4: Parameter Sensitivity</a></li></ul></li></ul></nav></div></aside></main></body></html>