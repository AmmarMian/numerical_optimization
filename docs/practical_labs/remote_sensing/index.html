<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Solving Inverse Problems in Remote Sensing
  #


  README
  #


To get all the material for the labs, clone the following git repo : https://github.com/y-mhiri/hsi_unmixing_lab.
You can follow this lab in multiple level of difficulty


The easiest way : Answer only the theoretical question and follow the notebooks in notebooks/ to do the programming
Intermediary level : Implement your own code to answer the lab questions using the helper functions you&rsquo;ll find in src/
Expert level : I guess you don&rsquo;t even need to clone the git repo&mldr;

If anything don&rsquo;t hesitate to reach me at yassine.mhiri@univ-smb.fr."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/practical_labs/remote_sensing/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="II - Remote Sensing Project"><meta property="og:description" content="Solving Inverse Problems in Remote Sensing # README # To get all the material for the labs, clone the following git repo : https://github.com/y-mhiri/hsi_unmixing_lab. You can follow this lab in multiple level of difficulty The easiest way : Answer only the theoretical question and follow the notebooks in notebooks/ to do the programming Intermediary level : Implement your own code to answer the lab questions using the helper functions you’ll find in src/ Expert level : I guess you don’t even need to clone the git repo… If anything don’t hesitate to reach me at yassine.mhiri@univ-smb.fr."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>II - Remote Sensing Project | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/practical_labs/remote_sensing/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.97d809c6151b65f7466e1fea32988f044c8baf093687f80d68b77937d2c0defe.js integrity="sha256-l9gJxhUbZfdGbh/qMpiPBEyLrwk2h/gNaLd5N9LA3v4=" crossorigin=anonymous></script><script>const chapterNum=0;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_projected/>5b. Constrained optimization - Projected Gradient Descent</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimization_linear/>6. Constrained optimization - Linear programming</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/ class=active>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li><li><a href=/numerical_optimization/docs/practical_labs/quasinewton/>Quasi-Newton methods memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>II - Remote Sensing Project</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#readme>README</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#learning-objectives>Learning Objectives</a></li><li><a href=#i---modelization-and-problem-setup>I - Modelization and Problem Setup</a><ul><li><a href=#1-what-is-a-hyperspectral-image>1. What is a Hyperspectral Image?</a></li><li><a href=#2-the-spectral-unmixing-linear-model>2. The Spectral Unmixing Linear Model</a></li><li><a href=#3-formulation-of-the-inverse-problem>3. Formulation of the Inverse Problem</a></li></ul></li><li><a href=#ii---solving-the-unconstrained-least-squares-inverse-problem>II - Solving the Unconstrained Least Squares Inverse Problem</a><ul><li><a href=#1-unconstrained-least-squares-solution>1. Unconstrained Least Squares Solution</a></li><li><a href=#2-performance-evaluation>2. Performance Evaluation</a></li></ul></li><li><a href=#iii---solving-constrained-least-squares-inverse-problems>III - Solving Constrained Least Squares Inverse Problems</a><ul><li><a href=#1-sum-to-one-constrained-least-squares>1. Sum-to-One Constrained Least Squares</a></li><li><a href=#2-non-negativity-constrained-least-squares>2. Non-Negativity Constrained Least Squares</a></li><li><a href=#3-fully-constrained-least-squares>3. Fully Constrained Least Squares</a></li></ul></li><li><a href=#iv---blind-hyperspectral-unmixing-bonus>IV - Blind Hyperspectral Unmixing (Bonus)</a><ul><li><a href=#block-coordinate-descent-algorithm>Block Coordinate Descent Algorithm</a></li><li><a href=#limitations-and-advanced-methods>Limitations and Advanced Methods</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=solving-inverse-problems-in-remote-sensing>Solving Inverse Problems in Remote Sensing
<a class=anchor href=#solving-inverse-problems-in-remote-sensing>#</a></h1><h2 id=readme>README
<a class=anchor href=#readme>#</a></h2><ul><li>To get all the material for the labs, clone the following git repo : <a href=https://github.com/y-mhiri/hsi_unmixing_lab>https://github.com/y-mhiri/hsi_unmixing_lab</a>.</li><li>You can follow this lab in multiple level of difficulty</li></ul><ol><li><strong>The easiest way</strong> : Answer only the theoretical question and follow the notebooks in <code>notebooks/</code> to do the programming</li><li><strong>Intermediary level</strong> : Implement your own code to answer the lab questions using the helper functions you&rsquo;ll find in <code>src/</code></li><li><strong>Expert level</strong> : I guess you don&rsquo;t even need to clone the git repo&mldr;</li></ol><p>If anything don&rsquo;t hesitate to reach me at <code>yassine.mhiri@univ-smb.fr</code>.</p><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>In this lab session, we will explore one of the many applications of numerical optimization, namely <strong>solving inverse problems</strong>. Inverse problems constitute a sub-field of applied mathematics with many applications in real-world data analysis. In this lab, you will work with <strong>hyperspectral images</strong>, commonly used in <strong>remote sensing</strong>.</p><p>We will begin with an introduction to hyperspectral images and derive a formulation of the <strong>hyperspectral unmixing problem</strong>. We will then express the inverse problem through an objective function to optimize. In the second part of the lab, we will experiment with multiple algorithms to solve the optimization problem and analyze their performance.</p><p>Hyperspectral unmixing is a fundamental problem in remote sensing that involves decomposing mixed pixel spectra into their constituent materials (endmembers) and their respective proportions (abundances). This problem is inherently an inverse problem where we seek to recover the underlying components from observed mixed signals.</p><p>As a bonus, we will explore blind hyperspectral unmixing where both endmembers and abundances are unknown, making the problem significantly more challenging.</p><h2 id=learning-objectives>Learning Objectives
<a class=anchor href=#learning-objectives>#</a></h2><p>By the end of the session, you should be able to:</p><ul><li>Derive a data model and objective function to solve an inverse problem</li><li>Implement descent algorithms to solve multi-objective optimization problems</li><li>Handle constrained optimization problems using Lagrange multipliers and projection methods</li><li>Benchmark optimization algorithms and measure their performance</li><li>Manipulate real-world hyperspectral data in Python</li><li>Understand the relationship between physical constraints and mathematical optimization</li></ul><h2 id=i---modelization-and-problem-setup>I - Modelization and Problem Setup
<a class=anchor href=#i---modelization-and-problem-setup>#</a></h2><h3 id=1-what-is-a-hyperspectral-image>1. What is a Hyperspectral Image?
<a class=anchor href=#1-what-is-a-hyperspectral-image>#</a></h3><p>A <strong>hyperspectral image (HSI)</strong> captures information across a wide range of the electromagnetic spectrum. Unlike traditional images that capture data in three bands (red, green, and blue), hyperspectral images can capture data in hundreds of contiguous spectral bands. This allows for detailed spectral analysis and identification of materials in <strong>remote sensing</strong> based on their spectral signatures.</p><p>Each pixel in a hyperspectral image contains a complete spectrum, which can be thought of as a &ldquo;fingerprint&rdquo; of the materials present in that pixel. The spectral dimension provides rich information about the chemical and physical properties of the observed scene.</p><center><figure id=fig:hsi_cube><img src=%20../../../../../png/hyperspectral/hsi.png alt="Hyperspectral image cube showing spatial and spectral dimensions" width=600px><figcaption><p><strong>Figure 0.1: </strong>A hyperspectral image cube with spatial dimensions (x,y) and spectral dimension (λ)</p></figcaption></figure></center><p>In this lab session, we will work with an open dataset for hyperspectral data analysis: the <strong>Pavia University HSI dataset</strong>. This dataset is widely used in the remote sensing community and contains agricultural fields with different crop types.</p><p><strong>Tasks:</strong></p><ol><li><p><strong>Open the PaviaU dataset. You can either use the helper function provided or use the <code>loadmat</code>function from <code>scipy.io</code></strong>.</p></li><li><p><strong>What is the size of the image and how many spectral bands does the image contain?</strong></p></li></ol><details><summary>Hint</summary><div>The Pavia University dataset typically has dimensions of 145×145 pixels with 200 spectral bands after noise removal. You can check the shape using <code>.shape</code> attribute in Python.</div></details><ol start=3><li><p><strong>Use <code>imshow</code> or the provided helper functions to display a few band images of the HSI cube at different wavelengths.</strong> Try displaying bands at different spectral regions (e.g., visible, near-infrared, short-wave infrared).</p></li><li><p><strong>Load and display the ground truth classification map.</strong> This shows the different crop types present in the scene.</p></li><li><p><strong>Extract and plot the mean spectrum of the first three classes</strong> from the ground truth. What differences do you observe between the spectral signatures?</p></li></ol><details><summary>Hint</summary><div>Use the ground truth labels to mask the hyperspectral data and compute the mean spectrum for each class. Different materials will have distinct spectral signatures, particularly in the near-infrared region.</div></details><ol start=6><li><strong>Save the matrix formed by the spectra of all the classes in a .npy using <code>np.save</code></strong></li></ol><h3 id=2-the-spectral-unmixing-linear-model>2. The Spectral Unmixing Linear Model
<a class=anchor href=#2-the-spectral-unmixing-linear-model>#</a></h3><p>In remote sensing, hyperspectral images of the Earth are composed of pixels that represent mixed spectral signatures of various materials. Due to the limited spatial resolution of sensors, each pixel often contains multiple materials. The spectrum observed at each pixel is the result of multiple constituent spectra called <strong>endmembers</strong>.</p><p><strong>Spectral unmixing</strong> consists of estimating the per-pixel <strong>abundances</strong> of each endmember, giving insight into the various constituents of the observed field. This is particularly important in applications such as:</p><ul><li>Agricultural monitoring (crop type identification)</li><li>Environmental monitoring (vegetation health assessment)</li><li>Geological surveys (mineral identification)</li><li>Urban planning (land cover classification)</li></ul><div class=center-container><div class=center-content><figure id=fig:unmixing><img src=%20../../../../../tikZ/hyperspectral_linear_mixing_model/main.svg alt="Illustration of spectral unmixing process" width=600px><figcaption><p><strong>Figure 0.2: </strong>Spectral unmixing: decomposing mixed pixel spectra into endmember spectra and abundances</p></figcaption></figure></div></div><p>It is commonly accepted to model the pixel spectra by a <strong>linear mixing model</strong> as follows:</p><p>$$
\mathbf{y}_p = \sum_{k=1}^K a_{kp} \mathbf{s}_k + \mathbf{n}_p
$$</p><p>where:</p><ul><li>$\mathbf{y}_p \in \mathbb{R}^m$ is the observed spectrum at pixel $p$ (with $m$ spectral bands)</li><li>$a_{kp} \in \mathbb{R}$ is the abundance (proportion) of the $k$-th endmember at pixel $p$</li><li>$\mathbf{s}_k \in \mathbb{R}^m$ is the spectral signature of the $k$-th endmember</li><li>$\mathbf{n}_p \in \mathbb{R}^m$ represents the noise at pixel $p$</li><li>$K$ is the number of endmembers</li></ul><p>The linear mixing model assumes that (i) the observed spectrum is a linear combination of endmember spectra, (ii) there are no multiple scattering effects and (iii) the endmembers are spectrally distinct.</p><p><strong>Tasks:</strong></p><ol><li><strong>Derive a matrix formulation of the linear mixing model</strong> in which images are vectorized. Define clearly:<ul><li>The data matrix $\mathbf{Y} \in \mathbb{R}^{m \times n}$ where $n$ is the number of pixels</li><li>The endmember matrix $\mathbf{S} \in \mathbb{R}^{m \times K}$</li><li>The abundance matrix $\mathbf{A} \in \mathbb{R}^{K \times n}$</li><li>The noise matrix $\mathbf{N} \in \mathbb{R}^{m \times n}$</li></ul></li></ol><details><summary>Hint</summary><div>Vectorize the spatial dimension by stacking the pixels column-wise.</div></details><ol start=2><li><strong>What are the physical constraints</strong> that should be imposed on the abundance matrix $\mathbf{A}$? Justify your answer.</li></ol><details><summary>Hint</summary><div>Think about what abundances represent physically. They are proportions of materials in a pixel. Remember that we assume that the endmember matrix includes all the material present in the scene.</div></details><h3 id=3-formulation-of-the-inverse-problem>3. Formulation of the Inverse Problem
<a class=anchor href=#3-formulation-of-the-inverse-problem>#</a></h3><p>The <strong>inverse problem</strong> in hyperspectral unmixing consists of estimating the endmember matrix $\mathbf{S}$ and the abundance matrix $\mathbf{A}$ from the observed data matrix $\mathbf{Y}$. This is essentially a <strong>matrix factorization problem</strong>.</p><p>The basic objective function for this optimization problem can be written as:</p><p>$$
\min_{\mathbf{A}, \mathbf{S}} \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_F^2
$$</p><p>where $\Vert\cdot\Vert_F^2$ is the squared Frobenius norm of the matrix, defined as:
$$
\Vert\mathbf{X}\Vert_F^2 = \sum_{i,j} x_{ij}^2 = \text{tr}(\mathbf{X}^T\mathbf{X})
$$</p><p><strong>Tasks:</strong></p><ol><li><p><strong>Explain why this is called an inverse problem.</strong> What makes it challenging compared to a forward problem?</p></li><li><p><strong>Is the objective function convex in both $\mathbf{A}$ and $\mathbf{S}$ simultaneously?</strong> Justify your answer.</p></li></ol><details><summary>Hint</summary><div>Is it convex in $\mathbf{A}$ when $\mathbf{S}$ is fixed, and convex in $\mathbf{S}$ when $\mathbf{A}$ is fixed ? Is it jointly convex in both variables ?</div></details><h2 id=ii---solving-the-unconstrained-least-squares-inverse-problem>II - Solving the Unconstrained Least Squares Inverse Problem
<a class=anchor href=#ii---solving-the-unconstrained-least-squares-inverse-problem>#</a></h2><p>An inverse problem involves determining the input or parameters of a system from its observed output. In the context of spectral unmixing, the inverse problem is to estimate the endmember spectra and their abundances from the observed hyperspectral data.</p><p><strong>We will mostly consider the case where the endmembers $\mathbf{S}$ are known</strong> (e.g., from a spectral library or field measurements). In this case, we only need to estimate the abundance matrix $\mathbf{A}$.</p><h3 id=1-unconstrained-least-squares-solution>1. Unconstrained Least Squares Solution
<a class=anchor href=#1-unconstrained-least-squares-solution>#</a></h3><p>When the endmembers are known, the problem becomes a <strong>linear least squares problem</strong> for each pixel:</p><p>$$
\min_{\mathbf{A}} \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_F^2
$$</p><p><strong>Tasks:</strong></p><ol><li><strong>Derive the analytical solution</strong> for the unconstrained least squares problem. Show that the optimal abundance matrix is given by:
$$
\mathbf{A}^* = (\mathbf{S}^T\mathbf{S})^{-1}\mathbf{S}^T\mathbf{Y}
$$</li></ol><details><summary>Hint</summary><div>This is similar to the multiple linear regression problem from Lab I. Use the fact that the Frobenius norm can be expressed as a sum of vector norms, and solve for each pixel independently.</div></details><ol start=2><li><p><strong>Under what conditions is this solution unique?</strong> What happens if $\mathbf{S}^T\mathbf{S}$ is not invertible?</p></li><li><p><strong>Implement the unconstrained least squares solution</strong> as a lambda function or regular function.</p></li></ol><h3 id=2-performance-evaluation>2. Performance Evaluation
<a class=anchor href=#2-performance-evaluation>#</a></h3><p>To evaluate the quality of spectral unmixing results, we need appropriate metrics that measure both spectral and spatial accuracy.</p><p><strong>Tasks:</strong></p><ol><li><strong>Implement a function that evaluates the following evaluation metrics:</strong><ul><li><strong>Spectral Angle Mapper (SAM)</strong>: Measures the angle between reconstructed and original spectra.</li><li><strong>Root Mean Square Error (RMSE)</strong>: Measures the pixel-wise reconstruction error.</li><li><strong>SSIM</strong> : Measure a perceptual similiarity between images.</li></ul></li></ol><p><strong>You can either use the helper function implemented in the repo or use implementation for external libraries (<code>scipy</code>, <code>sklearn</code>).</strong></p><ol start=2><li><p><strong>Implement a reconstruction and visualization function</strong> that:</p><ul><li>Reconstructs the hyperspectral image from estimated abundances</li><li>Displays RGB composite images (original vs. reconstructed)</li><li>Shows abundance maps for each endmember</li><li>Computes and displays the evaluation metrics</li></ul></li><li><p><strong>Apply your unconstrained least squares solution</strong> to the Pavia University dataset:</p><ul><li>Use the mean spectra of the first 3-5 classes as endmembers</li><li>Compute the abundance maps</li><li>Evaluate the reconstruction quality</li></ul></li><li><p><strong>Comment on the results.</strong> What do you observe about the abundance values? Are they physically meaningful?</p></li></ol><details><summary>Hint</summary><div>You&rsquo;ll likely observe that some abundance values are negative or that abundances for a pixel don&rsquo;t sum to one. This violates the physical constraints and motivates the need for constrained optimization.</div></details><h2 id=iii---solving-constrained-least-squares-inverse-problems>III - Solving Constrained Least Squares Inverse Problems
<a class=anchor href=#iii---solving-constrained-least-squares-inverse-problems>#</a></h2><p>Given the physical interpretation of the spectral linear mixing model, a set of constraints should be added to the optimization problem to ensure physically meaningful solutions.</p><p><strong>Tasks:</strong></p><ol><li><strong>Propose a set of equality and/or inequality constraints</strong> to ensure the interpretability of the solutions. Explain the physical meaning of each constraint.</li></ol><details><summary>Hint</summary><div><p>The two main physical constraints are:</p><ul><li><strong>Sum-to-one constraint</strong>: $\sum_{k=1}^K a_{kp} = 1$ for all pixels $p$ (abundances are proportions)</li><li><strong>Non-negativity constraint</strong>: $a_{kp} \geq 0$ for all $k, p$ (negative abundances are not physical)</li></ul></div></details><p>As you must notice, this makes the optimization problem more difficult to solve. In the next parts of the lab, we will derive methods to solve the relaxed version of the fully constrained problem.</p><h3 id=1-sum-to-one-constrained-least-squares>1. Sum-to-One Constrained Least Squares
<a class=anchor href=#1-sum-to-one-constrained-least-squares>#</a></h3><p>Let&rsquo;s first consider only the sum-to-one constraint. The optimization problem becomes:</p><p>$$
\begin{align}
\min_{\mathbf{A}} &\quad \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_F^2 \
\text{subject to} &\quad \mathbf{1}^T\mathbf{A} = \mathbf{1}^T
\end{align}
$$</p><p>where $\mathbf{1}$ is a vector of ones.</p><p><strong>Tasks:</strong></p><ol><li><strong>Derive the Lagrangian</strong> of the constrained optimization problem</li></ol><details><summary>Solution</summary><div>$$
L(\mathbf{A}, \lambda) = \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_2^2 + \sum_{i=1}^n\lambda_i\left(1 - \sum_{k=1}^K a_{ik}\right)
$$</div></details><ol start=2><li><strong>Compute the optimal solution</strong> by setting the gradients to zero.</li></ol><ol start=3><li><strong>Derive the closed-form expression</strong> for $\lambda^*$ and the final solution.</li></ol><details><summary>Solution</summary><div><p>Recall that the matrix vector product
$$
(\mathbf{c}\mathbf{A})_i = \sum_{k} c_k a_{ki}, (\mathbf{A}\mathbf{c})_k = \sum_{i} a_{ki} c_i
$$</p><p>In the problem below, we note $\lambda\in\mathbb{R}^{n\times 1}$, $\mathbf{A}\in\mathbb{R}^{K\times n}$ and $\mathbf{1}_{d} = [1, &mldr;, 1]^T \in\mathbb{R}^{d\times 1}$.</p><p>The Lagrangian below can thus be expressed as,
$$
\begin{aligned}
L(\mathbf{A}, \lambda) &= \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_2^2 + \sum_{i=1}^n\lambda_i - \sum_{i=1}^n \sum_{k=1}^K \lambda_i a_{ik} \\
&= \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_2^2 + \mathbf{1}_n^T \mathbf{\lambda} - \sum_{k=1}^K (\mathbf{A}\mathbf{\lambda})_k \\
&= \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_2^2 + \mathbf{1}_n^T \mathbf{\lambda} - \mathbf{1}_{K}^T\mathbf{A}\mathbf{\lambda}
\end{aligned}
$$</p><p>The gradient of $L$ w.r.t to $\mathbf{A}$ and $\lambda$ reads,
$$
\begin{aligned}
\nabla_{\mathbf{A}}f(\mathbf{A}) &= \mathbf{S}^T(\mathbf{S}\mathbf{A} - \mathbf{Y}) - \mathbf{1}_{K}\mathbf{\lambda}^T \\
\nabla_{\mathbf{\lambda}}f(\mathbf{\lambda}) &= -\mathbf{A}^T\mathbf{1}_{K} + \mathbf{1}_{n}
\end{aligned}
$$
Note that $\mathbf{1}_{K}\mathbf{\lambda}^T \in \mathbf{R}^{K\times n}$</p><p>We can now solve the system of linear equation that cancels the gradients,
$$
\begin{aligned}
\mathbf{S}^T(\mathbf{S}\mathbf{A} - \mathbf{Y}) - \mathbf{1}_{K}\mathbf{\lambda}^T &= 0\\
-\mathbf{A}^T\mathbf{1}_{K} + \mathbf{1}_{n} =& 0
\end{aligned}
$$</p><p>Without extensive linear algebra we get</p><p>$$
\begin{aligned}
\mathbf{A}^* &= (\mathbf{S}^T\mathbf{S})^{-1} \mathbf{S}^T \mathbf{Y} + (\mathbf{S}^T\mathbf{S})^{-1}\mathbf{1}_{K}\mathbf{\lambda}^T \\
\end{aligned}
$$
We notice that we can use the unconstrained solution to express this new constrained solution
$$
\mathbf{A}^* = \mathbf{A}_{\rm un}^* + (\mathbf{S}^T\mathbf{S})^{-1}\mathbf{1}_{K}\mathbf{\lambda}^T
$$
Injecting into the second equation we get
$$
\begin{aligned}
-(\mathbf{A}_{\rm un}^* + (\mathbf{S}^T\mathbf{S})^{-1}\mathbf{1}_{K}\mathbf{\lambda}^T)^T\mathbf{1}_{K} + \mathbf{1}_{n} &= 0 \\
-\mathbf{A}_{\rm un}^{*^T}\mathbf{1}_{K} - \mathbf{\lambda}\mathbf{1}_{K}^T(\mathbf{S}^T\mathbf{S})^{-1}\mathbf{1}_{K} + \mathbf{1}_{n} &= 0 \\
\end{aligned}
$$
$$
\mathbf{\lambda}^{*} = \frac{\mathbf{1}_{n} - \mathbf{A}_{\rm un}^{*^T}\mathbf{1}_{K}}{\mathbf{1}_{K}^T(\mathbf{S}^T\mathbf{S})^{-1}\mathbf{1}_{K} }
$$</p></div></details><ol start=4><li><p><strong>Implement the sum-to-one constrained solution</strong> and test it on the Pavia University dataset.</p></li><li><p><strong>Display the results</strong>: RGB composite, abundance maps, and compute evaluation metrics. <strong>Comment on the improvements</strong> compared to the unconstrained solution.</p></li></ol><h3 id=2-non-negativity-constrained-least-squares>2. Non-Negativity Constrained Least Squares
<a class=anchor href=#2-non-negativity-constrained-least-squares>#</a></h3><p>Now consider only the non-negativity constraints:</p><p>$$
\begin{align}
\min_{\mathbf{A}} &\quad \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_F^2 \
\text{subject to} &\quad \mathbf{A} \geq 0
\end{align}
$$</p><p>This is a <strong>quadratic programming problem</strong> with inequality constraints. The analytical solution is more complex, but we can use iterative methods.</p><hr><h4 id=what-is-a-quadratic-programming-problem>What is a Quadratic Programming Problem?
<a class=anchor href=#what-is-a-quadratic-programming-problem>#</a></h4><p>A <strong>Quadratic Programming Problem</strong> is an optimization problem where:</p><ul><li>The <strong>objective function</strong> is quadratic in the decision variables</li><li>The <strong>constraints</strong> are linear (equality and/or inequality constraints)</li></ul><p>The general form of a QPP is:
$$
\begin{align}
\min_{\mathbf{x}} &\quad \frac{1}{2}\mathbf{x}^T\mathbf{Q}\mathbf{x} + \mathbf{c}^T\mathbf{x} \
\text{subject to} &\quad \mathbf{A}_{eq}\mathbf{x} = \mathbf{b}_{eq} \
&\quad \mathbf{A}_{ineq}\mathbf{x} \leq \mathbf{b}_{ineq}
\end{align}
$$</p><p>In our case:</p><ul><li>$\mathbf{Q} = \mathbf{S}^T\mathbf{S}$ (positive semi-definite matrix)</li><li>The constraint $\mathbf{A} \geq 0$ represents simple bounds</li><li>QPPs are <strong>convex</strong> when $\mathbf{Q}$ is positive semi-definite, ensuring a unique global minimum</li></ul><h4 id=projected-gradient-algorithm>Projected Gradient Algorithm
<a class=anchor href=#projected-gradient-algorithm>#</a></h4><p>Since analytical solutions for QPPs with inequality constraints can be complex, we use iterative methods. The <strong>projected gradient algorithm</strong> is particularly effective for problems with simple constraints like non-negativity.</p><p>The algorithm works as follows:</p><ol><li><p><strong>Gradient Step</strong>: Take a standard gradient descent step
$$\tilde{\mathbf{A}}^{(k+1)} = \mathbf{A}^{(k)} - \alpha \nabla f(\mathbf{A}^{(k)})$$</p></li><li><p><strong>Projection Step</strong>: Project the result onto the feasible set
$$\mathbf{A}^{(k+1)} = \text{Proj}_{\mathcal{C}}(\tilde{\mathbf{A}}^{(k+1)})$$</p></li></ol><p>where $\mathcal{C}$ is the constraint set and $\text{Proj}_{\mathcal{C}}(\cdot)$ is the orthogonal projection operator.</p><p>For our problem:</p><ul><li>The gradient is: $\nabla f(\mathbf{A}) = 2\mathbf{S}^T(\mathbf{S}\mathbf{A} - \mathbf{Y})$</li><li>The projection onto the non-negative orthant is: $\text{Proj}_{\mathbb{R}_+}(\mathbf{x})_i = \max(0, x_i)$</li></ul><p>The projection step ensures that all iterates remain feasible while the gradient step minimizes the objective function.</p><p><strong>Tasks:</strong></p><ol><li><p><strong>Derive the gradient</strong> of the objective function $f(\mathbf{A}) = \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_F^2$ with respect to $\mathbf{A}$.</p></li><li><p><strong>Implement a projected gradient descent algorithm</strong> to solve the non-negativity constrained problem.</p></li><li><p><strong>Apply the non-negativity constrained method</strong> to the Pavia University dataset. Be carefull on how you choose the descent step size. Compare results with previous methods.</p></li></ol><h3 id=3-fully-constrained-least-squares>3. Fully Constrained Least Squares
<a class=anchor href=#3-fully-constrained-least-squares>#</a></h3><p>Finally, let&rsquo;s combine both constraints:</p><p>$$
\begin{align}
\min_{\mathbf{A}} &\quad \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_F^2 \
\text{subject to} &\quad \mathbf{1}^T\mathbf{A} = \mathbf{1}^T \
&\quad \mathbf{A} \geq 0
\end{align}
$$</p><h4 id=the-simplex-and-simplex-projection>The Simplex and Simplex Projection
<a class=anchor href=#the-simplex-and-simplex-projection>#</a></h4><p>The set defined by the intersection of the sum-to-one constraint and the non-negativity constraint defines the <strong>unit simplex</strong>.</p><p><strong>Definition</strong>: The unit simplex in $\mathbb{R}^K$ is defined as:
$$\Delta^{K-1} = \left\{\mathbf{x} \in \mathbb{R}^K : \sum_{i=1}^K x_i = 1, \forall i \leq K \ x_i \geq 0 \right\}$$</p><p><strong>Geometric Interpretation</strong>:</p><ul><li>In 2D ($K=2$): The simplex is a line segment from $(1,0)$ to $(0,1)$</li><li>In 3D ($K=3$): The simplex is a triangle with vertices at $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$</li><li>In general: The simplex is a $(K-1)$-dimensional polytope</li></ul><p><strong>Physical Meaning</strong>: In our context, each column of $\mathbf{A}$ (representing abundances for one pixel) must lie on the simplex, ensuring that abundances are non-negative and sum to one.</p><h4 id=projection-onto-the-simplex>Projection onto the Simplex
<a class=anchor href=#projection-onto-the-simplex>#</a></h4><p>The <strong>projection of a point $\mathbf{v}$ onto the simplex</strong> is the closest point in the simplex to $\mathbf{v}$ in the Euclidean sense:
$$\text{Proj}_{\Delta^{K-1}}(\mathbf{v}) = \arg\min_{\mathbf{x} \in \Delta^{K-1}} \Vert\mathbf{x} - \mathbf{v}\Vert_2^2$$</p><p><details><summary>An algorithm for simplex projection (Duchi et al., 2008)</summary><div><ol><li><strong>Sort</strong> the coordinates: $v_1 \geq v_2 \geq \ldots \geq v_K$</li><li><strong>Find the threshold</strong>: $\rho = \max{j : v_j - \frac{1}{j}(\sum_{i=1}^j v_i - 1) > 0}$</li><li><strong>Compute the Lagrange multiplier</strong>: $\lambda = \frac{1}{\rho}(\sum_{i=1}^\rho v_i - 1)$</li><li><strong>Project</strong>: $[\text{Proj}_{\Delta^{K-1}}(\mathbf{v})]_i = \max(0, v_i - \lambda)$</li></ol></div></details><strong>Tasks:</strong></p><ol><li><p><strong>Implement a function that perform projection on the simplex</strong>:</p></li><li><p><strong>Modify the previous projected gradient descent algorithm to the Fully Constrained Least Square problem.</strong></p></li><li><p><strong>Apply the fully constrained method</strong> to the Pavia University dataset.</p></li><li><p><strong>Compare all methods</strong> (unconstrained, sum-to-one, non-negativity, fully constrained) in terms of:</p><ul><li>Reconstruction quality (SAM, RMSE, SSIM)</li><li>Physical meaningfulness of abundances</li><li>Computational efficiency</li><li>Visual quality of abundance maps</li></ul></li><li><p><strong>Analyze the convergence behavior</strong> of the different projected gradient algorithms. Plot the objective function value vs. iteration number.</p></li></ol><h2 id=iv---blind-hyperspectral-unmixing-bonus>IV - Blind Hyperspectral Unmixing (Bonus)
<a class=anchor href=#iv---blind-hyperspectral-unmixing-bonus>#</a></h2><p><em>This section is intended for advanced students who have successfully completed the previous sections.</em></p><p>In the previous sections, we assumed that the endmembers were known. In practice, this is often not the case, and we need to estimate both the endmembers and abundances simultaneously. This is called <strong>blind hyperspectral unmixing</strong> or <strong>blind source separation</strong>.</p><p>The optimization problem becomes:</p><p>$$
\begin{align}
\min_{\mathbf{A}, \mathbf{S}} &\quad \Vert\mathbf{S}\mathbf{A} - \mathbf{Y}\Vert_F^2 \
\text{subject to} &\quad \mathbf{1}^T\mathbf{A} = \mathbf{1}^T \
&\quad \mathbf{A} \geq 0 \
&\quad \mathbf{S} \geq 0
\end{align}
$$</p><p>This problem is significantly more challenging because:</p><ul><li>It is <strong>non-convex</strong> in the joint variables $(\mathbf{A}, \mathbf{S})$</li><li>Multiple local minima exist</li><li>The solution is not unique (scaling ambiguity)</li></ul><h3 id=block-coordinate-descent-algorithm>Block Coordinate Descent Algorithm
<a class=anchor href=#block-coordinate-descent-algorithm>#</a></h3><p>Since the problem is not jointly convex, but is convex in each variable when the other is fixed, we can use <strong>Block Coordinate Descent (BCD)</strong>. This approach alternates between optimizing blocks of variables while keeping others fixed.</p><p><strong>Algorithm Structure:</strong></p><ol><li><strong>Initialize</strong> $\mathbf{S}^{(0)}$ and $\mathbf{A}^{(0)}$</li><li><strong>For</strong> $k = 0, 1, 2, \ldots$ <strong>until convergence:</strong><ul><li><p><strong>Fix</strong> $\mathbf{S} = \mathbf{S}^{(k)}$ and solve for $\mathbf{A}^{(k+1)}$:
$$\mathbf{A}^{(k+1)} = \arg\min_{\mathbf{A}} \Vert\mathbf{S}^{(k)}\mathbf{A} - \mathbf{Y}\Vert_F^2 \quad \text{s.t. } \mathbf{1}^T\mathbf{A} = \mathbf{1}^T, \mathbf{A} \geq 0$$</p></li><li><p><strong>Fix</strong> $\mathbf{A} = \mathbf{A}^{(k+1)}$ and solve for $\mathbf{S}^{(k+1)}$:
$$\mathbf{S}^{(k+1)} = \arg\min_{\mathbf{S}} \Vert\mathbf{S}\mathbf{A}^{(k+1)} - \mathbf{Y}\Vert_F^2 \quad \text{s.t. } \mathbf{S} \geq 0$$</p></li></ul></li></ol><p><strong>Key Insight:</strong> Each subproblem is a constrained least squares problem that we know how to solve from Section III!</p><p><strong>Tasks:</strong></p><ol><li><p><strong>Analyze the subproblems:</strong></p><ul><li>What type of optimization problem is the $\mathbf{A}$-subproblem? Which method from Section III can you use?</li><li>What type of optimization problem is the $\mathbf{S}$-subproblem? How does it differ from the abundance estimation?</li></ul></li><li><p><strong>Implement the Block Coordinate Descent algorithm:</strong></p></li><li><p><strong>Initialization strategies:</strong> Implement at least two initialization methods:</p><ul><li><strong>Random initialization:</strong> Random positive values with proper normalization</li><li><strong>Supervised initialization:</strong> Use the endmembers provided on section III.</li></ul></li><li><p><strong>Convergence analysis:</strong></p><ul><li>Implement a convergence criterion based on the relative change in objective function</li><li>Plot the objective function value vs. iteration number</li><li>Compare convergence behavior with different initializations</li></ul></li><li><p><strong>Algorithm analysis:</strong></p><ul><li>What are the main limitations of this approach?</li><li>How sensitive is the algorithm to initialization?</li><li>What happens when the number of endmembers $K$ is incorrectly specified?</li></ul></li></ol><h3 id=limitations-and-advanced-methods>Limitations and Advanced Methods
<a class=anchor href=#limitations-and-advanced-methods>#</a></h3><p>The Block Coordinate Descent approach presented here provides a good introduction to blind unmixing, but it has several limitations:</p><ul><li><strong>Local minima:</strong> The algorithm may converge to poor local solutions</li><li><strong>Initialization dependence:</strong> Results can vary significantly with different initializations</li><li><strong>Scaling ambiguity:</strong> Solutions are not unique up to scaling factors</li><li><strong>Slow convergence:</strong> Simple alternating optimization can be slow</li></ul><p><strong>Advanced methods exist</strong> that address these limitations and provide better performance:</p><ul><li><strong>Non-negative Matrix Factorization (NMF)</strong> with multiplicative updates</li><li><strong>Independent Component Analysis (ICA)</strong> for statistical independence
&mldr;</li></ul><p>These advanced methods often incorporate additional prior knowledge, regularization terms, or sophisticated optimization techniques to achieve more robust and accurate unmixing results.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#readme>README</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#learning-objectives>Learning Objectives</a></li><li><a href=#i---modelization-and-problem-setup>I - Modelization and Problem Setup</a><ul><li><a href=#1-what-is-a-hyperspectral-image>1. What is a Hyperspectral Image?</a></li><li><a href=#2-the-spectral-unmixing-linear-model>2. The Spectral Unmixing Linear Model</a></li><li><a href=#3-formulation-of-the-inverse-problem>3. Formulation of the Inverse Problem</a></li></ul></li><li><a href=#ii---solving-the-unconstrained-least-squares-inverse-problem>II - Solving the Unconstrained Least Squares Inverse Problem</a><ul><li><a href=#1-unconstrained-least-squares-solution>1. Unconstrained Least Squares Solution</a></li><li><a href=#2-performance-evaluation>2. Performance Evaluation</a></li></ul></li><li><a href=#iii---solving-constrained-least-squares-inverse-problems>III - Solving Constrained Least Squares Inverse Problems</a><ul><li><a href=#1-sum-to-one-constrained-least-squares>1. Sum-to-One Constrained Least Squares</a></li><li><a href=#2-non-negativity-constrained-least-squares>2. Non-Negativity Constrained Least Squares</a></li><li><a href=#3-fully-constrained-least-squares>3. Fully Constrained Least Squares</a></li></ul></li><li><a href=#iv---blind-hyperspectral-unmixing-bonus>IV - Blind Hyperspectral Unmixing (Bonus)</a><ul><li><a href=#block-coordinate-descent-algorithm>Block Coordinate Descent Algorithm</a></li><li><a href=#limitations-and-advanced-methods>Limitations and Advanced Methods</a></li></ul></li></ul></nav></div></aside></main></body></html>