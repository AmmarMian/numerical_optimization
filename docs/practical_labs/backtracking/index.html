<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Backtracking procedure for step size selection
  #


  Introduction
  #

The backtracking line search is a fundamental technique in optimization algorithms for determining an appropriate step size that ensures sufficient decrease in the objective function. This procedure is particularly useful in gradient-based methods where choosing an optimal step size analytically is difficult or computationally expensive.

  Mathematical setup
  #

Consider the optimization problem:
$\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$
where $f: \mathbb{R}^n \to \mathbb{R}$ is a continuously differentiable function. At iteration $k$, we have:"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/practical_labs/backtracking/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="Backtracking memo"><meta property="og:description" content="Backtracking procedure for step size selection # Introduction # The backtracking line search is a fundamental technique in optimization algorithms for determining an appropriate step size that ensures sufficient decrease in the objective function. This procedure is particularly useful in gradient-based methods where choosing an optimal step size analytically is difficult or computationally expensive.
Mathematical setup # Consider the optimization problem: $\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$
where $f: \mathbb{R}^n \to \mathbb{R}$ is a continuously differentiable function. At iteration $k$, we have:"><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Backtracking memo | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/practical_labs/backtracking/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.2f95a8412d3c4ad3086cd3b0e485d6c419c4581efb1ce467f255a757541f5712.js integrity="sha256-L5WoQS08StMIbNOw5IXWxBnEWB77HORn8lWnV1QfVxI=" crossorigin=anonymous></script><script>const chapterNum=0;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>6. Constrained optimization</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/ class=active>Backtracking memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>Backtracking memo</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#mathematical-setup>Mathematical setup</a></li><li><a href=#the-armijo-condition>The Armijo condition</a></li><li><a href=#backtracking-algorithm-steps>Backtracking algorithm steps</a><ul><li><a href=#step-1-initialize-parameters>Step 1: Initialize parameters</a></li><li><a href=#step-2-check-armijo-condition>Step 2: Check Armijo condition</a></li><li><a href=#step-3-backtrack-if-necessary>Step 3: Backtrack if necessary</a></li><li><a href=#step-4-update-iteration>Step 4: Update iteration</a></li></ul></li><li><a href=#algorithmic-description>Algorithmic description</a></li><li><a href=#theoretical-properties>Theoretical properties</a><ul><li><a href=#convergence-guarantee>Convergence guarantee</a></li><li><a href=#sufficient-decrease-property>Sufficient decrease property</a></li></ul></li><li><a href=#implementation-considerations>Implementation considerations</a><ul><li><a href=#choice-of-parameters>Choice of parameters</a></li><li><a href=#computational-complexity>Computational complexity</a></li><li><a href=#practical-modifications>Practical modifications</a></li></ul></li><li><a href=#applications>Applications</a></li><li><a href=#example-implementation>Example implementation</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=backtracking-procedure-for-step-size-selection>Backtracking procedure for step size selection
<a class=anchor href=#backtracking-procedure-for-step-size-selection>#</a></h1><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>The backtracking line search is a fundamental technique in optimization algorithms for determining an appropriate step size that ensures sufficient decrease in the objective function. This procedure is particularly useful in gradient-based methods where choosing an optimal step size analytically is difficult or computationally expensive.</p><h2 id=mathematical-setup>Mathematical setup
<a class=anchor href=#mathematical-setup>#</a></h2><p>Consider the optimization problem:
$\min_{\mathbf{x} \in \mathbb{R}^n} f(\mathbf{x})$</p><p>where $f: \mathbb{R}^n \to \mathbb{R}$ is a continuously differentiable function. At iteration $k$, we have:</p><ul><li>Current point: $\mathbf{x}_k$</li><li>Search direction: $\mathbf{p}_k$ (typically $\mathbf{p}_k = -\nabla f(\mathbf{x}_k)$ for steepest descent)</li><li>Step size: $\alpha_k > 0$</li></ul><h2 id=the-armijo-condition>The Armijo condition
<a class=anchor href=#the-armijo-condition>#</a></h2><p>The backtracking procedure is based on the Armijo condition, which requires:
$f(\mathbf{x}_k + \alpha_k \mathbf{p}_k) \leq f(\mathbf{x}_k) + c_1 \alpha_k \nabla f(\mathbf{x}_k)^{\mathrm{T}} \mathbf{p}_k$</p><p>where $c_1 \in (0, 1)$ is a constant, typically $c_1 = 10^{-4}$.</p><h2 id=backtracking-algorithm-steps>Backtracking algorithm steps
<a class=anchor href=#backtracking-algorithm-steps>#</a></h2><h3 id=step-1-initialize-parameters>Step 1: Initialize parameters
<a class=anchor href=#step-1-initialize-parameters>#</a></h3><ul><li>Choose initial step size $\alpha_0 > 0$ (e.g., $\alpha_0 = 1$)</li><li>Set reduction factor $\rho \in (0, 1)$ (typically $\rho = 0.5$)</li><li>Set Armijo parameter $c_1 \in (0, 1)$ (typically $c_1 = 10^{-4}$)</li><li>Set $\alpha = \alpha_0$</li></ul><h3 id=step-2-check-armijo-condition>Step 2: Check Armijo condition
<a class=anchor href=#step-2-check-armijo-condition>#</a></h3><p>Evaluate the condition:
$f(\mathbf{x}_k + \alpha \mathbf{p}_k) \leq f(\mathbf{x}_k) + c_1 \alpha \nabla f(\mathbf{x}_k)^{\mathrm{T}} \mathbf{p}_k$</p><h3 id=step-3-backtrack-if-necessary>Step 3: Backtrack if necessary
<a class=anchor href=#step-3-backtrack-if-necessary>#</a></h3><p><strong>If</strong> the Armijo condition is satisfied:</p><ul><li>Accept $\alpha_k = \alpha$</li><li><strong>Go to</strong> Step 4</li></ul><p><strong>Else:</strong></p><ul><li>Update $\alpha \leftarrow \rho \alpha$</li><li><strong>Return to</strong> Step 2</li></ul><h3 id=step-4-update-iteration>Step 4: Update iteration
<a class=anchor href=#step-4-update-iteration>#</a></h3><p>Compute the new iterate:
$\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k$</p><h2 id=algorithmic-description>Algorithmic description
<a class=anchor href=#algorithmic-description>#</a></h2><pre tabindex=0><code>Algorithm: Backtracking Line Search
Input: x_k, p_k, α₀, ρ, c₁
Output: α_k

1. Set α = α₀
2. While f(x_k + α·p_k) &gt; f(x_k) + c₁·α·∇f(x_k)ᵀ·p_k do
3.    α ← ρ·α
4. End while  
5. Return α_k = α
</code></pre><h2 id=theoretical-properties>Theoretical properties
<a class=anchor href=#theoretical-properties>#</a></h2><h3 id=convergence-guarantee>Convergence guarantee
<a class=anchor href=#convergence-guarantee>#</a></h3><p>Under mild conditions on $f$ and $\mathbf{p}_k$, the backtracking procedure terminates in finite steps. Specifically, if:</p><ul><li>$f$ is continuously differentiable</li><li>$\mathbf{p}_k$ is a descent direction: $\nabla f(\mathbf{x}_k)^{\mathrm{T}} \mathbf{p}_k &lt; 0$</li></ul><p>Then there exists a step size $\alpha > 0$ satisfying the Armijo condition.</p><h3 id=sufficient-decrease-property>Sufficient decrease property
<a class=anchor href=#sufficient-decrease-property>#</a></h3><p>The accepted step size $\alpha_k$ ensures:
$f(\mathbf{x}_{k+1}) - f(\mathbf{x}_k) \leq c_1 \alpha_k \nabla f(\mathbf{x}_k)^{\mathrm{T}} \mathbf{p}_k &lt; 0$</p><p>This guarantees that each iteration decreases the objective function value.</p><h2 id=implementation-considerations>Implementation considerations
<a class=anchor href=#implementation-considerations>#</a></h2><h3 id=choice-of-parameters>Choice of parameters
<a class=anchor href=#choice-of-parameters>#</a></h3><ul><li><strong>Initial step size</strong> $\alpha_0$: Common choices are $\alpha_0 = 1$ for Newton-type methods, or $\alpha_0 = 1/|\nabla f(\mathbf{x}_k)|$ for gradient methods</li><li><strong>Reduction factor</strong> $\rho$: Typically $\rho = 0.5$ or $\rho = 0.8$</li><li><strong>Armijo parameter</strong> $c_1$: Usually $c_1 = 10^{-4}$ or $c_1 = 10^{-3}$</li></ul><h3 id=computational-complexity>Computational complexity
<a class=anchor href=#computational-complexity>#</a></h3><p>Each backtracking iteration requires:</p><ul><li>One function evaluation: $f(\mathbf{x}_k + \alpha \mathbf{p}_k)$</li><li>One gradient evaluation: $\nabla f(\mathbf{x}_k)$ (if not already computed)</li><li>One vector operation: $\mathbf{x}_k + \alpha \mathbf{p}_k$</li></ul><h3 id=practical-modifications>Practical modifications
<a class=anchor href=#practical-modifications>#</a></h3><p><strong>Maximum iterations:</strong> Limit the number of backtracking steps to prevent infinite loops:</p><pre tabindex=0><code>max_backtracks = 50
iter = 0
while (Armijo condition not satisfied) and (iter &lt; max_backtracks):
    α ← ρ·α
    iter ← iter + 1
</code></pre><p><strong>Minimum step size:</strong> Set a lower bound $\alpha_{min}$ to avoid numerical issues:</p><pre tabindex=0><code>if α &lt; α_min:
    α = α_min
    break
</code></pre><h2 id=applications>Applications
<a class=anchor href=#applications>#</a></h2><p>The backtracking procedure is widely used in:</p><ul><li><strong>Gradient descent:</strong> $\mathbf{p}_k = -\nabla f(\mathbf{x}_k)$</li><li><strong>Newton&rsquo;s method:</strong> $\mathbf{p}_k = -(\mathbf{H}_k)^{-1} \nabla f(\mathbf{x}_k)$ where $\mathbf{H}_k$ is the Hessian</li><li><strong>Quasi-Newton methods:</strong> $\mathbf{p}_k = -\mathbf{B}_k^{-1} \nabla f(\mathbf{x}_k)$ where $\mathbf{B}_k$ approximates the Hessian</li><li><strong>Conjugate gradient methods</strong></li></ul><h2 id=example-implementation>Example implementation
<a class=anchor href=#example-implementation>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backtracking_line_search</span>(f, grad_f, x_k, p_k, alpha_0<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>, rho<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, c1<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-4</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Backtracking line search for step size selection
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - f: objective function
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - grad_f: gradient function  
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - x_k: current point
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - p_k: search direction
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - alpha_0: initial step size
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - rho: reduction factor
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - c1: Armijo parameter
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    - alpha_k: accepted step size
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    alpha <span style=color:#f92672>=</span> alpha_0
</span></span><span style=display:flex><span>    f_k <span style=color:#f92672>=</span> f(x_k)
</span></span><span style=display:flex><span>    grad_k <span style=color:#f92672>=</span> grad_f(x_k)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Armijo condition right-hand side</span>
</span></span><span style=display:flex><span>    armijo_rhs <span style=color:#f92672>=</span> f_k <span style=color:#f92672>+</span> c1 <span style=color:#f92672>*</span> alpha <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>dot(grad_k, p_k)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> f(x_k <span style=color:#f92672>+</span> alpha <span style=color:#f92672>*</span> p_k) <span style=color:#f92672>&gt;</span> armijo_rhs:
</span></span><span style=display:flex><span>        alpha <span style=color:#f92672>*=</span> rho
</span></span><span style=display:flex><span>        armijo_rhs <span style=color:#f92672>=</span> f_k <span style=color:#f92672>+</span> c1 <span style=color:#f92672>*</span> alpha <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>dot(grad_k, p_k)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> alpha
</span></span></code></pre></div><p><strong>Exercises</strong></p><ol><li><p>Implement the backtracking line search for the quadratic function $f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^{\mathrm{T}} \mathbf{Q} \mathbf{x} - \mathbf{b}^{\mathrm{T}} \mathbf{x}$, where $\mathbf{Q}$ is positive definite.</p></li><li><p>Compare the performance of different values of $\rho$ and $c_1$ on a test optimization problem.</p></li><li><p>Analyze the number of backtracking steps required as a function of the condition number of the Hessian matrix.</p></li></ol></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#mathematical-setup>Mathematical setup</a></li><li><a href=#the-armijo-condition>The Armijo condition</a></li><li><a href=#backtracking-algorithm-steps>Backtracking algorithm steps</a><ul><li><a href=#step-1-initialize-parameters>Step 1: Initialize parameters</a></li><li><a href=#step-2-check-armijo-condition>Step 2: Check Armijo condition</a></li><li><a href=#step-3-backtrack-if-necessary>Step 3: Backtrack if necessary</a></li><li><a href=#step-4-update-iteration>Step 4: Update iteration</a></li></ul></li><li><a href=#algorithmic-description>Algorithmic description</a></li><li><a href=#theoretical-properties>Theoretical properties</a><ul><li><a href=#convergence-guarantee>Convergence guarantee</a></li><li><a href=#sufficient-decrease-property>Sufficient decrease property</a></li></ul></li><li><a href=#implementation-considerations>Implementation considerations</a><ul><li><a href=#choice-of-parameters>Choice of parameters</a></li><li><a href=#computational-complexity>Computational complexity</a></li><li><a href=#practical-modifications>Practical modifications</a></li></ul></li><li><a href=#applications>Applications</a></li><li><a href=#example-implementation>Example implementation</a></li></ul></nav></div></aside></main></body></html>