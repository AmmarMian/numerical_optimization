<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Linear Regression models
  #


  Introduction
  #

In this lab session, we will explore the fundamental concepts of numerical optimization through the lens of linear regression. We&rsquo;ll begin with the simplest case and gradually build up to more complex scenarios, comparing analytical solutions with numerical methods at each step.
Linear regression is perhaps the most fundamental problem in machine learning and statistics. While it has a closed-form solution, implementing numerical optimization methods for this problem provides excellent intuition for more complex optimization scenarios where analytical solutions don&rsquo;t exist."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/practical_labs/linear_regression/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="I - Linear Regression models"><meta property="og:description" content="Linear Regression models # Introduction # In this lab session, we will explore the fundamental concepts of numerical optimization through the lens of linear regression. We’ll begin with the simplest case and gradually build up to more complex scenarios, comparing analytical solutions with numerical methods at each step.
Linear regression is perhaps the most fundamental problem in machine learning and statistics. While it has a closed-form solution, implementing numerical optimization methods for this problem provides excellent intuition for more complex optimization scenarios where analytical solutions don’t exist."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>I - Linear Regression models | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/practical_labs/linear_regression/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.0bb2f74293a9dcb3092cd06f4d19ca8daa763c792eb06051e0fd9ac8f19dd215.js integrity="sha256-C7L3QpOp3LMJLNBvTRnKjap2PHkusGBR4P2ayPGd0hU=" crossorigin=anonymous></script><script>const chapterNum=0;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/ class=active>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>I - Linear Regression models</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#learning-objectives>Learning objectives</a></li><li><a href=#i---one-dimensional-case>I - One dimensional case</a><ul><li><a href=#1-modeling-and-solving-the-problem>1. Modeling and solving the problem</a></li><li><a href=#2-gradient-descent-for-the-one-dimensional-case>2. Gradient descent for the one-dimensional case</a></li></ul></li><li><a href=#ii---multiple-variables-case>II - Multiple variables case</a><ul><li><a href=#1-modeling-and-solving-the-problem-1>1. Modeling and solving the problem</a></li><li><a href=#2-gradient-descent-for-the-multiple-variables-case>2. Gradient descent for the multiple variables case</a></li><li><a href=#3-experimenting-with--backtracking-line-search>3. Experimenting with backtracking line search</a></li><li><a href=#4-using-more-complex-linesearch-techniques-using-toolboxes>4. Using more complex linesearch techniques using toolboxes</a></li></ul></li><li><a href=#iii---bonus-the-general-case>III - (Bonus) The general case</a></li><li><a href=#iv---bonus-regularization>IV - (Bonus) Regularization</a></li><li><a href=#v---bonus-nonlinear-regression>V - (Bonus) Nonlinear regression</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=linear-regression-models>Linear Regression models
<a class=anchor href=#linear-regression-models>#</a></h1><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>In this lab session, we will explore the fundamental concepts of numerical optimization through the lens of linear regression. We&rsquo;ll begin with the simplest case and gradually build up to more complex scenarios, comparing analytical solutions with numerical methods at each step.</p><p>Linear regression is perhaps the most fundamental problem in machine learning and statistics. While it has a closed-form solution, implementing numerical optimization methods for this problem provides excellent intuition for more complex optimization scenarios where analytical solutions don&rsquo;t exist.</p><h2 id=learning-objectives>Learning objectives
<a class=anchor href=#learning-objectives>#</a></h2><p>By the end of this session, you should be able to:</p><ul><li>Derive the analytical solution for simple linear regression</li><li>Implement gradient descent with various step size strategies</li><li>Understand the connection between the one-dimensional and multi-dimensional cases</li><li>Apply line search techniques to improve convergence</li></ul><h2 id=i---one-dimensional-case>I - One dimensional case
<a class=anchor href=#i---one-dimensional-case>#</a></h2><p>Let us first start with the form that most people are familiar with, the linear regression model in one dimension. The setup is as follows:</p><ul><li>We have a set of data points $\{(x_i, y_i)\}_{i=1}^n$. Here the $x_i$ are the input features and the $y_i$ are the target values.</li><li>Assuming there is a linear relationship between and target because of some underlying phenomenon, we model the observations as:
\begin{equation}
y_i = \alpha x_i + \beta + \epsilon
\label{eq:linear_model_1d}
\end{equation}
where $\epsilon$ is a random noise term that we assume to be normally distributed with mean 0 and variance $\sigma^2$.</li></ul><p>Our goal is then to find the parameters $\alpha$ and $\beta$ that &ldquo;best match&rdquo; the data points. Such a program can is illustrated with following <a href=../../../interactive/linear_regression_1d.html>interactive plot</a>.</p><h3 id=1-modeling-and-solving-the-problem>1. Modeling and solving the problem
<a class=anchor href=#1-modeling-and-solving-the-problem>#</a></h3><ol><li>Propose a loss function that quantifies the difference between the observed $y_i$ and the predicted values $\hat{y}_i = \alpha x_i + \beta$.</li></ol><details><summary>Hint</summary><div>The most common loss function for regression problems is the mean squared error (MSE):
$$
L(\alpha, \beta) = \frac{1}{2} \sum_{i=1}^n (y_i - (\alpha x_i + \beta))^2
$$</div></details><ol start=2><li>Show that the loss function is convex in the parameters $\alpha$ and $\beta$.</li></ol><details><summary>Hint</summary><div>To show convexity, we need to demonstrate that the Hessian matrix of second derivatives is positive semi-definite. Or that the function is a positive linear combination of convex functions.
The loss function is a quadratic function in $\alpha$ and $\beta$, which is convex. The Hessian matrix will have positive eigenvalues, confirming convexity.</div></details><ol start=3><li>Derive the analytical solution for the parameters $\alpha$ and $\beta$ by setting the gradients of the loss function with respect to these parameters to zero.</li></ol><details><summary>Hint</summary><div><p>It is often useful to express the gradients in terms of the means and variances of the data points:</p><ul><li>means : $$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i, \quad \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i$$</li><li>variance: $$s_{xx} = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2$$</li><li>covariance: $$s_{xy} = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})$$</li></ul></div></details><ol start=4><li>Implement the analytical solution in Python and compute the optimal parameters for a given dataset. To generate dataset, you can use following code snippet:</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Set random seed for reproducibility</span>
</span></span><span style=display:flex><span>rng <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>default_rng(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate synthetic data</span>
</span></span><span style=display:flex><span>n_samples <span style=color:#f92672>=</span> <span style=color:#ae81ff>50</span>
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>10</span>, n_samples)
</span></span><span style=display:flex><span><span style=color:#75715e># True parameters</span>
</span></span><span style=display:flex><span>alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>2.5</span>
</span></span><span style=display:flex><span>beta <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add Gaussian noise</span>
</span></span><span style=display:flex><span>noise <span style=color:#f92672>=</span> rng<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> alpha <span style=color:#f92672>*</span> x <span style=color:#f92672>+</span> beta <span style=color:#f92672>+</span> noise
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Visualize the data</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x, y, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Data points&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;x&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;y&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Synthetic linear data with noise&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><ol start=5><li>(Bonus) Show that doing a Maximum Likelihood Estimation (MLE) for the parameters $\alpha$ and $\beta$ leads to the same solution as minimizing the loss function derived above.</li></ol><details><summary>Hint</summary><div>The MLE for the parameters in a linear regression model with Gaussian noise leads to minimizing the negative log-likelihood, which is equivalent to minimizing the mean squared error loss function. The derivation involves taking the logarithm of the Gaussian probability density function and simplifying it, leading to the same equations for $\alpha$ and $\beta$ as derived from the loss function.</div></details><h3 id=2-gradient-descent-for-the-one-dimensional-case>2. Gradient descent for the one-dimensional case
<a class=anchor href=#2-gradient-descent-for-the-one-dimensional-case>#</a></h3><p>Now that we have a good understanding of the problem and have implemented the analytical solution, let&rsquo;s explore how we can solve this problem using numerical optimization techniques, specifically steepest gradient descent, i.e always taking the gradient as the direction.</p><ol><li>Recalling the update rule for steepest gradient descent of a point $\theta_k$ at iteration $k$:
$$
\theta_{k+1} = \theta_k - \alpha_k \nabla L(\theta_k)
$$
where $\alpha_k$ is the step size at iteration $k$,</li></ol><p>give the update rule for the parameters $\alpha$ and $\beta$ in the context of our linear regression problem.</p><details><summary>Hint</summary><div>The update rules for the parameters $\alpha$ and $\beta$ can be derived from the gradients of the loss function:
$$
\begin{align*}
\alpha_{k+1} &= \alpha_k - \alpha_k \frac{\partial L}{\partial \alpha}(\alpha_k, \beta_k) \\
\beta_{k+1} &= \beta_k - \alpha_k \frac{\partial L}{\partial \beta}(\alpha_k, \beta_k)
\end{align*}
$$
where the gradients are computed as follows:
$$
\begin{align*}
\frac{\partial L}{\partial \alpha} &= -\sum_{i=1}^n (y_i - (\alpha_k x_i + \beta_k)) x_i \\
\frac{\partial L}{\partial \beta} &= -\sum_{i=1}^n (y_i - (\alpha_k x_i + \beta_k))
\end{align*}
$$</div></details><ol start=2><li><p>Implement gradient descent with a constant step size $\alpha_k = \alpha$ for all iterations. Your function should:</p><ul><li>Take initial parameters $(\alpha_0, \beta_0)$, step size $\alpha$, and number of iterations as inputs.</li><li>Return the trajectory of parameters and loss values.</li><li>Include a stopping criterion based on gradient magnitude.</li></ul></li><li><p>Experiment with different step sizes: $\alpha \in \{0.0001, 0.001, 0.01, 0.1\}$. Plot the loss function over iterations for each case. What do you observe?</p></li></ol><details><summary>Hint</summary><div>The loss function should decrease over iterations, but the rate of decrease will depend on the step size. A very small step size will lead to slow convergence, while a very large step size may cause divergence or oscillations.</div></details><ol start=4><li>For a fixed number of iterations (say 1000), plot the final error as a function of step size on a logarithmic scale. What is the optimal range for $\alpha$?</li></ol><details><summary>Hint</summary><div>The optimal range for $\alpha$ is typically small enough to ensure convergence but large enough to allow for reasonable speed of convergence. You may find that values around $0.001$ to $0.01$ work well, but this can depend on the specific dataset and problem.</div></details><ol start=5><li>Let&rsquo;s try a first experiment with a decreasing step size. Implement a linear decay strategy:
$$
\alpha_k = \alpha_0 - k \cdot \gamma,
$$
where $\gamma$ is a small constant (e.g., $0.0001$). Experiment with different values of $\alpha_0$ and $\gamma$.</li></ol><p>Compare the convergence behavior with constant step size. Plot the loss function and parameter trajectories over iterations.</p><ol start=6><li>Why might decreasing step sizes be beneficial? What are the trade-offs between aggressive and conservative decay rates?</li></ol><details><summary>Hint</summary><div><p>Decreasing step sizes can help avoid overshooting the minimum and allow for finer adjustments as the algorithm converges.</p><p>In nonconvex problmes, aggressive decay rates may lead to faster convergence initially but can cause the algorithm to get stuck in local minima, while conservative rates may lead to slower convergence but better exploration of the parameter space.</p></div></details><ol start=7><li>Try implementing an exponential decay strategy:
$$
\alpha_k = \alpha_0 \cdot \gamma^k$$
where $\gamma \in (0, 1)$ is the decay rate. Experiment with different values of $\gamma$ (e.g., $0.9$, $0.95$, $0.99$) and compare the convergence behavior with constant and linear decay strategies.</li></ol><details><summary>Hint</summary><div>Exponential decay is more aggressive, thus it may also cause the step size to become too small too quickly, leading to slow convergence in later iterations. The choice of $\gamma$ can significantly affect the convergence behavior.</div></details><h2 id=ii---multiple-variables-case>II - Multiple variables case
<a class=anchor href=#ii---multiple-variables-case>#</a></h2><p>Now that we have a good understanding of the one-dimensional case, let&rsquo;s generalize our approach to multiple dimensions. The setup is similar, but now we have multiple features and parameters:</p><ul><li>We have a set of data points
$\{(\mathbf{x}_i, y_i)\}_{i=1}^n$, where $\mathbf{x}_i \in \mathbb{R}^d$ are the input features and $y_i \in \mathbb{R}$ are the target values.</li><li>We model the observations as:
\begin{equation}
y_i = \mathbf{w}^{\mathrm{T}} \mathbf{x}_i + \beta + \epsilon
\label{eq:linear_model_d}
\end{equation}
where $\mathbf{w} \in \mathbb{R}^d$ is the weight vector, $\beta \in \mathbb{R}$ is the bias term, and $\epsilon$ is a random noise term that we assume to be normally distributed with mean 0 and variance $\sigma^2$.</li></ul><div class=center-container><div class=center-content><figure id=linear_regression_2d><img src=%20../../../../../tikZ/2D_linear/main.svg alt="2D linear regression" width=400px><figcaption><p><strong>Figure 0.1: </strong>2D linear regression</p></figcaption></figure></div></div><p>An example of this model is illustrated in Figure
<a href=#linear_regression_2d>0.1</a>
, where the data points are represented in a two-dimensional space, and the linear regression model fits a plane to the data.</p><p>This model is more general than the one-dimensional case, as it allows for multiple features to influence the target variable. Notably, we can also augment the feature vectors with a constant term to simplify the notation so that we don&rsquo;t have to deal with the bias term separately. We define:
\begin{equation}
\tilde{\mathbf{x}}_i = [1, \mathbf{x}_i^{\mathrm{T}}]^{\mathrm{T}} \in \mathbb{R}^{d+1}
\end{equation}
and
\begin{equation}
\tilde{\mathbf{w}} = [\beta, \mathbf{w}^{\mathrm{T}}]^{\mathrm{T}} \in \mathbb{R}^{d+1}
\end{equation}
so that we can rewrite the model as:
\begin{equation}
y_i = \tilde{\mathbf{w}}^{\mathrm{T}} \tilde{\mathbf{x}}_i + \epsilon
\label{eq:linear_model_d_augmented}
\end{equation}</p><p>Our goal is then to find the parameters $\mathbf{w}$ and $\beta$ that &ldquo;best match&rdquo; the data points, or in augmented notation, to find $\tilde{\mathbf{w}}$ that minimizes the loss function.</p><h3 id=1-modeling-and-solving-the-problem-1>1. Modeling and solving the problem
<a class=anchor href=#1-modeling-and-solving-the-problem-1>#</a></h3><p>While the augmented formulation is nice, we can also express the model in matrix form for the observed data. We define the design matrix $\mathbf{X} \in \mathbb{R}^{n \times (d+1)}$ as:
\begin{equation}
\mathbf{X} = \begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1d} \\
1 & x_{21} & x_{22} & \ldots & x_{2d} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nd}
\end{bmatrix}
\end{equation}
and the target vector $\mathbf{y} \in \mathbb{R}^n$ as:
\begin{equation}
\mathbf{y} = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
\end{equation}</p><p>Then we can express the model as:
\begin{equation}
\mathbf{y} = \mathbf{X} \tilde{\mathbf{w}} + \boldsymbol{\epsilon},
\label{eq:linear_model_matrix}
\end{equation}</p><p>where $\boldsymbol{\epsilon} \in \mathbb{R}^n$ is the noise vector. This more compact formulation is interesting for several reasons:</p><ul><li>It already encapsulates the observed data in the model and we consider all the $y_i$ as a vector, which allows us to work with the entire dataset at once.</li><li>the matrix form allow us to obtain solutions that will be expressed as matrix operations, which is more efficient for larger datasets</li><li>it allows us to use linear algebra techniques to derive the solution</li></ul><ol><li>Propose a loss function that quantifies the difference between the observed $y_i$ and the predicted values $\hat{y}_i = \mathbf{X} \tilde{\mathbf{w}}$.</li></ol><details><summary>Hint</summary><div>The most common loss function for regression problems is the mean squared error (MSE):
$$
L(\tilde{\mathbf{w}}) = \frac{1}{2} \|\mathbf{y} - \mathbf{X} \tilde{\mathbf{w}}\|^2_2
$$</div></details><ol start=2><li>Show that the loss function is convex in the parameters $\tilde{\mathbf{w}}$.</li></ol><details><summary>Hint</summary><div>As in the one-dimensional case, the loss function is a quadratic function in $\tilde{\mathbf{w}}$, which is convex. The Hessian matrix of second derivatives will be positive semi-definite, confirming convexity.</div></details><ol start=3><li>Derive the gradient of the loss function with respect to $\tilde{\mathbf{w}}$ in matrix form. To help yourselves, you can use the properties of matrix derivatives from matrix cookbook available <a href=https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf>here</a> and identity of vector norms:
$$
\lVert \mathbf{u} - \mathbf{v} \rVert^2_2 = \mathbf{u}^{\mathrm{T}} \mathbf{u} - 2 \mathbf{u}^{\mathrm{T}} \mathbf{v} + \mathbf{v}^{\mathrm{T}} \mathbf{v}.
$$</li></ol><p>and show that optimal solution $\tilde{\mathbf{w}}$ satisfies the normal equations:
$$
\mathbf{X}^{\mathrm{T}} \mathbf{X} \tilde{\mathbf{w}} = \mathbf{X}^{\mathrm{T}} \mathbf{y}.
$$</p><details><summary>Hint</summary><div><p>Make use of</p><ul><li>$\frac{\partial}{\partial \mathbf{x}} \mathbf{x}^{\mathrm{T}} \mathbf{a} = \frac{\partial}{\partial \mathbf{x}} \mathbf{a}^{\mathrm{T}} \mathbf{x} = \mathbf{a}$</li><li>$\frac{\partial}{\partial \mathbf{x}} \|\mathbf{x}\|^2_2 = \frac{\partial}{\partial \mathbf{x}} \mathbf{x}^{\mathrm{T}} \mathbf{x} = 2 \mathbf{x}$</li><li>$\frac{\partial}{\partial \mathbf{x}} \lVert \mathbf{A} \mathbf{x} \rVert^2_2 = 2 \mathbf{A}^{\mathrm{T}} \mathbf{A} \mathbf{x}$</li></ul></div></details><p>Thus, to obtain optimal parameters $\tilde{\mathbf{w}}$, we can solve the normal equations:
$$
\tilde{\mathbf{w}} = (\mathbf{X}^{\mathrm{T}} \mathbf{X})^{-1} \mathbf{X}^{\mathrm{T}} \mathbf{y}.
$$</p><blockquote><p>Note: The matrix $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ is known as the Gram matrix, and it is positive semi-definite. If $\mathbf{X}$ has full column rank, then $\mathbf{X}^{\mathrm{T}} \mathbf{X}$ is invertible, and we can compute the unique solution for $\tilde{\mathbf{w}}$. Otherwise, the solution is not unique, and we may need to use regularization techniques (e.g., ridge regression) to obtain a stable solution, but we will not cover this in this lab.</p></blockquote><ol start=4><li>Implement the analytical solution using NumPy&rsquo;s linear algebra functions. Compare your result with <code>np.linalg.lstsq</code>. To generate dataset, you can use following code snippet:</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate multi-dimensional data</span>
</span></span><span style=display:flex><span>d <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>  <span style=color:#75715e># number of features</span>
</span></span><span style=display:flex><span>n_samples <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate random features</span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(n_samples, d)
</span></span><span style=display:flex><span><span style=color:#75715e># Add intercept term</span>
</span></span><span style=display:flex><span>X_augmented <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>column_stack([np<span style=color:#f92672>.</span>ones(n_samples), X])
</span></span><span style=display:flex><span><span style=color:#75715e># True parameters</span>
</span></span><span style=display:flex><span>w_true <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(d <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)  <span style=color:#75715e># including bias</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate targets with noise</span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> X_augmented <span style=color:#f92672>@</span> w_true <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randn(n_samples)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Data shape: </span><span style=color:#e6db74>{</span>X<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Augmented data shape: </span><span style=color:#e6db74>{</span>X_augmented<span style=color:#f92672>.</span>shape<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;True parameters: </span><span style=color:#e6db74>{</span>w_true<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><h3 id=2-gradient-descent-for-the-multiple-variables-case>2. Gradient descent for the multiple variables case
<a class=anchor href=#2-gradient-descent-for-the-multiple-variables-case>#</a></h3><p>Rather than inverting the matrix $\mathbf{X}^{\mathrm{T}} \mathbf{X}$, which can be computationally expensive for large datasets, we can use gradient descent to find the optimal parameters $\tilde{\mathbf{w}}$.</p><ol><li>Give the update rule for the parameters $\tilde{\mathbf{w}}$ in the context of our linear regression problem using steepest gradient descent.</li></ol><details><summary>Hint</summary><div>The update rule for the parameters $\tilde{\mathbf{w}}$ can be derived from the gradient of the loss function:
$$
\tilde{\mathbf{w}}_{k+1} = \tilde{\mathbf{w}}_k - \alpha_k \nabla L(\tilde{\mathbf{w}}_k)
$$
where the gradient is given by:
$$
\nabla L(\tilde{\mathbf{w}}) = -\mathbf{X}^{\mathrm{T}} (\mathbf{y} - \mathbf{X} \tilde{\mathbf{w}})
$$</div></details><ol start=2><li>Implement gradient descent with a constant step size $\alpha_k = \alpha$ for all iterations. Your function should:<ul><li>Take initial parameters $\tilde{\mathbf{w}}_0$, step size $\alpha$, and number of iterations as inputs.</li><li>Return the trajectory of parameters and loss values.</li><li>Include a stopping criterion based on gradient magnitude.</li></ul></li></ol><h3 id=3-experimenting-with--backtracking-line-search>3. Experimenting with backtracking line search
<a class=anchor href=#3-experimenting-with--backtracking-line-search>#</a></h3><p>From implementing gradient descent, we have seen that the choice of step size $\alpha$ can significantly affect the convergence behavior. A fixed step size may not be optimal for all iterations, leading to slow convergence or oscillations. On the other hand, a decreasing step size is not the best choice as it may lead to very small step sizes in later iterations, causing slow convergence. Let us put in practice the theory we have set in place around line search techniques to adaptively choose the step size at each iteration.</p><ol><li><p>Implement a backtracking line search algorithm to adaptively choose the step size $\alpha_k$ at each iteration. For a reminder, check the memo <a href=http://ammarmian.github.io/numerical_optimization/docs/practical_labs/backtracking/>here</a>.</p></li><li><p>Use the backtracking line search to find the optimal step size for each iteration of gradient descent. Compare the convergence behavior with constant and decreasing step sizes.</p></li><li><p>Experiment with different parameters for the backtracking line search, such as the initial step size $\alpha_0$, reduction factor $\rho$, and Armijo parameter $c_1$. How do these parameters affect the convergence behavior?</p></li></ol><details><summary>Hint</summary><div>The backtracking line search will adaptively adjust the step size based on the Armijo condition, allowing for more efficient convergence. The choice of $\alpha_0$, $\rho$, and $c_1$ can significantly affect the speed of convergence and stability of the algorithm.</div></details><h3 id=4-using-more-complex-linesearch-techniques-using-toolboxes>4. Using more complex linesearch techniques using toolboxes
<a class=anchor href=#4-using-more-complex-linesearch-techniques-using-toolboxes>#</a></h3><p>In practice, we often use more sophisticated line search techniques that are not so easy to implement from scratch. One such technique is the <code>line_search</code> function from SciPy&rsquo;s optimization module, which implements interpolation techniques to find an optimal step size.</p><ol><li>Use the <code>line_search</code> function from SciPy&rsquo;s optimization module to find the optimal step size for each iteration of gradient descent. Compare the convergence behavior with the backtracking line search.
Documentation is available <a href=https://docs.scipy.org/doc/scipy-1.15.3/reference/generated/scipy.optimize.line_search.html>here</a>.</li></ol><h2 id=iii---bonus-the-general-case>III - (Bonus) The general case
<a class=anchor href=#iii---bonus-the-general-case>#</a></h2><p>Consider the general case where the target is also a vector, i.e., we have a multi-output linear regression problem. The model can be expressed as:
\begin{equation}
\mathbf{Y} = \mathbf{X} \tilde{\mathbf{W}} + \boldsymbol{\epsilon},
\label{eq:linear_model_multi_output}
\end{equation}
where $\mathbf{Y} \in \mathbb{R}^{n \times m}$ is the target matrix with $m$ outputs, and $\tilde{\mathbf{W}} \in \mathbb{R}^{(d+1) \times m}$ is the weight matrix.</p><p>Derive all the necessary tools to solve this problem using the same techniques as in the previous sections.</p><h2 id=iv---bonus-regularization>IV - (Bonus) Regularization
<a class=anchor href=#iv---bonus-regularization>#</a></h2><p>In practice, we often encounter situations where the model is overfitting the data, especially in high-dimensional settings. Regularization techniques are used to prevent overfitting by adding a penalty term to the loss function.</p><p>Implement L2 regularization (ridge regression) by adding a penalty term to the loss function:
\begin{equation}
L(\tilde{\mathbf{w}}) = \frac{1}{2} \|\mathbf{y} - \mathbf{X} \tilde{\mathbf{w}}\|^2_2 + \frac{\lambda}{2} \|\tilde{\mathbf{w}}\|^2_2,
\end{equation}
where $\lambda > 0$ is the regularization parameter.</p><h2 id=v---bonus-nonlinear-regression>V - (Bonus) Nonlinear regression
<a class=anchor href=#v---bonus-nonlinear-regression>#</a></h2><p>It&rsquo;s actually possible to extend the linear regression model to nonlinear regression by setting up the design matrix $\mathbf{X}$ to include nonlinear features of the input data. For example, we can include polynomial features suc as:
\begin{equation}
\mathbf{X} = \begin{bmatrix}
1 & x_1 & x_1^2 & \ldots & x_1^d \\
1 & x_2 & x_2^2 & \ldots & x_2^d \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \ldots & x_n^d
\end{bmatrix}
\end{equation}</p><p>From this information and your own research , implement a nonlinear regression model using polynomial features. You can use the <code>PolynomialFeatures</code> class from <code>sklearn.preprocessing</code> to generate polynomial features.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#learning-objectives>Learning objectives</a></li><li><a href=#i---one-dimensional-case>I - One dimensional case</a><ul><li><a href=#1-modeling-and-solving-the-problem>1. Modeling and solving the problem</a></li><li><a href=#2-gradient-descent-for-the-one-dimensional-case>2. Gradient descent for the one-dimensional case</a></li></ul></li><li><a href=#ii---multiple-variables-case>II - Multiple variables case</a><ul><li><a href=#1-modeling-and-solving-the-problem-1>1. Modeling and solving the problem</a></li><li><a href=#2-gradient-descent-for-the-multiple-variables-case>2. Gradient descent for the multiple variables case</a></li><li><a href=#3-experimenting-with--backtracking-line-search>3. Experimenting with backtracking line search</a></li><li><a href=#4-using-more-complex-linesearch-techniques-using-toolboxes>4. Using more complex linesearch techniques using toolboxes</a></li></ul></li><li><a href=#iii---bonus-the-general-case>III - (Bonus) The general case</a></li><li><a href=#iv---bonus-regularization>IV - (Bonus) Regularization</a></li><li><a href=#v---bonus-nonlinear-regression>V - (Bonus) Nonlinear regression</a></li></ul></nav></div></aside></main></body></html>