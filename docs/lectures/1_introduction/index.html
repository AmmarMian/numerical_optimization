<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Introduction
  #


  Notations
  #

Let us start by defining the notation used troughout all the lectures and practical labs.

  Basic Notation
  #

Scalars are represented by italic letters (e.g., $x$, $y$, $\lambda$). Vectors are denoted by bold lowercase letters (e.g., $\mathbf{v}$, $\mathbf{x}$), while matrices are represented by bold uppercase letters (e.g., $\mathbf{A}$, $\mathbf{B}$). The dimensionality of a vector $\mathbf{v} \in \mathbb{R}^n$ indicates it contains $n$ elements, and similarly, a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$ has $m$ rows and $n$ columns."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/1_introduction/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="Introduction"><meta property="og:description" content="Introduction # Notations # Let us start by defining the notation used troughout all the lectures and practical labs.
Basic Notation # Scalars are represented by italic letters (e.g., $x$, $y$, $\lambda$). Vectors are denoted by bold lowercase letters (e.g., $\mathbf{v}$, $\mathbf{x}$), while matrices are represented by bold uppercase letters (e.g., $\mathbf{A}$, $\mathbf{B}$). The dimensionality of a vector $\mathbf{v} \in \mathbb{R}^n$ indicates it contains $n$ elements, and similarly, a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$ has $m$ rows and $n$ columns."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Introduction | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/1_introduction/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.3058cdc089369a1f9b1e578d894abc25d547aae5ffda963ef2c1a1da95fe6189.js integrity="sha256-MFjNwIk2mh+bHleNiUq8JdVHquX/2pY+8sGh2pX+YYk=" crossorigin=anonymous></script><script>const chapterNum=0;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/ class=active>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_projected/>5b. Constrained optimization - Projected Gradient Descent</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimization_linear/>6. Constrained optimization - Linear programming</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li><li><a href=/numerical_optimization/docs/practical_labs/quasinewton/>Quasi-Newton methods memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>Introduction</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#notations>Notations</a><ul><li><a href=#basic-notation>Basic Notation</a></li><li><a href=#matrix-operations>Matrix Operations</a></li><li><a href=#vector-operations>Vector Operations</a></li><li><a href=#eigenvalues-and-eigenvectors>Eigenvalues and Eigenvectors</a></li><li><a href=#matrix-decompositions>Matrix Decompositions</a></li><li><a href=#multivariate-calculus>Multivariate Calculus</a></li><li><a href=#special-matrices-and-properties>Special Matrices and Properties</a></li><li><a href=#derivatives-of-matrix-expressions>Derivatives of Matrix Expressions</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h1><h2 id=notations>Notations
<a class=anchor href=#notations>#</a></h2><p>Let us start by defining the notation used troughout all the lectures and practical labs.</p><h3 id=basic-notation>Basic Notation
<a class=anchor href=#basic-notation>#</a></h3><p>Scalars are represented by italic letters (e.g., $x$, $y$, $\lambda$). Vectors are denoted by bold lowercase letters (e.g., $\mathbf{v}$, $\mathbf{x}$), while matrices are represented by bold uppercase letters (e.g., $\mathbf{A}$, $\mathbf{B}$). The dimensionality of a vector $\mathbf{v} \in \mathbb{R}^n$ indicates it contains $n$ elements, and similarly, a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$ has $m$ rows and $n$ columns.</p><h3 id=matrix-operations>Matrix Operations
<a class=anchor href=#matrix-operations>#</a></h3><p>The transpose of a matrix $\mathbf{A}$ is denoted as $\mathbf{A}^\mathrm{T}$, which reflects the matrix across its diagonal. The trace of a square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$, written as $\mathrm{tr}(\mathbf{A})$, is the sum of its diagonal elements, i.e., $\mathrm{tr}(\mathbf{A}) = \sum_{i=1}^{n} a_{ii}$. The determinant of $\mathbf{A}$ is represented as $\det(\mathbf{A})$ or $|\mathbf{A}|$. A matrix $\mathbf{A}$ is invertible if and only if $\det(\mathbf{A}) \neq 0$, and its inverse is denoted as $\mathbf{A}^{-1}$, satisfying $\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$, where $\mathbf{I}$ is the identity matrix.</p><h3 id=vector-operations>Vector Operations
<a class=anchor href=#vector-operations>#</a></h3><p>The dot product between two vectors $\mathbf{a}$ and $\mathbf{b}$ of the same dimension is written as $\mathbf{a} \cdot \mathbf{b}$ or $\mathbf{a}^\mathrm{T}\mathbf{b}$, resulting in a scalar value.</p><p>The p-norm of a vector $\mathbf{v}$ is denoted as $\lVert\mathbf{v}\rVert_p$ and defined as $\lVert\mathbf{v}\rVert_p = \left(\sum_{i=1}^{n} |v_i|^p\right)^{1/p}$ for $p \geq 1$, with common choices being $p=1$ (Manhattan norm), $p=2$ (Euclidean norm), and $p=\infty$ (maximum norm, defined as $\lVert\mathbf{v}\rVert_{\infty} = \max_i |v_i|$); when the subscript $p$ is omitted, as in $\lVert\mathbf{v}\rVert$, it is conventionally understood to refer to the Euclidean (L2) norm. The Euclidean norm (or length) of a vector $\mathbf{v}$ is represented as $\lVert\mathbf{v}\rVert$ or $\lVert\mathbf{v}\rVert_2$, defined as $\lVert\mathbf{v}\rVert = \sqrt{\mathbf{v}^\mathrm{T}\mathbf{v}} = \sqrt{\sum_{i=1}^{n} v_i^2}$. A unit vector in the direction of $\mathbf{v}$ is given by $\hat{\mathbf{v}} = \mathbf{v}/\lVert\mathbf{v}\rVert$, having a norm of 1.</p><h3 id=eigenvalues-and-eigenvectors>Eigenvalues and Eigenvectors
<a class=anchor href=#eigenvalues-and-eigenvectors>#</a></h3><p>For a square matrix $\mathbf{A}$, a scalar $\lambda$ is an eigenvalue if there exists a non-zero vector $\mathbf{v}$ such that $\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$. The vector $\mathbf{v}$ is called an eigenvector corresponding to the eigenvalue $\lambda$. The characteristic polynomial of $\mathbf{A}$ is defined as $p(\lambda) = \det(\lambda\mathbf{I} - \mathbf{A})$, and its roots are the eigenvalues of $\mathbf{A}$. The spectrum of $\mathbf{A}$, denoted by $\sigma(\mathbf{A})$, is the set of all eigenvalues of $\mathbf{A}$.</p><h3 id=matrix-decompositions>Matrix Decompositions
<a class=anchor href=#matrix-decompositions>#</a></h3><p>The singular value decomposition (SVD) of a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$ is expressed as $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^\mathrm{T}$, where $\mathbf{U} \in \mathbb{R}^{m \times m}$ and $\mathbf{V} \in \mathbb{R}^{n \times n}$ are orthogonal matrices, and $\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$ is a diagonal matrix containing the singular values of $\mathbf{A}$. The eigendecomposition of a diagonalizable matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is given by $\mathbf{A} = \mathbf{P}\mathbf{\Lambda}\mathbf{P}^{-1}$, where $\mathbf{P}$ is a matrix whose columns are the eigenvectors of $\mathbf{A}$, and $\mathbf{\Lambda}$ is a diagonal matrix containing the corresponding eigenvalues.</p><h3 id=multivariate-calculus>Multivariate Calculus
<a class=anchor href=#multivariate-calculus>#</a></h3><p>The gradient of a scalar function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is denoted as $\nabla f$ or $\mathrm{grad}(f)$, resulting in a vector of partial derivatives $\nabla f = [\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}]^\mathrm{T}$. The Jacobian matrix of a vector-valued function $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is represented as $\mathbf{J}_\mathbf{f}$ or $\nabla \mathbf{f}^\mathrm{T}$, where $ (\mathbf{J}_\mathbf{f})_{ij} = \frac{\partial f_i}{\partial x_j} $.</p><p>The Hessian matrix of a twice-differentiable scalar function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is denoted as $\mathbf{H}_f$ or $\nabla^2 f$, where $(\mathbf{H}_f)_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$.</p><h3 id=special-matrices-and-properties>Special Matrices and Properties
<a class=anchor href=#special-matrices-and-properties>#</a></h3><p>A symmetric matrix satisfies $\mathbf{A} = \mathbf{A}^\mathrm{T}$, while a skew-symmetric matrix has $\mathbf{A} = -\mathbf{A}^\mathrm{T}$. An orthogonal matrix $\mathbf{Q}$ satisfies $\mathbf{Q}^\mathrm{T}\mathbf{Q} = \mathbf{Q}\mathbf{Q}^\mathrm{T} = \mathbf{I}$, meaning its inverse equals its transpose: $\mathbf{Q}^{-1} = \mathbf{Q}^\mathrm{T}$. A matrix $\mathbf{A}$ is positive definite if $\mathbf{x}^\mathrm{T}\mathbf{A}\mathbf{x} > 0$ for all non-zero vectors $\mathbf{x}$, and positive semidefinite if $\mathbf{x}^\mathrm{T}\mathbf{A}\mathbf{x} \geq 0$.</p><h3 id=derivatives-of-matrix-expressions>Derivatives of Matrix Expressions
<a class=anchor href=#derivatives-of-matrix-expressions>#</a></h3><p>The derivative of a scalar function with respect to a vector $\mathbf{x}$ is denoted as $\frac{\partial f}{\partial \mathbf{x}}$, resulting in a vector of the same dimension as $\mathbf{x}$. For matrix functions, the derivative with respect to a matrix $\mathbf{X}$ is written as $\frac{\partial f}{\partial \mathbf{X}}$, producing a matrix of the same dimensions as $\mathbf{X}$. Common matrix derivatives include $\frac{\partial}{\partial \mathbf{X}}\mathrm{tr}(\mathbf{AX}) = \mathbf{A}^\mathrm{T}$ and $\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^\mathrm{T}\mathbf{A}\mathbf{x}) = \mathbf{A}\mathbf{x} + \mathbf{A}^\mathrm{T}\mathbf{x}$ (with $\mathbf{A}\mathbf{x} + \mathbf{A}^\mathrm{T}\mathbf{x} = 2\mathbf{A}\mathbf{x}$ when $\mathbf{A}$ is symmetric).</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#notations>Notations</a><ul><li><a href=#basic-notation>Basic Notation</a></li><li><a href=#matrix-operations>Matrix Operations</a></li><li><a href=#vector-operations>Vector Operations</a></li><li><a href=#eigenvalues-and-eigenvectors>Eigenvalues and Eigenvectors</a></li><li><a href=#matrix-decompositions>Matrix Decompositions</a></li><li><a href=#multivariate-calculus>Multivariate Calculus</a></li><li><a href=#special-matrices-and-properties>Special Matrices and Properties</a></li><li><a href=#derivatives-of-matrix-expressions>Derivatives of Matrix Expressions</a></li></ul></li></ul></nav></div></aside></main></body></html>