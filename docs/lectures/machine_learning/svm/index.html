<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Classification and support vector machines
  #


Note: This lecture extends our optimization framework to classification problems, introducing the perceptron algorithm and support vector machines (SVMs). We&rsquo;ll see how the constrained optimization techniques from previous lectures lead to powerful classification methods.


  Classification problems in depth
  #


  Types of classification
  #

In our previous lecture, we introduced binary classification through logistic regression. Let&rsquo;s now formalize the broader classification framework and understand how different formulations connect to optimization problems."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/svm/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="2. Classification and support vector machines"><meta property="og:description" content="Classification and support vector machines # Note: This lecture extends our optimization framework to classification problems, introducing the perceptron algorithm and support vector machines (SVMs). We’ll see how the constrained optimization techniques from previous lectures lead to powerful classification methods.
Classification problems in depth # Types of classification # In our previous lecture, we introduced binary classification through logistic regression. Let’s now formalize the broader classification framework and understand how different formulations connect to optimization problems."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>2. Classification and support vector machines | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/machine_learning/svm/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.c8136f89692432684b7768e41dc799210664a14f5868691247e588c4590249dd.js integrity="sha256-yBNviWkkMmhLd2jkHceZIQZkoU9YaGkSR+WIxFkCSd0=" crossorigin=anonymous></script><script>const chapterNum=2;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_projected/>5b. Constrained optimization - Projected Gradient Descent</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimization_linear/>6. Constrained optimization - Linear programming</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/fundamentals/>1. Machine learning fundamentals</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/ class=active>2. Classification and support vector machines</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>II - MNIST and Fashion-MNIST Classification</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li><li><a href=/numerical_optimization/docs/practical_labs/quasinewton/>Quasi-Newton methods memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>2. Classification and support vector machines</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#classification-problems-in-depth>Classification problems in depth</a><ul><li><a href=#types-of-classification>Types of classification</a></li><li><a href=#performance-metrics-for-classification>Performance metrics for classification</a></li></ul></li><li><a href=#the-perceptron-algorithm>The perceptron algorithm</a><ul><li><a href=#model-definition-and-geometry>Model definition and geometry</a></li><li><a href=#the-perceptron-loss-function>The perceptron loss function</a></li><li><a href=#gradient-computation-and-update-rule>Gradient computation and update rule</a></li><li><a href=#introduction-to-stochastic-optimization>Introduction to stochastic optimization</a></li></ul></li><li><a href=#support-vector-machines>Support vector machines</a><ul><li><a href=#motivation-which-hyperplane-is-best>Motivation: which hyperplane is best?</a></li><li><a href=#mathematical-formulation>Mathematical formulation</a></li><li><a href=#the-primal-optimization-problem>The primal optimization problem</a></li><li><a href=#lagrangian-formulation-and-kkt-conditions>Lagrangian formulation and KKT conditions</a></li><li><a href=#the-dual-problem>The dual problem</a></li><li><a href=#computing-the-final-classifier>Computing the final classifier</a></li></ul></li><li><a href=#cross-validation>Cross-validation</a><ul><li><a href=#k-fold-cross-validation>K-fold cross-validation</a></li><li><a href=#stratified-cross-validation>Stratified cross-validation</a></li><li><a href=#applications-of-cross-validation>Applications of cross-validation</a></li></ul></li><li><a href=#summary-and-exercises>Summary and exercises</a><ul><li><a href=#exercises>Exercises</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=classification-and-support-vector-machines>Classification and support vector machines
<a class=anchor href=#classification-and-support-vector-machines>#</a></h1><blockquote><p><strong>Note</strong>: This lecture extends our optimization framework to classification problems, introducing the perceptron algorithm and support vector machines (SVMs). We&rsquo;ll see how the constrained optimization techniques from previous lectures lead to powerful classification methods.</p></blockquote><h2 id=classification-problems-in-depth>Classification problems in depth
<a class=anchor href=#classification-problems-in-depth>#</a></h2><h3 id=types-of-classification>Types of classification
<a class=anchor href=#types-of-classification>#</a></h3><p>In our previous lecture, we introduced binary classification through logistic regression. Let&rsquo;s now formalize the broader classification framework and understand how different formulations connect to optimization problems.</p><p><strong>Binary classification</strong> remains the fundamental building block. We can choose between two label conventions:</p><ul><li>$y \in \{0, 1\}$: Natural for probabilistic models like logistic regression</li><li>$y \in \{-1, +1\}$: Convenient for geometric algorithms like perceptron and SVM</li></ul><p>The choice affects our loss functions and update rules, but the underlying problem remains the same: partition the input space into two regions.</p><p><strong>Multiclass classification</strong> extends to $K > 2$ classes with labels $y \in \{1, 2, \ldots, K\}$. We often use one-hot encoding, transforming each label into a vector $\mathbf{y} \in \{0,1\}^K$ where exactly one component equals 1. For instance, in digit recognition with $K=10$, the digit &ldquo;3&rdquo; becomes $\mathbf{y} = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]^{\mathrm{T}}$. This encoding naturally extends to probabilistic models where we predict a distribution over classes.</p><h3 id=performance-metrics-for-classification>Performance metrics for classification
<a class=anchor href=#performance-metrics-for-classification>#</a></h3><p>Classification performance requires more nuanced evaluation than regression&rsquo;s simple squared error. Consider a binary classifier&rsquo;s predictions compared to true labels, summarized in the confusion matrix:</p><pre tabindex=0><code>              Predicted
              0    1
Actual   0   TN   FP
         1   FN   TP
</code></pre><p>where TN = True Negatives, FP = False Positives, FN = False Negatives, and TP = True Positives.</p><div id=classification_metrics class=theorem-box><p class=theorem-title><strong>Definition 2.1 (Classification metrics)</strong></p><div class=theorem-content><p><strong>Accuracy</strong> measures overall correctness: $\frac{TP + TN}{TP + TN + FP + FN}$</p><p><strong>Precision</strong> answers &ldquo;of predicted positives, how many are correct?&rdquo;: $\frac{TP}{TP + FP}$</p><p><strong>Recall (Sensitivity)</strong> answers &ldquo;of actual positives, how many did we detect?&rdquo;: $\frac{TP}{TP + FN}$</p><p><strong>Specificity</strong> answers &ldquo;of actual negatives, how many did we correctly identify?&rdquo;: $\frac{TN}{TN + FP}$</p><p><strong>F1-Score</strong> balances precision and recall: $\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$</p></div></div><p>The choice of metric depends critically on the problem context. Consider spam detection where 95% of emails are legitimate. A classifier that always predicts &ldquo;not spam&rdquo; achieves 95% accuracy but has zero recall—it catches no spam at all! In such imbalanced scenarios, precision and recall provide more meaningful evaluation. Medical diagnosis presents another perspective: high recall ensures we catch most disease cases (minimizing false negatives), while high precision reduces unnecessary treatments from false positives.</p><h2 id=the-perceptron-algorithm>The perceptron algorithm
<a class=anchor href=#the-perceptron-algorithm>#</a></h2><h3 id=model-definition-and-geometry>Model definition and geometry
<a class=anchor href=#model-definition-and-geometry>#</a></h3><p>The perceptron, introduced by Rosenblatt in 1958, provides our first glimpse into linear classification beyond the probabilistic framework of logistic regression. Using the $\{-1, +1\}$ label convention, the perceptron model is:</p><p>\begin{equation}
f(\mathbf{x}) = \text{sign}(\mathbf{w}^{\mathrm{T}} \mathbf{x} + b)
\label{eq:perceptron_model}
\end{equation}</p><p>where $\text{sign}(z) = +1$ if $z > 0$ and $-1$ otherwise.</p><p>The geometric interpretation reveals the elegance of this formulation. The decision boundary satisfies $\mathbf{w}^{\mathrm{T}} \mathbf{x} + b = 0$, defining a hyperplane in $\mathbb{R}^d$. The vector $\mathbf{w}$ serves as the normal to this hyperplane, pointing toward the positive class region. The bias $b$ controls the hyperplane&rsquo;s offset from the origin—specifically, the perpendicular distance from the origin to the hyperplane equals $\frac{|b|}{|\mathbf{w}|}$.</p><h3 id=the-perceptron-loss-function>The perceptron loss function
<a class=anchor href=#the-perceptron-loss-function>#</a></h3><p>Unlike logistic regression&rsquo;s smooth cross-entropy loss, the perceptron employs a more direct approach: it only penalizes misclassified points. This leads to the perceptron loss:</p><p>\begin{equation}
L(\mathbf{w}, b) = \sum_{i \in \mathcal{M}} -y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b)
\label{eq:perceptron_loss}
\end{equation}</p><p>where $\mathcal{M} = \{i : y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) \leq 0\}$ denotes the set of misclassified points.</p><p>To understand this loss, consider a point $(\mathbf{x}_i, y_i)$:</p><ul><li>If correctly classified: $y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) > 0$, contributing zero to the loss</li><li>If misclassified: $y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) &lt; 0$, contributing $|y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b)|$ to the loss</li></ul><p>The loss measures how &ldquo;wrong&rdquo; we are on misclassified points, with larger violations incurring greater penalties.</p><h3 id=gradient-computation-and-update-rule>Gradient computation and update rule
<a class=anchor href=#gradient-computation-and-update-rule>#</a></h3><p>Computing gradients of the perceptron loss yields remarkably simple expressions:</p><p>\begin{equation}
\frac{\partial L}{\partial \mathbf{w}} = -\sum_{i \in \mathcal{M}} y_i \mathbf{x}_i
\label{eq:perceptron_grad_w}
\end{equation}</p><p>\begin{equation}
\frac{\partial L}{\partial b} = -\sum_{i \in \mathcal{M}} y_i
\label{eq:perceptron_grad_b}
\end{equation}</p><p>These gradients lead to the classic perceptron update rule. For each misclassified point $(\mathbf{x}_i, y_i)$:</p><p>\begin{equation}
\begin{aligned}
\mathbf{w} &\leftarrow \mathbf{w} + \alpha y_i \mathbf{x}_i \\
b &\leftarrow b + \alpha y_i
\end{aligned}
\label{eq:perceptron_update}
\end{equation}</p><p>The geometric intuition is compelling: we adjust the hyperplane to better classify the misclassified point by moving the normal vector $\mathbf{w}$ in the direction that reduces the violation.</p><h3 id=introduction-to-stochastic-optimization>Introduction to stochastic optimization
<a class=anchor href=#introduction-to-stochastic-optimization>#</a></h3><p>The perceptron naturally motivates stochastic optimization. Consider a dataset with millions of points—computing the full gradient over all misclassified points becomes computationally prohibitive. This challenge extends beyond the perceptron to all large-scale machine learning.</p><p><strong>Full batch gradient descent</strong> processes all data at each iteration:</p><p>\begin{equation}
\boldsymbol{\theta}^{(k+1)} = \boldsymbol{\theta}^{(k)} - \alpha \frac{1}{n} \sum_{i=1}^n \nabla_{\boldsymbol{\theta}} L_i(\boldsymbol{\theta}^{(k)})
\label{eq:batch_gd}
\end{equation}</p><p>with computational cost $O(n)$ per iteration.</p><p><strong>Stochastic gradient descent (SGD)</strong> instead uses a single randomly selected example:</p><p>\begin{equation}
\boldsymbol{\theta}^{(k+1)} = \boldsymbol{\theta}^{(k)} - \alpha \nabla_{\boldsymbol{\theta}} L_i(\boldsymbol{\theta}^{(k)})
\label{eq:sgd}
\end{equation}</p><p>where $i$ is sampled uniformly from $\{1, 2, \ldots, n\}$. The computational cost drops to $O(1)$ per iteration.</p><p>The key insight: while each SGD update is noisy, it provides an unbiased estimate of the true gradient. Mathematically, $\mathbb{E}[\nabla_{\boldsymbol{\theta}} L_i(\boldsymbol{\theta})] = \frac{1}{n} \sum_{j=1}^n \nabla_{\boldsymbol{\theta}} L_j(\boldsymbol{\theta})$. Over many iterations, these noisy steps average out, leading to convergence (with appropriate learning rate decay).</p><p><strong>Mini-batch SGD</strong> strikes a balance, using a small subset $B$ of examples:</p><p>\begin{equation}
\boldsymbol{\theta}^{(k+1)} = \boldsymbol{\theta}^{(k)} - \alpha \frac{1}{|B|} \sum_{i \in B} \nabla_{\boldsymbol{\theta}} L_i(\boldsymbol{\theta}^{(k)})
\label{eq:minibatch}
\end{equation}</p><p>Typical batch sizes range from 32 to 256, leveraging vectorized computation while maintaining reasonable variance in gradient estimates.</p><div id=perceptron_convergence class=theorem-box><p class=theorem-title><strong>Theorem 2.1 (Perceptron convergence theorem)</strong></p><div class=theorem-content>If the training data is linearly separable with margin $\gamma > 0$ and all $|\mathbf{x}_i| \leq R$, then the perceptron algorithm converges in at most $(R/\gamma)^2$ iterations.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>The proof relies on two key observations: (1) each update increases $\mathbf{w}^{\mathrm{T}} \mathbf{w}^\star$ where $\mathbf{w}^\star$ is a separating hyperplane, and (2) $|\mathbf{w}|$ grows slowly. The ratio of these quantities bounds the number of updates. See Novikoff (1962) for the complete proof.</div><div class=qed>■</div></div><h2 id=support-vector-machines>Support vector machines
<a class=anchor href=#support-vector-machines>#</a></h2><h3 id=motivation-which-hyperplane-is-best>Motivation: which hyperplane is best?
<a class=anchor href=#motivation-which-hyperplane-is-best>#</a></h3><p>The perceptron finds <em>a</em> separating hyperplane when data is linearly separable, but infinitely many such hyperplanes exist. Which should we prefer? Support vector machines answer this question elegantly: choose the hyperplane that maximizes the margin—the distance to the nearest data points.</p><p>This maximum margin principle embodies a form of regularization. By staying as far as possible from all training points, we build in robustness to small perturbations and improve generalization to unseen data. The connection to our optimization framework becomes clear: we&rsquo;ll formulate margin maximization as a constrained optimization problem.</p><h3 id=mathematical-formulation>Mathematical formulation
<a class=anchor href=#mathematical-formulation>#</a></h3><p>Consider a linearly separable dataset with labels $y_i \in \{-1, +1\}$. Any separating hyperplane satisfies $y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) > 0$ for all $i$. The SVM introduces a crucial normalization: we scale $\mathbf{w}$ and $b$ such that the closest points to the hyperplane satisfy:</p><p>\begin{equation}
y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) = 1
\label{eq:svm_normalization}
\end{equation}</p><p>This canonical form ensures all points satisfy:</p><p>\begin{equation}
y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) \geq 1 \quad \forall i = 1, \ldots, n
\label{eq:svm_constraints}
\end{equation}</p><p>The geometric distance from a point $\mathbf{x}_0$ to the hyperplane $\mathbf{w}^{\mathrm{T}} \mathbf{x} + b = 0$ equals:</p><p>$$\text{distance} = \frac{|\mathbf{w}^{\mathrm{T}} \mathbf{x}_0 + b|}{|\mathbf{w}|}$$</p><p>For the support vectors (points achieving equality in \eqref{eq:svm_constraints}), this distance equals $\frac{1}{|\mathbf{w}|}$. The margin—the total separation between the two classes—therefore equals $\frac{2}{|\mathbf{w}|}$.</p><h3 id=the-primal-optimization-problem>The primal optimization problem
<a class=anchor href=#the-primal-optimization-problem>#</a></h3><p>Maximizing the margin $\frac{2}{|\mathbf{w}|}$ is equivalent to minimizing $|\mathbf{w}|$. For mathematical convenience, we minimize $\frac{1}{2}|\mathbf{w}|^2$, leading to the SVM primal problem:</p><p>\begin{equation}
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2} |\mathbf{w}|^2 \\
\text{subject to} \quad & y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) \geq 1, \quad i = 1, \ldots, n
\end{aligned}
\label{eq:svm_primal}
\end{equation}</p><p>This is a convex quadratic program—the objective is a positive definite quadratic function, and the constraints are linear. Our optimization theory guarantees a unique global minimum.</p><h3 id=lagrangian-formulation-and-kkt-conditions>Lagrangian formulation and KKT conditions
<a class=anchor href=#lagrangian-formulation-and-kkt-conditions>#</a></h3><p>Introducing Lagrange multipliers $\alpha_i \geq 0$ for each constraint, we form the Lagrangian:</p><p>\begin{equation}
\mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}|\mathbf{w}|^2 - \sum_{i=1}^n \alpha_i [y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) - 1]
\label{eq:svm_lagrangian}
\end{equation}</p><p>The KKT conditions for this problem are:</p><p><strong>Stationarity:</strong></p><p>\begin{equation}
\nabla_{\mathbf{w}} \mathcal{L} = \mathbf{w} - \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i = 0
\label{eq:kkt_stationarity_w}
\end{equation}</p><p>\begin{equation}
\nabla_b \mathcal{L} = -\sum_{i=1}^n \alpha_i y_i = 0
\label{eq:kkt_stationarity_b}
\end{equation}</p><p><strong>Primal feasibility:</strong>
$$y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) - 1 \geq 0$$</p><p><strong>Dual feasibility:</strong>
$$\alpha_i \geq 0$$</p><p><strong>Complementary slackness:</strong>
\begin{equation}
\alpha_i [y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) - 1] = 0
\label{eq:complementary_slackness}
\end{equation}</p><p>The complementary slackness condition reveals the sparse nature of the SVM solution. For each point, either $\alpha_i = 0$ (the point doesn&rsquo;t influence the solution) or $y_i(\mathbf{w}^{\mathrm{T}} \mathbf{x}_i + b) = 1$ (the point lies exactly on the margin boundary). Points with $\alpha_i > 0$ are called <strong>support vectors</strong>—they alone determine the hyperplane.</p><h3 id=the-dual-problem>The dual problem
<a class=anchor href=#the-dual-problem>#</a></h3><p>From the stationarity condition \eqref{eq:kkt_stationarity_w}, we have:</p><p>$$\mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i$$</p><p>Substituting this and the constraint $\sum_{i=1}^n \alpha_i y_i = 0$ back into the Lagrangian yields the dual objective:</p><p>\begin{equation}
\mathcal{L}_D(\boldsymbol{\alpha}) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \mathbf{x}_i^{\mathrm{T}} \mathbf{x}_j
\label{eq:svm_dual_objective}
\end{equation}</p><p>The SVM dual problem becomes:</p><p>\begin{equation}
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad & \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \mathbf{x}_i^{\mathrm{T}} \mathbf{x}_j \\
\text{subject to} \quad & \sum_{i=1}^n \alpha_i y_i = 0 \\
& \alpha_i \geq 0, \quad i = 1, \ldots, n
\end{aligned}
\label{eq:svm_dual}
\end{equation}</p><p>This dual formulation offers several advantages. The problem size depends on the number of training examples $n$, not the dimension $d$. More importantly, the objective depends on the data only through inner products $\mathbf{x}_i^{\mathrm{T}} \mathbf{x}_j$—this observation leads to the kernel trick for nonlinear classification.</p><h3 id=computing-the-final-classifier>Computing the final classifier
<a class=anchor href=#computing-the-final-classifier>#</a></h3><p>Once we solve for the optimal $\boldsymbol{\alpha}^\star$, we reconstruct the primal variables:</p><p>$$\mathbf{w}^\star = \sum_{i=1}^n \alpha_i^\star y_i \mathbf{x}_i$$</p><p>To find $b^\star$, we use any support vector $\mathbf{x}_s$ (where $\alpha_s^\star > 0$). From complementary slackness, $y_s(\mathbf{w}^{*\mathrm{T}} \mathbf{x}_s + b^\star) = 1$, giving:</p><p>\begin{equation}
b^\star = y_s - \mathbf{w}^{*\mathrm{T}} \mathbf{x}_s = y_s - \sum_{i=1}^n \alpha_i^\star y_i \mathbf{x}_i^{\mathrm{T}} \mathbf{x}_s
\label{eq:svm_compute_b}
\end{equation}</p><p>The final classification function is:</p><p>\begin{equation}
f(\mathbf{x}) = \text{sign}\left(\sum_{i=1}^n \alpha_i^\star y_i \mathbf{x}_i^{\mathrm{T}} \mathbf{x} + b^\star\right)
\label{eq:svm_classifier}
\end{equation}</p><p>Remarkably, only support vectors contribute to this sum—most $\alpha_i^\star$ equal zero. This sparsity makes SVMs computationally efficient at test time and provides insight into which training examples are most informative.</p><h2 id=cross-validation>Cross-validation
<a class=anchor href=#cross-validation>#</a></h2><h3 id=k-fold-cross-validation>K-fold cross-validation
<a class=anchor href=#k-fold-cross-validation>#</a></h3><p>Machine learning models contain hyperparameters that cannot be learned through optimization—for instance, the learning rate in gradient descent or the regularization strength in SVMs. Cross-validation provides a principled approach to hyperparameter selection and performance estimation.</p><div id=kfold_cv class=theorem-box><p class=theorem-title><strong>Definition 2.2 (K-fold cross-validation)</strong></p><div class=theorem-content><p><strong>K-fold cross-validation</strong> partitions the training data into $K$ equal-sized folds. For each fold $k$:</p><ol><li>Train the model on folds $\{1, 2, \ldots, K\} \setminus \{k\}$</li><li>Evaluate on fold $k$</li><li>Record the validation performance</li></ol><p>The final performance estimate is the average across all $K$ folds.</p></div></div><p>The algorithm proceeds as follows:</p><ol><li>Randomly shuffle the dataset</li><li>Split into $K$ approximately equal folds</li><li>For $k = 1$ to $K$:<ul><li>Combine folds $\{1, \ldots, K\} \setminus \{k\}$ as training set</li><li>Use fold $k$ as validation set</li><li>Train model and compute validation metric</li></ul></li><li>Return mean and standard deviation of $K$ validation scores</li></ol><p>Common choices include $K = 5$ or $K = 10$, balancing computational cost against variance in the estimate. The extreme case $K = n$ (leave-one-out cross-validation) provides an nearly unbiased estimate but becomes computationally prohibitive for large datasets.</p><h3 id=stratified-cross-validation>Stratified cross-validation
<a class=anchor href=#stratified-cross-validation>#</a></h3><p>For classification problems, especially with imbalanced classes, <strong>stratified K-fold</strong> maintains the class distribution in each fold. If 10% of examples are positive in the full dataset, each fold should contain approximately 10% positive examples. This prevents pessimistic estimates from folds that lack examples from minority classes.</p><h3 id=applications-of-cross-validation>Applications of cross-validation
<a class=anchor href=#applications-of-cross-validation>#</a></h3><p>Cross-validation serves three primary purposes in machine learning:</p><p><strong>Model selection</strong> compares different algorithms (e.g., logistic regression vs. SVM) using their cross-validated performance. The model with the best average validation score is selected for final training on the full dataset.</p><p><strong>Hyperparameter tuning</strong> searches over hyperparameter values (e.g., regularization strength $C$ in SVM) to find settings that maximize cross-validated performance. Grid search exhaustively tries combinations, while random search samples from distributions over hyperparameters.</p><p><strong>Performance estimation</strong> provides a robust estimate of how the model will perform on unseen data. The standard deviation across folds indicates the stability of this estimate.</p><h2 id=summary-and-exercises>Summary and exercises
<a class=anchor href=#summary-and-exercises>#</a></h2><p>We&rsquo;ve extended our optimization framework to classification, introducing two fundamental algorithms. The perceptron uses a simple loss that only penalizes misclassified points, naturally leading to stochastic gradient descent. Support vector machines formulate classification as constrained optimization, finding the maximum margin hyperplane through the elegant machinery of Lagrangian duality.</p><p>The key insights connecting to our optimization foundations:</p><ul><li>Classification losses differ from regression but still yield optimization problems</li><li>Stochastic gradient descent addresses computational challenges of large datasets</li><li>Constrained optimization and KKT conditions lead to powerful algorithms like SVM</li><li>The dual formulation reveals computational advantages and enables extensions</li></ul><h3 id=exercises>Exercises
<a class=anchor href=#exercises>#</a></h3><ol><li><p><strong>Perceptron implementation</strong>: Implement the perceptron algorithm from scratch. Test on a 2D dataset where you can visualize the decision boundary evolution during training. Compare batch versus stochastic updates.</p></li><li><p><strong>SVM with sklearn</strong>: Apply SVM to a subset of MNIST digits (e.g., 3 vs. 8). First use PCA to reduce dimensionality to 50 components, then tune the regularization parameter $C$ using 5-fold cross-validation. Report test accuracy and number of support vectors.</p></li><li><p><strong>Derive the soft-margin SVM</strong>: Extend the hard-margin SVM to handle non-separable data by introducing slack variables $\xi_i \geq 0$. Write the primal problem, derive the KKT conditions, and show how the dual problem changes.</p></li></ol></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#classification-problems-in-depth>Classification problems in depth</a><ul><li><a href=#types-of-classification>Types of classification</a></li><li><a href=#performance-metrics-for-classification>Performance metrics for classification</a></li></ul></li><li><a href=#the-perceptron-algorithm>The perceptron algorithm</a><ul><li><a href=#model-definition-and-geometry>Model definition and geometry</a></li><li><a href=#the-perceptron-loss-function>The perceptron loss function</a></li><li><a href=#gradient-computation-and-update-rule>Gradient computation and update rule</a></li><li><a href=#introduction-to-stochastic-optimization>Introduction to stochastic optimization</a></li></ul></li><li><a href=#support-vector-machines>Support vector machines</a><ul><li><a href=#motivation-which-hyperplane-is-best>Motivation: which hyperplane is best?</a></li><li><a href=#mathematical-formulation>Mathematical formulation</a></li><li><a href=#the-primal-optimization-problem>The primal optimization problem</a></li><li><a href=#lagrangian-formulation-and-kkt-conditions>Lagrangian formulation and KKT conditions</a></li><li><a href=#the-dual-problem>The dual problem</a></li><li><a href=#computing-the-final-classifier>Computing the final classifier</a></li></ul></li><li><a href=#cross-validation>Cross-validation</a><ul><li><a href=#k-fold-cross-validation>K-fold cross-validation</a></li><li><a href=#stratified-cross-validation>Stratified cross-validation</a></li><li><a href=#applications-of-cross-validation>Applications of cross-validation</a></li></ul></li><li><a href=#summary-and-exercises>Summary and exercises</a><ul><li><a href=#exercises>Exercises</a></li></ul></li></ul></nav></div></aside></main></body></html>