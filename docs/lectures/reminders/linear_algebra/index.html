<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Fundamentals of Linear Algebra
  #


  1 - Introduction
  #


Linear algebra is one of the foundational branches of mathematics, with applications spanning from engineering and computer science to economics and physics. This document is extracted from the textbook Matrix Differential Calculus with Applications in Statistics and Econometrics from Jan R. Magnus; Heinz Neudecker, adapting the notations to the ones used in the lectures.

In this chapter, we summarize some of the well-known definitions and theorems of matrix algebra. Most of the theorems will be proved."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/reminders/linear_algebra/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="Linear Algebra"><meta property="og:description" content="Fundamentals of Linear Algebra # 1 - Introduction # Linear algebra is one of the foundational branches of mathematics, with applications spanning from engineering and computer science to economics and physics. This document is extracted from the textbook Matrix Differential Calculus with Applications in Statistics and Econometrics from Jan R. Magnus; Heinz Neudecker, adapting the notations to the ones used in the lectures.
In this chapter, we summarize some of the well-known definitions and theorems of matrix algebra. Most of the theorems will be proved."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Linear Algebra | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/reminders/linear_algebra/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.685635e46869170dc4e585ef7f203f7af22844553caf412b42931aea040bbfcd.js integrity="sha256-aFY15GhpFw3E5YXvfyA/evIoRFU8r0ErQpMa6gQLv80=" crossorigin=anonymous></script><script>const chapterNum=0;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle checked>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/ class=active>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>Linear Algebra</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#1---introduction>1 - Introduction</a></li><li><a href=#2---sets>2 - Sets</a></li><li><a href=#3---matrices-addition-and-multiplication>3 - Matrices: Addition and Multiplication</a></li><li><a href=#4---the-transpose-of-a-matrix>4 - The transpose of a matrix</a></li><li><a href=#5---square-matrices>5 - Square matrices</a></li><li><a href=#6----linear-forms-and-quadratic-forms>6 - Linear forms and quadratic forms</a></li><li><a href=#7---the-rank-of-a-matrix>7 - The rank of a matrix</a></li><li><a href=#8---the-inverse>8 - The Inverse</a></li><li><a href=#9---the-determinant>9 - The Determinant</a></li><li><a href=#10---the-trace>10 - The trace</a></li><li><a href=#11----partitioned-matrices>11 - Partitioned matrices</a></li><li><a href=#12---complex-matrices>12 - Complex Matrices</a></li><li><a href=#13----eigenvalues-and-eigenvectors>13 - Eigenvalues and Eigenvectors</a></li><li><a href=#14---schurs-decomposition-theorem>14 - Schur&rsquo;s decomposition theorem</a></li><li><a href=#15---the-jordan-decomposition>15 - The Jordan decomposition</a></li><li><a href=#16---the-singular-value-decomposition>16 - The singular value decomposition</a></li><li><a href=#17---further-results-concerning-eigenvalues>17 - Further results concerning eigenvalues</a></li><li><a href=#positive-semidefinite-matrices>Positive (semi)definite matrices</a></li><li><a href=#three-further-results-for-positive-definite-matrices>Three further results for positive definite matrices</a></li><li><a href=#a-useful-result>A useful result</a></li><li><a href=#symmetric-matrix-functions>Symmetric matrix functions</a></li><li><a href=#miscellaneous-exercises>Miscellaneous exercises</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=fundamentals-of-linear-algebra>Fundamentals of Linear Algebra
<a class=anchor href=#fundamentals-of-linear-algebra>#</a></h1><h2 id=1---introduction>1 - Introduction
<a class=anchor href=#1---introduction>#</a></h2><blockquote><p>Linear algebra is one of the foundational branches of mathematics, with applications spanning from engineering and computer science to economics and physics. This document is extracted from the textbook <strong>Matrix Differential Calculus with Applications in Statistics and Econometrics</strong> from Jan R. Magnus; Heinz Neudecker, adapting the notations to the ones used in the lectures.</p></blockquote><p>In this chapter, we summarize some of the well-known definitions and theorems of matrix algebra. Most of the theorems will be proved.</p><h2 id=2---sets>2 - Sets
<a class=anchor href=#2---sets>#</a></h2><div id=set_definition class=theorem-box><p class=theorem-title><strong>Definition 0.1 (Set)</strong></p><div class=theorem-content>A <strong>set</strong> is a collection of objects, called the elements (or members) of the set. We write $x \in S$ to mean &lsquo;$x$ is an element of $S$&rsquo; or &lsquo;$x$ belongs to $S$&rsquo;. If $x$ does not belong to $S$, we write $x \notin S$. The set that contains no elements is called the <strong>empty set</strong>, denoted by $\emptyset$.</div></div><p>Sometimes a set can be defined by displaying the elements in braces. For example, $A={0,1}$ or</p><p>$$
\mathbb{N}={1,2,3, \ldots}
$$</p><p>Notice that $A$ is a finite set (contains a finite number of elements), whereas $\mathbb{N}$ is an infinite set. If $P$ is a property that any element of $S$ has or does not have, then</p><p>$$
{x: x \in S, x \text { satisfies } P}
$$</p><p>denotes the set of all the elements of $S$ that have property $P$.</p><div id=subset_definition class=theorem-box><p class=theorem-title><strong>Definition 0.2 (Subset)</strong></p><div class=theorem-content>A set $A$ is called a <strong>subset</strong> of $B$, written $A \subset B$, whenever every element of $A$ also belongs to $B$. The notation $A \subset B$ does not rule out the possibility that $A=B$. If $A \subset B$ and $A \neq B$, then we say that $A$ is a <strong>proper subset</strong> of $B$.</div></div><p>If $A$ and $B$ are two subsets of $S$, we define</p><p>$$
A \cup B,
$$</p><p>the union of $A$ and $B$, as the set of elements of $S$ that belong to $A$ or to $B$ or to both, and</p><p>$$
A \cap B,
$$</p><p>the intersection of $A$ and $B$, as the set of elements of $S$ that belong to both $A$ and $B$. We say that $A$ and $B$ are (mutually) disjoint if they have no common elements, that is, if</p><p>$$
A \cap B=\emptyset .
$$</p><p>The complement of $A$ relative to $B$, denoted by $B-A$, is the set ${x: x \in B$, but $x \notin A}$. The complement of $A$ (relative to $S$) is sometimes denoted by $A^{c}$.</p><div id=cartesian_product class=theorem-box><p class=theorem-title><strong>Definition 0.3 (Cartesian Product)</strong></p><div class=theorem-content><p>The <strong>Cartesian product</strong> of two sets $A$ and $B$, written $A \times B$, is the set of all ordered pairs $(a, b)$ such that $a \in A$ and $b \in B$. More generally, the Cartesian product of $n$ sets $A_{1}, A_{2}, \ldots, A_{n}$, written</p><p>$$
\prod_{i=1}^{n} A_{i}
$$</p><p>is the set of all ordered $n$-tuples $\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ such that $a_{i} \in A_{i}(i=1, \ldots, n)$.</p></div></div><p>The set of (finite) real numbers (the one-dimensional Euclidean space) is denoted by $\mathbb{R}$. The $n$-dimensional Euclidean space $\mathbb{R}^{n}$ is the Cartesian product of $n$ sets equal to $\mathbb{R}$:</p><p>$$
\mathbb{R}^{n}=\mathbb{R} \times \mathbb{R} \times \cdots \times \mathbb{R} \quad (n \text { times })
$$</p><p>The elements of $\mathbb{R}^{n}$ are thus the ordered $n$-tuples $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ of real numbers $x_{1}, x_{2}, \ldots, x_{n}$.</p><div id=bounded_set class=theorem-box><p class=theorem-title><strong>Definition 0.4 (Bounded Set)</strong></p><div class=theorem-content>A set $S$ of real numbers is said to be <strong>bounded</strong> if there exists a number $M$ such that $|x| \leq M$ for all $x \in S$.</div></div><h2 id=3---matrices-addition-and-multiplication>3 - Matrices: Addition and Multiplication
<a class=anchor href=#3---matrices-addition-and-multiplication>#</a></h2><div id=real_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.5 (Real Matrix)</strong></p><div class=theorem-content><p>A <strong>real</strong> $m \times n$ <strong>matrix</strong> $\mathbf{A}$ is a rectangular array of real numbers</p><p>$$
\mathbf{A}=\left(\begin{array}{cccc}
a_{11} & a_{12} & \ldots & a_{1 n} \\
a_{21} & a_{22} & \ldots & a_{2 n} \\
\vdots & \vdots & & \vdots \\
a_{m 1} & a_{m 2} & \ldots & a_{m n}
\end{array}\right)
$$</p><p>We sometimes write $\mathbf{A}=\left(a_{i j}\right)$.</p></div></div><p>If one or more of the elements of $\mathbf{A}$ is complex, we say that $\mathbf{A}$ is a complex matrix. Almost all matrices in this book are real and the word &lsquo;matrix&rsquo; is assumed to be a real matrix, unless explicitly stated otherwise.</p><p>An $m \times n$ matrix can be regarded as a point in $\mathbb{R}^{m \times n}$. The real numbers $a_{i j}$ are called the elements of $\mathbf{A}$. An $m \times 1$ matrix is a point in $\mathbb{R}^{m \times 1}$ (that is, in $\mathbb{R}^{m}$) and is called a (column) vector of order $m \times 1$. A $1 \times n$ matrix is called a row vector (of order $1 \times n$). The elements of a vector are usually called its components. Matrices are always denoted by capital letters and vectors by lower-case letters.</p><div id=matrix_addition class=theorem-box><p class=theorem-title><strong>Definition 0.6 (Matrix Addition)</strong></p><div class=theorem-content><p>The <strong>sum</strong> of two matrices $\mathbf{A}$ and $\mathbf{B}$ of the same order is defined as</p><p>$$
\mathbf{A}+\mathbf{B}=\left(a_{i j}\right)+\left(b_{i j}\right)=\left(a_{i j}+b_{i j}\right)
$$</p></div></div><div id=scalar_multiplication class=theorem-box><p class=theorem-title><strong>Definition 0.7 (Scalar Multiplication)</strong></p><div class=theorem-content><p>The <strong>product</strong> of a matrix by a scalar $\lambda$ is</p><p>$$
\lambda \mathbf{A}=\mathbf{A} \lambda=\left(\lambda a_{i j}\right)
$$</p></div></div><p>The following properties are now easily proved for matrices $\mathbf{A}, \mathbf{B}$, and $\mathbf{C}$ of the same order and scalars $\lambda$ and $\mu$:</p><p>\begin{equation}
\begin{aligned}
\mathbf{A}+\mathbf{B} & =\mathbf{B}+\mathbf{A}, \\
(\mathbf{A}+\mathbf{B})+\mathbf{C} & =\mathbf{A}+(\mathbf{B}+\mathbf{C}), \\
(\lambda+\mu) \mathbf{A} & =\lambda \mathbf{A}+\mu \mathbf{A}, \\
\lambda(\mathbf{A}+\mathbf{B}) & =\lambda \mathbf{A}+\lambda \mathbf{B}, \\
\lambda(\mu \mathbf{A}) & =(\lambda \mu) \mathbf{A} .
\end{aligned}
\end{equation}</p><p>A matrix whose elements are all zero is called a null matrix and denoted by $\mathbf{0}$. We have, of course,</p><p>$$
\mathbf{A}+(-1) \mathbf{A}=\mathbf{0}
$$</p><div id=matrix_multiplication class=theorem-box><p class=theorem-title><strong>Definition 0.8 (Matrix Multiplication)</strong></p><div class=theorem-content><p>If $\mathbf{A}$ is an $m \times n$ matrix and $\mathbf{B}$ an $n \times p$ matrix (so that $\mathbf{A}$ has the same number of columns as $\mathbf{B}$ has rows), then we define the <strong>product</strong> of $\mathbf{A}$ and $\mathbf{B}$ as</p><p>$$
\mathbf{A} \mathbf{B}=\left(\sum_{j=1}^{n} a_{i j} b_{j k}\right)
$$</p><p>Thus, $\mathbf{A} \mathbf{B}$ is an $m \times p$ matrix and its $ik$th element is $\sum_{j=1}^{n} a_{i j} b_{j k}$.</p></div></div><p>The following properties of the matrix product can be established:</p><p>\begin{equation}
\begin{aligned}
(\mathbf{A} \mathbf{B}) \mathbf{C} & =\mathbf{A}(\mathbf{B} \mathbf{C}) \\
\mathbf{A}(\mathbf{B}+\mathbf{C}) & =\mathbf{A} \mathbf{B}+\mathbf{A} \mathbf{C} \\
(\mathbf{A}+\mathbf{B}) \mathbf{C} & =\mathbf{A} \mathbf{C}+\mathbf{B} \mathbf{C}
\end{aligned}
\end{equation}</p><p>These relations hold provided the matrix products exist.</p><p>We note that the existence of $\mathbf{A} \mathbf{B}$ does not imply the existence of $\mathbf{B} \mathbf{A}$, and even when both products exist, they are not generally equal. (Two matrices $\mathbf{A}$ and $\mathbf{B}$ for which</p><p>$$
\mathbf{A} \mathbf{B}=\mathbf{B} \mathbf{A}
$$</p><p>are said to commute.) We therefore distinguish between premultiplication and postmultiplication: a given $m \times n$ matrix $\mathbf{A}$ can be premultiplied by a $p \times m$ matrix $\mathbf{B}$ to form the product $\mathbf{B} \mathbf{A}$; it can also be postmultiplied by an $n \times q$ matrix $\mathbf{C}$ to form $\mathbf{A} \mathbf{C}$.</p><h2 id=4---the-transpose-of-a-matrix>4 - The transpose of a matrix
<a class=anchor href=#4---the-transpose-of-a-matrix>#</a></h2><div id=transpose class=theorem-box><p class=theorem-title><strong>Definition 0.9 (Transpose)</strong></p><div class=theorem-content>The <strong>transpose</strong> of an $m \times n$ matrix $\mathbf{A}=\left(a_{i j}\right)$ is the $n \times m$ matrix, denoted by $\mathbf{A}^{\mathrm{T}}$, whose $ij$th element is $a_{j i}$.</div></div><p>We have</p><p>\begin{equation}
\begin{aligned}
\left(\mathbf{A}^{\mathrm{T}}\right)^{\mathrm{T}} & =\mathbf{A} \\
(\mathbf{A}+\mathbf{B})^{\mathrm{T}} & =\mathbf{A}^{\mathrm{T}}+\mathbf{B}^{\mathrm{T}} \\
(\mathbf{A} \mathbf{B})^{\mathrm{T}} & =\mathbf{B}^{\mathrm{T}} \mathbf{A}^{\mathrm{T}}
\end{aligned}
\end{equation}</p><p>If $\mathbf{x}$ is an $n \times 1$ vector, then $\mathbf{x}^{\mathrm{T}}$ is a $1 \times n$ row vector and</p><p>$$
\mathbf{x}^{\mathrm{T}} \mathbf{x}=\sum_{i=1}^{n} x_{i}^{2}
$$</p><div id=euclidean_norm class=theorem-box><p class=theorem-title><strong>Definition 0.10 (Euclidean Norm)</strong></p><div class=theorem-content><p>The <strong>(Euclidean) norm</strong> of $\mathbf{x}$ is defined as</p><p>$$
|\mathbf{x}|=\left(\mathbf{x}^{\mathrm{T}} \mathbf{x}\right)^{1 / 2}
$$</p></div></div><h2 id=5---square-matrices>5 - Square matrices
<a class=anchor href=#5---square-matrices>#</a></h2><div id=square_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.11 (Square Matrix)</strong></p><div class=theorem-content>A matrix is said to be <strong>square</strong> if it has as many rows as it has columns.</div></div><p>A square matrix $\mathbf{A}=\left(a_{i j}\right)$, real or complex, is said to be</p><table><thead><tr><th style=text-align:left>type</th><th style=text-align:left>if</th></tr></thead><tbody><tr><td style=text-align:left>lower triangular</td><td style=text-align:left>if $a_{i j}=0 \quad(i&lt;j)$,</td></tr><tr><td style=text-align:left>strictly lower triangular</td><td style=text-align:left>if $a_{i j}=0 \quad(i \leq j)$,</td></tr><tr><td style=text-align:left>unit lower triangular</td><td style=text-align:left>if $a_{i j}=0 \quad(i&lt;j)$ and $a_{i i}=1$ (all $i$),</td></tr><tr><td style=text-align:left>upper triangular</td><td style=text-align:left>if $a_{i j}=0 \quad(i>j)$,</td></tr><tr><td style=text-align:left>strictly upper triangular</td><td style=text-align:left>if $a_{i j}=0 \quad(i \geq j)$,</td></tr><tr><td style=text-align:left>unit upper triangular</td><td style=text-align:left>if $a_{i j}=0 \quad(i>j)$ and $a_{i i}=1$ (all $i$),</td></tr><tr><td style=text-align:left>idempotent</td><td style=text-align:left>if $\mathbf{A}^{2}=\mathbf{A}$.</td></tr></tbody></table><p>A square matrix $\mathbf{A}$ is triangular if it is either lower triangular or upper triangular (or both).</p><p>A real square matrix $\mathbf{A}=\left(a_{i j}\right)$ is said to be</p><table><thead><tr><th>type</th><th>if</th></tr></thead><tbody><tr><td>symmetric</td><td>if $\mathbf{A}^{\mathrm{T}} = \mathbf{A}$,</td></tr><tr><td>skew-symmetric</td><td>if $\mathbf{A}^{\mathrm{T}} = -\mathbf{A}$.</td></tr></tbody></table><p>For any square $n \times n$ matrix $\mathbf{A}=\left(a_{i j}\right)$, we define $\operatorname{dg} \mathbf{A}$ or $\operatorname{dg}(\mathbf{A})$ as</p><p>$$
\operatorname{dg} \mathbf{A}=\left(\begin{array}{cccc}
a_{11} & 0 & \ldots & 0 \\
0 & a_{22} & \ldots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \ldots & a_{n n}
\end{array}\right)
$$</p><p>or, alternatively,</p><p>$$
\operatorname{dg} \mathbf{A}=\operatorname{diag}\left(a_{11}, a_{22}, \ldots, a_{n n}\right)
$$</p><div id=diagonal_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.12 (Diagonal Matrix)</strong></p><div class=theorem-content>If $\mathbf{A}=\operatorname{dg} \mathbf{A}$, we say that $\mathbf{A}$ is <strong>diagonal</strong>.</div></div><div id=identity_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.13 (Identity Matrix)</strong></p><div class=theorem-content><p>A particular diagonal matrix is the <strong>identity matrix</strong> $\mathbf{I}_n$ (of order $n \times n$),</p><p>$$
\left (\begin{array}{cccc}
1 & 0 & \ldots & 0 \\
0 & 1 & \ldots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \ldots & 1
\end{array}\right )=\left(\delta_{i j}\right)
$$</p><p>where $\delta_{i j}=1$ if $i=j$ and $\delta_{i j}=0$ if $i \neq j$ ($\delta_{i j}$ is called the Kronecker delta).</p></div></div><p>We sometimes write $\mathbf{I}$ instead of $\mathbf{I}_{n}$ when the order is obvious or irrelevant. We have</p><p>$$
\mathbf{I} \mathbf{A}=\mathbf{A} \mathbf{I}=\mathbf{A},
$$</p><p>if $\mathbf{A}$ and $\mathbf{I}$ have the same order.</p><div id=orthogonal_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.14 (Orthogonal Matrix)</strong></p><div class=theorem-content><p>A real square matrix $\mathbf{A}$ is said to be <strong>orthogonal</strong> if</p><p>$$
\mathbf{A} \mathbf{A}^{\mathrm{T}}=\mathbf{A}^{\mathrm{T}} \mathbf{A}=\mathbf{I}
$$</p><p>and its columns are said to be <strong>orthonormal</strong>.</p></div></div><p>A rectangular (not square) matrix can still have the property that $\mathbf{A} \mathbf{A}^{\mathrm{T}}=\mathbf{I}$ or $\mathbf{A}^{\mathrm{T}} \mathbf{A}=\mathbf{I}$, but not both. Such a matrix is called semi-orthogonal.</p><p>Note carefully that the concepts of symmetry, skew-symmetry, and orthogonality are defined only for real square matrices. Hence, a complex matrix $\mathbf{Z}$ satisfying $\mathbf{Z}^{\mathrm{T}}=\mathbf{Z}$ is not called symmetric (in spite of what some textbooks do). This is important because complex matrices can be Hermitian, skew-Hermitian, or unitary, and there are many important results about these classes of matrices. These results should specialize to matrices that are symmetric, skew-symmetric, or orthogonal in the special case that the matrices are real. Thus, a symmetric matrix is just a real Hermitian matrix, a skew-symmetric matrix is a real skew-Hermitian matrix, and an orthogonal matrix is a real unitary matrix; see also Section 1.12.</p><h2 id=6----linear-forms-and-quadratic-forms>6 - Linear forms and quadratic forms
<a class=anchor href=#6----linear-forms-and-quadratic-forms>#</a></h2><p>Let $\mathbf{a}$ be an $n \times 1$ vector, $\mathbf{A}$ an $n \times n$ matrix, and $\mathbf{B}$ an $n \times m$ matrix. The expression $\mathbf{a}^{\mathrm{T}} \mathbf{x}$ is called a linear form in $\mathbf{x}$, the expression $\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}$ is a quadratic form in $\mathbf{x}$, and the expression $\mathbf{x}^{\mathrm{T}} \mathbf{B} \mathbf{y}$ a bilinear form in $\mathbf{x}$ and $\mathbf{y}$. In quadratic forms we may, without loss of generality, assume that $\mathbf{A}$ is symmetric, because if not then we can replace $\mathbf{A}$ by $\left(\mathbf{A}+\mathbf{A}^{\mathrm{T}}\right) / 2$, since</p><p>$$
\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}=\mathbf{x}^{\mathrm{T}}\left(\frac{\mathbf{A}+\mathbf{A}^{\mathrm{T}}}{2}\right) \mathbf{x} .
$$</p><p>Thus, let $\mathbf{A}$ be a symmetric matrix. We say that $\mathbf{A}$ is</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left></th></tr></thead><tbody><tr><td style=text-align:left>positive definite</td><td style=text-align:left>if $\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}>0$ for all $\mathbf{x} \neq \mathbf{0}$,</td></tr><tr><td style=text-align:left>positive semidefinite</td><td style=text-align:left>if $\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x} \geq 0$ for all $\mathbf{x}$,</td></tr><tr><td style=text-align:left>negative definite</td><td style=text-align:left>if $\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}&lt;0$ for all $\mathbf{x} \neq \mathbf{0}$,</td></tr><tr><td style=text-align:left>negative semidefinite</td><td style=text-align:left>if $\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x} \leq 0$ for all $\mathbf{x}$,</td></tr><tr><td style=text-align:left>indefinite</td><td style=text-align:left>if $\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}>0$ for some $\mathbf{x}$ and $\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}&lt;0$ for some $\mathbf{x}$.</td></tr></tbody></table><p>It is clear that the matrices $\mathbf{B} \mathbf{B}^{\mathrm{T}}$ and $\mathbf{B}^{\mathrm{T}} \mathbf{B}$ are positive semidefinite, and that $\mathbf{A}$ is negative (semi)definite if and only if $-\mathbf{A}$ is positive (semi)definite. A square null matrix is both positive and negative semidefinite.</p><div id=square_root_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.15 (Square Root Matrix)</strong></p><div class=theorem-content><p>If $\mathbf{A}$ is positive semidefinite, then there are many matrices $\mathbf{B}$ satisfying</p><p>$$
\mathbf{B}^{2}=\mathbf{A} .
$$</p><p>But there is only one positive semidefinite matrix $\mathbf{B}$ satisfying $\mathbf{B}^{2}=\mathbf{A}$. This matrix is called the <strong>square root</strong> of $\mathbf{A}$, denoted by $\mathbf{A}^{1 / 2}$.</p></div></div><p>The following two theorems are often useful.</p><div id=matrix_equality_conditions class=theorem-box><p class=theorem-title><strong>Theorem 0.1 (Matrix Equality Conditions)</strong></p><div class=theorem-content>Let $\mathbf{A}$ be an $m \times n$ matrix, $\mathbf{B}$ and $\mathbf{C}$ $n \times p$ matrices, and let $\mathbf{x}$ be an $n \times 1$ vector. Then,
(a) $\mathbf{A} \mathbf{x}=\mathbf{0} \Longleftrightarrow \mathbf{A}^{\mathrm{T}} \mathbf{A} \mathbf{x}=\mathbf{0}$,
(b) $\mathbf{A} \mathbf{B}=\mathbf{0} \Longleftrightarrow \mathbf{A}^{\mathrm{T}} \mathbf{A} \mathbf{B}=\mathbf{0}$,
(c) $\mathbf{A}^{\mathrm{T}} \mathbf{A} \mathbf{B}=\mathbf{A}^{\mathrm{T}} \mathbf{A} \mathbf{C} \Longleftrightarrow \mathbf{A} \mathbf{B}=\mathbf{A} \mathbf{C}$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>(a) Clearly $\mathbf{A} \mathbf{x}=\mathbf{0}$ implies $\mathbf{A}^{\mathrm{T}} \mathbf{A} \mathbf{x}=\mathbf{0}$. Conversely, if $\mathbf{A}^{\mathrm{T}} \mathbf{A} \mathbf{x}=\mathbf{0}$, then $(\mathbf{A} \mathbf{x})^{\mathrm{T}}(\mathbf{A} \mathbf{x})=\mathbf{x}^{\mathrm{T}} \mathbf{A}^{\mathrm{T}} \mathbf{A} \mathbf{x}=0$ and hence $\mathbf{A} \mathbf{x}=\mathbf{0}$. (b) follows from (a), and (c) follows from (b) by substituting $\mathbf{B}-\mathbf{C}$ for $\mathbf{B}$ in (b).</div><div class=qed>■</div></div><div id=zero_matrix_conditions class=theorem-box><p class=theorem-title><strong>Theorem 0.2 (Zero Matrix Conditions)</strong></p><div class=theorem-content>Let $\mathbf{A}$ be an $m \times n$ matrix, $\mathbf{B}$ and $\mathbf{C}$ $n \times n$ matrices, $\mathbf{B}$ symmetric. Then,
(a) $\mathbf{A} \mathbf{x}=\mathbf{0}$ for all $n \times 1$ vectors $\mathbf{x}$ if and only if $\mathbf{A}=\mathbf{0}$,
(b) $\mathbf{x}^{\mathrm{T}} \mathbf{B} \mathbf{x}=0$ for all $n \times 1$ vectors $\mathbf{x}$ if and only if $\mathbf{B}=\mathbf{0}$,
(c) $\mathbf{x}^{\mathrm{T}} \mathbf{C} \mathbf{x}=0$ for all $n \times 1$ vectors $\mathbf{x}$ if and only if $\mathbf{C}^{\mathrm{T}}=-\mathbf{C}$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>The proof is easy and is left to the reader.</div><div class=qed>■</div></div><h2 id=7---the-rank-of-a-matrix>7 - The rank of a matrix
<a class=anchor href=#7---the-rank-of-a-matrix>#</a></h2><div id=linear_independence class=theorem-box><p class=theorem-title><strong>Definition 0.16 (Linear Independence)</strong></p><div class=theorem-content>A set of vectors $\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}$
is said to be <strong>linearly independent</strong> if $\sum_{i} \alpha_{i} \mathbf{x}_{i}=\mathbf{0}$ implies that all $\alpha_{i}=0$. If $\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}$ are not linearly independent, they are said to be <strong>linearly dependent</strong>.</div></div><div id=matrix_rank class=theorem-box><p class=theorem-title><strong>Definition 0.17 (Matrix Rank)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be an $m \times n$ matrix. The <strong>column rank</strong> of $\mathbf{A}$ is the maximum number of linearly independent columns it contains. The <strong>row rank</strong> of $\mathbf{A}$ is the maximum number of linearly independent rows it contains. It may be shown that the column rank of $\mathbf{A}$ is equal to its row rank. Hence, the concept of <strong>rank</strong> is unambiguous. We denote the rank of $\mathbf{A}$ by</p><p>$$
r(\mathbf{A}) .
$$</p></div></div><p>It is clear that</p><p>$$
r(\mathbf{A}) \leq \min (m, n)
$$</p><p>If $r(\mathbf{A})=m$, we say that $\mathbf{A}$ has full row rank. If $r(\mathbf{A})=n$, we say that $\mathbf{A}$ has full column rank. If $r(\mathbf{A})=0$, then $\mathbf{A}$ is the null matrix, and conversely, if $\mathbf{A}$ is the null matrix, then $r(\mathbf{A})=0$.</p><p>We have the following important results concerning ranks:</p><p>\begin{equation}
\begin{gathered}
r(\mathbf{A})=r\left(\mathbf{A}^{\mathrm{T}}\right)=r\left(\mathbf{A}^{\mathrm{T}} \mathbf{A}\right)=r\left(\mathbf{A} \mathbf{A}^{\mathrm{T}}\right) \\
r(\mathbf{A} \mathbf{B}) \leq \min (r(\mathbf{A}), r(\mathbf{B})) \\
r(\mathbf{A} \mathbf{B})=r(\mathbf{A}) \quad \text { if } \mathbf{B} \text { is square and of full rank, } \\
r(\mathbf{A}+\mathbf{B}) \leq r(\mathbf{A})+r(\mathbf{B})
\end{gathered}
\end{equation}</p><p>and finally, if $\mathbf{A}$ is an $m \times n$ matrix and $\mathbf{A} \mathbf{x}=\mathbf{0}$ for some $\mathbf{x} \neq \mathbf{0}$, then</p><p>$$
r(\mathbf{A}) \leq n-1
$$</p><div id=column_space class=theorem-box><p class=theorem-title><strong>Definition 0.18 (Column Space)</strong></p><div class=theorem-content><p>The <strong>column space</strong> of $\mathbf{A}(m \times n)$, denoted by $\mathcal{M}(\mathbf{A})$, is the set of vectors</p><p>$$
\mathcal{M}(\mathbf{A})=\left\{\mathbf{y}: \mathbf{y}=\mathbf{A} \mathbf{x} \text { for some } \mathbf{x} \text { in } \mathbb{R}^{n}\right\}
$$</p><p>Thus, $\mathcal{M}(\mathbf{A})$ is the vector space generated by the columns of $\mathbf{A}$.</p></div></div><p>The dimension of this vector space is $r(\mathbf{A})$. We have</p><p>$
\mathcal{M}(\mathbf{A})=\mathcal{M}\left(\mathbf{A} \mathbf{A}^{\mathrm{T}}\right)
$</p><p>for any matrix $\mathbf{A}$.</p><p><strong>Exercises:</strong></p><ol><li>If $\mathbf{A}$ has full column rank and $\mathbf{C}$ has full row rank, then $r(\mathbf{A} \mathbf{B} \mathbf{C})=r(\mathbf{B})$.</li><li>Let $\mathbf{A}$ be partitioned as $\mathbf{A}=\left(\mathbf{A}_{1}: \mathbf{A}_{2}\right)$. Then $r(\mathbf{A})=r\left(\mathbf{A}_{1}\right)$ if and only if $\mathcal{M}\left(\mathbf{A}_{2}\right) \subset \mathcal{M}\left(\mathbf{A}_{1}\right)$.</li></ol><h2 id=8---the-inverse>8 - The Inverse
<a class=anchor href=#8---the-inverse>#</a></h2><div id=nonsingular_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.19 (Nonsingular Matrix)</strong></p><div class=theorem-content>Let $\mathbf{A}$ be a square matrix of order $n \times n$. We say that $\mathbf{A}$ is <strong>nonsingular</strong> if $r(\mathbf{A})=n$, and that $\mathbf{A}$ is <strong>singular</strong> if $r(\mathbf{A})&lt;n$.</div></div><div id=matrix_inverse class=theorem-box><p class=theorem-title><strong>Definition 0.20 (Matrix Inverse)</strong></p><div class=theorem-content><p>If $\mathbf{A}$ is nonsingular, then there exists a nonsingular matrix $\mathbf{B}$ such that</p><p>$
\mathbf{A} \mathbf{B}=\mathbf{B} \mathbf{A}=\mathbf{I}_{n} .
$</p><p>The matrix $\mathbf{B}$, denoted by $\mathbf{A}^{-1}$, is unique and is called the <strong>inverse</strong> of $\mathbf{A}$.</p></div></div><p>We have</p><p>\begin{equation}
\begin{aligned}
\left(\mathbf{A}^{-1}\right)^{\mathrm{T}} & =\left(\mathbf{A}^{\mathrm{T}}\right)^{-1}, \\
(\mathbf{A} \mathbf{B})^{-1} & =\mathbf{B}^{-1} \mathbf{A}^{-1},
\end{aligned}
\end{equation}</p><p>if the inverses exist.</p><div id=permutation_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.21 (Permutation Matrix)</strong></p><div class=theorem-content>A square matrix $\mathbf{P}$ is said to be a <strong>permutation matrix</strong> if each row and each column of $\mathbf{P}$ contain a single element one, and the remaining elements are zero. An $n \times n$ permutation matrix thus contains $n$ ones and $n(n-1)$ zeros.</div></div><p>It can be proved that any permutation matrix is nonsingular. In fact, it is even true that $\mathbf{P}$ is orthogonal, that is,</p><p>$
\mathbf{P}^{-1}=\mathbf{P}^{\mathrm{T}}
$</p><p>for any permutation matrix $\mathbf{P}$.</p><h2 id=9---the-determinant>9 - The Determinant
<a class=anchor href=#9---the-determinant>#</a></h2><div id=determinant class=theorem-box><p class=theorem-title><strong>Definition 0.22 (Determinant)</strong></p><div class=theorem-content><p>Associated with any $n \times n$ matrix $\mathbf{A}$ is the <strong>determinant</strong> $|\mathbf{A}|$ defined by</p><p>$
|\mathbf{A}|=\sum(-1)^{\phi\left(j_{1}, \ldots, j_{n}\right)} \prod_{i=1}^{n} a_{i j_{i}}
$</p><p>where the summation is taken over all permutations $\left(j_{1}, \ldots, j_{n}\right)$ of the set of integers $(1, \ldots, n)$, and $\phi\left(j_{1}, \ldots, j_{n}\right)$ is the number of transpositions required to change $(1, \ldots, n)$ into $\left(j_{1}, \ldots, j_{n}\right)$.</p></div></div><p>We have</p><p>\begin{equation}
\begin{aligned}
|\mathbf{A} \mathbf{B}| & =|\mathbf{A}||\mathbf{B}| \\
\left|\mathbf{A}^{\mathrm{T}}\right| & =|\mathbf{A}| \\
|\alpha \mathbf{A}| & =\alpha^{n}|\mathbf{A}| \quad \text { for any scalar } \alpha \\
\left|\mathbf{A}^{-1}\right| & =|\mathbf{A}|^{-1} \quad \text { if } \mathbf{A} \text { is nonsingular, } \\
\left|\mathbf{I}_{n}\right| & =1
\end{aligned}
\end{equation}</p><div id=minor_cofactor class=theorem-box><p class=theorem-title><strong>Definition 0.23 (Minor and Cofactor)</strong></p><div class=theorem-content>A <strong>submatrix</strong> of $\mathbf{A}$ is the rectangular array obtained from $\mathbf{A}$ by deleting some of its rows and/or some of its columns. A <strong>minor</strong> is the determinant of a square submatrix of $\mathbf{A}$. The <strong>minor</strong> of an element $a_{i j}$ is the determinant of the submatrix of $\mathbf{A}$ obtained by deleting the $i$th row and $j$th column. The <strong>cofactor</strong> of $a_{i j}$, say $c_{i j}$, is $(-1)^{i+j}$ times the minor of $a_{i j}$.</div></div><p>The matrix $\mathbf{C}=\left(c_{i j}\right)$ is called the cofactor matrix of $\mathbf{A}$. The transpose of $\mathbf{C}$ is called the adjoint of $\mathbf{A}$ and will be denoted by $\mathbf{A}^{\#}$.</p><p>We have</p><p>\begin{equation}
\begin{aligned}
|\mathbf{A}|=\sum_{j=1}^{n} a_{i j} c_{i j} & =\sum_{j=1}^{n} a_{j k} c_{j k} \quad(i, k=1, \ldots, n), \\
\mathbf{A} \mathbf{A}^{\#} & =\mathbf{A}^{\#} \mathbf{A}=|\mathbf{A}| \mathbf{I}, \\
(\mathbf{A} \mathbf{B})^{\#} & =\mathbf{B}^{\#} \mathbf{A}^{\#} .
\end{aligned}
\end{equation}</p><div id=principal_minor class=theorem-box><p class=theorem-title><strong>Definition 0.24 (Principal Minor)</strong></p><div class=theorem-content>For any square matrix $\mathbf{A}$, a <strong>principal submatrix</strong> of $\mathbf{A}$ is obtained by deleting corresponding rows and columns. The determinant of a principal submatrix is called a <strong>principal minor</strong>.</div></div><p><strong>Exercises:</strong></p><ol><li>If $\mathbf{A}$ is nonsingular, show that $\mathbf{A}^{\#}=|\mathbf{A}| \mathbf{A}^{-1}$.</li><li>Prove that the determinant of a triangular matrix is the product of its diagonal elements.</li></ol><h2 id=10---the-trace>10 - The trace
<a class=anchor href=#10---the-trace>#</a></h2><div id=trace class=theorem-box><p class=theorem-title><strong>Definition 0.25 (Trace)</strong></p><div class=theorem-content><p>The <strong>trace</strong> of a square $n \times n$ matrix $\mathbf{A}$, denoted by $\operatorname{tr} \mathbf{A}$ or $\operatorname{tr}(\mathbf{A})$, is the sum of its diagonal elements:</p><p>$
\operatorname{tr} \mathbf{A}=\sum_{i=1}^{n} a_{i i} .
$</p></div></div><p>We have</p><p>\begin{equation}
\begin{aligned}
\operatorname{tr}(\mathbf{A}+\mathbf{B}) & =\operatorname{tr} \mathbf{A}+\operatorname{tr} \mathbf{B} \\
\operatorname{tr}(\lambda \mathbf{A}) & =\lambda \operatorname{tr} \mathbf{A} \quad \text { if } \lambda \text { is a scalar } \\
\operatorname{tr} \mathbf{A}^{\mathrm{T}} & =\operatorname{tr} \mathbf{A} \\
\operatorname{tr} \mathbf{A} \mathbf{B} & =\operatorname{tr} \mathbf{B} \mathbf{A}
\end{aligned}
\end{equation}</p><p>We note in (25) that $\mathbf{A} \mathbf{B}$ and $\mathbf{B} \mathbf{A}$, though both square, need not be of the same order.</p><p>Corresponding to the vector (Euclidean) norm</p><p>$
|\mathbf{x}|=\left(\mathbf{x}^{\mathrm{T}} \mathbf{x}\right)^{1 / 2},
$</p><p>given in (4), we now define the matrix (Euclidean) norm as</p><div id=matrix_norm class=theorem-box><p class=theorem-title><strong>Definition 0.26 (Matrix Norm)</strong></p><div class=theorem-content>$
|\mathbf{A}|=\left(\operatorname{tr} \mathbf{A}^{\mathrm{T}} \mathbf{A}\right)^{1 / 2}
$</div></div><p>We have</p><p>$
\operatorname{tr} \mathbf{A}^{\mathrm{T}} \mathbf{A} \geq 0
$</p><p>with equality if and only if $\mathbf{A}=\mathbf{0}$.</p><h2 id=11----partitioned-matrices>11 - Partitioned matrices
<a class=anchor href=#11----partitioned-matrices>#</a></h2><div id=partitioned_matrix class=theorem-box><p class=theorem-title><strong>Definition 0.27 (Partitioned Matrix)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be an $m \times n$ matrix. We can <strong>partition</strong> $\mathbf{A}$ as</p><p>$$
\mathbf{A}=\left(\begin{array}{ll}
\mathbf{A}_{11} & \mathbf{A}_{12} \\
\mathbf{A}_{21} & \mathbf{A}_{22}
\end{array}\right),
$$</p><p>where $\mathbf{A}_{11}$ is $m_1 \times n_1$, $\mathbf{A}_{12}$ is $m_1 \times n_2$, $\mathbf{A}_{21}$ is $m_2 \times n_1$, $\mathbf{A}_{22}$ is $m_2 \times n_2$, and $m_1+m_2=m$ and $n_1+n_2=n$.</p></div></div><p>Let $\mathbf{B}(m \times n)$ be similarly partitioned into submatrices $\mathbf{B}_{ij}(i, j=1,2)$. Then,</p><p>$$
\mathbf{A}+\mathbf{B}=\left(\begin{array}{cc}
\mathbf{A}_{11}+\mathbf{B}_{11} & \mathbf{A}_{12}+\mathbf{B}_{12} \\
\mathbf{A}_{21}+\mathbf{B}_{21} & \mathbf{A}_{22}+\mathbf{B}_{22}
\end{array}\right)
$$</p><p>Now let $\mathbf{C}(n \times p)$ be partitioned into submatrices $\mathbf{C}_{ij}(i, j=1,2)$ such that $\mathbf{C}_{11}$ has $n_1$ rows (and hence $\mathbf{C}_{12}$ also has $n_1$ rows and $\mathbf{C}_{21}$ and $\mathbf{C}_{22}$ have $n_2$ rows). Then we may postmultiply $\mathbf{A}$ by $\mathbf{C}$ yielding</p><p>$$
\mathbf{A} \mathbf{C}=\left(\begin{array}{cc}
\mathbf{A}_{11} \mathbf{C}_{11}+\mathbf{A}_{12} \mathbf{C}_{21} & \mathbf{A}_{11} \mathbf{C}_{12}+\mathbf{A}_{12} \mathbf{C}_{22} \\
\mathbf{A}_{21} \mathbf{C}_{11}+\mathbf{A}_{22} \mathbf{C}_{21} & \mathbf{A}_{21} \mathbf{C}_{12}+\mathbf{A}_{22} \mathbf{C}_{22}
\end{array}\right)
$$</p><p>The transpose of the matrix $\mathbf{A}$ given in (28) is</p><p>$$
\mathbf{A}^{\mathrm{T}}=\left(\begin{array}{cc}
\mathbf{A}_{11}^{\mathrm{T}} & \mathbf{A}_{21}^{\mathrm{T}} \\
\mathbf{A}_{12}^{\mathrm{T}} & \mathbf{A}_{22}^{\mathrm{T}}
\end{array}\right)
$$</p><p>If the off-diagonal blocks $\mathbf{A}_{12}$ and $\mathbf{A}_{21}$ are both zero, and $\mathbf{A}_{11}$ and $\mathbf{A}_{22}$ are square and nonsingular, then $\mathbf{A}$ is also nonsingular and its inverse is</p><p>$$
\mathbf{A}^{-1}=\left(\begin{array}{cc}
\mathbf{A}_{11}^{-1} & \mathbf{0} \\
\mathbf{0} & \mathbf{A}_{22}^{-1}
\end{array}\right)
$$</p><p>More generally, if $\mathbf{A}$ as given in (28) is nonsingular and $\mathbf{D}=\mathbf{A}_{22}-\mathbf{A}_{21} \mathbf{A}_{11}^{-1} \mathbf{A}_{12}$ is also nonsingular, then</p><p>$$
\mathbf{A}^{-1}=\left(\begin{array}{cc}
\mathbf{A}_{11}^{-1}+\mathbf{A}_{11}^{-1} \mathbf{A}_{12} \mathbf{D}^{-1} \mathbf{A}_{21} \mathbf{A}_{11}^{-1} & -\mathbf{A}_{11}^{-1} \mathbf{A}_{12} \mathbf{D}^{-1} \\
-\mathbf{D}^{-1} \mathbf{A}_{21} \mathbf{A}_{11}^{-1} & \mathbf{D}^{-1}
\end{array}\right)
$$</p><p>Alternatively, if $\mathbf{A}$ is nonsingular and $\mathbf{E}=\mathbf{A}_{11}-\mathbf{A}_{12} \mathbf{A}_{22}^{-1} \mathbf{A}_{21}$ is also nonsingular, then</p><p>$$
\mathbf{A}^{-1}=\left(\begin{array}{cc}
\mathbf{E}^{-1} & -\mathbf{E}^{-1} \mathbf{A}_{12} \mathbf{A}_{22}^{-1} \\
-\mathbf{A}_{22}^{-1} \mathbf{A}_{21} \mathbf{E}^{-1} & \mathbf{A}_{22}^{-1}+\mathbf{A}_{22}^{-1} \mathbf{A}_{21} \mathbf{E}^{-1} \mathbf{A}_{12} \mathbf{A}_{22}^{-1}
\end{array}\right) .
$$</p><p>Of course, if both $\mathbf{D}$ and $\mathbf{E}$ are nonsingular, blocks in (29) and (30) can be interchanged. The results (29) and (30) can be easily extended to a $3 \times 3$ matrix partition. We only consider the following symmetric case where two of the off-diagonal blocks are null matrices.</p><div id=symmetric_3x3_inverse class=theorem-box><p class=theorem-title><strong>Theorem 0.3 (3x3 Symmetric Partitioned Matrix Inverse)</strong></p><div class=theorem-content><p>If the matrix</p><p>$$
\left(\begin{array}{lll}
\mathbf{A} & \mathbf{B} & \mathbf{C} \\
\mathbf{B}^{\mathrm{T}} & \mathbf{D} & \mathbf{0} \\
\mathbf{C}^{\mathrm{T}} & \mathbf{0} & \mathbf{E}
\end{array}\right)
$$</p><p>is symmetric and nonsingular, its inverse is given by</p><p>$$
\left(\begin{array}{ccc}
\mathbf{Q}^{-1} & -\mathbf{Q}^{-1} \mathbf{B} \mathbf{D}^{-1} & -\mathbf{Q}^{-1} \mathbf{C} \mathbf{E}^{-1} \\
-\mathbf{D}^{-1} \mathbf{B}^{\mathrm{T}} \mathbf{Q}^{-1} & \mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{B}^{\mathrm{T}} \mathbf{Q}^{-1} \mathbf{B} \mathbf{D}^{-1} & \mathbf{D}^{-1} \mathbf{B}^{\mathrm{T}} \mathbf{Q}^{-1} \mathbf{C} \mathbf{E}^{-1} \\
-\mathbf{E}^{-1} \mathbf{C}^{\mathrm{T}} \mathbf{Q}^{-1} & \mathbf{E}^{-1} \mathbf{C}^{\mathrm{T}} \mathbf{Q}^{-1} \mathbf{B} \mathbf{D}^{-1} & \mathbf{E}^{-1}+\mathbf{E}^{-1} \mathbf{C}^{\mathrm{T}} \mathbf{Q}^{-1} \mathbf{C} \mathbf{E}^{-1}
\end{array}\right)
$$</p><p>where</p><p>$$
\mathbf{Q}=\mathbf{A}-\mathbf{B} \mathbf{D}^{-1} \mathbf{B}^{\mathrm{T}}-\mathbf{C} \mathbf{E}^{-1} \mathbf{C}^{\mathrm{T}} .
$$</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>The proof is left to the reader.</div><div class=qed>■</div></div><p>As to the determinants of partitioned matrices, we note that</p><p>$$
\left|\begin{array}{ll}
\mathbf{A}_{11} & \mathbf{A}_{12} \\
\mathbf{0} & \mathbf{A}_{22}
\end{array}\right|=\left|\mathbf{A}_{11}\right|\left|\mathbf{A}_{22}\right|=\left|\begin{array}{ll}
\mathbf{A}_{11} & \mathbf{0} \\
\mathbf{A}_{21} & \mathbf{A}_{22}
\end{array}\right|
$$</p><p>if both $\mathbf{A}_{11}$ and $\mathbf{A}_{22}$ are square matrices.</p><p><strong>Exercises:</strong></p><ol><li>Find the determinant and inverse (if it exists) of</li></ol><p>$$
\mathbf{B}=\left(\begin{array}{cc}
\mathbf{A} & \mathbf{0} \\
\mathbf{a}^{\mathrm{T}} & 1
\end{array}\right) .
$$</p><ol start=2><li>If $|\mathbf{A}| \neq 0$, prove that</li></ol><p>$$
\left|\begin{array}{cc}
\mathbf{A} & \mathbf{b} \\
\mathbf{a}^{\mathrm{T}} & \alpha
\end{array}\right|=\left(\alpha-\mathbf{a}^{\mathrm{T}} \mathbf{A}^{-1} \mathbf{b}\right)|\mathbf{A}| .
$$</p><ol start=3><li>If $\alpha \neq 0$, prove that</li></ol><p>$$
\left|\begin{array}{cc}
\mathbf{A} & \mathbf{b} \\
\mathbf{a}^{\mathrm{T}} & \alpha
\end{array}\right|=\alpha\left|\mathbf{A}-(1 / \alpha) \mathbf{b} \mathbf{a}^{\mathrm{T}}\right| .
$$</p><h2 id=12---complex-matrices>12 - Complex Matrices
<a class=anchor href=#12---complex-matrices>#</a></h2><p>If $\mathbf{A}$ and $\mathbf{B}$ are real matrices of the same order, then a complex matrix $\mathbf{Z}$ can be defined as</p><p>$$
\mathbf{Z}=\mathbf{A}+i \mathbf{B}
$$</p><p>where $i$ denotes the imaginary unit with the property $i^2=-1$. The complex conjugate of $\mathbf{Z}$, denoted by $\mathbf{Z}^mathrm{H}$, is defined as</p><p>$$
\mathbf{Z}^\mathrm{H}=\mathbf{A}^{\mathrm{T}}-i \mathbf{B}^{\mathrm{T}}
$$</p><p>If $\mathbf{Z}$ is real, then $\mathbf{Z}^\mathrm{H}=\mathbf{Z}^{\mathrm{T}}$. If $\mathbf{Z}$ is a scalar, say $\zeta$, we usually write $\bar{\zeta}$ instead of $\zeta^mathrm{H}$.</p><p>A square complex matrix $\mathbf{Z}$ is said to be Hermitian if $\mathbf{Z}^{\mathrm{H}}=\mathbf{Z}$ (the complex equivalent to a symmetric matrix), skew-Hermitian if $\mathbf{Z}^{\mathrm{H}}=-\mathbf{Z}$ (the complex equivalent to a skew-symmetric matrix), and unitary if $\mathbf{Z}^{\mathrm{H}} \mathbf{Z}=\mathbf{I}$ (the complex equivalent to an orthogonal matrix).</p><p>We shall see in <a href=#symmetric_eigenvalues>this theorem</a> that the eigenvalues of a symmetric matrix are real. In general, however, eigenvalues (and hence eigenvectors) are complex. In this book, complex numbers appear only in connection with eigenvalues and eigenvectors of matrices that are not symmetric (Chapter 8). A detailed treatment is therefore omitted. Matrices and vectors are assumed to be real, unless it is explicitly specified that they are complex.</p><h2 id=13----eigenvalues-and-eigenvectors>13 - Eigenvalues and Eigenvectors
<a class=anchor href=#13----eigenvalues-and-eigenvectors>#</a></h2><div id=eigenvalues_eigenvectors class=theorem-box><p class=theorem-title><strong>Definition 0.28 (Eigenvalues and Eigenvectors)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be a square matrix, say $n \times n$. The <strong>eigenvalues</strong> of $\mathbf{A}$ are defined as the roots of the characteristic equation</p><p>$$
\left|\lambda \mathbf{I}_n-\mathbf{A}\right|=0
$$</p><p>The characteristic equation (31) has $n$ roots, in general complex. Let $\lambda$ be an eigenvalue of $\mathbf{A}$. Then there exist vectors $\mathbf{x}$ and $\mathbf{y}(\mathbf{x} \neq \mathbf{0}, \mathbf{y} \neq \mathbf{0})$ such that</p><p>$$
(\lambda \mathbf{I}-\mathbf{A}) \mathbf{x}=\mathbf{0}, \quad \mathbf{y}^{\mathrm{T}}(\lambda \mathbf{I}-\mathbf{A})=\mathbf{0}
$$</p><p>That is,</p><p>$$
\mathbf{A} \mathbf{x}=\lambda \mathbf{x}, \quad \mathbf{y}^{\mathrm{T}} \mathbf{A}=\lambda \mathbf{y}^{\mathrm{T}}
$$</p><p>The vectors $\mathbf{x}$ and $\mathbf{y}$ are called a <strong>(column) eigenvector</strong> and <strong>row eigenvector</strong> of $\mathbf{A}$ associated with the eigenvalue $\lambda$.</p></div></div><p>Eigenvectors are usually normalized in some way to make them unique, for example, by $\mathbf{x}^{\mathrm{T}} \mathbf{x}=\mathbf{y}^{\mathrm{T}} \mathbf{y}=1$ (when $\mathbf{x}$ and $\mathbf{y}$ are real).</p><p>Not all roots of the characteristic equation need to be different. Each root is counted a number of times equal to its multiplicity. When a root (eigenvalue) appears more than once it is called a multiple eigenvalue; if it appears only once it is called a simple eigenvalue.</p><p>Although eigenvalues are in general complex, the eigenvalues of a symmetric matrix are always real.</p><div id=symmetric_eigenvalues class=theorem-box><p class=theorem-title><strong>Theorem 0.4 (Symmetric Matrix Eigenvalues)</strong></p><div class=theorem-content>A symmetric matrix has only real eigenvalues.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Let $\lambda$ be an eigenvalue of a symmetric matrix $\mathbf{A}$ and let $\mathbf{x}=\mathbf{u}+i \mathbf{v}$ be an associated eigenvector. Then,</p><p>$$
\mathbf{A}(\mathbf{u}+i \mathbf{v})=\lambda(\mathbf{u}+i \mathbf{v})
$$</p><p>and hence</p><p>$$
(\mathbf{u}-i \mathbf{v})^{\mathrm{T}} \mathbf{A}(\mathbf{u}+i \mathbf{v})=\lambda(\mathbf{u}-i \mathbf{v})^{\mathrm{T}}(\mathbf{u}+i \mathbf{v})
$$</p><p>which leads to</p><p>$$
\mathbf{u}^{\mathrm{T}} \mathbf{A} \mathbf{u}+\mathbf{v}^{\mathrm{T}} \mathbf{A} \mathbf{v}=\lambda\left(\mathbf{u}^{\mathrm{T}} \mathbf{u}+\mathbf{v}^{\mathrm{T}} \mathbf{v}\right)
$$</p><p>because of the symmetry of $\mathbf{A}$. This implies that $\lambda$ is real.</p></div><div class=qed>■</div></div><p>Let us prove the following three results, which will be useful to us later.</p><div id=similar_matrices_eigenvalues class=theorem-box><p class=theorem-title><strong>Theorem 0.5 (Similar Matrices Eigenvalues)</strong></p><div class=theorem-content>If $\mathbf{A}$ is an $n \times n$ matrix and $\mathbf{G}$ is a nonsingular $n \times n$ matrix, then $\mathbf{A}$ and $\mathbf{G}^{-1} \mathbf{A} \mathbf{G}$ have the same set of eigenvalues (with the same multiplicities).</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>From</p><p>$$
\lambda \mathbf{I}_n-\mathbf{G}^{-1} \mathbf{A} \mathbf{G}=\mathbf{G}^{-1}\left(\lambda \mathbf{I}_n-\mathbf{A}\right) \mathbf{G}
$$</p><p>we obtain</p><p>$$
\left|\lambda \mathbf{I}_n-\mathbf{G}^{-1} \mathbf{A} \mathbf{G}\right|=\left|\mathbf{G}^{-1}\right|\left|\lambda \mathbf{I}_n-\mathbf{A}\right||\mathbf{G}|=\left|\lambda \mathbf{I}_n-\mathbf{A}\right|
$$</p><p>and the result follows.</p></div><div class=qed>■</div></div><div id=singular_zero_eigenvalue class=theorem-box><p class=theorem-title><strong>Theorem 0.6 (Singular Matrix Zero Eigenvalue)</strong></p><div class=theorem-content>A singular matrix has at least one zero eigenvalue.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>If $\mathbf{A}$ is singular, then $|\mathbf{A}|=0$ and hence $|\lambda \mathbf{I}-\mathbf{A}|=0$ for $\lambda=0$.</div><div class=qed>■</div></div><div id=special_matrix_eigenvalues class=theorem-box><p class=theorem-title><strong>Theorem 0.7 (Special Matrix Eigenvalues)</strong></p><div class=theorem-content>An idempotent matrix has only eigenvalues 0 or 1. All eigenvalues of a unitary matrix have unit modulus.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Let $\mathbf{A}$ be idempotent. Then $\mathbf{A}^2=\mathbf{A}$. Thus, if $\mathbf{A} \mathbf{x}=\lambda \mathbf{x}$, then</p><p>$$
\lambda \mathbf{x}=\mathbf{A} \mathbf{x}=\mathbf{A}(\mathbf{A} \mathbf{x})=\mathbf{A}(\lambda \mathbf{x})=\lambda(\mathbf{A} \mathbf{x})=\lambda^2 \mathbf{x}
$$</p><p>and hence $\lambda=\lambda^2$, which implies $\lambda=0$ or $\lambda=1$.</p><p>If $\mathbf{A}$ is unitary, then $\mathbf{A}^{\mathrm{H}} \mathbf{A}=\mathbf{I}$. Thus, if $\mathbf{A} \mathbf{x}=\lambda \mathbf{x}$, then</p><p>$$
\mathbf{x}^{\mathrm{H}} \mathbf{A}^{\mathrm{H}}=\bar{\lambda} \mathbf{x}^{\mathrm{H}}
$$</p><p>using the notation of Section 1.12. Hence,</p><p>$$
\mathbf{x}^{\mathrm{H}} \mathbf{x}=\mathbf{x}^{\mathrm{H}} \mathbf{A}^{\mathrm{H}} \mathbf{A} \mathbf{x}=\bar{\lambda} \lambda \mathbf{x}^{\mathrm{H}} \mathbf{x}
$$</p><p>Since $\mathbf{x}^{\mathrm{H}} \mathbf{x} \neq 0$, we obtain $\bar{\lambda} \lambda=1$ and hence $|\lambda|=1$.</p></div><div class=qed>■</div></div><p>An important theorem regarding positive definite matrices is stated below.</p><div id=positive_definite_eigenvalues class=theorem-box><p class=theorem-title><strong>Theorem 0.8 (Positive Definite Eigenvalues)</strong></p><div class=theorem-content>A symmetric matrix is positive definite if and only if all its eigenvalues are positive.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>If $\mathbf{A}$ is positive definite and $\mathbf{A}\mathbf{x}=\lambda \mathbf{x}$, then $\mathbf{x}^\mathrm{T} \mathbf{A} \mathbf{x}=\lambda \mathbf{x}^\mathrm{T} \mathbf{x}$. Now, $\mathbf{x}^\mathrm{T} \mathbf{A} \mathbf{x}>0$ and $\mathbf{x}^\mathrm{T} \mathbf{x}>0$ imply $\lambda>0$. The converse will not be proved here. (It follows from <a href=#schur_decomposition>this theorem</a>.)</div><div class=qed>■</div></div><p>Next, let us prove <a href=#eigenvalue_identity>this theorem</a>.</p><div id=eigenvalue_identity class=theorem-box><p class=theorem-title><strong>Theorem 0.9 (Eigenvalue Identity)</strong></p><div class=theorem-content>Let $\mathbf{A}$ be $m \times n$ and let $\mathbf{B}$ be $n \times m(n \geq m)$. Then the nonzero eigenvalues of $\mathbf{B}\mathbf{A}$ and $\mathbf{A}\mathbf{B}$ are identical, and $\left|I_{m}-\mathbf{A}\mathbf{B}\right|=\left|I_{n}-\mathbf{B}\mathbf{A}\right|$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Taking determinants on both sides of the equality</p><p>\begin{equation}
\left(\begin{array}{cc}
I_{m}-\mathbf{A}\mathbf{B} & \mathbf{A} \\
0 & I_{n}
\end{array}\right)\left(\begin{array}{cc}
I_{m} & 0 \\
\mathbf{B} & I_{n}
\end{array}\right)=\left(\begin{array}{cc}
I_{m} & 0 \\
\mathbf{B} & I_{n}
\end{array}\right)\left(\begin{array}{cc}
I_{m} & \mathbf{A} \\
0 & I_{n}-\mathbf{B}\mathbf{A}
\end{array}\right),
\end{equation}</p><p>we obtain</p><p>\begin{equation}
\left|I_{m}-\mathbf{A}\mathbf{B}\right|=\left|I_{n}-\mathbf{B}\mathbf{A}\right| .
\end{equation}</p><p>Now let $\lambda \neq 0$. Then,</p><p>\begin{equation}
\begin{aligned}
\left|\lambda I_{n}-\mathbf{B}\mathbf{A}\right| & =\lambda^{n}\left|I_{n}-\mathbf{B}\left(\lambda^{-1} \mathbf{A}\right)\right| \\
& =\lambda^{n}\left|I_{m}-\left(\lambda^{-1} \mathbf{A}\right) \mathbf{B}\right|=\lambda^{n-m}\left|\lambda I_{m}-\mathbf{A}\mathbf{B}\right| .
\end{aligned}
\end{equation}</p><p>Hence, the nonzero eigenvalues of $\mathbf{B}\mathbf{A}$ are the same as the nonzero eigenvalues of $\mathbf{A}\mathbf{B}$, and this is equivalent to the statement in the theorem.</p></div><div class=qed>■</div></div><p>Without proof we state the following famous result.</p><div id=cayley_hamilton class=theorem-box><p class=theorem-title><strong>Theorem 0.10 (Cayley-Hamilton)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be an $n \times n$ matrix with eigenvalues $\lambda_{1}, \ldots, \lambda_{n}$. Then,</p><p>\begin{equation}
\prod_{i=1}^{n}\left(\lambda_{i} I_{n}-\mathbf{A}\right)=0 .
\end{equation}</p></div></div><p>Finally, we present the following result on eigenvectors.</p><div id=eigenvector_independence class=theorem-box><p class=theorem-title><strong>Theorem 0.11 (Linear Independence of Eigenvectors)</strong></p><div class=theorem-content>Eigenvectors associated with distinct eigenvalues are linearly independent.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Let $\mathbf{A}\mathbf{x}_{1}=\lambda_{1} \mathbf{x}_{1}$, $\mathbf{A}\mathbf{x}_{2}=\lambda_{2} \mathbf{x}_{2}$, and $\lambda_{1} \neq \lambda_{2}$. Assume that $\mathbf{x}_{1}$ and $\mathbf{x}_{2}$ are linearly dependent. Then there is an $\alpha \neq 0$ such that $\mathbf{x}_{2}=\alpha \mathbf{x}_{1}$, and hence</p><p>\begin{equation}
\alpha \lambda_{1} \mathbf{x}_{1}=\alpha \mathbf{A} \mathbf{x}_{1}=\mathbf{A} \mathbf{x}_{2}=\lambda_{2} \mathbf{x}_{2}=\alpha \lambda_{2} \mathbf{x}_{1}
\end{equation}</p><p>that is</p><p>\begin{equation}
\alpha\left(\lambda_{1}-\lambda_{2}\right) \mathbf{x}_{1}=0
\end{equation}</p><p>Since $\alpha \neq 0$ and $\lambda_{1} \neq \lambda_{2}$, this implies that $\mathbf{x}_{1}=0$, a contradiction.</p></div><div class=qed>■</div></div><p><strong>Exercices</strong></p><ol><li>Show that</li></ol><p>\begin{equation}
\left|\begin{array}{ll}
0 & I_{m} \\
I_{m} & 0
\end{array}\right|=(-1)^{m}
\end{equation}</p><ol start=2><li>Show that, for $n=2$,</li></ol><p>\begin{equation}
|I+\epsilon \mathbf{A}|=1+\epsilon \operatorname{tr} \mathbf{A}+\epsilon^{2}|\mathbf{A}|
\end{equation}</p><ol start=3><li>Show that, for $n=3$,</li></ol><p>\begin{equation}
|I+\epsilon \mathbf{A}|=1+\epsilon \operatorname{tr} \mathbf{A}+\frac{\epsilon^{2}}{2}\left((\operatorname{tr} \mathbf{A})^{2}-\operatorname{tr} \mathbf{A}^{2}\right)+\epsilon^{3}|\mathbf{A}|
\end{equation}</p><h2 id=14---schurs-decomposition-theorem>14 - Schur&rsquo;s decomposition theorem
<a class=anchor href=#14---schurs-decomposition-theorem>#</a></h2><p>In the next few sections, we present three decomposition theorems: Schur&rsquo;s theorem, Jordan&rsquo;s theorem, and the singular-value decomposition. Each of these theorems will prove useful later in this book. We first state Schur&rsquo;s theorem.</p><div id=schur_decomposition class=theorem-box><p class=theorem-title><strong>Theorem 0.12 (Schur decomposition)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be an $n \times n$ matrix, possibly complex. Then there exist a unitary $n \times n$ matrix $\mathbf{S}$ (that is, $\mathbf{S}^\mathrm{H} \mathbf{S}=I_{n}$ ) and an upper triangular matrix $\mathbf{M}$ whose diagonal elements are the eigenvalues of $\mathbf{A}$, such that</p><p>\begin{equation}
\mathbf{S}^\mathrm{H} \mathbf{A} \mathbf{S}=\mathbf{M}
\end{equation}</p></div></div><p>The most important special case of Schur&rsquo;s decomposition theorem is the case where $\mathbf{A}$ is symmetric.</p><div id=symmetric_decomposition class=theorem-box><p class=theorem-title><strong>Theorem 0.13 (Symmetric Matrix Decomposition)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be a symmetric $n \times n$ matrix. Then there exist an orthogonal $n \times n$ matrix $\mathbf{S}$ (that is, $\mathbf{S}^\mathrm{T} \mathbf{S}=I_{n}$ ) whose columns are eigenvectors of $\mathbf{A}$ and a diagonal matrix $\boldsymbol{\Lambda}$ whose diagonal elements are the eigenvalues of $\mathbf{A}$, such that</p><p>\begin{equation}
\mathbf{S}^\mathrm{T} \mathbf{A} \mathbf{S}=\boldsymbol{\Lambda}
\end{equation}</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Using <a href=#schur_decomposition>Theorem 0.12</a>, there exists a unitary matrix $\mathbf{S}=\mathbf{R}+i \mathbf{T}$ with real $\mathbf{R}$ and $\mathbf{T}$ and an upper triangular matrix $\mathbf{M}$ such that $\mathbf{S}^\mathrm{H} \mathbf{A} \mathbf{S}=\mathbf{M}$. Then,</p><p>\begin{equation}
\begin{aligned}
\mathbf{M} & =\mathbf{S}^\mathrm{H} \mathbf{A} \mathbf{S}=(\mathbf{R}-i \mathbf{T})^\mathrm{T} \mathbf{A}(\mathbf{R}+i \mathbf{T}) \\
& =\left(\mathbf{R}^\mathrm{T} \mathbf{A} \mathbf{R}+\mathbf{T}^\mathrm{T} \mathbf{A} \mathbf{T}\right)+i\left(\mathbf{R}^\mathrm{T} \mathbf{A} \mathbf{T}-\mathbf{T}^\mathrm{T} \mathbf{A} \mathbf{R}\right)
\end{aligned}
\end{equation}</p><p>and hence, using the symmetry of $\mathbf{A}$,</p><p>\begin{equation}
\mathbf{M}+\mathbf{M}^\mathrm{T}=2\left(\mathbf{R}^\mathrm{T} \mathbf{A} \mathbf{R}+\mathbf{T}^\mathrm{T} \mathbf{A} \mathbf{T}\right) .
\end{equation}</p><p>It follows that $\mathbf{M}+\mathbf{M}^\mathrm{T}$ is a real matrix and hence, since $\mathbf{M}$ is triangular, that $\mathbf{M}$ is a real matrix. We thus obtain, from (32),</p><p>\begin{equation}
\mathbf{M}=\mathbf{R}^\mathrm{T} \mathbf{A} \mathbf{R}+\mathbf{T}^\mathrm{T} \mathbf{A} \mathbf{T} .
\end{equation}</p><p>Since $\mathbf{A}$ is symmetric, $\mathbf{M}$ is symmetric. But, since $\mathbf{M}$ is also triangular, $\mathbf{M}$ must be diagonal. The columns of $\mathbf{S}$ are then eigenvectors of $\mathbf{A}$, and since the diagonal elements of $\mathbf{M}$ are real, $\mathbf{S}$ can be chosen to be real as well.</p></div><div class=qed>■</div></div><p><strong>Exercices</strong></p><ol><li>Let $\mathbf{A}$ be a symmetric $n \times n$ matrix with eigenvalues $\lambda_{1} \leq \lambda_{2} \leq \cdots \leq \lambda_{n}$. Use <a href=#symmetric_decomposition>Theorem 0.13</a> to prove that</li></ol><p>\begin{equation}
\lambda_{1} \leq \frac{\mathbf{x}^\mathrm{T} \mathbf{A} \mathbf{x}}{\mathbf{x}^\mathrm{T} \mathbf{x}} \leq \lambda_{n} .
\end{equation}</p><ol start=2><li>Hence show that, for any $m \times n$ matrix $\mathbf{A}$,</li></ol><p>\begin{equation}
|\mathbf{A} \mathbf{x}| \leq \mu|\mathbf{x}|,
\end{equation}</p><p>where $\mu^{2}$ denotes the largest eigenvalue of $\mathbf{A}^\mathrm{T} \mathbf{A}$.</p><ol start=3><li>Let $\mathbf{A}$ be an $m \times n$ matrix of rank $r$. Show that there exists an $n \times(n-r)$ matrix $\mathbf{S}$ such that</li></ol><p>\begin{equation}
\mathbf{A} \mathbf{S}=0, \quad \mathbf{S}^\mathrm{T} \mathbf{S}=I_{n-r}
\end{equation}</p><ol start=4><li>Let $\mathbf{A}$ be an $m \times n$ matrix of rank $r$. Let $\mathbf{S}$ be a matrix such that $\mathbf{A} \mathbf{S}=0$. Show that $r(\mathbf{S}) \leq n-r$.</li></ol><h2 id=15---the-jordan-decomposition>15 - The Jordan decomposition
<a class=anchor href=#15---the-jordan-decomposition>#</a></h2><p>Schur&rsquo;s theorem tells us that there exists, for every square matrix $\mathbf{A}$, a unitary (possibly orthogonal) matrix $\mathbf{S}$ which &rsquo;transforms&rsquo; $\mathbf{A}$ into an upper triangular matrix $\mathbf{M}$ whose diagonal elements are the eigenvalues of $\mathbf{A}$.</p><p>Jordan&rsquo;s theorem similarly states that there exists a nonsingular matrix, say $\mathbf{T}$, which transforms $\mathbf{A}$ into an upper triangular matrix $\mathbf{M}$ whose diagonal elements are the eigenvalues of $\mathbf{A}$. The difference between the two decomposition theorems is that in Jordan&rsquo;s theorem less structure is put on the matrix $\mathbf{T}$ (nonsingular, but not necessarily unitary) and more structure on the matrix $\mathbf{M}$.</p><div id=jordan_decomposition class=theorem-box><p class=theorem-title><strong>Theorem 0.14 (Jordan decomposition)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be an $n \times n$ matrix and denote by $\mathbf{J}_{k}(\lambda)$ a $k \times k$ matrix of the form</p><p>\begin{equation}
\mathbf{J}_{k}(\lambda)=\left(\begin{array}{cccccc}
\lambda & 1 & 0 & \ldots & 0 & 0 \\
0 & \lambda & 1 & \ldots & 0 & 0 \\
\vdots & \vdots & \vdots & & \vdots & \vdots \\
0 & 0 & 0 & \ldots & \lambda & 1 \\
0 & 0 & 0 & \ldots & 0 & \lambda
\end{array}\right)
\end{equation}</p><p>(a so-called Jordan block), where $\mathbf{J}_{1}(\lambda)=\lambda$. Then there exists a nonsingular $n \times n$ matrix $\mathbf{T}$ such that</p><p>\begin{equation}
\mathbf{T}^{-1} \mathbf{A} \mathbf{T}=\left(\begin{array}{cccc}
\mathbf{J}_{k_{1}}\left(\lambda_{1}\right) & 0 & \ldots & 0 \\
0 & \mathbf{J}_{k_{2}}\left(\lambda_{2}\right) & \ldots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \ldots & \mathbf{J}_{k_{r}}\left(\lambda_{r}\right)
\end{array}\right)
\end{equation}</p><p>with $k_{1}+k_{2}+\cdots+k_{r}=n$. The $\lambda_{i}$ are the eigenvalues of $\mathbf{A}$, not necessarily distinct.</p></div></div><p>The most important special case of <a href=#jordan_decomposition>Theorem 0.14</a> is <a href=#distinct_eigenvalues>this theorem</a>.</p><div id=distinct_eigenvalues class=theorem-box><p class=theorem-title><strong>Theorem 0.15 (Distinct Eigenvalues Decomposition)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be an $n \times n$ matrix with distinct eigenvalues. Then there exist a nonsingular $n \times n$ matrix $\mathbf{T}$ and a diagonal $n \times n$ matrix $\boldsymbol{\Lambda}$ whose diagonal elements are the eigenvalues of $\mathbf{A}$, such that</p><p>\begin{equation}
\mathbf{T}^{-1} \mathbf{A} \mathbf{T}=\boldsymbol{\Lambda}
\end{equation}</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>Immediate from <a href=#jordan_decomposition>Theorem 0.14</a> (or <a href=#eigenvector_independence>Theorem 0.11</a>).</div><div class=qed>■</div></div><p><strong>Exercices</strong></p><ol><li>Show that $\left(\lambda I_{k}-\mathbf{J}_{k}(\lambda)\right)^{k}=0$ and use this fact to prove <a href=#cayley_hamilton>Theorem 0.10</a>.</li><li>Show that <a href=#distinct_eigenvalues>Theorem 0.15</a> remains valid when $\mathbf{A}$ is complex.</li></ol><h2 id=16---the-singular-value-decomposition>16 - The singular value decomposition
<a class=anchor href=#16---the-singular-value-decomposition>#</a></h2><p>The third important decomposition theorem is the singular-value decomposition.</p><div id=svd class=theorem-box><p class=theorem-title><strong>Theorem 0.16 (Singular-value decomposition)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be a real $m \times n$ matrix with $r(\mathbf{A})=r>0$. Then there exist an $m \times r$ matrix $\mathbf{S}$ such that $\mathbf{S}^\mathrm{T} \mathbf{S}=I_{r}$, an $n \times r$ matrix $\mathbf{T}$ such that $\mathbf{T}^\mathrm{T} \mathbf{T}=I_{r}$ and an $r \times r$ diagonal matrix $\boldsymbol{\Lambda}$ with positive diagonal elements, such that</p><p>\begin{equation}
\mathbf{A}=\mathbf{S} \boldsymbol{\Lambda}^{1 / 2} \mathbf{T}^\mathrm{T}
\end{equation}</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Since $\mathbf{A} \mathbf{A}^\mathrm{T}$ is an $m \times m$ positive semidefinite matrix of rank $r$ (by (6)), its nonzero eigenvalues are all positive (<a href=#eigenvalue_positive>this theorem</a>). From <a href=#symmetric_decomposition>Theorem 0.13</a> we know that there exists an orthogonal $m \times m$ matrix $( \mathbf{S}: \mathbf{S}_{2})$ such that</p><p>\begin{equation}
\mathbf{A} \mathbf{A}^\mathrm{T} \mathbf{S}=\mathbf{S} \boldsymbol{\Lambda}, \quad \mathbf{A} \mathbf{A}^\mathrm{T} \mathbf{S}_{2}=0, \quad \mathbf{S} \mathbf{S}^\mathrm{T}+\mathbf{S}_{2} \mathbf{S}_{2}^\mathrm{T}=I_{m}
\end{equation}</p><p>where $\boldsymbol{\Lambda}$ is an $r \times r$ diagonal matrix having these $r$ positive eigenvalues as its diagonal elements. Define $\mathbf{T}=\mathbf{A}^\mathrm{T} \mathbf{S} \boldsymbol{\Lambda}^{-1 / 2}$. Then we see that</p><p>\begin{equation}
\mathbf{A}^\mathrm{T} \mathbf{A} \mathbf{T}=\mathbf{T} \boldsymbol{\Lambda}, \quad \mathbf{T}^\mathrm{T} \mathbf{T}=I_{r}
\label{eq:33}
\end{equation}</p><p>Thus, since $\mathbf{A}^\mathrm{T} \mathbf{S}_{2}=0$, we have</p><p>\begin{equation}
\mathbf{A}=\left(\mathbf{S} \mathbf{S}^\mathrm{T}+\mathbf{S}_{2} \mathbf{S}_{2}^\mathrm{T}\right) \mathbf{A}=\mathbf{S} \mathbf{S}^\mathrm{T} \mathbf{A}=\mathbf{S} \boldsymbol{\Lambda}^{1 / 2}\left(\mathbf{A}^\mathrm{T} \mathbf{S} \boldsymbol{\Lambda}^{-1 / 2}\right)^\mathrm{T}=\mathbf{S} \boldsymbol{\Lambda}^{1 / 2} \mathbf{T}^\mathrm{T}
\label{eq:34}
\end{equation}</p><p>which concludes the proof.</p></div><div class=qed>■</div></div><p>We see from \eqref{eq:33} and \eqref{eq:34} that the semi-orthogonal matrices $\mathbf{S}$ and $\mathbf{T}$ satisfy</p><p>\begin{equation}
\mathbf{A} \mathbf{A}^\mathrm{T} \mathbf{S}=\mathbf{S} \boldsymbol{\Lambda}, \quad \mathbf{A}^\mathrm{T} \mathbf{A} \mathbf{T}=\mathbf{T} \boldsymbol{\Lambda}
\end{equation}</p><p>Hence, $\boldsymbol{\Lambda}$ contains the $r$ nonzero eigenvalues of $\mathbf{A} \mathbf{A}^\mathrm{T}$ (and of $\mathbf{A}^\mathrm{T} \mathbf{A}$ ) and $\mathbf{S}$ (by construction) and $\mathbf{T}$ contain corresponding eigenvectors. A common mistake in applying the singular-value decomposition is to find $\mathbf{S}$, $\mathbf{T}$, and $\boldsymbol{\Lambda}$ from (35). This is incorrect because, given $\mathbf{S}$, $\mathbf{T}$ is not unique. The correct procedure is to find $\mathbf{S}$ and $\boldsymbol{\Lambda}$ from $\mathbf{A} \mathbf{A}^\mathrm{T} \mathbf{S}=\mathbf{S} \boldsymbol{\Lambda}$ and then define $\mathbf{T}=\mathbf{A}^\mathrm{T} \mathbf{S} \boldsymbol{\Lambda}^{-1 / 2}$. Alternatively, we can find $\mathbf{T}$ and $\boldsymbol{\Lambda}$ from $\mathbf{A}^\mathrm{T} \mathbf{A} \mathbf{T}=\mathbf{T} \boldsymbol{\Lambda}$ and define $\mathbf{S}=\mathbf{A} \mathbf{T} \boldsymbol{\Lambda}^{-1 / 2}$.</p><h2 id=17---further-results-concerning-eigenvalues>17 - Further results concerning eigenvalues
<a class=anchor href=#17---further-results-concerning-eigenvalues>#</a></h2><p>Let us now prove the following five theorems, all of which concern eigenvalues. <a href=#trace_determinant>this theorem</a> deals with the sum and the product of the eigenvalues. <a href=#rank_eigenvalues>this theorem</a> and <a href=#rank_eigenvalues_symmetric>this theorem</a> discuss the relationship between the rank and the number of nonzero eigenvalues, and <a href=#idempotent_eigenvalues>this theorem</a> concerns idempotent matrices.</p><div id=trace_determinant class=theorem-box><p class=theorem-title><strong>Theorem 0.17 (Trace and Determinant)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be a square, possibly complex, $n \times n$ matrix with eigenvalues $\lambda_{1}, \ldots, \lambda_{n}$. Then,</p><p>\begin{equation}
\operatorname{tr} \mathbf{A}=\sum_{i=1}^{n} \lambda_{i}, \quad |\mathbf{A}|=\prod_{i=1}^{n} \lambda_{i}
\end{equation}</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>We write, using <a href=#schur_decomposition>Theorem 0.12</a>, $\mathbf{S}^\mathrm{H} \mathbf{A} \mathbf{S}=\mathbf{M}$. Then,</p><p>\begin{equation}
\operatorname{tr} \mathbf{A}=\operatorname{tr} \mathbf{S} \mathbf{M} \mathbf{S}^\mathrm{H}=\operatorname{tr} \mathbf{M} \mathbf{S}^\mathrm{H} \mathbf{S}=\operatorname{tr} \mathbf{M}=\sum_{i} \lambda_{i}
\end{equation}</p><p>and</p><p>\begin{equation}
|\mathbf{A}|=\left|\mathbf{S} \mathbf{M} \mathbf{S}^\mathrm{H}\right|=|\mathbf{S}||\mathbf{M}|\left|\mathbf{S}^\mathrm{H}\right|=|\mathbf{M}|=\prod_{i} \lambda_{i}
\end{equation}</p><p>and the result follows.</p></div><div class=qed>■</div></div><div id=rank_eigenvalues class=theorem-box><p class=theorem-title><strong>Theorem 0.18 (Rank and Nonzero Eigenvalues)</strong></p><div class=theorem-content>If $\mathbf{A}$ has $r$ nonzero eigenvalues, then $r(\mathbf{A}) \geq r$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>We write again, using <a href=#schur_decomposition>Theorem 0.12</a>, $\mathbf{S}^\mathrm{H} \mathbf{A} \mathbf{S}=\mathbf{M}$. We partition</p><p>\begin{equation}
\mathbf{M}=\left(\begin{array}{cc}
\mathbf{M}_{1} & \mathbf{M}_{2} \\
0 & \mathbf{M}_{3}
\end{array}\right)
\end{equation}</p><p>where $\mathbf{M}_{1}$ is a nonsingular upper triangular $r \times r$ matrix and $\mathbf{M}_{3}$ is strictly upper triangular. Since $r(\mathbf{A})=r(\mathbf{M}) \geq r\left(\mathbf{M}_{1}\right)=r$, the result follows.</p></div><div class=qed>■</div></div><p>The following example shows that it is indeed possible that $r(\mathbf{A})>r$. Let</p><p>\begin{equation}
\mathbf{A}=\left(\begin{array}{ll}
1 & -1 \\
1 & -1
\end{array}\right)
\end{equation}</p><p>Then $r(\mathbf{A})=1$ and both eigenvalues of $\mathbf{A}$ are zero.</p><div id=simple_eigenvalue_rank class=theorem-box><p class=theorem-title><strong>Theorem 0.19 (Simple Eigenvalue Rank)</strong></p><div class=theorem-content>Let $\mathbf{A}$ be an $n \times n$ matrix. If $\lambda$ is a simple eigenvalue of $\mathbf{A}$, then $r(\lambda I-\mathbf{A})=n-1$. Conversely, if $r(\lambda I-\mathbf{A})=n-1$, then $\lambda$ is an eigenvalue of $\mathbf{A}$, but not necessarily a simple eigenvalue.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>Let $\lambda_{1}, \ldots, \lambda_{n}$ be the eigenvalues of $\mathbf{A}$. Then $\mathbf{B}=\lambda I-\mathbf{A}$ has eigenvalues $\lambda-\lambda_{i}(i=1, \ldots, n)$, and since $\lambda$ is a simple eigenvalue of $\mathbf{A}$, $\mathbf{B}$ has a simple eigenvalue zero. Hence, $r(\mathbf{B}) \leq n-1$. Also, since $\mathbf{B}$ has $n-1$ nonzero eigenvalues, $r(\mathbf{B}) \geq n-1$ (<a href=#rank_eigenvalues>Theorem 0.18</a>). Hence $r(\mathbf{B})=n-1$. Conversely, if $r(\mathbf{B})=n-1$, then $\mathbf{B}$ has at least one zero eigenvalue and hence $\lambda=\lambda_{i}$ for at least one $i$.</div><div class=qed>■</div></div><div id=simple_zero_corollary class=theorem-box><p class=theorem-title><strong>Definition 0.29 (Simple Zero Eigenvalue Corollary)</strong></p><div class=theorem-content>An $n \times n$ matrix with a simple zero eigenvalue has rank $n-1$.</div></div><div id=rank_eigenvalues_symmetric class=theorem-box><p class=theorem-title><strong>Theorem 0.20 (Symmetric Matrix Rank and Eigenvalues)</strong></p><div class=theorem-content>If $\mathbf{A}$ is a symmetric matrix with $r$ nonzero eigenvalues, then $r(\mathbf{A})=r$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Using <a href=#symmetric_decomposition>Theorem 0.13</a>, we have $\mathbf{S}^\mathrm{T} \mathbf{A} \mathbf{S}=\boldsymbol{\Lambda}$ and hence</p><p>\begin{equation}
r(\mathbf{A})=r\left(\mathbf{S} \boldsymbol{\Lambda} \mathbf{S}^\mathrm{T}\right)=r(\boldsymbol{\Lambda})=r,
\end{equation}</p><p>and the result follows.</p></div><div class=qed>■</div></div><p><div id=idempotent_1_12 class=theorem-box><p class=theorem-title><strong>Theorem 0.21 (Idempotent Matrix Properties)</strong></p><div class=theorem-content>If $\mathbf{A}$ is an idempotent matrix, possibly complex, with $r$ eigenvalues equal to one, then $r(\mathbf{A})=\operatorname{tr} \mathbf{A}=r$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>By <a href=#theorem_1_12>this theorem</a>, $\mathbf{S}^{*} \mathbf{A} \mathbf{S}=\mathbf{M}$ (upper triangular), where</p><p>\begin{equation}
\mathbf{M}=\left(\begin{array}{cc}
\mathbf{M}_{1} & \mathbf{M}_{2} \\
0 & \mathbf{M}_{3}
\end{array}\right)
\end{equation}</p><p>with $\mathbf{M}_{1}$ a unit upper triangular $r \times r$ matrix and $\mathbf{M}_{3}$ a strictly upper triangular matrix. Since $\mathbf{A}$ is idempotent, so is $\mathbf{M}$ and hence</p><p>\begin{equation}
\left(\begin{array}{cc}
\mathbf{M}_{1}^{2} & \mathbf{M}_{1} \mathbf{M}_{2}+\mathbf{M}_{2} \mathbf{M}_{3} \\
0 & \mathbf{M}_{3}^{2}
\end{array}\right)=\left(\begin{array}{cc}
\mathbf{M}_{1} & \mathbf{M}_{2} \\
0 & \mathbf{M}_{3}
\end{array}\right) .
\end{equation}</p><p>This implies that $\mathbf{M}_{1}$ is idempotent; it is nonsingular, hence $\mathbf{M}_{1}=\mathbf{I}_{r}$ (see Exercise 1 below). Also, $\mathbf{M}_{3}$ is idempotent and all its eigenvalues are zero, hence $\mathbf{M}_{3}=0$ (see Exercise 2 below), so that</p><p>\begin{equation}
\mathbf{M}=\left(\begin{array}{cc}
\mathbf{I}_{r} & \mathbf{M}_{2} \\
0 & 0
\end{array}\right)
\end{equation}</p><p>Hence,</p><p>\begin{equation}
r(\mathbf{A})=r(\mathbf{M})=r
\end{equation}</p><p>Also, by:</p><p>\begin{equation}
\operatorname{tr} \mathbf{A}=\text { sum of eigenvalues of } \mathbf{A}=r,
\end{equation}</p><p>thus completing the proof.</p></div><div class=qed>■</div></div></p><p>We note that in, the matrix $\mathbf{A}$ is not required to be symmetric. If $\mathbf{A}$ is idempotent and symmetric, then it is positive semidefinite. Since its eigenvalues are only 0 and 1 and its rank equals $r$, it that $\mathbf{A}$ can be written as</p><p>\begin{equation}
\mathbf{A}=\mathbf{P} \mathbf{P}^{\mathrm{T}}, \quad \mathbf{P}^{\mathrm{T}} \mathbf{P}=\mathbf{I}_{r}
\end{equation}</p><p><strong>Exercises</strong></p><ol><li>The only nonsingular idempotent matrix is the identity matrix.</li><li>The only idempotent matrix whose eigenvalues are all zero is the null matrix.</li><li>If $\mathbf{A}$ is a positive semidefinite $n \times n$ matrix with $r(\mathbf{A})=r$, then there exists an $n \times r$ matrix $\mathbf{P}$ such that</li></ol><p>\begin{equation}
\mathbf{A}=\mathbf{P} \mathbf{P}^{\mathrm{T}}, \quad \mathbf{P}^{\mathrm{T}} \mathbf{P}=\mathbf{\Lambda}
\end{equation}</p><p>where $\mathbf{\Lambda}$ is an $r \times r$ diagonal matrix containing the positive eigenvalues of $\mathbf{A}$.</p><h2 id=positive-semidefinite-matrices>Positive (semi)definite matrices
<a class=anchor href=#positive-semidefinite-matrices>#</a></h2><p>Positive (semi)definite matrices were introduced in Section 1.6. We have already seen that $\mathbf{A} \mathbf{A}^{\mathrm{T}}$ and $\mathbf{A}^{\mathrm{T}} \mathbf{A}$ are both positive semidefinite and that the eigenvalues of a positive (semi)definite matrix are all positive (nonnegative). We now present some more properties of positive (semi)definite matrices.</p><div id=theorem_1_22 class=theorem-box><p class=theorem-title><strong>Theorem 0.22 (Determinant inequality for positive definite matrices)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be positive definite and $\mathbf{B}$ positive semidefinite. Then,</p><p>\begin{equation}
|\mathbf{A}+\mathbf{B}| \geq|\mathbf{A}|
\end{equation}</p><p>with equality if and only if $\mathbf{B}=0$.</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Let $\mathbf{\Lambda}$ be a positive definite diagonal matrix such that</p><p>\begin{equation}
\mathbf{S}^{\mathrm{T}} \mathbf{A} \mathbf{S}=\mathbf{\Lambda}, \quad \mathbf{S}^{\mathrm{T}} \mathbf{S}=\mathbf{I} .
\end{equation}</p><p>Then, $\mathbf{S} \mathbf{S}^{\mathrm{T}}=\mathbf{I}$ and</p><p>\begin{equation}
\mathbf{A}+\mathbf{B}=\mathbf{S} \mathbf{\Lambda}^{1 / 2}\left(\mathbf{I}+\mathbf{\Lambda}^{-1 / 2} \mathbf{S}^{\mathrm{T}} \mathbf{B} \mathbf{S} \mathbf{\Lambda}^{-1 / 2}\right) \mathbf{\Lambda}^{1 / 2} \mathbf{S}^{\mathrm{T}} .
\end{equation}</p><p>Hence, using determinant results,</p><p>\begin{equation}
\begin{aligned}
|\mathbf{A}+\mathbf{B}| & =\left|\mathbf{S} \mathbf{\Lambda}^{1 / 2}\right|\left|\mathbf{I}+\mathbf{\Lambda}^{-1 / 2} \mathbf{S}^{\mathrm{T}} \mathbf{B} \mathbf{S} \mathbf{\Lambda}^{-1 / 2}\right|\left|\mathbf{\Lambda}^{1 / 2} \mathbf{S}^{\mathrm{T}}\right| \\
& =\left|\mathbf{S} \mathbf{\Lambda}^{1 / 2} \mathbf{\Lambda}^{1 / 2} \mathbf{S}^{\mathrm{T}}\right|\left|\mathbf{I}+\mathbf{\Lambda}^{-1 / 2} \mathbf{S}^{\mathrm{T}} \mathbf{B} \mathbf{S} \mathbf{\Lambda}^{-1 / 2}\right| \\
& =|\mathbf{A}|\left|\mathbf{I}+\mathbf{\Lambda}^{-1 / 2} \mathbf{S}^{\mathrm{T}} \mathbf{B} \mathbf{S} \mathbf{\Lambda}^{-1 / 2}\right|
\end{aligned}
\end{equation}</p><p>If $\mathbf{B}=0$ then $|\mathbf{A}+\mathbf{B}|=|\mathbf{A}|$. If $\mathbf{B} \neq 0$, then the matrix $\mathbf{\Lambda}^{-1 / 2} \mathbf{S}^{\mathrm{T}} \mathbf{B} \mathbf{S} \mathbf{\Lambda}^{-1 / 2}$ will be positive semidefinite with at least one positive eigenvalue. Hence we have $\left|\mathbf{I}+\mathbf{\Lambda}^{-1 / 2} \mathbf{S}^{\mathrm{T}} \mathbf{B} \mathbf{S} \mathbf{\Lambda}^{-1 / 2}\right|>1$ and $|\mathbf{A}+\mathbf{B}|>|\mathbf{A}|$.</p></div><div class=qed>■</div></div><div id=theorem_1_23 class=theorem-box><p class=theorem-title><strong>Theorem 0.23 (Simultaneous diagonalization)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be positive definite and $\mathbf{B}$ symmetric of the same order. Then there exist a nonsingular matrix $\mathbf{P}$ and a diagonal matrix $\mathbf{\Lambda}$ such that</p><p>\begin{equation}
\mathbf{A}=\mathbf{P} \mathbf{P}^{\mathrm{T}}, \quad \mathbf{B}=\mathbf{P} \mathbf{\Lambda} \mathbf{P}^{\mathrm{T}}
\end{equation}</p><p>If $\mathbf{B}$ is positive semidefinite, then so is $\mathbf{\Lambda}$.</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Let $\mathbf{C}=\mathbf{A}^{-1 / 2} \mathbf{B} \mathbf{A}^{-1 / 2}$. Since $\mathbf{C}$ is symmetric, there exist an orthogonal matrix $\mathbf{S}$ and a diagonal matrix $\mathbf{\Lambda}$ such that</p><p>\begin{equation}
\mathbf{S}^{\mathrm{T}} \mathbf{C} \mathbf{S}=\mathbf{\Lambda}, \quad \mathbf{S}^{\mathrm{T}} \mathbf{S}=\mathbf{I}
\end{equation}</p><p>Now define $\mathbf{P}=\mathbf{A}^{1 / 2} \mathbf{S}$. Then,</p><p>\begin{equation}
\mathbf{P} \mathbf{P}^{\mathrm{T}}=\mathbf{A}^{1 / 2} \mathbf{S} \mathbf{S}^{\mathrm{T}} \mathbf{A}^{1 / 2}=\mathbf{A}^{1 / 2} \mathbf{A}^{1 / 2}=\mathbf{A}
\end{equation}</p><p>and</p><p>\begin{equation}
\mathbf{P} \mathbf{\Lambda} \mathbf{P}^{\mathrm{T}}=\mathbf{A}^{1 / 2} \mathbf{S} \mathbf{\Lambda} \mathbf{S}^{\mathrm{T}} \mathbf{A}^{1 / 2}=\mathbf{A}^{1 / 2} \mathbf{C} \mathbf{A}^{1 / 2}=\mathbf{A}^{1 / 2} \mathbf{A}^{-1 / 2} \mathbf{B} \mathbf{A}^{-1 / 2} \mathbf{A}^{1 / 2}=\mathbf{B} .
\end{equation}</p><p>If $\mathbf{B}$ is positive semidefinite, then so is $\mathbf{C}$ and so is $\mathbf{\Lambda}$.</p></div><div class=qed>■</div></div><p>For two symmetric matrices $\mathbf{A}$ and $\mathbf{B}$, we shall write $\mathbf{A} \geq \mathbf{B}$ (or $\mathbf{B} \leq \mathbf{A}$ ) if $\mathbf{A}-\mathbf{B}$ is positive semidefinite, and $\mathbf{A}>\mathbf{B}$ (or $\mathbf{B}&lt;\mathbf{A}$ ) if $\mathbf{A}-\mathbf{B}$ is positive definite.</p><div id=theorem_1_24 class=theorem-box><p class=theorem-title><strong>Theorem 0.24 (Inverse order for positive definite matrices)</strong></p><div class=theorem-content>Let $\mathbf{A}$ and $\mathbf{B}$ be positive definite $n \times n$ matrices. Then $\mathbf{A}>\mathbf{B}$ if and only if $\mathbf{B}^{-1}>\mathbf{A}^{-1}$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>By <a href=#theorem_1_23>Theorem 0.23</a>, there exist a nonsingular matrix $\mathbf{P}$ and a positive definite diagonal matrix $\mathbf{\Lambda}=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{n}\right)$ such that</p><p>\begin{equation}
\mathbf{A}=\mathbf{P} \mathbf{P}^{\mathrm{T}}, \quad \mathbf{B}=\mathbf{P} \mathbf{\Lambda} \mathbf{P}^{\mathrm{T}}
\end{equation}</p><p>Then,</p><p>\begin{equation}
\mathbf{A}-\mathbf{B}=\mathbf{P}(\mathbf{I}-\mathbf{\Lambda}) \mathbf{P}^{\mathrm{T}}, \quad \mathbf{B}^{-1}-\mathbf{A}^{-1}=\mathbf{P}^{\mathrm{T}-1}\left(\mathbf{\Lambda}^{-1}-\mathbf{I}\right) \mathbf{P}^{-1} .
\end{equation}</p><p>If $\mathbf{A}-\mathbf{B}$ is positive definite, then $\mathbf{I}-\mathbf{\Lambda}$ is positive definite and hence $0&lt;\lambda_{i}&lt;$ $1(i=1, \ldots, n)$. This implies that $\mathbf{\Lambda}^{-1}-\mathbf{I}$ is positive definite and hence that $\mathbf{B}^{-1}-\mathbf{A}^{-1}$ is positive definite.</p></div><div class=qed>■</div></div><div id=theorem_1_25 class=theorem-box><p class=theorem-title><strong>Theorem 0.25 (Determinant monotonicity)</strong></p><div class=theorem-content>Let $\mathbf{A}$ and $\mathbf{B}$ be positive definite matrices such that $\mathbf{A}-\mathbf{B}$ is positive semidefinite. Then, $|\mathbf{A}| \geq|\mathbf{B}|$ with equality if and only if $\mathbf{A}=\mathbf{B}$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>Let $\mathbf{C}=\mathbf{A}-\mathbf{B}$. Then $\mathbf{A}=\mathbf{B}+\mathbf{C}$, where $\mathbf{B}$ is positive definite and $\mathbf{C}$ is positive semidefinite. Thus, by <a href=#theorem_1_22>Theorem 0.22</a>, $|\mathbf{B}+\mathbf{C}| \geq|\mathbf{B}|$ with equality if and only if $\mathbf{C}=0$, that is, $|\mathbf{A}| \geq|\mathbf{B}|$ with equality if and only if $\mathbf{A}=\mathbf{B}$.</div><div class=qed>■</div></div><p>A useful special case of <a href=#theorem_1_25>Theorem 0.25</a> is <a href=#theorem_1_26>this theorem</a>.</p><div id=theorem_1_26 class=theorem-box><p class=theorem-title><strong>Theorem 0.26 (Identity characterization)</strong></p><div class=theorem-content>Let $\mathbf{A}$ be positive definite with $|\mathbf{A}|=1$. If $\mathbf{I}-\mathbf{A}$ is also positive semidefinite, then $\mathbf{A}=\mathbf{I}$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>This follows immediately from <a href=#theorem_1_25>Theorem 0.25</a>.</div><div class=qed>■</div></div><h2 id=three-further-results-for-positive-definite-matrices>Three further results for positive definite matrices
<a class=anchor href=#three-further-results-for-positive-definite-matrices>#</a></h2><p>Let us now prove <a href=#theorem_1_27>this theorem</a>.</p><div id=theorem_1_27 class=theorem-box><p class=theorem-title><strong>Theorem 0.27 (Block matrix determinant and positive definiteness)</strong></p><div class=theorem-content><p>Let $\mathbf{A}$ be a positive definite $n \times n$ matrix, and let $\mathbf{B}$ be the $(n+1) \times(n+1)$ matrix</p><p>\begin{equation}
\mathbf{B}=\left(\begin{array}{ll}
\mathbf{A} & \mathbf{b} \\
\mathbf{b}^{\mathrm{T}} & \alpha
\end{array}\right)
\end{equation}</p><p>Then,
(i) $|\mathbf{B}| \leq \alpha|\mathbf{A}|$ with equality if and only if $\mathbf{b}=0$; and
(ii) $\mathbf{B}$ is positive definite if and only if $|\mathbf{B}|>0$.</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Define the $(n+1) \times(n+1)$ matrix</p><p>\begin{equation}
\mathbf{P}=\left(\begin{array}{cc}
\mathbf{I}_{n} & -\mathbf{A}^{-1} \mathbf{b} \\
\mathbf{0}^{\mathrm{T}} & 1
\end{array}\right)
\end{equation}</p><p>Then,</p><p>\begin{equation}
\mathbf{P}^{\mathrm{T}} \mathbf{B} \mathbf{P}=\left(\begin{array}{cc}
\mathbf{A} & 0 \\
\mathbf{0}^{\mathrm{T}} & \alpha-\mathbf{b}^{\mathrm{T}} \mathbf{A}^{-1} \mathbf{b}
\end{array}\right)
\end{equation}</p><p>so that</p><p>\begin{equation}
|\mathbf{B}|=\left|\mathbf{P}^{\mathrm{T}} \mathbf{B} \mathbf{P}\right|=|\mathbf{A}|\left(\alpha-\mathbf{b}^{\mathrm{T}} \mathbf{A}^{-1} \mathbf{b}\right) .
\label{eq:det_block}
\end{equation}</p><p>(Compare Exercise 2 in Section 1.11.) Then (i) is an immediate consequence of \eqref{eq:det_block}. To prove (ii) we note that $|\mathbf{B}|>0$ if and only if $\alpha-\mathbf{b}^{\mathrm{T}} \mathbf{A}^{-1} \mathbf{b}>0$ (from \eqref{eq:det_block}), which is the case if and only if $\mathbf{P}^{\mathrm{T}} \mathbf{B} \mathbf{P}$ is positive definite (from the previous equation). This in turn is true if and only if $\mathbf{B}$ is positive definite.</p></div><div class=qed>■</div></div><p>An immediate consequence of <a href=#theorem_1_27>Theorem 0.27</a>, proved by induction, is the following.</p><div id=theorem_1_28 class=theorem-box><p class=theorem-title><strong>Theorem 0.28 (Hadamard's inequality)</strong></p><div class=theorem-content><p>If $\mathbf{A}=\left(a_{ij}\right)$ is a positive definite $n \times n$ matrix, then</p><p>\begin{equation}
|\mathbf{A}| \leq \prod_{i=1}^{n} a_{ii}
\end{equation}</p><p>with equality if and only if $\mathbf{A}$ is diagonal.</p></div></div><p>Another consequence of <a href=#theorem_1_27>Theorem 0.27</a> is <a href=#theorem_1_29>this theorem</a>.</p><div id=theorem_1_29 class=theorem-box><p class=theorem-title><strong>Theorem 0.29 (Principal minor test)</strong></p><div class=theorem-content>A symmetric $n \times n$ matrix $\mathbf{A}$ is positive definite if and only if all principal minors $\left|\mathbf{A}_{k}\right|(k=1, \ldots, n)$ are positive.</div></div><p>Note. The $k \times k$ matrix $\mathbf{A}_{k}$ is obtained from $\mathbf{A}$ by deleting the last $n-k$ rows and columns of $\mathbf{A}$. Notice that $\mathbf{A}_{n}=\mathbf{A}$.</p><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Let $\mathbf{E}_{k}=\left(\mathbf{I}_{k}: 0\right)$ be a $k \times n$ matrix, so that $\mathbf{A}_{k}=\mathbf{E}_{k} \mathbf{A} \mathbf{E}_{k}^{\mathrm{T}}$. Let $\mathbf{y}$ be an arbitrary $k \times 1$ vector, $\mathbf{y} \neq 0$. Then,</p><p>\begin{equation}
\mathbf{y}^{\mathrm{T}} \mathbf{A}_{k} \mathbf{y}=\left(\mathbf{E}_{k}^{\mathrm{T}} \mathbf{y}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{E}_{k}^{\mathrm{T}} \mathbf{y}\right)>0
\end{equation}</p><p>since $\mathbf{E}_{k}^{\mathrm{T}} \mathbf{y} \neq 0$ and $\mathbf{A}$ is positive definite. Hence, $\mathbf{A}_{k}$ is positive definite and, in particular, $\left|\mathbf{A}_{k}\right|>0$. The converse follows by repeated application of <a href=#theorem_1_27>Theorem 0.27</a>(ii).</p></div><div class=qed>■</div></div><p><strong>Exercises</strong></p><ol><li>If $\mathbf{A}$ is positive definite show that the matrix</li></ol><p>\begin{equation}
\left(\begin{array}{cc}
\mathbf{A} & \mathbf{b} \\
\mathbf{b}^{\mathrm{T}} & \mathbf{b}^{\mathrm{T}} \mathbf{A}^{-1} \mathbf{b}
\end{array}\right)
\end{equation}</p><p>is positive semidefinite and singular, and find the eigenvector associated with the zero eigenvalue.</p><ol start=2><li>Hence show that, for positive definite $\mathbf{A}$,</li></ol><p>\begin{equation}
\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}-2 \mathbf{b}^{\mathrm{T}} \mathbf{x} \geq-\mathbf{b}^{\mathrm{T}} \mathbf{A}^{-1} \mathbf{b}
\end{equation}</p><p>for every $\mathbf{x}$, with equality if and only if $\mathbf{x}=\mathbf{A}^{-1} \mathbf{b}$.</p><h2 id=a-useful-result>A useful result
<a class=anchor href=#a-useful-result>#</a></h2><p>If $\mathbf{A}$ is a positive definite $n \times n$ matrix, then, in accordance with <a href=#theorem_1_28>Theorem 0.28</a>,</p><p>\begin{equation}
|\mathbf{A}|=\prod_{i=1}^{n} a_{ii}
\label{eq:diagonal_det}
\end{equation}</p><p>if and only if $\mathbf{A}$ is diagonal. If $\mathbf{A}$ is merely symmetric, then \eqref{eq:diagonal_det}, while obviously necessary, is no longer sufficient for the diagonality of $\mathbf{A}$. For example, the matrix</p><p>\begin{equation}
\mathbf{A}=\left(\begin{array}{lll}
2 & 3 & 3 \\
3 & 2 & 3 \\
3 & 3 & 2
\end{array}\right)
\end{equation}</p><p>has determinant $|\mathbf{A}|=8$ (its eigenvalues are $-1,-1$, and 8 ), thus satisfying \eqref{eq:diagonal_det}, but $\mathbf{A}$ is not diagonal.</p><p><a href=#theorem_1_30>this theorem</a> gives a necessary and sufficient condition for the diagonality of a symmetric matrix.</p><div id=theorem_1_30 class=theorem-box><p class=theorem-title><strong>Theorem 0.30 (Diagonal matrix characterization)</strong></p><div class=theorem-content>A symmetric matrix is diagonal if and only if its eigenvalues and its diagonal elements coincide.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Let $\mathbf{A}=\left(a_{ij}\right)$ be a symmetric $n \times n$ matrix. The &lsquo;only if&rsquo; part of the theorem is trivial. To prove the &lsquo;if&rsquo; part, assume that $\lambda_{i}(\mathbf{A})=a_{ii}, i=1, \ldots, n$, and consider the matrix</p><p>\begin{equation}
\mathbf{B}=\mathbf{A}+k \mathbf{I},
\end{equation}</p><p>where $k>0$ is such that $\mathbf{B}$ is positive definite. Then,</p><p>\begin{equation}
\lambda_{i}(\mathbf{B})=\lambda_{i}(\mathbf{A})+k=a_{ii}+k=b_{ii} \quad(i=1, \ldots, n),
\end{equation}</p><p>and hence</p><p>\begin{equation}
|\mathbf{B}|=\prod_{1}^{n} \lambda_{i}(\mathbf{B})=\prod_{i=1}^{n} b_{ii} .
\end{equation}</p><p>It then follows from <a href=#theorem_1_28>Theorem 0.28</a> that $\mathbf{B}$ is diagonal, and hence that $\mathbf{A}$ is diagonal.</p></div><div class=qed>■</div></div><h2 id=symmetric-matrix-functions>Symmetric matrix functions
<a class=anchor href=#symmetric-matrix-functions>#</a></h2><p>Let $\mathbf{A}$ be a square matrix of order $n \times n$. The $\operatorname{trace} \operatorname{tr} \mathbf{A}$ and the determinant $|\mathbf{A}|$ are examples of scalar functions of $\mathbf{A}$. We can also consider matrix functions, for example, the inverse $\mathbf{A}^{-1}$. The general definition of a matrix function is somewhat complicated, but for symmetric matrices it is easy. So, let us assume that $\mathbf{A}$ is symmetric.</p><p>We known from that any symmetric $n \times n$ matrix $\mathbf{A}$ can be diagonalized, which means that there exists an orthogonal matrix $\mathbf{S}$ and a diagonal matrix $\mathbf{\Lambda}$ (containing the eigenvalues of $\mathbf{A}$ ) such that $\mathbf{S}^{\mathrm{T}} \mathbf{A} \mathbf{S}=\mathbf{\Lambda}$. Let $\lambda_{i}$ denote the $i$ th diagonal element of $\mathbf{\Lambda}$ and let $\phi$ be a function so that $\phi(\lambda)$ is defined, for example, $\phi(\lambda)=\sqrt{\lambda}$ or $1 / \lambda$ or $\log \lambda$ or $e^{\lambda}$.</p><p>We now define the matrix function $F$ as</p><p>\begin{equation}
F(\mathbf{\Lambda})=\left(\begin{array}{cccc}
\phi\left(\lambda_{1}\right) & 0 & \ldots & 0 \\
0 & \phi\left(\lambda_{2}\right) & \ldots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \ldots & \phi\left(\lambda_{n}\right)
\end{array}\right),
\end{equation}</p><p>and then</p><p>\begin{equation}
F(\mathbf{A})=\mathbf{S} F(\mathbf{\Lambda}) \mathbf{S}^{\mathrm{T}}
\end{equation}</p><p>For example, if $\mathbf{A}$ is nonsingular then all $\lambda_{i}$ are nonzero, and letting $\phi(\lambda)=$ $1 / \lambda$, we have</p><p>\begin{equation}
F(\mathbf{\Lambda})=\mathbf{\Lambda}^{-1}=\left(\begin{array}{cccc}
1 / \lambda_{1} & 0 & \ldots & 0 \\
0 & 1 / \lambda_{2} & \ldots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \ldots & 1 / \lambda_{n}
\end{array}\right)
\end{equation}</p><p>and hence $\mathbf{A}^{-1}=\mathbf{S} \mathbf{\Lambda}^{-1} \mathbf{S}^{\mathrm{T}}$. To check, we have</p><p>\begin{equation}
\mathbf{A} \mathbf{A}^{-1}=\mathbf{S} \mathbf{\Lambda} \mathbf{S}^{\mathrm{T}} \mathbf{S} \mathbf{\Lambda}^{-1} \mathbf{S}^{\mathrm{T}}=\mathbf{S} \mathbf{\Lambda} \mathbf{\Lambda}^{-1} \mathbf{S}^{\mathrm{T}}=\mathbf{S} \mathbf{S}^{\mathrm{T}}=\mathbf{I}_{n}
\end{equation}</p><p>as we should.
Similarly, if $\mathbf{A}$ is positive semidefinite, then $\mathbf{A}^{1 / 2}=\mathbf{S} \mathbf{\Lambda}^{1 / 2} \mathbf{S}^{\mathrm{T}}$ and</p><p>$$
\mathbf{A}^{1 / 2} \mathbf{A}^{1 / 2}=\mathbf{S} \boldsymbol{\Lambda}^{1 / 2} \mathbf{S}^{\mathrm{T}} \mathbf{S} \boldsymbol{\Lambda}^{1 / 2} \mathbf{S}^{\mathrm{T}}=\mathbf{S} \boldsymbol{\Lambda}^{1 / 2} \boldsymbol{\Lambda}^{1 / 2} \mathbf{S}^{\mathrm{T}}=\mathbf{S} \boldsymbol{\Lambda} \mathbf{S}^{\mathrm{T}}=\mathbf{A},
$$</p><p>again as it should be. Also, when $\mathbf{A}$ is positive definite (hence nonsingular), then $\mathbf{A}^{-1 / 2}=\mathbf{S} \boldsymbol{\Lambda}^{-1 / 2} \mathbf{S}^{\mathrm{T}}$ and</p><p>$$
\begin{aligned}
\left(\mathbf{A}^{1 / 2}\right)^{-1} & =\left(\mathbf{S} \boldsymbol{\Lambda}^{1 / 2} \mathbf{S}^{\mathrm{T}}\right)^{-1}=\mathbf{S}\left(\boldsymbol{\Lambda}^{1 / 2}\right)^{-1} \mathbf{S}^{\mathrm{T}}=\mathbf{S}\left(\boldsymbol{\Lambda}^{-1}\right)^{1 / 2} \mathbf{S}^{\mathrm{T}} \\
& =\left(\mathbf{S} \boldsymbol{\Lambda}^{-1} \mathbf{S}^{\mathrm{T}}\right)^{1 / 2}=\left(\mathbf{A}^{-1}\right)^{1 / 2}
\end{aligned}
$$</p><p>so that this expression is unambiguously defined.
Symmetric matrix functions are thus always defined through their eigenvalues. For example, the logarithm or exponential of $\mathbf{A}$ is not the matrix with elements $\log a_{i j}$ or $e^{a_{i j}}$, but rather a matrix whose eigenvalues are $\log \lambda_{i}$ or $e^{\lambda_{i}}$ and whose eigenvectors are the same as the eigenvectors of $\mathbf{A}$. This is similar to the definition of a positive definite matrix, which is not a matrix all whose elements are positive, but rather a matrix all whose eigenvalues are positive.</p><h2 id=miscellaneous-exercises>Miscellaneous exercises
<a class=anchor href=#miscellaneous-exercises>#</a></h2><p><strong>Exercises</strong></p><ol><li>If $\mathbf{A}$ and $\mathbf{B}$ are square matrices such that $\mathbf{A}\mathbf{B}=0, \mathbf{A} \neq 0, \mathbf{B} \neq 0$, then prove that $|\mathbf{A}|=|\mathbf{B}|=0$.</li><li>If $\mathbf{x}$ and $\mathbf{y}$ are vectors of the same order, prove that $\mathbf{x}^{\mathrm{T}} \mathbf{y}=\operatorname{tr} \mathbf{y} \mathbf{x}^{\mathrm{T}}$.</li><li>Let</li></ol><p>$$
\mathbf{A}=\left(\begin{array}{ll}
\mathbf{A}_{11} & \mathbf{A}_{12} \\
\mathbf{A}_{21} & \mathbf{A}_{22}
\end{array}\right)
$$</p><p>Show that</p><p>$$
|\mathbf{A}|=\left|\mathbf{A}_{11}\right|\left|\mathbf{A}_{22}-\mathbf{A}_{21} \mathbf{A}_{11}^{-1} \mathbf{A}_{12}\right|
$$</p><p>if $\mathbf{A}_{11}$ is nonsingular, and</p><p>$$
|\mathbf{A}|=\left|\mathbf{A}_{22}\right|\left|\mathbf{A}_{11}-\mathbf{A}_{12} \mathbf{A}_{22}^{-1} \mathbf{A}_{21}\right|
$$</p><p>if $\mathbf{A}_{22}$ is nonsingular.
4. Show that $(\mathbf{I}-\mathbf{A}\mathbf{B})^{-1}=\mathbf{I}+\mathbf{A}(\mathbf{I}-\mathbf{B}\mathbf{A})^{-1}\mathbf{B}$, if the inverses exist.
5. Show that</p><p>$$
(\alpha \mathbf{I}-\mathbf{A})^{-1}-(\beta \mathbf{I}-\mathbf{A})^{-1}=(\beta-\alpha)(\beta \mathbf{I}-\mathbf{A})^{-1}(\alpha \mathbf{I}-\mathbf{A})^{-1} .
$$</p><ol start=6><li>If $\mathbf{A}$ is positive definite, show that $\mathbf{A}+\mathbf{A}^{-1}-2 \mathbf{I}$ is positive semidefinite.</li><li>For any symmetric matrices $\mathbf{A}$ and $\mathbf{B}$, show that $\mathbf{A}\mathbf{B}-\mathbf{B}\mathbf{A}$ is skewsymmetric.</li><li>Let $\mathbf{A}$ and $\mathbf{B}$ be two $m \times n$ matrices of rank $r$. If $\mathbf{A}\mathbf{A}^{\mathrm{T}}=\mathbf{B}\mathbf{B}^{\mathrm{T}}$ then $\mathbf{A}=\mathbf{B}\mathbf{Q}$, where $\mathbf{Q}\mathbf{Q}^{\mathrm{T}}$ (and hence $\mathbf{Q}^{\mathrm{T}} \mathbf{Q}$ ) is idempotent of rank $k \geq r$ (Neudecker and van de Velden 2000).</li><li>Let $\mathbf{A}$ be an $m \times n$ matrix partitioned as $\mathbf{A}=\left(\mathbf{A}_{1}: \mathbf{A}_{2}\right)$ and satisfying $\mathbf{A}_{1}^{\mathrm{T}} \mathbf{A}_{2}=0$ and $r\left(\mathbf{A}_{1}\right)+r\left(\mathbf{A}_{2}\right)=m$. Then, for any positive semidefinite matrix $\mathbf{V}$, we have</li></ol><p>$$
r(\mathbf{V})=r\left(\mathbf{A}_{1}\right)+r\left(\mathbf{A}_{2}^{\mathrm{T}} \mathbf{V} \mathbf{A}_{2}\right) \Longleftrightarrow r(\mathbf{V})=r\left(\mathbf{V}: \mathbf{A}_{1}\right)
$$</p><ol start=10><li>Prove that the eigenvalues $\lambda_{i}$ of $(\mathbf{A}+\mathbf{B})^{-1} \mathbf{A}$, where $\mathbf{A}$ is positive semidefinite and $\mathbf{B}$ is positive definite, satisfy $0 \leq \lambda_{i}&lt;1$.</li><li>Let $\mathbf{x}$ and $\mathbf{y}$ be $n \times 1$ vectors. Prove that $\mathbf{x} \mathbf{y}^{\mathrm{T}}$ has $n-1$ zero eigenvalues and one eigenvalue $\mathbf{x}^{\mathrm{T}} \mathbf{y}$.</li><li>Show that $\left|\mathbf{I}+\mathbf{x} \mathbf{y}^{\mathrm{T}}\right|=1+\mathbf{x}^{\mathrm{T}} \mathbf{y}$.</li><li>Let $\mu=1+\mathbf{x}^{\mathrm{T}} \mathbf{y}$. If $\mu \neq 0$, show that $\left(\mathbf{I}+\mathbf{x} \mathbf{y}^{\mathrm{T}}\right)^{-1}=\mathbf{I}-(1 / \mu) \mathbf{x} \mathbf{y}^{\mathrm{T}}$.</li><li>Show that $\left(\mathbf{I}+\mathbf{A} \mathbf{A}^{\mathrm{T}}\right)^{-1} \mathbf{A}=\mathbf{A}\left(\mathbf{I}+\mathbf{A}^{\mathrm{T}} \mathbf{A}\right)^{-1}$.</li><li>Show that $\mathbf{A}\left(\mathbf{A}^{\mathrm{T}} \mathbf{A}\right)^{1 / 2}=\left(\mathbf{A} \mathbf{A}^{\mathrm{T}}\right)^{1 / 2} \mathbf{A}$.</li><li>(Monotonicity of the entropic complexity.) Let $\mathbf{A}_{n}$ be a positive definite $n \times n$ matrix and define</li></ol><p>$$
\phi(n)=\frac{n}{2} \log \operatorname{tr}\left(\mathbf{A}_{n} / n\right)-\frac{1}{2} \log \left|\mathbf{A}_{n}\right| .
$$</p><p>Let $\mathbf{A}_{n+1}$ be a positive definite $(n+1) \times(n+1)$ matrix such that</p><p>$$
\mathbf{A}_{n+1}=\left(\begin{array}{cc}
\mathbf{A}_{n} & \mathbf{a}_{n} \\
\mathbf{a}_{n}^{\mathrm{T}} & \alpha_{n}
\end{array}\right)
$$</p><p>Then,</p><p>$$
\phi(n+1) \geq \phi(n)
$$</p><p>with equality if and only if</p><p>$$
\mathbf{a}_{n}=0, \quad \alpha_{n}=\operatorname{tr} \mathbf{A}_{n} / n
$$</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#1---introduction>1 - Introduction</a></li><li><a href=#2---sets>2 - Sets</a></li><li><a href=#3---matrices-addition-and-multiplication>3 - Matrices: Addition and Multiplication</a></li><li><a href=#4---the-transpose-of-a-matrix>4 - The transpose of a matrix</a></li><li><a href=#5---square-matrices>5 - Square matrices</a></li><li><a href=#6----linear-forms-and-quadratic-forms>6 - Linear forms and quadratic forms</a></li><li><a href=#7---the-rank-of-a-matrix>7 - The rank of a matrix</a></li><li><a href=#8---the-inverse>8 - The Inverse</a></li><li><a href=#9---the-determinant>9 - The Determinant</a></li><li><a href=#10---the-trace>10 - The trace</a></li><li><a href=#11----partitioned-matrices>11 - Partitioned matrices</a></li><li><a href=#12---complex-matrices>12 - Complex Matrices</a></li><li><a href=#13----eigenvalues-and-eigenvectors>13 - Eigenvalues and Eigenvectors</a></li><li><a href=#14---schurs-decomposition-theorem>14 - Schur&rsquo;s decomposition theorem</a></li><li><a href=#15---the-jordan-decomposition>15 - The Jordan decomposition</a></li><li><a href=#16---the-singular-value-decomposition>16 - The singular value decomposition</a></li><li><a href=#17---further-results-concerning-eigenvalues>17 - Further results concerning eigenvalues</a></li><li><a href=#positive-semidefinite-matrices>Positive (semi)definite matrices</a></li><li><a href=#three-further-results-for-positive-definite-matrices>Three further results for positive definite matrices</a></li><li><a href=#a-useful-result>A useful result</a></li><li><a href=#symmetric-matrix-functions>Symmetric matrix functions</a></li><li><a href=#miscellaneous-exercises>Miscellaneous exercises</a></li></ul></nav></div></aside></main></body></html>