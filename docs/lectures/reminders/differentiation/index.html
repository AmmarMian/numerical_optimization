<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Differentiation in Multiple Dimensions
  #


  1 - Introduction
  #


Differentiation provides the mathematical framework for understanding how functions change locally. While single-variable calculus introduces derivatives, most applications require working with functions of multiple variables. This chapter extends differentiation concepts to multivariate and matrix-valued functions, building the tools needed for optimization and analysis in higher dimensions.


  2 - Monovariate Reminders
  #


  Derivative of a Function
  #












  Definition 0.1 (Derivative)"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/reminders/differentiation/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="Differentiation"><meta property="og:description" content="Differentiation in Multiple Dimensions # 1 - Introduction # Differentiation provides the mathematical framework for understanding how functions change locally. While single-variable calculus introduces derivatives, most applications require working with functions of multiple variables. This chapter extends differentiation concepts to multivariate and matrix-valued functions, building the tools needed for optimization and analysis in higher dimensions.
2 - Monovariate Reminders # Derivative of a Function # Definition 0.1 (Derivative)"><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Differentiation | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/reminders/differentiation/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.f28b8237862d934dafc8da537aa3936a35a91e90c6c9ff097abeb367aa09a7f9.js integrity="sha256-8ouCN4Ytk02vyNpTeqOTajWpHpDGyf8Jer6zZ6oJp/k=" crossorigin=anonymous></script><script>const chapterNum=0;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_projected/>5b. Constrained optimization - Projected Gradient Descent</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimization_linear/>6. Constrained optimization - Linear programming</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/fundamentals/>1. Machine learning fundamentals</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Classification and support vector machines</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle checked>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/ class=active>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - MNIST and Fashion-MNIST Classification</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li><li><a href=/numerical_optimization/docs/practical_labs/quasinewton/>Quasi-Newton methods memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>Differentiation</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#1---introduction>1 - Introduction</a></li><li><a href=#2---monovariate-reminders>2 - Monovariate Reminders</a><ul><li><a href=#derivative-of-a-function>Derivative of a Function</a></li></ul></li><li><a href=#3---extension-to-multivariate-setup-fmathbbrd-to-mathbbr>3 - Extension to Multivariate Setup: $f:\mathbb{R}^d \to \mathbb{R}$</a><ul><li><a href=#limits-and-continuity>Limits and Continuity</a></li><li><a href=#directional-derivative>Directional Derivative</a></li><li><a href=#gradient>Gradient</a></li><li><a href=#gradient-and-partial-derivatives>Gradient and Partial Derivatives</a></li><li><a href=#gradient-properties-and-practical-computation>Gradient Properties and Practical Computation</a></li><li><a href=#hessian-matrix>Hessian Matrix</a></li><li><a href=#hessian-matrix-properties>Hessian Matrix Properties</a></li></ul></li><li><a href=#4---multivariate-case-fmathbbrd-to-mathbbrp>4 - Multivariate Case: $f:\mathbb{R}^d \to \mathbb{R}^p$</a><ul><li><a href=#multivariate-functions>Multivariate Functions</a></li><li><a href=#gradient-and-jacobian>Gradient and Jacobian</a></li><li><a href=#jacobian-and-directional-derivative>Jacobian and Directional Derivative</a></li><li><a href=#chain-rule-for-composition-of-functions>Chain Rule for Composition of Functions</a></li><li><a href=#chain-rule-special-cases>Chain Rule: Special Cases</a></li><li><a href=#worked-examples>Worked Examples</a></li></ul></li><li><a href=#5---matrix-functions-fmathbbrm-times-n-to-mathbbr>5 - Matrix Functions: $f:\mathbb{R}^{m \times n} \to \mathbb{R}$</a><ul><li><a href=#fréchet-derivative>Fréchet Derivative</a></li><li><a href=#matrix-to-scalar-functions>Matrix-to-Scalar Functions</a></li><li><a href=#examples-of-matrix-to-scalar-functions>Examples of Matrix-to-Scalar Functions</a></li></ul></li><li><a href=#6---matrix-functions-fmathbbrm-times-n-to-mathbbrp-times-q>6 - Matrix Functions: $f:\mathbb{R}^{m \times n} \to \mathbb{R}^{p \times q}$</a><ul><li><a href=#matrix-to-matrix-functions>Matrix-to-Matrix Functions</a></li><li><a href=#vectorization-identities>Vectorization Identities</a></li><li><a href=#examples-of-matrix-to-matrix-functions>Examples of Matrix-to-Matrix Functions</a></li><li><a href=#properties-of-matrix-function-derivatives>Properties of Matrix Function Derivatives</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=differentiation-in-multiple-dimensions>Differentiation in Multiple Dimensions
<a class=anchor href=#differentiation-in-multiple-dimensions>#</a></h1><h2 id=1---introduction>1 - Introduction
<a class=anchor href=#1---introduction>#</a></h2><blockquote><p>Differentiation provides the mathematical framework for understanding how functions change locally. While single-variable calculus introduces derivatives, most applications require working with functions of multiple variables. This chapter extends differentiation concepts to multivariate and matrix-valued functions, building the tools needed for optimization and analysis in higher dimensions.</p></blockquote><h2 id=2---monovariate-reminders>2 - Monovariate Reminders
<a class=anchor href=#2---monovariate-reminders>#</a></h2><h3 id=derivative-of-a-function>Derivative of a Function
<a class=anchor href=#derivative-of-a-function>#</a></h3><div id=derivative_definition class=theorem-box><p class=theorem-title><strong>Definition 0.1 (Derivative)</strong></p><div class=theorem-content>The <strong>derivative</strong> of a function $f:\mathbb{R}\to\mathbb{R}$ at a point $x_0$ is defined as:
$$f&rsquo;(x_0) = \lim_{h\to 0} \frac{f(x_0 + h) - f(x_0)}{h}$$
provided this limit exists.</div></div><p>The derivative represents the instantaneous rate of change of the function at a specific point. Geometrically, $f&rsquo;(x_0)$ gives the slope of the tangent line to the curve $y = f(x)$ at the point $(x_0, f(x_0))$. This tangent line provides the best linear approximation to the function near $x_0$.</p><p>For practical computation, we use two fundamental rules:</p><ul><li><strong>Product rule</strong>: $(uv)&rsquo; = u&rsquo;v + uv'$</li><li><strong>Chain rule</strong>: $(f(g(x)))&rsquo; = f&rsquo;(g(x))g&rsquo;(x)$</li></ul><p>These rules allow us to differentiate complex expressions by breaking them down into simpler components.</p><h2 id=3---extension-to-multivariate-setup-fmathbbrd-to-mathbbr>3 - Extension to Multivariate Setup: $f:\mathbb{R}^d \to \mathbb{R}$
<a class=anchor href=#3---extension-to-multivariate-setup-fmathbbrd-to-mathbbr>#</a></h2><h3 id=limits-and-continuity>Limits and Continuity
<a class=anchor href=#limits-and-continuity>#</a></h3><div id=open_disk class=theorem-box><p class=theorem-title><strong>Definition 0.2 (Open Disk)</strong></p><div class=theorem-content>An <strong>open disk</strong> of radius $\epsilon > 0$ centered at a point $\mathbf{x}_0 \in \mathbb{R}^d$ is defined as:
$$\mathcal{B}(\mathbf{x}_0, \epsilon) = {\mathbf{x} \in \mathbb{R}^d : |\mathbf{x} - \mathbf{x}_0|_2 &lt; \epsilon}$$</div></div><div id=limit_definition class=theorem-box><p class=theorem-title><strong>Definition 0.3 (Limit)</strong></p><div class=theorem-content>The <strong>limit</strong> of a function $f:\mathbb{R}^d \to \mathbb{R}$ at a point $\mathbf{x}_0$ is defined as:
$$\lim_{\mathbf{x} \to \mathbf{x}_0} f(\mathbf{x}) = L$$
if for every $\epsilon > 0$, there exists $\delta > 0$ such that whenever $|\mathbf{x} - \mathbf{x}_0|_2 &lt; \delta$, we have $|f(\mathbf{x}) - L| &lt; \epsilon$.</div></div><p>A function is <strong>continuous</strong> at a point $\mathbf{x}_0$ if $\lim_{\mathbf{x} \to \mathbf{x}_0} f(\mathbf{x}) = f(\mathbf{x}_0)$. These definitions generalize the single-variable concepts using the Euclidean norm to measure distances in $\mathbb{R}^d$.</p><h3 id=directional-derivative>Directional Derivative
<a class=anchor href=#directional-derivative>#</a></h3><div id=directional_derivative class=theorem-box><p class=theorem-title><strong>Definition 0.4 (Directional Derivative)</strong></p><div class=theorem-content>The <strong>directional derivative</strong> of a function $f:\mathbb{R}^d \to \mathbb{R}$ at a point $\mathbf{x}_0$ in the direction of a vector $\mathbf{v} \in \mathbb{R}^d$ is defined as:
$$Df(\mathbf{x}_0)[\mathbf{v}] = \lim_{h \to 0} \frac{f(\mathbf{x}_0 + h\mathbf{v}) - f(\mathbf{x}_0)}{h}$$</div></div><p>When $|\mathbf{v}|_2 = 1$, the directional derivative $Df(\mathbf{x}_0)[\mathbf{v}]$ represents the rate of change of $f$ in the direction of $\mathbf{v}$ at the point $\mathbf{x}_0$. This generalizes the concept of derivative to any direction in the input space.</p><p>We also use the notation $\nabla_{\mathbf{v}}f(\mathbf{x}_0)$ for the directional derivative.</p><h3 id=gradient>Gradient
<a class=anchor href=#gradient>#</a></h3><div id=gradient_definition class=theorem-box><p class=theorem-title><strong>Definition 0.5 (Gradient)</strong></p><div class=theorem-content>The <strong>gradient</strong> of a function $f:\mathbb{R}^d \to \mathbb{R}$ at a point $\mathbf{x}_0$ is defined as the vector of all directional derivatives in the standard basis directions:
$$\nabla f(\mathbf{x}_0) = \left( Df(\mathbf{x}_0)[\mathbf{e}_1], Df(\mathbf{x}_0)[\mathbf{e}_2], \ldots, Df(\mathbf{x}_0)[\mathbf{e}_d] \right)^\mathrm{T}$$
where ${\mathbf{e}_1, \ldots, \mathbf{e}_d}$ is the standard basis of $\mathbb{R}^d$.</div></div><p>The gradient points in the direction of steepest ascent of the function $f$ at the point $\mathbf{x}_0$. It encodes all the first-order information about how the function changes locally.</p><p>For any vector $\mathbf{v} \in \mathbb{R}^d$, the directional derivative can be expressed as:
$$Df(\mathbf{x}_0)[\mathbf{v}] = \nabla f(\mathbf{x}_0)^\mathrm{T} \mathbf{v}$$</p><p>This shows that the gradient contains all the information needed to compute directional derivatives in any direction.</p><h3 id=gradient-and-partial-derivatives>Gradient and Partial Derivatives
<a class=anchor href=#gradient-and-partial-derivatives>#</a></h3><div id=partial_derivative class=theorem-box><p class=theorem-title><strong>Definition 0.6 (Partial Derivative)</strong></p><div class=theorem-content>The <strong>partial derivative</strong> of a function $f:\mathbb{R}^d \to \mathbb{R}$ with respect to the $i$-th variable is defined as:
$$\frac{\partial f}{\partial x_i}(\mathbf{x}_0) = \lim_{h \to 0} \frac{f(\mathbf{x}_0 + h\mathbf{e}_i) - f(\mathbf{x}_0)}{h}$$
where $\mathbf{e}_i$ is the $i$-th standard basis vector.</div></div><p>The gradient can be expressed in terms of partial derivatives as:
$$\nabla f(\mathbf{x}_0) = \left( \frac{\partial f}{\partial x_1}(\mathbf{x}_0), \frac{\partial f}{\partial x_2}(\mathbf{x}_0), \ldots, \frac{\partial f}{\partial x_d}(\mathbf{x}_0) \right)^\mathrm{T}$$</p><p>This representation makes it clear that the gradient is a vector containing all the partial derivatives of the function at the point $\mathbf{x}_0$.</p><h3 id=gradient-properties-and-practical-computation>Gradient Properties and Practical Computation
<a class=anchor href=#gradient-properties-and-practical-computation>#</a></h3><p>When computing gradients in practice, we use the following rules:</p><div id=theorem-0-1 class=theorem-box><p class=theorem-title><strong>Theorem 0.1 (Product Rule for Gradients)</strong></p><div class=theorem-content>Let $g:\mathbb{R}^d \to \mathbb{R}$ and $h:\mathbb{R}^d \to \mathbb{R}$ be two functions. Then the gradient of their product $f(\mathbf{x}) = g(\mathbf{x})h(\mathbf{x})$ is:
$$\nabla f(\mathbf{x}) = g(\mathbf{x})\nabla h(\mathbf{x}) + h(\mathbf{x})\nabla g(\mathbf{x})$$</div></div><div id=theorem-0-2 class=theorem-box><p class=theorem-title><strong>Theorem 0.2 (Chain Rule for Gradients)</strong></p><div class=theorem-content><p>For composition of functions, we have two main cases:</p><ol><li>If $f=h\circ g$ where $h:\mathbb{R}\to\mathbb{R}$ and $g:\mathbb{R}^d\to\mathbb{R}$, then:
$$\nabla f(\mathbf{x}) = h&rsquo;(g(\mathbf{x}))\nabla g(\mathbf{x})$$
where $h&rsquo;$ is the derivative of $h$.</li><li>If $f=h\circ g$ where $h:\mathbb{R}^d\to\mathbb{R}$ and $g:\mathbb{R}^{d&rsquo;}\to\mathbb{R}^d$, we need the more general chain rule discussed later.</li></ol></div></div><h3 id=hessian-matrix>Hessian Matrix
<a class=anchor href=#hessian-matrix>#</a></h3><div id=hessian_definition class=theorem-box><p class=theorem-title><strong>Definition 0.7 (Hessian Matrix)</strong></p><div class=theorem-content>The <strong>Hessian matrix</strong> of a function $f:\mathbb{R}^d \to \mathbb{R}$ at a point $\mathbf{x}_0$ is defined as the square matrix of second-order partial derivatives:
$$\mathbf{H}(\mathbf{x}_0) = \begin{pmatrix}
\frac{\partial^2 f}{\partial x_1^2}(\mathbf{x}_0) & \frac{\partial^2 f}{\partial x_1 \partial x_2}(\mathbf{x}_0) & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_d}(\mathbf{x}_0) \\
\frac{\partial^2 f}{\partial x_2 \partial x_1}(\mathbf{x}_0) & \frac{\partial^2 f}{\partial x_2^2}(\mathbf{x}_0) & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_d}(\mathbf{x}_0) \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_d \partial x_1}(\mathbf{x}_0) & \frac{\partial^2 f}{\partial x_d \partial x_2}(\mathbf{x}_0) & \cdots & \frac{\partial^2 f}{\partial x_d^2}(\mathbf{x}_0)
\end{pmatrix}$$</div></div><p>The Hessian matrix captures the second-order behavior of the function, providing information about its curvature at the point $\mathbf{x}_0$.</p><p><strong>Exercise 1</strong>: Compute the gradient and Hessian matrix of the function $f(x,y) = x^2 + 3xy + y^2$ at the point $(1,2)$.</p><p><strong>Exercise 2</strong>: Using the chain rule, compute the gradient of $f(\mathbf{x}) = \left(\sum_{i=1}^{d}x_i^2\right)^{1/2}$.</p><h3 id=hessian-matrix-properties>Hessian Matrix Properties
<a class=anchor href=#hessian-matrix-properties>#</a></h3><p>The Hessian matrix has several important properties:</p><ul><li><p><strong>Symmetry</strong>: If $f$ is twice continuously differentiable, then $\mathbf{H}(\mathbf{x}_0) = \mathbf{H}(\mathbf{x}_0)^\mathrm{T}$ because mixed partial derivatives are equal: $\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$.</p></li><li><p><strong>Curvature information</strong>: The eigenvalues of the Hessian determine the local curvature:</p><ul><li>All eigenvalues positive: $f$ is locally convex at $\mathbf{x}_0$</li><li>All eigenvalues negative: $f$ is locally concave at $\mathbf{x}_0$</li><li>Mixed positive and negative eigenvalues: $f$ has a saddle point at $\mathbf{x}_0$</li></ul></li></ul><p><strong>Exercise 3 (Rosenbrock function)</strong>: The Rosenbrock function is defined as:
$$f(x,y) = (a - x)^2 + b(y - x^2)^2$$
where $a$ and $b$ are constants (commonly $a=1$ and $b=100$).</p><ol><li>Compute the gradient $\nabla f(x,y)$ and find stationary points.</li><li>Compute the Hessian matrix $\mathbf{H}(x,y)$ and analyze local curvature at the stationary points.</li></ol><h2 id=4---multivariate-case-fmathbbrd-to-mathbbrp>4 - Multivariate Case: $f:\mathbb{R}^d \to \mathbb{R}^p$
<a class=anchor href=#4---multivariate-case-fmathbbrd-to-mathbbrp>#</a></h2><h3 id=multivariate-functions>Multivariate Functions
<a class=anchor href=#multivariate-functions>#</a></h3><div id=vector_valued_function class=theorem-box><p class=theorem-title><strong>Definition 0.8 (Vector-Valued Function)</strong></p><div class=theorem-content>A <strong>vector-valued function</strong> $f:\mathbb{R}^d \to \mathbb{R}^p$ maps a vector $\mathbf{x} \in \mathbb{R}^d$ to a vector $\mathbf{y} \in \mathbb{R}^p$. We can write:
$$f(\mathbf{x}) = \begin{pmatrix}
f_1(\mathbf{x}) \\
f_2(\mathbf{x}) \\
\vdots \\
f_p(\mathbf{x})
\end{pmatrix}$$
where each component $f_i:\mathbb{R}^d \to \mathbb{R}$ is a scalar function.</div></div><h3 id=gradient-and-jacobian>Gradient and Jacobian
<a class=anchor href=#gradient-and-jacobian>#</a></h3><p>For scalar-valued functions, we defined the gradient. For vector-valued functions, we need the Jacobian matrix.</p><div id=jacobian_definition class=theorem-box><p class=theorem-title><strong>Definition 0.9 (Jacobian Matrix)</strong></p><div class=theorem-content>The <strong>Jacobian matrix</strong> of a function $f:\mathbb{R}^d \to \mathbb{R}^p$ is defined as:
$$\mathbf{J}_f(\mathbf{x}) = \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_d} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_d} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_p}{\partial x_1} & \frac{\partial f_p}{\partial x_2} & \cdots & \frac{\partial f_p}{\partial x_d}
\end{pmatrix} \in \mathbb{R}^{p \times d}$$</div></div><p>The Jacobian matrix generalizes the gradient to vector-valued functions. Each row is the gradient of one component function.</p><h3 id=jacobian-and-directional-derivative>Jacobian and Directional Derivative
<a class=anchor href=#jacobian-and-directional-derivative>#</a></h3><p>The directional derivative of a vector-valued function $f:\mathbb{R}^d \to \mathbb{R}^p$ in the direction of a vector $\mathbf{v} \in \mathbb{R}^d$ is:
$$Df(\mathbf{x})[\mathbf{v}] = \mathbf{J}_f(\mathbf{x})\mathbf{v} = \begin{pmatrix} \nabla f_1(\mathbf{x})^T \mathbf{v} \\ \nabla f_2(\mathbf{x})^T \mathbf{v} \\ \vdots \\ \nabla f_p(\mathbf{x})^T \mathbf{v} \end{pmatrix} \in \mathbb{R}^p$$</p><p>This shows how the Jacobian matrix encodes all directional derivative information.</p><h3 id=chain-rule-for-composition-of-functions>Chain Rule for Composition of Functions
<a class=anchor href=#chain-rule-for-composition-of-functions>#</a></h3><div id=theorem-0-3 class=theorem-box><p class=theorem-title><strong>Theorem 0.3 (General Chain Rule)</strong></p><div class=theorem-content>If $f:\mathbb{R}^d \to \mathbb{R}^p$ and $g:\mathbb{R}^m \to \mathbb{R}^d$, then the composition $h = f \circ g : \mathbb{R}^m \to \mathbb{R}^p$ is defined as:
$$h(\mathbf{y}) = f(g(\mathbf{y}))$$
The Jacobian of $h$ can be computed using the chain rule:
$$\mathbf{J}_h(\mathbf{y}) = \mathbf{J}_f(g(\mathbf{y})) \mathbf{J}_g(\mathbf{y})$$
where $\mathbf{J}_h(\mathbf{y}) \in \mathbb{R}^{p \times m}$, $\mathbf{J}_f(g(\mathbf{y})) \in \mathbb{R}^{p \times d}$, and $\mathbf{J}_g(\mathbf{y}) \in \mathbb{R}^{d \times m}$.</div></div><h3 id=chain-rule-special-cases>Chain Rule: Special Cases
<a class=anchor href=#chain-rule-special-cases>#</a></h3><p><strong>Case 1</strong>: If $f:\mathbb{R}^d \to \mathbb{R}$ and $g:\mathbb{R}^m \to \mathbb{R}^d$, then for $h = f \circ g : \mathbb{R}^m \to \mathbb{R}$:
$$\nabla h(\mathbf{y}) = \mathbf{J}_g(\mathbf{y})^T \nabla f(g(\mathbf{y}))$$</p><p><strong>Case 2</strong>: If $f:\mathbb{R} \to \mathbb{R}$ and $g:\mathbb{R}^m \to \mathbb{R}$, then for $h = f \circ g : \mathbb{R}^m \to \mathbb{R}$:
$$\nabla h(\mathbf{y}) = f&rsquo;(g(\mathbf{y})) \nabla g(\mathbf{y})$$</p><h3 id=worked-examples>Worked Examples
<a class=anchor href=#worked-examples>#</a></h3><p><strong>Example 1</strong>: Given:</p><ul><li>$f(\mathbf{x}) = \mathbf{x}^T\mathbf{x}$ where $f: \mathbb{R}^2 \to \mathbb{R}$</li><li>$g(\mathbf{y}) = \begin{pmatrix} y_1 + y_2 \\ y_1 - y_2 \end{pmatrix}$ where $g: \mathbb{R}^2 \to \mathbb{R}^2$</li><li>$h = f \circ g$</li></ul><p>Find $\nabla h(\mathbf{y})$ using the chain rule.</p><p><strong>Solution</strong>:</p><ol><li>First, $\nabla f(\mathbf{x}) = 2\mathbf{x}$</li><li>The Jacobian is $\mathbf{J}_g(\mathbf{y}) = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}$</li><li>Applying the chain rule:
$$\nabla h(\mathbf{y}) = \mathbf{J}_g(\mathbf{y})^T \nabla f(g(\mathbf{y})) = \begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix} \cdot 2g(\mathbf{y})$$
$$= 2\begin{pmatrix} 1 & 1 \\ 1 & -1 \end{pmatrix}\begin{pmatrix} y_1 + y_2 \ y_1 - y_2 \end{pmatrix} = \begin{pmatrix} 4y_1 \ 4y_2 \end{pmatrix}$$</li></ol><p><strong>Example 2 (General Quadratic Forms)</strong>: Given:</p><ul><li>$f(\mathbf{x}) = \mathbf{x}^T\mathbf{A}\mathbf{x} + \mathbf{b}^T\mathbf{x}$ where $\mathbf{A}$ is symmetric</li><li>$g(\mathbf{y}) = \mathbf{C}\mathbf{y}$ (linear transformation)</li></ul><p>Find $\nabla h(\mathbf{y})$ for $h = f \circ g$.</p><p><strong>Solution</strong>:</p><ol><li>$\nabla f(\mathbf{x}) = 2\mathbf{A}\mathbf{x} + \mathbf{b}$</li><li>$\mathbf{J}_g(\mathbf{y}) = \mathbf{C}$</li><li>Therefore:
$$\nabla h(\mathbf{y}) = \mathbf{C}^T [2\mathbf{A}(\mathbf{C}\mathbf{y}) + \mathbf{b}] = 2\mathbf{C}^T\mathbf{A}\mathbf{C}\mathbf{y} + \mathbf{C}^T\mathbf{b}$$</li></ol><h2 id=5---matrix-functions-fmathbbrm-times-n-to-mathbbr>5 - Matrix Functions: $f:\mathbb{R}^{m \times n} \to \mathbb{R}$
<a class=anchor href=#5---matrix-functions-fmathbbrm-times-n-to-mathbbr>#</a></h2><h3 id=fréchet-derivative>Fréchet Derivative
<a class=anchor href=#fr%c3%a9chet-derivative>#</a></h3><div id=frechet_differentiability class=theorem-box><p class=theorem-title><strong>Definition 0.10 (Fréchet Differentiability)</strong></p><div class=theorem-content>A function $f:\mathbb{R}^{m \times n}\to\mathbb{R}^{p \times q}$ is <strong>Fréchet differentiable</strong> at $\mathbf{X}$ if there exists a linear mapping $Df(\mathbf{X}):\mathbb{R}^{m \times n}\to\mathbb{R}^{p \times q}$ such that
$$\lim_{|\mathbf{V}|_F\to 0} \frac{|f(\mathbf{X}+\mathbf{V}) - f(\mathbf{X}) - Df(\mathbf{X})[\mathbf{V}]|_F}{|\mathbf{V}|_F} = 0$$</div></div><p>The Fréchet derivative can also be characterized using the <strong>Gateaux derivative</strong>:
$$Df(\mathbf{X})[\mathbf{V}] = \left.\frac{d}{dt}\right|_{t=0} f(\mathbf{X}+t\mathbf{V}) = \lim_{t\to 0} \frac{f(\mathbf{X}+t\mathbf{V}) - f(\mathbf{X})}{t}$$</p><p>If this limit is not linear in $\mathbf{V}$, then $f$ is not Fréchet differentiable.</p><p>Often it is useful to see this derivative as a linear operator such that:
$$ \mathbf{D} f(\mathbf{X})[\boldsymbol{\xi}] = f(\mathbf{X}+\mathbf{\xi}) - f(\mathbf{X}) + o(\lVert\boldsymbol{\xi}\rVert)$$</p><h3 id=matrix-to-scalar-functions>Matrix-to-Scalar Functions
<a class=anchor href=#matrix-to-scalar-functions>#</a></h3><p>For a function $f:\mathbb{R}^{m \times n} \to \mathbb{R}$, the directional derivative at $\mathbf{X}$ in direction $\mathbf{V}$ is:
$$Df(\mathbf{X})[\mathbf{V}] = \lim_{h \to 0} \frac{f(\mathbf{X} + h\mathbf{V}) - f(\mathbf{X})}{h}$$</p><div id=matrix_gradient class=theorem-box><p class=theorem-title><strong>Definition 0.11 (Matrix Gradient)</strong></p><div class=theorem-content>For $f:\mathbb{R}^{m \times n} \to \mathbb{R}$, the <strong>gradient</strong> $\nabla f(\mathbf{X}) \in \mathbb{R}^{m \times n}$ satisfies:
$$Df(\mathbf{X})[\mathbf{V}] = \mathrm{Tr}(\nabla f(\mathbf{X})^\mathrm{T} \mathbf{V})$$
where $\mathrm{Tr}(\cdot)$ denotes the trace of a matrix.</div></div><p>The gradient can be computed element-wise as:
$$\nabla f(\mathbf{X}) = \begin{pmatrix}
\frac{\partial f}{\partial X_{11}} & \frac{\partial f}{\partial X_{12}} & \cdots & \frac{\partial f}{\partial X_{1n}} \\
\frac{\partial f}{\partial X_{21}} & \frac{\partial f}{\partial X_{22}} & \cdots & \frac{\partial f}{\partial X_{2n}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f}{\partial X_{m1}} & \frac{\partial f}{\partial X_{m2}} & \cdots & \frac{\partial f}{\partial X_{mn}}
\end{pmatrix}$$</p><h3 id=examples-of-matrix-to-scalar-functions>Examples of Matrix-to-Scalar Functions
<a class=anchor href=#examples-of-matrix-to-scalar-functions>#</a></h3><p><strong>Example 1</strong>: $f(\mathbf{X}) = |\mathbf{X}|_F^2 = \mathrm{Tr}(\mathbf{X}^\mathrm{T}\mathbf{X})$</p><p>Using the Gateaux derivative:
$$Df(\mathbf{X})[\mathbf{V}] = \left.\frac{d}{dt}\right|_{t=0} \mathrm{Tr}((\mathbf{X}+t\mathbf{V})^\mathrm{T}(\mathbf{X}+t\mathbf{V}))$$</p><p>Expanding and differentiating:
$$= \left.\frac{d}{dt}\right|_{t=0} [\mathrm{Tr}(\mathbf{X}^\mathrm{T}\mathbf{X}) + 2t\mathrm{Tr}(\mathbf{X}^\mathrm{T}\mathbf{V}) + t^2\mathrm{Tr}(\mathbf{V}^\mathrm{T}\mathbf{V})]$$
$$= 2\mathrm{Tr}(\mathbf{X}^\mathrm{T}\mathbf{V})$$</p><p>Therefore: $\nabla f(\mathbf{X}) = 2\mathbf{X}$</p><p><strong>Example 2</strong>: $f(\mathbf{X}) = \log\det(\mathbf{X})$ (for invertible $\mathbf{X}$)</p><p>For this function:
$$Df(\mathbf{X})[\mathbf{V}] = \left.\frac{d}{dt}\right|_{t=0} \log\det(\mathbf{X}+t\mathbf{V}) = \mathrm{Tr}(\mathbf{X}^{-1}\mathbf{V})$$</p><p>Therefore: $\nabla f(\mathbf{X}) = \mathbf{X}^{-\mathrm{T}}$</p><h2 id=6---matrix-functions-fmathbbrm-times-n-to-mathbbrp-times-q>6 - Matrix Functions: $f:\mathbb{R}^{m \times n} \to \mathbb{R}^{p \times q}$
<a class=anchor href=#6---matrix-functions-fmathbbrm-times-n-to-mathbbrp-times-q>#</a></h2><h3 id=matrix-to-matrix-functions>Matrix-to-Matrix Functions
<a class=anchor href=#matrix-to-matrix-functions>#</a></h3><p>For a function $f:\mathbb{R}^{m \times n} \to \mathbb{R}^{p \times q}$, the directional derivative $Df(\mathbf{X})[\mathbf{V}]$ is a linear mapping from $\mathbb{R}^{m \times n}$ to $\mathbb{R}^{p \times q}$.</p><p>Since $Df(\mathbf{X})$ is linear, there exists a matrix $\mathbf{M}_{\mathbf{X}} \in \mathbb{R}^{pq \times mn}$ such that:
$$\mathrm{vec}(Df(\mathbf{X})[\mathbf{V}]) = \mathbf{M}_{\mathbf{X}} \mathrm{vec}(\mathbf{V})$$
where $\mathrm{vec}(\cdot)$ stacks matrix columns into a vector.</p><p>This representation transforms the problem of computing matrix derivatives into standard matrix-vector multiplication. The matrix $\mathbf{M}_{\mathbf{X}}$ is sometimes called the <strong>derivative matrix</strong> or <strong>Jacobian matrix</strong> of the vectorized function.</p><p>The power of this representation becomes clear when combined with the Kronecker product identity:</p><div id=theorem-0-4 class=theorem-box><p class=theorem-title><strong>Theorem 0.4 (Kronecker Product Identity)</strong></p><div class=theorem-content>For matrices $\mathbf{A} \in \mathbb{R}^{p \times m}$, $\mathbf{B} \in \mathbb{R}^{n \times q}$, and $\mathbf{X} \in \mathbb{R}^{m \times n}$:
$$\mathrm{vec}(\mathbf{A}\mathbf{X}\mathbf{B}) = (\mathbf{B}^\mathrm{T} \otimes \mathbf{A}) \mathrm{vec}(\mathbf{X})$$</div></div><p><strong>Example</strong>: Consider $f(\mathbf{X}) = \mathbf{A}\mathbf{X}\mathbf{B}$ where $\mathbf{A} \in \mathbb{R}^{p \times m}$ and $\mathbf{B} \in \mathbb{R}^{n \times q}$ are fixed matrices.</p><p>To find the derivative, we compute:
$$Df(\mathbf{X})[\mathbf{V}] = f(\mathbf{X} + \mathbf{V}) - f(\mathbf{X}) = \mathbf{A}(\mathbf{X} + \mathbf{V})\mathbf{B} - \mathbf{A}\mathbf{X}\mathbf{B} = \mathbf{A}\mathbf{V}\mathbf{B}$$</p><p>Using the Kronecker product identity:
$$\mathrm{vec}(Df(\mathbf{X})[\mathbf{V}]) = \mathrm{vec}(\mathbf{A}\mathbf{V}\mathbf{B}) = (\mathbf{B}^\mathrm{T} \otimes \mathbf{A}) \mathrm{vec}(\mathbf{V})$$</p><p>Therefore, $\mathbf{M}_{\mathbf{X}} = \mathbf{B}^\mathrm{T} \otimes \mathbf{A}$, which is independent of $\mathbf{X}$ since $f$ is linear.</p><h3 id=vectorization-identities>Vectorization Identities
<a class=anchor href=#vectorization-identities>#</a></h3><p>Key identities for working with matrix derivatives:</p><ul><li>$\mathrm{vec}(\mathbf{A}\mathbf{B}\mathbf{C}) = (\mathbf{C}^\mathrm{T} \otimes \mathbf{A}) \mathrm{vec}(\mathbf{B})$</li><li>$\mathrm{Tr}(\mathbf{A}\mathbf{B}) = \mathrm{vec}(\mathbf{A})^\mathrm{T}\mathrm{vec}(\mathbf{B})$</li><li>$\mathrm{Tr}(\mathbf{A}^\mathrm{T}\mathbf{B}) = \mathrm{vec}(\mathbf{A})^\mathrm{T}\mathrm{vec}(\mathbf{B})$</li></ul><p>where $\otimes$ denotes the Kronecker product.</p><h3 id=examples-of-matrix-to-matrix-functions>Examples of Matrix-to-Matrix Functions
<a class=anchor href=#examples-of-matrix-to-matrix-functions>#</a></h3><p><strong>Example 1</strong>: $f(\mathbf{X}) = \mathbf{X}^2$</p><p>Using the Gateaux derivative:
$$Df(\mathbf{X})[\mathbf{V}] = \left.\frac{d}{dt}\right|_{t=0} (\mathbf{X}+t\mathbf{V})^2 = \mathbf{X}\mathbf{V} + \mathbf{V}\mathbf{X}$$</p><p><strong>Example 2</strong>: $f(\mathbf{X}) = \mathbf{X}^{-1}$ (for invertible $\mathbf{X}$)</p><p>From the identity $\mathbf{X}\mathbf{X}^{-1} = \mathbf{I}$ and differentiating:
$$Df(\mathbf{X})[\mathbf{V}] = -\mathbf{X}^{-1}\mathbf{V}\mathbf{X}^{-1}$$</p><h3 id=properties-of-matrix-function-derivatives>Properties of Matrix Function Derivatives
<a class=anchor href=#properties-of-matrix-function-derivatives>#</a></h3><p>The derivatives of matrix functions follow familiar rules:</p><ul><li><p><strong>Linearity</strong>: For $f = \alpha g + \beta h$:
$$Df(\mathbf{X})[\mathbf{V}] = \alpha , Dg(\mathbf{X})[\mathbf{V}] + \beta , Dh(\mathbf{X})[\mathbf{V}]$$</p></li><li><p><strong>Product rule</strong>: For $f(\mathbf{X}) = g(\mathbf{X}) \cdot h(\mathbf{X})$:
$$Df(\mathbf{X})[\mathbf{V}] = Dg(\mathbf{X})[\mathbf{V}] \cdot h(\mathbf{X}) + g(\mathbf{X}) \cdot Dh(\mathbf{X})[\mathbf{V}]$$</p></li><li><p><strong>Chain rule</strong>: For $f(\mathbf{X}) = g(h(\mathbf{X}))$:
$$Df(\mathbf{X})[\mathbf{V}] = Dg(h(\mathbf{X}))[Dh(\mathbf{X})[\mathbf{V}]]$$</p></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#1---introduction>1 - Introduction</a></li><li><a href=#2---monovariate-reminders>2 - Monovariate Reminders</a><ul><li><a href=#derivative-of-a-function>Derivative of a Function</a></li></ul></li><li><a href=#3---extension-to-multivariate-setup-fmathbbrd-to-mathbbr>3 - Extension to Multivariate Setup: $f:\mathbb{R}^d \to \mathbb{R}$</a><ul><li><a href=#limits-and-continuity>Limits and Continuity</a></li><li><a href=#directional-derivative>Directional Derivative</a></li><li><a href=#gradient>Gradient</a></li><li><a href=#gradient-and-partial-derivatives>Gradient and Partial Derivatives</a></li><li><a href=#gradient-properties-and-practical-computation>Gradient Properties and Practical Computation</a></li><li><a href=#hessian-matrix>Hessian Matrix</a></li><li><a href=#hessian-matrix-properties>Hessian Matrix Properties</a></li></ul></li><li><a href=#4---multivariate-case-fmathbbrd-to-mathbbrp>4 - Multivariate Case: $f:\mathbb{R}^d \to \mathbb{R}^p$</a><ul><li><a href=#multivariate-functions>Multivariate Functions</a></li><li><a href=#gradient-and-jacobian>Gradient and Jacobian</a></li><li><a href=#jacobian-and-directional-derivative>Jacobian and Directional Derivative</a></li><li><a href=#chain-rule-for-composition-of-functions>Chain Rule for Composition of Functions</a></li><li><a href=#chain-rule-special-cases>Chain Rule: Special Cases</a></li><li><a href=#worked-examples>Worked Examples</a></li></ul></li><li><a href=#5---matrix-functions-fmathbbrm-times-n-to-mathbbr>5 - Matrix Functions: $f:\mathbb{R}^{m \times n} \to \mathbb{R}$</a><ul><li><a href=#fréchet-derivative>Fréchet Derivative</a></li><li><a href=#matrix-to-scalar-functions>Matrix-to-Scalar Functions</a></li><li><a href=#examples-of-matrix-to-scalar-functions>Examples of Matrix-to-Scalar Functions</a></li></ul></li><li><a href=#6---matrix-functions-fmathbbrm-times-n-to-mathbbrp-times-q>6 - Matrix Functions: $f:\mathbb{R}^{m \times n} \to \mathbb{R}^{p \times q}$</a><ul><li><a href=#matrix-to-matrix-functions>Matrix-to-Matrix Functions</a></li><li><a href=#vectorization-identities>Vectorization Identities</a></li><li><a href=#examples-of-matrix-to-matrix-functions>Examples of Matrix-to-Matrix Functions</a></li><li><a href=#properties-of-matrix-function-derivatives>Properties of Matrix Function Derivatives</a></li></ul></li></ul></nav></div></aside></main></body></html>