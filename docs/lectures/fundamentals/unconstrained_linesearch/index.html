<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Unconstrained optimization - Linesearch methods
  #

All algorithms for unconstrained minimization require the user to supply a starting point, which we usually denote by $\mathbf{x}_0$. The user with knowledge about the application and the data set may be in a good position to choose $\mathbf{x}_0$ to be a reasonable estimate of the solution. Otherwise, the starting point must be chosen in some arbitrary manner.
Beginning at $\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\left\{\mathbf{x}_k\right\}_{k=0}^{\infty}$ that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy. In deciding how to move from one iterate $\mathbf{x}_k$ to the next, the algorithms use information about the function $f$ at $\mathbf{x}_k$, and possibly also information from earlier iterates $\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_{k-1}$. They use this information to find a new iterate $\mathbf{x}_{k+1}$ with a lower function value than $\mathbf{x}_k$. (There exist nonmonotone algorithms that do not insist on a decrease in $f$ at every step, but even these algorithms require $f$ to be decreased after some prescribed number $m$ of iterations. That is, they enforce $f\left(\mathbf{x}_k\right)<f\left(\mathbf{x}_{k-m}\right)$.)"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="4. Unconstrained optimization : linesearch"><meta property="og:description" content="Unconstrained optimization - Linesearch methods # All algorithms for unconstrained minimization require the user to supply a starting point, which we usually denote by $\mathbf{x}_0$. The user with knowledge about the application and the data set may be in a good position to choose $\mathbf{x}_0$ to be a reasonable estimate of the solution. Otherwise, the starting point must be chosen in some arbitrary manner.
Beginning at $\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\left\{\mathbf{x}_k\right\}_{k=0}^{\infty}$ that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy. In deciding how to move from one iterate $\mathbf{x}_k$ to the next, the algorithms use information about the function $f$ at $\mathbf{x}_k$, and possibly also information from earlier iterates $\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_{k-1}$. They use this information to find a new iterate $\mathbf{x}_{k+1}$ with a lower function value than $\mathbf{x}_k$. (There exist nonmonotone algorithms that do not insist on a decrease in $f$ at every step, but even these algorithms require $f$ to be decreased after some prescribed number $m$ of iterations. That is, they enforce $f\left(\mathbf{x}_k\right)<f\left(\mathbf{x}_{k-m}\right)$.)"><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>4. Unconstrained optimization : linesearch | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.81d91c7b674cc3e32b1a540eb6f48726b6d7ccbb6f384fb092556e6d6c756d4d.js integrity="sha256-gdkce2dMw+MrGlQOtvSHJrbXzLtvOE+wklVubWx1bU0=" crossorigin=anonymous></script><script>const chapterNum=4;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/ class=active>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_newton/>5. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>6. Constrained optimization</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>1. Proximal methods</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/stochastic/>2. Stochastic optimization</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression with gradient</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>4. Unconstrained optimization : linesearch</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#two-strategies-line-search-and-trust-region>Two strategies: line search and trust region</a></li><li><a href=#search-directions-for-line-search-methods>Search directions for line search methods</a></li><li><a href=#step-length-conditions>Step-length conditions</a><ul><li><a href=#wolfe-conditions>Wolfe conditions</a></li><li><a href=#goldenstein-conditions>Goldenstein conditions</a></li></ul></li><li><a href=#search-directions>Search directions</a></li><li><a href=#rate-of-convergence>Rate of convergence</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=unconstrained-optimization---linesearch-methods>Unconstrained optimization - Linesearch methods
<a class=anchor href=#unconstrained-optimization---linesearch-methods>#</a></h1><p>All algorithms for unconstrained minimization require the user to supply a starting point, which we usually denote by $\mathbf{x}_0$. The user with knowledge about the application and the data set may be in a good position to choose $\mathbf{x}_0$ to be a reasonable estimate of the solution. Otherwise, the starting point must be chosen in some arbitrary manner.</p><p>Beginning at $\mathbf{x}_0$, optimization algorithms generate a sequence of iterates $\left\{\mathbf{x}_k\right\}_{k=0}^{\infty}$ that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy. In deciding how to move from one iterate $\mathbf{x}_k$ to the next, the algorithms use information about the function $f$ at $\mathbf{x}_k$, and possibly also information from earlier iterates $\mathbf{x}_0, \mathbf{x}_1, \ldots, \mathbf{x}_{k-1}$. They use this information to find a new iterate $\mathbf{x}_{k+1}$ with a lower function value than $\mathbf{x}_k$. (There exist nonmonotone algorithms that do not insist on a decrease in $f$ at every step, but even these algorithms require $f$ to be decreased after some prescribed number $m$ of iterations. That is, they enforce $f\left(\mathbf{x}_k\right)&lt;f\left(\mathbf{x}_{k-m}\right)$.)</p><p>There are two fundamental strategies for moving from the current point $\mathbf{x}_k$ to a new iterate $\mathbf{x}_{k+1}$. Most of the algorithms described in this book follow one of these approaches.</p><h2 id=two-strategies-line-search-and-trust-region>Two strategies: line search and trust region
<a class=anchor href=#two-strategies-line-search-and-trust-region>#</a></h2><p>In the line search strategy, the algorithm chooses a direction $\mathbf{p}_k$ and searches along this direction from the current iterate $\mathbf{x}_k$ for a new iterate with a lower function value. The distance to move along $\mathbf{p}_k$ can be found by approximately solving the following one-dimensional minimization problem to find a step length $\alpha$ :</p><p>\begin{equation}
\min _{\alpha>0} f\left(\mathbf{x}_k+\alpha \mathbf{p}_k\right)
\label{eq:line_search_min}
\end{equation}</p><p>By solving \eqref{eq:line_search_min} exactly, we would derive the maximum benefit from the direction $\mathbf{p}_k$, but an exact minimization is expensive and unnecessary. Instead, the line search algorithm generates a limited number of trial step lengths until it finds one that loosely approximates the minimum of \eqref{eq:line_search_min}. At the new point a new search direction and step length are computed, and the process is repeated.</p><p>In the second algorithmic strategy, known as trust region, the information gathered about $f$ is used to construct a model function $m_k$ whose behavior near the current point $\mathbf{x}_k$ is similar to that of the actual objective function $f$. Because the model $m_k$ may not be a good approximation of $f$ when $\mathbf{x}$ is far from $\mathbf{x}_k$, we restrict the search for a minimizer of $m_k$ to some region around $\mathbf{x}_k$. In other words, we find the candidate step $\mathbf{p}$ by approximately
solving the following subproblem:</p><p>\begin{equation}
\min _{\mathbf{p}} m_k\left(\mathbf{x}_k+\mathbf{p}\right), \quad \text { where } \mathbf{x}_k+\mathbf{p} \text { lies inside the trust region. }
\label{eq:trust_region_subproblem}
\end{equation}</p><p>If the candidate solution does not produce a sufficient decrease in $f$, we conclude that the trust region is too large, and we shrink it and re-solve \eqref{eq:trust_region_subproblem}. Usually, the trust region is a ball defined by $\lVert\mathbf{p}\rVert_2 \leq \Delta$, where the scalar $\Delta>0$ is called the trust-region radius. Elliptical and box-shaped trust regions may also be used.</p><p>The model $m_k$ in \eqref{eq:trust_region_subproblem} is usually defined to be a quadratic function of the form</p><p>\begin{equation}
m_k\left(\mathbf{x}_k+\mathbf{p}\right)=f_k+\mathbf{p}^{\mathrm{T}} \nabla f_k+\frac{1}{2} \mathbf{p}^{\mathrm{T}} \mathbf{B}_k \mathbf{p}
\label{eq:quadratic_model}
\end{equation}</p><p>where $f_k$, $\nabla f_k$, and $\mathbf{B}_k$ are a scalar, vector, and matrix, respectively. As the notation indicates, $f_k$ and $\nabla f_k$ are chosen to be the function and gradient values at the point $\mathbf{x}_k$, so that $m_k$ and $f$ are in agreement to first order at the current iterate $\mathbf{x}_k$. The matrix $\mathbf{B}_k$ is either the Hessian $\nabla^2 f_k$ or some approximation to it.</p><p>Suppose that the objective function is given by $f(\mathbf{x})=10\left(x_2-x_1^2\right)^2+\left(1-x_1\right)^2$. At the point $\mathbf{x}_k=(0,1)$ its gradient and Hessian are</p><p>$$
\nabla f_k=\left[\begin{array}{c}
-2 \\
20
\end{array}\right], \quad \nabla^2 f_k=\left[\begin{array}{cc}
-38 & 0 \\
0 & 20
\end{array}\right]
$$</p><p>Note that each time we decrease the size of the trust region after failure of a candidate iterate, the step from $\mathbf{x}_k$ to the new candidate will be shorter, and it usually points in a different direction from the previous candidate. The trust-region strategy differs in this respect from line search, which stays with a single search direction.</p><p>In a sense, the line search and trust-region approaches differ in the order in which they choose the direction and distance of the move to the next iterate. Line search starts by fixing the direction $\mathbf{p}_k$ and then identifying an appropriate distance, namely the step length $\alpha_k$. In trust region, we first choose a maximum distance-the trust-region radius $\Delta_k$-and then seek a direction and step that attain the best improvement possible subject to this distance constraint. If this step proves to be unsatisfactory, we reduce the distance measure $\Delta_k$ and try again.</p><p>The line search approach is discussed in more detail in this lecture while the trust-region strategy, is left to the reader to study.</p><h2 id=search-directions-for-line-search-methods>Search directions for line search methods
<a class=anchor href=#search-directions-for-line-search-methods>#</a></h2><p>The steepest-descent direction $-\nabla f_k$ is the most obvious choice for search direction for a line search method. It is intuitive; among all the directions we could move from $\mathbf{x}_k$, it is the one along which $f$ decreases most rapidly. To verify this claim, we appeal again to Taylor&rsquo;s theorem, which tells us that for any search direction $\mathbf{p}$ and step-length parameter $\alpha$, we have</p><p>\begin{equation}
f\left(\mathbf{x}_k+\alpha \mathbf{p}\right)=f\left(\mathbf{x}_k\right)+\alpha \mathbf{p}^{\mathrm{T}} \nabla f_k+\frac{1}{2} \alpha^2 \mathbf{p}^{\mathrm{T}} \nabla^2 f\left(\mathbf{x}_k+t \mathbf{p}\right) \mathbf{p}, \quad \text { for some } t \in(0, \alpha)
\label{eq:taylor_expansion}
\end{equation}</p><p>The rate of change in $f$ along the direction $\mathbf{p}$ at $\mathbf{x}_k$ is simply the coefficient of $\alpha$, namely, $\mathbf{p}^{\mathrm{T}} \nabla f_k$. Hence, the unit direction $\mathbf{p}$ of most rapid decrease is the solution to the problem</p><p>\begin{equation}
\min _{\mathbf{p}} \mathbf{p}^{\mathrm{T}} \nabla f_k, \quad \text { subject to }\lVert\mathbf{p}\rVert=1
\label{eq:steepest_descent_problem}
\end{equation}</p><p>Since $\mathbf{p}^{\mathrm{T}} \nabla f_k=\lVert\mathbf{p}\rVert\lVert\nabla f_k\rVert \cos \theta$, where $\theta$ is the angle between $\mathbf{p}$ and $\nabla f_k$, we have from $\lVert\mathbf{p}\rVert=1$ that $\mathbf{p}^{\mathrm{T}} \nabla f_k=\lVert\nabla f_k\rVert \cos \theta$, so the objective in \eqref{eq:steepest_descent_problem} is minimized when $\cos \theta$ takes on its minimum value of -1 at $\theta=\pi$ radians. In other words, the solution to \eqref{eq:steepest_descent_problem} is</p><p>$$
\mathbf{p}=-\nabla f_k /\lVert\nabla f_k\rVert
$$</p><p>as claimed. This direction is orthogonal to the contours of the function.</p><p>The steepest descent method is a line search method that moves along $\mathbf{p}_k=-\nabla f_k$ at every step. It can choose the step length $\alpha_k$ in a variety of ways, as we will see in next chapter. One advantage of the steepest descent direction is that it requires calculation of the gradient $\nabla f_k$ but not of second derivatives. However, it can be excruciatingly slow on difficult problems.</p><p>Line search methods may use search directions other than the steepest descent direction. In general, any descent direction-one that makes an angle of strictly less than $\pi / 2$ radians with $-\nabla f_k$-is guaranteed to produce a decrease in $f$, provided that the step length is sufficiently small. We can verify this claim by using Taylor&rsquo;s theorem. From \eqref{eq:taylor_expansion}, we have that</p><p>$$
f\left(\mathbf{x}_k+\epsilon \mathbf{p}_k\right)=f\left(\mathbf{x}_k\right)+\epsilon \mathbf{p}_k^{\mathrm{T}} \nabla f_k+O\left(\epsilon^2\right)
$$</p><p>When $\mathbf{p}_k$ is a downhill direction, the angle $\theta_k$ between $\mathbf{p}_k$ and $\nabla f_k$ has $\cos \theta_k&lt;0$, so that</p><p>$$
\mathbf{p}_k^{\mathrm{T}} \nabla f_k=\lVert\mathbf{p}_k\rVert\lVert\nabla f_k\rVert \cos \theta_k&lt;0
$$</p><p>It follows that $f\left(\mathbf{x}_k+\epsilon \mathbf{p}_k\right)&lt;f\left(\mathbf{x}_k\right)$ for all positive but sufficiently small values of $\epsilon$.</p><p>Another important search direction-perhaps the most important one of all-is the Newton direction. This direction is derived from the second-order Taylor series approximation to $f\left(\mathbf{x}_k+\mathbf{p}\right)$, which is
$$
f\left(\mathbf{x}_k+\mathbf{p}\right) \approx f_k+\mathbf{p}^{\mathrm{T}} \nabla f_k+\frac{1}{2} \mathbf{p}^{\mathrm{T}} \nabla^2 f_k \mathbf{p} \stackrel{\text { def }}{=} m_k(\mathbf{p})
$$</p><p>Assuming for the moment that $\nabla^2 f_k$ is positive definite, we obtain the Newton direction by finding the vector $\mathbf{p}$ that minimizes $m_k(\mathbf{p})$. By simply setting the derivative of $m_k(\mathbf{p})$ to zero, we obtain the following explicit formula:</p><p>\begin{equation}
\mathbf{p}_k^{\mathrm{N}}=-\nabla^2 f_k^{-1} \nabla f_k
\label{eq:newton_direction}
\end{equation}</p><p>The Newton direction is reliable when the difference between the true function $f\left(\mathbf{x}_k+ \mathbf{p}\right)$ and its quadratic model $m_k(\mathbf{p})$ is not too large. By comparing \eqref{eq:quadratic_model} with \eqref{eq:taylor_expansion}, we see that the only difference between these functions is that the matrix $\nabla^2 f\left(\mathbf{x}_k+t \mathbf{p}\right)$ in the third term of the expansion has been replaced by $\nabla^2 f_k=\nabla^2 f\left(\mathbf{x}_k\right)$. If $\nabla^2 f(\cdot)$ is sufficiently smooth, this difference introduces a perturbation of only $O\left(\lVert\mathbf{p}\rVert^3\right)$ into the expansion, so that when $\lVert\mathbf{p}\rVert$ is small, the approximation $f\left(\mathbf{x}_k+\mathbf{p}\right) \approx m_k(\mathbf{p})$ is very accurate indeed.</p><p>The Newton direction can be used in a line search method when $\nabla^2 f_k$ is positive definite, for in this case we have</p><p>$$
\nabla f_k^{\mathrm{T}} \mathbf{p}_k^{\mathrm{N}}=-\mathbf{p}_k^{\mathrm{N} \mathrm{T}} \nabla^2 f_k \mathbf{p}_k^{\mathrm{N}} \leq-\sigma_k\lVert\mathbf{p}_k^{\mathrm{N}}\rVert^2
$$</p><p>for some $\sigma_k>0$. Unless the gradient $\nabla f_k$ (and therefore the step $\mathbf{p}_k^N$) is zero, we have that $\nabla f_k^{\mathrm{T}} \mathbf{p}_k^{\mathrm{N}}&lt;0$, so the Newton direction is a descent direction. Unlike the steepest descent direction, there is a &ldquo;natural&rdquo; step length of 1 associated with the Newton direction. Most
line search implementations of Newton&rsquo;s method use the unit step $\alpha=1$ where possible and adjust this step length only when it does not produce a satisfactory reduction in the value of $f$.</p><p>When $\nabla^2 f_k$ is not positive definite, the Newton direction may not even be defined, since $\nabla^2 f_k^{-1}$ may not exist. Even when it is defined, it may not satisfy the descent property $\nabla f_k^{\mathrm{T}} \mathbf{p}_k^{\mathrm{N}}&lt;0$, in which case it is unsuitable as a search direction. In these situations, line search methods modify the definition of $\mathbf{p}_k$ to make it satisfy the downhill condition while retaining the benefit of the second-order information contained in $\nabla^2 f_k$.</p><p>Methods that use the Newton direction have a fast rate of local convergence, typically quadratic. When a neighborhood of the solution is reached, convergence to high accuracy often occurs in just a few iterations. The main drawback of the Newton direction is the need for the Hessian $\nabla^2 f(\mathbf{x})$. Explicit computation of this matrix of second derivatives is sometimes, though not always, a cumbersome, error-prone, and expensive process.</p><p>Quasi-Newton search directions provide an attractive alternative in that they do not require computation of the Hessian and yet still attain a superlinear rate of convergence. In place of the true Hessian $\nabla^2 f_k$, they use an approximation $\mathbf{B}_k$, which is updated after each step to take account of the additional knowledge gained during the step. The updates make use of the fact that changes in the gradient $\mathbf{g}$ provide information about the second derivative of $f$ along the search direction. By using the expression from our statement of Taylor&rsquo;s theorem, we have by adding and subtracting the term $\nabla^2 f(\mathbf{x}) \mathbf{p}$ that</p><p>$$
\nabla f(\mathbf{x}+\mathbf{p})=\nabla f(\mathbf{x})+\nabla^2 f(\mathbf{x}) \mathbf{p}+\int_0^1\left[\nabla^2 f(\mathbf{x}+t \mathbf{p})-\nabla^2 f(\mathbf{x})\right] \mathbf{p} d t
$$</p><p>Because $\nabla f(\cdot)$ is continuous, the size of the final integral term is $o(\lVert\mathbf{p}\rVert)$. By setting $\mathbf{x}=\mathbf{x}_k$ and $\mathbf{p}=\mathbf{x}_{k+1}-\mathbf{x}_k$, we obtain</p><p>$$
\nabla f_{k+1}=\nabla f_k+\nabla^2 f_{k+1}\left(\mathbf{x}_{k+1}-\mathbf{x}_k\right)+o\left(\lVert\mathbf{x}_{k+1}-\mathbf{x}_k\rVert\right)
$$</p><p>When $\mathbf{x}_k$ and $\mathbf{x}_{k+1}$ lie in a region near the solution $\mathbf{x}^*$, within which $\nabla f$ is positive definite, the final term in this expansion is eventually dominated by the $\nabla^2 f_k\left(\mathbf{x}_{k+1}-\mathbf{x}_k\right)$ term, and we can write</p><p>$$
\nabla^2 f_{k+1}\left(\mathbf{x}_{k+1}-\mathbf{x}_k\right) \approx \nabla f_{k+1}-\nabla f_k
$$</p><p>We choose the new Hessian approximation $\mathbf{B}_{k+1}$ so that it mimics this property of the true Hessian, that is, we require it to satisfy the following condition, known as the secant equation:</p><p>\begin{equation}
\mathbf{B}_{k+1} \mathbf{s}_k=\mathbf{y}_k
\label{eq:secant_equation}
\end{equation}</p><p>where</p><p>$$
\mathbf{s}_k=\mathbf{x}_{k+1}-\mathbf{x}_k, \quad \mathbf{y}_k=\nabla f_{k+1}-\nabla f_k
$$</p><p>Typically, we impose additional requirements on $\mathbf{B}_{k+1}$, such as symmetry (motivated by symmetry of the exact Hessian), and a restriction that the difference between successive approximation $\mathbf{B}_k$ to $\mathbf{B}_{k+1}$ have low rank. The initial approximation $\mathbf{B}_0$ must be chosen by the user.</p><p>Two of the most popular formulae for updating the Hessian approximation $\mathbf{B}_k$ are the symmetric-rank-one (SR1) formula, defined by</p><p>\begin{equation}
\mathbf{B}_{k+1}=\mathbf{B}_k+\frac{\left(\mathbf{y}_k-\mathbf{B}_k \mathbf{s}_k\right)\left(\mathbf{y}_k-\mathbf{B}_k \mathbf{s}_k\right)^{\mathrm{T}}}{\left(\mathbf{y}_k-\mathbf{B}_k \mathbf{s}_k\right)^{\mathrm{T}} \mathbf{s}_k}
\label{eq:sr1_formula}
\end{equation}</p><p>and the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno, which is defined by</p><p>\begin{equation}
\mathbf{B}_{k+1}=\mathbf{B}_k-\frac{\mathbf{B}_k \mathbf{s}_k \mathbf{s}_k^{\mathrm{T}} \mathbf{B}_k}{\mathbf{s}_k^{\mathrm{T}} \mathbf{B}_k \mathbf{s}_k}+\frac{\mathbf{y}_k \mathbf{y}_k^{\mathrm{T}}}{\mathbf{y}_k^{\mathrm{T}} \mathbf{s}_k}
\label{eq:bfgs_formula}
\end{equation}</p><p>Note that the difference between the matrices $\mathbf{B}_k$ and $\mathbf{B}_{k+1}$ is a rank-one matrix in the case of \eqref{eq:sr1_formula}, and a rank-two matrix in the case of \eqref{eq:bfgs_formula}. Both updates satisfy the secant equation and both maintain symmetry. One can show that BFGS update \eqref{eq:bfgs_formula} generates positive definite approximations whenever the initial approximation $\mathbf{B}_0$ is positive definite and $\mathbf{s}_k^{\mathrm{T}} \mathbf{y}_k>0$.</p><p>The quasi-Newton search direction is given by using $\mathbf{B}_k$ in place of the exact Hessian in the formula \eqref{eq:newton_direction}, that is,</p><p>\begin{equation}
\mathbf{p}_k=-\mathbf{B}_k^{-1} \nabla f_k
\label{eq:quasi_newton_direction}
\end{equation}</p><p>Some practical implementations of quasi-Newton methods avoid the need to factorize $\mathbf{B}_k$ at each iteration by updating the inverse of $\mathbf{B}_k$, instead of $\mathbf{B}_k$ itself. In fact, the equivalent formula for \eqref{eq:sr1_formula} and \eqref{eq:bfgs_formula}, applied to the inverse approximation $\mathbf{H}_k \stackrel{\text { def }}{=} \mathbf{B}_k^{-1}$, is</p><p>\begin{equation}
\mathbf{H}_{k+1}=\left(\mathbf{I}-\rho_k \mathbf{s}_k \mathbf{y}_k^{\mathrm{T}}\right) \mathbf{H}_k\left(\mathbf{I}-\rho_k \mathbf{y}_k \mathbf{s}_k^{\mathrm{T}}\right)+\rho_k \mathbf{s}_k \mathbf{s}_k^{\mathrm{T}}, \quad \rho_k=\frac{1}{\mathbf{y}_k^{\mathrm{T}} \mathbf{s}_k}
\label{eq:inverse_bfgs}
\end{equation}</p><p>Calculation of $\mathbf{p}_k$ can then be performed by using the formula $\mathbf{p}_k=-\mathbf{H}_k \nabla f_k$. This can be implemented as a matrix-vector multiplication, which is typically simpler than the factorization/back-substitution procedure that is needed to implement the formula \eqref{eq:quasi_newton_direction}.</p><p>All of the search directions discussed so far can be used directly in a line search framework. They give rise to the steepest descent, Newton, quasi-Newton, and conjugate gradient line search methods. All except conjugate gradients have an analogue in the trustregion framework, as we now discuss.</p><h2 id=step-length-conditions>Step-length conditions
<a class=anchor href=#step-length-conditions>#</a></h2><h3 id=wolfe-conditions>Wolfe conditions
<a class=anchor href=#wolfe-conditions>#</a></h3><iframe style=border:none scrolling=no src=../../../../interactive/line-search-conditions.html width=700px height=500px title="Wolfe conditions visualisation"></iframe><h3 id=goldenstein-conditions>Goldenstein conditions
<a class=anchor href=#goldenstein-conditions>#</a></h3><iframe style=border:none scrolling=no src=../../../../interactive/goldstein-conditions-visualization.html width=700px height=700px title="Wolfe conditions visualisation"></iframe><h2 id=search-directions>Search directions
<a class=anchor href=#search-directions>#</a></h2><h2 id=rate-of-convergence>Rate of convergence
<a class=anchor href=#rate-of-convergence>#</a></h2></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#two-strategies-line-search-and-trust-region>Two strategies: line search and trust region</a></li><li><a href=#search-directions-for-line-search-methods>Search directions for line search methods</a></li><li><a href=#step-length-conditions>Step-length conditions</a><ul><li><a href=#wolfe-conditions>Wolfe conditions</a></li><li><a href=#goldenstein-conditions>Goldenstein conditions</a></li></ul></li><li><a href=#search-directions>Search directions</a></li><li><a href=#rate-of-convergence>Rate of convergence</a></li></ul></nav></div></aside></main></body></html>