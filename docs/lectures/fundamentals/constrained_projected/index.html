<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Understanding Saddle Points in Constrained Optimization: From KKT Conditions to Projected Gradient Methods
  #

When we first encounter the Karush-Kuhn-Tucker (KKT) conditions in constrained optimization, they often appear as a collection of mathematical requirements that characterize optimal solutions. However, these conditions actually emerge from a deeper geometric structure that reveals why constrained optimization problems possess fundamentally different mathematical properties than their unconstrained counterparts. This exploration will guide you through understanding how constraint conflicts create saddle point structures in the Lagrangian, and how this mathematical insight leads naturally to practical algorithms."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/constrained_projected/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="5b. Constrained optimization - Projected Gradient Descent"><meta property="og:description" content="Understanding Saddle Points in Constrained Optimization: From KKT Conditions to Projected Gradient Methods # When we first encounter the Karush-Kuhn-Tucker (KKT) conditions in constrained optimization, they often appear as a collection of mathematical requirements that characterize optimal solutions. However, these conditions actually emerge from a deeper geometric structure that reveals why constrained optimization problems possess fundamentally different mathematical properties than their unconstrained counterparts. This exploration will guide you through understanding how constraint conflicts create saddle point structures in the Lagrangian, and how this mathematical insight leads naturally to practical algorithms."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>5b. Constrained optimization - Projected Gradient Descent | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/constrained_projected/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.f28b8237862d934dafc8da537aa3936a35a91e90c6c9ff097abeb367aa09a7f9.js integrity="sha256-8ouCN4Ytk02vyNpTeqOTajWpHpDGyf8Jer6zZ6oJp/k=" crossorigin=anonymous></script><script>const chapterNum=5;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_projected/ class=active>5b. Constrained optimization - Projected Gradient Descent</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimization_linear/>6. Constrained optimization - Linear programming</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/fundamentals/>1. Machine learning fundamentals</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Classification and support vector machines</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - MNIST and Fashion-MNIST Classification</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li><li><a href=/numerical_optimization/docs/practical_labs/quasinewton/>Quasi-Newton methods memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>5b. Constrained optimization - Projected Gradient Descent</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#the-foundation-kkt-conditions-and-the-lagrangian>The foundation: KKT conditions and the Lagrangian</a></li><li><a href=#the-emergence-of-saddle-point-structure>The emergence of saddle point structure</a></li><li><a href=#illustrating-the-saddle-point-through-a-concrete-example>Illustrating the saddle point through a concrete example</a></li><li><a href=#from-theory-to-computation-the-projected-gradient-method>From theory to computation: the projected gradient method</a></li><li><a href=#exercise-constrained-optimization-with-mixed-constraints><strong>Exercise</strong>: constrained optimization with mixed constraints</a><ul><li><a href=#problem-setup>Problem Setup</a></li><li><a href=#lagrangian-construction>Lagrangian Construction</a></li><li><a href=#gradient-computations>Gradient Computations</a></li><li><a href=#projected-gradient-updates>Projected Gradient Updates</a></li><li><a href=#analytical-solution>Analytical Solution</a></li><li><a href=#verification>Verification</a></li><li><a href=#geometric-interpretation>Geometric Interpretation</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=understanding-saddle-points-in-constrained-optimization-from-kkt-conditions-to-projected-gradient-methods>Understanding Saddle Points in Constrained Optimization: From KKT Conditions to Projected Gradient Methods
<a class=anchor href=#understanding-saddle-points-in-constrained-optimization-from-kkt-conditions-to-projected-gradient-methods>#</a></h1><p>When we first encounter the Karush-Kuhn-Tucker (KKT) conditions in constrained optimization, they often appear as a collection of mathematical requirements that characterize optimal solutions. However, these conditions actually emerge from a deeper geometric structure that reveals why constrained optimization problems possess fundamentally different mathematical properties than their unconstrained counterparts. This exploration will guide you through understanding how constraint conflicts create saddle point structures in the Lagrangian, and how this mathematical insight leads naturally to practical algorithms.</p><h2 id=the-foundation-kkt-conditions-and-the-lagrangian>The foundation: KKT conditions and the Lagrangian
<a class=anchor href=#the-foundation-kkt-conditions-and-the-lagrangian>#</a></h2><p>Consider a general constrained optimization problem where we seek to minimize an objective function subject to both equality and inequality constraints. The mathematical framework begins with defining our constraint sets and constructing the Lagrangian function that will encode the relationship between our objective and constraints.</p><div id=constrained_opt class=theorem-box><p class=theorem-title><strong>Definition 5.1 (Constrained optimization problem)</strong></p><div class=theorem-content>We seek to solve:
\begin{equation}
\begin{aligned}
\text{minimize} \quad & f(\mathbf{x}) \\
\text{subject to} \quad & c_i(\mathbf{x}) = 0, \quad i \in \mathcal{E} \\
& c_i(\mathbf{x}) \geq 0, \quad i \in \mathcal{I}
\end{aligned}
\label{eq:constrained_problem}
\end{equation}
where $\mathcal{E}$ represents the set of equality constraint indices and $\mathcal{I}$ represents the set of inequality constraint indices.</div></div><p>The Lagrangian function serves as the mathematical bridge that connects our objective function with the constraint structure. Rather than treating constraints as separate mathematical entities, the Lagrangian weaves them together into a single function that encodes the fundamental trade-offs inherent in constrained optimization.</p><div id=lagrangian_def class=theorem-box><p class=theorem-title><strong>Definition 5.2 (Lagrangian function)</strong></p><div class=theorem-content>For the constrained optimization problem in \eqref{eq:constrained_problem}, the <strong>Lagrangian function</strong> is defined as:
\begin{equation}
\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}) = f(\mathbf{x}) - \sum_{i \in \mathcal{E} \cup \mathcal{I}} \lambda_i c_i(\mathbf{x})
\label{eq:lagrangian}
\end{equation}
where $\boldsymbol{\lambda} = (\lambda_1, \lambda_2, \ldots, \lambda_m)$ are the <strong>Lagrange multipliers</strong> associated with the constraints.</div></div><p>The KKT conditions emerge as necessary conditions that any optimal solution must satisfy, provided certain regularity assumptions hold. These conditions capture the essential balance that must exist at an optimal point between the desire to improve the objective function and the requirement to respect the constraints.</p><div id=kkt_necessary class=theorem-box><p class=theorem-title><strong>Theorem 5.1 (Karush-Kuhn-Tucker necessary conditions)</strong></p><div class=theorem-content>If $\mathbf{x}^\star$ is a local solution to \eqref{eq:constrained_problem} and the Linear Independence Constraint Qualification (LICQ) holds at $\mathbf{x}^\star$, then there exists a vector $\boldsymbol{\lambda}^\star$ such that:
\begin{equation}
\begin{aligned}
\nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}^\star, \boldsymbol{\lambda}^\star) &= \mathbf{0} && \text{(Stationarity)} \\
c_i(\mathbf{x}^\star) &= 0, \quad i \in \mathcal{E} && \text{(Equality feasibility)} \\
c_i(\mathbf{x}^\star) &\geq 0, \quad i \in \mathcal{I} && \text{(Inequality feasibility)} \\
\lambda_i^\star &\geq 0, \quad i \in \mathcal{I} && \text{(Dual feasibility)} \\
\lambda_i^\star c_i(\mathbf{x}^\star) &= 0, \quad i \in \mathcal{E} \cup \mathcal{I} && \text{(Complementarity)}
\end{aligned}
\label{eq:kkt_conditions}
\end{equation}</div></div><p>While these conditions tell us what an optimal solution must look like, they leave a crucial question unanswered: how should we actually optimize the Lagrangian function to find such a solution? This question leads us to discover one of the most elegant structures in mathematical optimization.</p><h2 id=the-emergence-of-saddle-point-structure>The emergence of saddle point structure
<a class=anchor href=#the-emergence-of-saddle-point-structure>#</a></h2><p>The key insight that transforms our understanding comes from recognizing that the Lagrangian possesses a special geometric property called a saddle point structure. This property emerges naturally from the mathematical conflict between objectives and constraints, and it explains why constrained optimization requires fundamentally different approaches than unconstrained problems.</p><p>To understand why this structure arises, let us examine what happens when we attempt different optimization strategies on the Lagrangian. Consider what would occur if we tried to minimize the Lagrangian with respect to both the primal variables $\mathbf{x}$ and the dual variables $\boldsymbol{\lambda}$ simultaneously.</p><p>For an inequality constraint $c_i(\mathbf{x}) \geq 0$, suppose we have a point where $c_i(\mathbf{x}) > 0$, meaning the constraint is satisfied with some slack. The Lagrangian contains the term $-\lambda_i c_i(\mathbf{x})$, which becomes increasingly negative as $\lambda_i$ increases. If we were minimizing over $\lambda_i$, this would drive $\lambda_i$ toward positive infinity, creating an unbounded minimization problem. This mathematical behavior makes no economic sense and violates the dual feasibility requirement $\lambda_i \geq 0$.</p><p>The resolution to this apparent contradiction reveals the fundamental insight: we must maximize over the dual variables rather than minimize. When $c_i(\mathbf{x}) > 0$ and we maximize over $\lambda_i \geq 0$, the maximization process naturally drives $\lambda_i$ toward zero, which aligns perfectly with the complementarity condition $\lambda_i c_i(\mathbf{x}) = 0$.</p><p>This mathematical necessity gives rise to the saddle point property, which provides both the theoretical foundation and the algorithmic guidance for solving constrained optimization problems.</p><div id=saddle_point class=theorem-box><p class=theorem-title><strong>Theorem 5.2 (Saddle point characterization)</strong></p><div class=theorem-content>A point $(\mathbf{x}^\star, \boldsymbol{\lambda}^\star)$ solves the constrained optimization problem \eqref{eq:constrained_problem} if and only if it constitutes a saddle point of the Lagrangian function:
\begin{equation}
\mathcal{L}(\mathbf{x}^\star, \boldsymbol{\lambda}) \leq \mathcal{L}(\mathbf{x}^\star, \boldsymbol{\lambda}^\star) \leq \mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}^\star)
\label{eq:saddle_point}
\end{equation}
for all feasible $\mathbf{x}$ and all $\boldsymbol{\lambda} \geq 0$.</div></div><p>The saddle point inequality \eqref{eq:saddle_point} encodes a beautiful mathematical principle. The left inequality tells us that $\mathcal{L}(\mathbf{x}^\star, \boldsymbol{\lambda})$ achieves its maximum over $\boldsymbol{\lambda}$ at $\boldsymbol{\lambda}^\star$, while the right inequality indicates that $\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}^\star)$ achieves its minimum over $\mathbf{x}$ at $\mathbf{x}^\star$. This creates the characteristic saddle shape: the surface curves downward (like a valley) in the primal direction and upward (like a ridge) in the dual direction.</p><h2 id=illustrating-the-saddle-point-through-a-concrete-example>Illustrating the saddle point through a concrete example
<a class=anchor href=#illustrating-the-saddle-point-through-a-concrete-example>#</a></h2><p>To make these abstract concepts concrete, let us examine a specific problem that clearly demonstrates how constraint conflicts create saddle point structures. Consider the problem of minimizing $f(x) = -(x-3)^2$ subject to the constraint $x \geq 1$.</p><p>This example creates a compelling mathematical conflict. The objective function $f(x) = -(x-3)^2$ wants to make the expression $-(x-3)^2$ as small as possible. Since $(x-3)^2$ is always non-negative, the term $-(x-3)^2$ is always non-positive, reaching its maximum value of zero when $x = 3$. To minimize $-(x-3)^2$, we need $(x-3)^2$ to be as large as possible, which drives $x$ away from 3 toward negative infinity.</p><p>However, the constraint $x \geq 1$ acts as a mathematical barrier that prevents this natural tendency. The objective desperately wants to push $x$ toward $-\infty$ where $f(x) \to -\infty$, but the constraint forces the solution to occur at the boundary $x^\star = 1$.</p><p>The Lagrangian for this problem becomes:
\begin{equation}
\mathcal{L}(x,\lambda) = -(x-3)^2 - \lambda(x-1)
\label{eq:example_lagrangian}
\end{equation}</p><p>To find the optimal point, we apply the stationarity condition:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial x} = -2(x-3) - \lambda = 0
\label{eq:stationarity_condition}
\end{equation}</p><p>At the constrained optimum $x^\star = 1$, this gives us:
\begin{equation}
-2(1-3) - \lambda = 0 \Rightarrow 4 - \lambda = 0 \Rightarrow \lambda^\star = 4
\label{eq:optimal_multiplier}
\end{equation}</p><p>We can verify the saddle point property by examining cross-sections of the Lagrangian. When we fix $\lambda = 4$ and vary $x$, we obtain:
\begin{equation}
\mathcal{L}(x,4) = -(x-3)^2 - 4(x-1) = -(x-3)^2 - 4x + 4
\label{eq:primal_cross_section}
\end{equation}</p><p>This function has a unique minimum at $x = 1$, confirming that we should minimize over the primal variable. When we fix $x = 1$ and vary $\lambda$, we get:
\begin{equation}
\mathcal{L}(1,\lambda) = -(1-3)^2 - \lambda(1-1) = -4
\label{eq:dual_cross_section}
\end{equation}</p><p>The Lagrangian becomes constant with respect to $\lambda$ when the constraint is exactly satisfied. This apparent insensitivity to $\lambda$ actually illustrates a profound principle: the dual variable value is uniquely determined by the stationarity requirement, and it encodes the economic value of constraint relaxation.</p><p>The Lagrange multiplier $\lambda^\star = 4$ represents the shadow price of the constraint. If we could relax the constraint from $x \geq 1$ to $x \geq 1 - \epsilon$ for some small $\epsilon > 0$, the optimal objective value would improve by approximately $4\epsilon$. We can verify this directly: with the relaxed constraint, the new optimum would be $x^\star = 1 - \epsilon$, giving $f(1-\epsilon) = -((1-\epsilon)-3)^2 = -(2+\epsilon)^2 = -4 - 4\epsilon - \epsilon^2 \approx -4 - 4\epsilon$ for small $\epsilon$. The improvement of $4\epsilon$ confirms that $\lambda^\star = 4$ correctly captures the economic value of constraint relaxation.</p><h2 id=from-theory-to-computation-the-projected-gradient-method>From theory to computation: the projected gradient method
<a class=anchor href=#from-theory-to-computation-the-projected-gradient-method>#</a></h2><p>The saddle point structure provides both theoretical insight and algorithmic guidance. Since we need to minimize over primal variables and maximize over dual variables, the natural computational approach involves alternating between these two types of updates. This leads to the projected gradient method, which implements the saddle point structure through iterative optimization.</p><div id=projected_gradient class=theorem-box><p class=theorem-title><strong>Theorem 5.3 (Projected gradient algorithm)</strong></p><div class=theorem-content><p>Given the Lagrangian $\mathcal{L}(\mathbf{x}, \boldsymbol{\lambda})$ from \eqref{eq:lagrangian}, the <strong>projected gradient method</strong> alternates between primal minimization and dual maximization:</p><p><strong>Initialize:</strong> $\mathbf{x}^0$, $\boldsymbol{\lambda}^0 \geq \mathbf{0}$</p><p><strong>For</strong> $k = 0, 1, 2, \ldots$ <strong>until convergence:</strong>
\begin{equation}
\begin{aligned}
\mathbf{x}^{k+1} &= \mathbf{x}^k - \alpha_k \nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}^k, \boldsymbol{\lambda}^k) \\
\boldsymbol{\lambda}^{k+1} &= \max(\mathbf{0}, \boldsymbol{\lambda}^k + \beta_k \nabla_{\boldsymbol{\lambda}} \mathcal{L}(\mathbf{x}^{k+1}, \boldsymbol{\lambda}^k))
\end{aligned}
\label{eq:projected_gradient}
\end{equation}
where $\alpha_k > 0$ and $\beta_k > 0$ are step size parameters.</p></div></div><p>The algorithm embodies the saddle point structure through its alternating updates. The primal step performs gradient descent on the Lagrangian with respect to $\mathbf{x}$, following the downward-curving direction of the saddle surface. The dual step performs projected gradient ascent on the Lagrangian with respect to $\boldsymbol{\lambda}$, following the upward-curving direction while maintaining the constraint $\boldsymbol{\lambda} \geq \mathbf{0}$ through the projection operation $\max(\mathbf{0}, \cdot)$.</p><p>To understand the specific update formulas, we need to compute the gradients of the Lagrangian. For our general formulation:</p><p>\begin{equation}
\nabla_{\mathbf{x}} \mathcal{L}(\mathbf{x}, \boldsymbol{\lambda}) = \nabla f(\mathbf{x}) - \sum_{i} \lambda_i \nabla c_i(\mathbf{x})
\label{eq:primal_gradient}
\end{equation}</p><p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \lambda_i} = -c_i(\mathbf{x})
\label{eq:dual_gradient}
\end{equation}</p><p>These gradients reveal the intuitive behavior of the algorithm. The primal update balances the objective gradient against the weighted constraint gradients, with the dual variables serving as the weights that encode constraint importance. The dual update increases $\lambda_i$ when the constraint $c_i(\mathbf{x}) &lt; 0$ is violated and decreases $\lambda_i$ when $c_i(\mathbf{x}) > 0$ provides slack, naturally driving the algorithm toward complementarity.</p><h2 id=exercise-constrained-optimization-with-mixed-constraints><strong>Exercise</strong>: constrained optimization with mixed constraints
<a class=anchor href=#exercise-constrained-optimization-with-mixed-constraints>#</a></h2><p>Let us apply our understanding to a concrete problem that illustrates both the theoretical principles and the algorithmic implementation. This exercise demonstrates how the projected gradient method handles a mixture of equality and inequality constraints.</p><h3 id=problem-setup>Problem Setup
<a class=anchor href=#problem-setup>#</a></h3><p>$$
\begin{aligned}
\text{minimize} \quad & f(x,y) = (x-2)^2 + (y-2)^2 \\
\text{subject to:} \quad & g(x,y) = x + y - 2 = 0 \\
& h_1(x,y) = x \geq 0 \\
& h_2(x,y) = y \geq 0
\end{aligned}
$$</p><p>This problem seeks the point closest to $(2,2)$ that lies on the line $x + y = 2$ while remaining in the first quadrant. The geometric intuition suggests that since the unconstrained minimizer $(2,2)$ lies on the line $x + y = 4$, and our constraint line is $x + y = 2$, the optimal point should be the point on $x + y = 2$ that is closest to $(2,2)$.</p><h3 id=lagrangian-construction>Lagrangian Construction
<a class=anchor href=#lagrangian-construction>#</a></h3><p>For a problem with equality constraints $g_j(\mathbf{x}) = 0$ and inequality constraints $h_i(\mathbf{x}) \geq 0$, the standard Lagrangian is:</p><p>$$\mathcal{L}(\mathbf{x}, \lambda, \mu) = f(\mathbf{x}) - \sum_j \lambda_j g_j(\mathbf{x}) - \sum_i \mu_i h_i(\mathbf{x})$$</p><p>With our constraints:</p><ul><li>Equality: $g(x,y) = x + y - 2 = 0$</li><li>Inequalities: $h_1(x,y) = x \geq 0$ and $h_2(x,y) = y \geq 0$</li></ul><p>The Lagrangian becomes:
$$\mathcal{L}(x,y,\lambda,\mu_1,\mu_2) = (x-2)^2 + (y-2)^2 - \lambda(x + y - 2) - \mu_1 x - \mu_2 y$$</p><h3 id=gradient-computations>Gradient Computations
<a class=anchor href=#gradient-computations>#</a></h3><p>The gradients required for the projected gradient algorithm are:</p><p>$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial x} &= 2(x-2) - \lambda - \mu_1 \\
\frac{\partial \mathcal{L}}{\partial y} &= 2(y-2) - \lambda - \mu_2 \\
\frac{\partial \mathcal{L}}{\partial \lambda} &= -(x + y - 2) \\
\frac{\partial \mathcal{L}}{\partial \mu_1} &= -x \\
\frac{\partial \mathcal{L}}{\partial \mu_2} &= -y
\end{aligned}
$$</p><h3 id=projected-gradient-updates>Projected Gradient Updates
<a class=anchor href=#projected-gradient-updates>#</a></h3><p>The algorithm seeks to minimize $\mathcal{L}$ with respect to primal variables $(x,y)$ and maximize with respect to dual variables $(\lambda, \mu_1, \mu_2)$. The updates become:</p><p>$$
\begin{aligned}
x^{k+1} &= x^k - \alpha \frac{\partial \mathcal{L}}{\partial x} = x^k - \alpha(2(x^k-2) - \lambda^k - \mu_1^k) \\
y^{k+1} &= y^k - \alpha \frac{\partial \mathcal{L}}{\partial y} = y^k - \alpha(2(y^k-2) - \lambda^k - \mu_2^k) \\
\lambda^{k+1} &= \lambda^k + \beta \frac{\partial \mathcal{L}}{\partial \lambda} = \lambda^k + \beta(x^{k+1} + y^{k+1} - 2) \\
\mu_1^{k+1} &= \Pi_{[0,\infty)} \left[\mu_1^k + \beta \frac{\partial \mathcal{L}}{\partial \mu_1}\right] = \max(0, \mu_1^k - \beta x^{k+1}) \\
\mu_2^{k+1} &= \Pi_{[0,\infty)} \left[\mu_2^k + \beta \frac{\partial \mathcal{L}}{\partial \mu_2}\right] = \max(0, \mu_2^k - \beta y^{k+1})
\end{aligned}
$$</p><p>Note that:</p><ul><li>For the equality constraint, we do not project $\lambda^{k+1}$ since equality constraint multipliers can take any real value</li><li>For inequality constraints, we project $\mu_i$ onto $[0,\infty)$ to maintain dual feasibility</li></ul><h3 id=analytical-solution>Analytical Solution
<a class=anchor href=#analytical-solution>#</a></h3><p>To understand what the algorithm should converge to, let us solve the problem analytically using the KKT conditions. At the optimal point, we need:</p><h4 id=kkt-conditions>KKT Conditions:
<a class=anchor href=#kkt-conditions>#</a></h4><ol><li><strong>Stationarity:</strong> $\nabla_x \mathcal{L} = 0$ and $\nabla_y \mathcal{L} = 0$</li><li><strong>Primal feasibility:</strong> $x + y - 2 = 0$, $x \geq 0$, $y \geq 0$</li><li><strong>Dual feasibility:</strong> $\mu_1 \geq 0$, $\mu_2 \geq 0$</li><li><strong>Complementary slackness:</strong> $\mu_1 x = 0$, $\mu_2 y = 0$</li></ol><p>Since the solution likely lies in the interior of the first quadrant (given the symmetry of the problem), we expect both inequality constraints to be inactive, meaning $\mu_1^\star = \mu_2^\star = 0$.</p><p>With $\mu_1^\star = \mu_2^\star = 0$, the stationarity conditions become:
$$
\begin{aligned}
2(x^<em>-2) - \lambda^</em> &= 0 \\
2(y^<em>-2) - \lambda^</em> &= 0 \\
x^* + y^* - 2 &= 0
\end{aligned}
$$</p><p>From the first two equations: $2(x^<em>-2) = 2(y^</em>-2)$, which implies $x^* = y^*$.</p><p>Substituting into the equality constraint:
$$x^* + x^* = 2 \Rightarrow x^* = y^* = 1$$</p><p>The Lagrange multiplier for the equality constraint is:
$$\lambda^* = 2(1-2) = -2$$</p><p>The negative value of $\lambda^*$ indicates that relaxing the constraint (increasing the right-hand side from 2 to $2 + \epsilon$) would worsen the objective by approximately $2\epsilon$.</p><h3 id=verification>Verification
<a class=anchor href=#verification>#</a></h3><p>We verify that $(x^\star, y^\star) = (1, 1)$ with $\lambda^\star = -2$ and $\mu_1^\star = \mu_2^\star = 0$ satisfies all KKT conditions:</p><ul><li><strong>Stationarity:</strong><ul><li>$\frac{\partial \mathcal{L}}{\partial x} = 2(1-2) - (-2) - 0 = -2 + 2 = 0$ ✓</li><li>$\frac{\partial \mathcal{L}}{\partial y} = 2(1-2) - (-2) - 0 = -2 + 2 = 0$ ✓</li></ul></li><li><strong>Primal feasibility:</strong><ul><li>$1 + 1 - 2 = 0$ ✓</li><li>$1 \geq 0$ and $1 \geq 0$ ✓</li></ul></li><li><strong>Dual feasibility:</strong> $\mu_1^* = 0 \geq 0$ and $\mu_2^* = 0 \geq 0$ ✓</li><li><strong>Complementary slackness:</strong> $\mu_1^\star \cdot 1 = 0$ and $\mu_2^\star \cdot 1 = 0$ ✓</li></ul><h3 id=geometric-interpretation>Geometric Interpretation
<a class=anchor href=#geometric-interpretation>#</a></h3><p>The solution $(1,1)$ is indeed the point on the line $x + y = 2$ that is closest to $(2,2)$. The projected gradient algorithm will converge to this solution, automatically determining that the inequality constraints are inactive through the projection steps that drive $\mu_1$ and $\mu_2$ toward zero.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#the-foundation-kkt-conditions-and-the-lagrangian>The foundation: KKT conditions and the Lagrangian</a></li><li><a href=#the-emergence-of-saddle-point-structure>The emergence of saddle point structure</a></li><li><a href=#illustrating-the-saddle-point-through-a-concrete-example>Illustrating the saddle point through a concrete example</a></li><li><a href=#from-theory-to-computation-the-projected-gradient-method>From theory to computation: the projected gradient method</a></li><li><a href=#exercise-constrained-optimization-with-mixed-constraints><strong>Exercise</strong>: constrained optimization with mixed constraints</a><ul><li><a href=#problem-setup>Problem Setup</a></li><li><a href=#lagrangian-construction>Lagrangian Construction</a></li><li><a href=#gradient-computations>Gradient Computations</a></li><li><a href=#projected-gradient-updates>Projected Gradient Updates</a></li><li><a href=#analytical-solution>Analytical Solution</a></li><li><a href=#verification>Verification</a></li><li><a href=#geometric-interpretation>Geometric Interpretation</a></li></ul></li></ul></nav></div></aside></main></body></html>