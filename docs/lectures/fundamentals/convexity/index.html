<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Convexity theory
  #

Convexity is a powerful property of functions and sets that simplifies the analysis of optimization problems and the characterization of global minimizers. In this chapter, we will explore the concepts of convex sets, convex functions, and their implications for unconstrained optimization.

  Convex sets
  #

Let us first start by defining the convexity of a given set $\mathcal{S}\subset\mathbb{R}^d$:











  Definition 3.1 (Convex set)
  
    Let $\mathcal{S}\subset\mathbb{R}^d$ be a set. The set $\mathcal{S}$ is convex if, for any two points $\mathbf{x}, \mathbf{y} \in \mathcal{S}$, the line segment that connects them is also contained in $\mathcal{S}$, that is,
\begin{equation}
\mathbf{x}, \mathbf{y} \in \mathcal{S} \implies \lambda \mathbf{x} + (1-\lambda) \mathbf{y} \in \mathcal{S}, \quad \forall \lambda \in [0, 1].
\label{eq:convex_set}
\end{equation}
  




  
    









  
    
      Figure 3.1: Convex set"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/convexity/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="3. Convexity theory"><meta property="og:description" content="Convexity theory # Convexity is a powerful property of functions and sets that simplifies the analysis of optimization problems and the characterization of global minimizers. In this chapter, we will explore the concepts of convex sets, convex functions, and their implications for unconstrained optimization.
Convex sets # Let us first start by defining the convexity of a given set $\mathcal{S}\subset\mathbb{R}^d$:
Definition 3.1 (Convex set)
Let $\mathcal{S}\subset\mathbb{R}^d$ be a set. The set $\mathcal{S}$ is convex if, for any two points $\mathbf{x}, \mathbf{y} \in \mathcal{S}$, the line segment that connects them is also contained in $\mathcal{S}$, that is, \begin{equation} \mathbf{x}, \mathbf{y} \in \mathcal{S} \implies \lambda \mathbf{x} + (1-\lambda) \mathbf{y} \in \mathcal{S}, \quad \forall \lambda \in [0, 1]. \label{eq:convex_set} \end{equation} Figure 3.1: Convex set"><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>3. Convexity theory | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/convexity/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.6698db5ec0b50ada94dfb50e4977481fdc66b0a9d0e104277817f19b95d6da6f.js integrity="sha256-ZpjbXsC1CtqU37UOSXdIH9xmsKnQ4QQneBfxm5XW2m8=" crossorigin=anonymous></script><script>const chapterNum=3;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/ class=active>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_projected/>5b. Constrained optimization - Projected Gradient Descent</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimization_linear/>6. Constrained optimization - Linear programming</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/fundamentals/>1. Machine learning fundamentals</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Classification and support vector machines</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - MNIST and Fashion-MNIST Classification</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li><li><a href=/numerical_optimization/docs/practical_labs/quasinewton/>Quasi-Newton methods memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>3. Convexity theory</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#convex-sets>Convex sets</a></li><li><a href=#convex-functions>Convex functions</a></li><li><a href=#convexity-and-unconstrained-optimization>Convexity and unconstrained optimization</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=convexity-theory>Convexity theory
<a class=anchor href=#convexity-theory>#</a></h1><p>Convexity is a powerful property of functions and sets that simplifies the analysis of optimization problems and the characterization of global minimizers. In this chapter, we will explore the concepts of convex sets, convex functions, and their implications for unconstrained optimization.</p><h2 id=convex-sets>Convex sets
<a class=anchor href=#convex-sets>#</a></h2><p>Let us first start by defining the convexity of a given set $\mathcal{S}\subset\mathbb{R}^d$:</p><div id=convex_set class=theorem-box><p class=theorem-title><strong>Definition 3.1 (Convex set)</strong></p><div class=theorem-content>Let $\mathcal{S}\subset\mathbb{R}^d$ be a set. The set $\mathcal{S}$ is convex if, for any two points $\mathbf{x}, \mathbf{y} \in \mathcal{S}$, the line segment that connects them is also contained in $\mathcal{S}$, that is,
\begin{equation}
\mathbf{x}, \mathbf{y} \in \mathcal{S} \implies \lambda \mathbf{x} + (1-\lambda) \mathbf{y} \in \mathcal{S}, \quad \forall \lambda \in [0, 1].
\label{eq:convex_set}
\end{equation}</div></div><div class=center-container><div class=center-content><figure id=convex_set><img src=%20../../../../../../tikZ/convex_set/main.svg alt="Zig zag" width=400px><figcaption><p><strong>Figure 3.1: </strong>Convex set</p></figcaption></figure></div></div><p>This is illustrated by Figure
<a href=#convex_set>3.1</a>
, where the set $\mathcal{S}$ is convex, as the line segment between any two points $\mathbf{x}$ and $\mathbf{y}$ lies entirely within $\mathcal{S}$. If this property does not hold, then the set is called non-convex.</p><p>While we will not do deeper now, this property is desirable for the constraints of an optimization problem, .because it means that for a given algorithm, any subsequent step is feasible by staying true to the given constraints for the problem.</p><h2 id=convex-functions>Convex functions
<a class=anchor href=#convex-functions>#</a></h2><p>A function $f:\mathbb{R}^n\to\mathbb{R}$ is convex if its domain is a convex set and it satisfies the following property:</p><div id=convex_function class=theorem-box><p class=theorem-title><strong>Definition 3.2 (Convex function)</strong></p><div class=theorem-content>A function $f:\mathbb{R}^n\to\mathbb{R}$ is convex if, for any two points $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ and for all $\lambda \in [0, 1]$, the following inequality holds:
\begin{equation}
f(\lambda \mathbf{x} + (1-\lambda) \mathbf{y}) \leq \lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}).
\label{eq:convex_function}
\end{equation}</div></div><div class=center-container><div class=center-content><figure id=convex_func><img src=%20../../../../../../tikZ/convex_func/main.svg alt="Zig zag" width=600px><figcaption><p><strong>Figure 3.2: </strong>Convex function</p></figcaption></figure></div></div><p>This means that the line segment connecting the points $(\mathbf{x}, f(\mathbf{x}))$ and $(\mathbf{y}, f(\mathbf{y}))$ lies above the graph of the function $f$. In other words, the function is &ldquo;bowl-shaped&rdquo; or &ldquo;curves upwards&rdquo;. Such an illustration is given for a 1-dimensional function in Figure
<a href=#convex_func>3.2</a>
, where the function $f$ is convex, as the line segment between any two points $(\mathbf{x}, f(\mathbf{x}))$ and $(\mathbf{y}, f(\mathbf{y}))$ lies above the graph of $f$.</p><p>In practice to show that a function is convex, we can make use of the following properties, given convex functions $f$ and $g$:</p><ul><li>let $\alpha, \beta>0$, then $\alpha f + \beta g$ is convex</li><li>$f \circ g$ is convex</li></ul><h2 id=convexity-and-unconstrained-optimization>Convexity and unconstrained optimization
<a class=anchor href=#convexity-and-unconstrained-optimization>#</a></h2><p>When the objective function is convex, local and global minimizers are simple to characterize.</p><p><div id=local_global_convex class=theorem-box><p class=theorem-title><strong>Theorem 3.1</strong></p><div class=theorem-content>When $f$ is convex, any local minimizer $\mathbf{x}^\star$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $\mathbf{x}^\star$ is a global minimizer of $f$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Suppose that $\mathbf{x}^\star$ is a local but not a global minimizer. Then we can find a point $\mathbf{z} \in \mathbb{R}^n$ with $f(\mathbf{z})&lt;f\left(\mathbf{x}^\star\right)$. Consider the line segment that joins $\mathbf{x}^\star$ to $\mathbf{z}$, that is,
\begin{equation}
\mathbf{x}=\lambda \mathbf{z}+(1-\lambda) \mathbf{x}^\star, \quad \text { for some } \lambda \in(0,1]
\label{eq:line_segment}
\end{equation}
By the convexity property for $f$, we have
\begin{equation}
f(\mathbf{x}) \leq \lambda f(\mathbf{z})+(1-\lambda) f\left(\mathbf{x}^\star\right)&lt;f\left(\mathbf{x}^\star\right)
\label{eq:convexity}
\end{equation}</p><p>Any neighborhood $\mathcal{N}$ of $\mathbf{x}^\star$ contains a piece of the line segment \eqref{eq:line_segment}, so there will always be points $\mathbf{x} \in \mathcal{N}$ at which \eqref{eq:convexity} is satisfied. Hence, $\mathbf{x}^\star$ is not a local minimizer.
For the second part of the theorem, suppose that $\mathbf{x}^\star$ is not a global minimizer and choose $\mathbf{z}$ as above. Then, from convexity, we have</p><p>\begin{equation}
\begin{aligned}
\nabla f\left(\mathbf{x}^\star\right)^{\mathrm{T}}\left(\mathbf{z}-\mathbf{x}^\star\right) & =\left.\frac{d}{d \lambda} f\left(\mathbf{x}^\star+\lambda\left(\mathbf{z}-\mathbf{x}^\star\right)\right)\right|_{\lambda=0} \\
& =\lim _{\lambda \downarrow 0} \frac{f\left(\mathbf{x}^\star+\lambda\left(\mathbf{z}-\mathbf{x}^\star\right)\right)-f\left(\mathbf{x}^\star\right)}{\lambda} \\
& \leq \lim _{\lambda \downarrow 0} \frac{\lambda f(\mathbf{z})+(1-\lambda) f\left(\mathbf{x}^\star\right)-f\left(\mathbf{x}^\star\right)}{\lambda} \\
& =f(\mathbf{z})-f\left(\mathbf{x}^\star\right)&lt;0
\end{aligned}
\end{equation}</p><p>Therefore, $\nabla f\left(\mathbf{x}^\star\right) \neq 0$, and so $\mathbf{x}^\star$ is not a stationary point.</p></div><div class=qed>■</div></div></p><p>This result is fundamental in optimization, as it guarantees that if we find a local minimizer of a convex function, we can be sure that it is also the global minimizer. This property greatly simplifies the search for optimal solutions. As such, finding that the function we minimize is convex often means that the problem is easier to solve, as we can use algorithms that are guaranteed to converge to the global minimum.</p><p>Conversely, in the design stage, we might prefer to design a convex function, or try to find a convex approximation of a non-convex function, to ensure that the optimization problem is well-behaved and that we can find the global minimum efficiently.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#convex-sets>Convex sets</a></li><li><a href=#convex-functions>Convex functions</a></li><li><a href=#convexity-and-unconstrained-optimization>Convexity and unconstrained optimization</a></li></ul></nav></div></aside></main></body></html>