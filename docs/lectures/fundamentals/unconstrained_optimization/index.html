<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Unconstrained optimization - basics
  #

We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve:
$$
\underset{\mathbf{x}\in\mathbb{R}^d}{\operatorname{argmin}} f(\mathbf{x}).
$$
Let us try to characterizes the nature of the solutions under this setup.

  What is a solution ?
  #










  
    
      Figure 2.1: Local and global minimum can coexist.
  


Generally, we would be happiest if we found a global minimizer of $f$ , a point where the
function attains its least value. A formal definition is :"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="2. Unconstrained optimization : basics"><meta property="og:description" content="Unconstrained optimization - basics # We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve: $$ \underset{\mathbf{x}\in\mathbb{R}^d}{\operatorname{argmin}} f(\mathbf{x}). $$
Let us try to characterizes the nature of the solutions under this setup.
What is a solution ? # Figure 2.1: Local and global minimum can coexist.
Generally, we would be happiest if we found a global minimizer of $f$ , a point where the function attains its least value. A formal definition is :"><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>2. Unconstrained optimization : basics | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.c50c2548bdfada0b54f1fbad3fc4be995c047f4e83d36b02c31176f4d5c86c3b.js integrity="sha256-xQwlSL362gtU8futP8S+mVwEf06D02sCwxF29NXIbDs=" crossorigin=anonymous></script><script>const chapterNum=2;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/ class=active>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>6. Constrained optimization</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>2. Unconstrained optimization : basics</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#what-is-a-solution->What is a solution ?</a></li><li><a href=#taylors-theorem>Taylor&rsquo;s theorem</a></li><li><a href=#sufficient-and-necessary-conditions-for-local-minima>Sufficient and necessary conditions for local minima</a></li><li><a href=#the-need-for-algorithms>The need for algorithms</a></li><li><a href=#steepest-descent-approach>Steepest-descent approach</a><ul><li><a href=#understanding-the-algorithm-step-by-step>Understanding the algorithm step by step</a></li><li><a href=#why-steepest-descent-can-struggle>Why steepest descent can struggle</a></li><li><a href=#convergence-properties>Convergence properties</a></li></ul></li><li><a href=#newton-method>Newton method</a><ul><li><a href=#the-mathematical-foundation>The mathematical foundation</a></li><li><a href=#the-geometric-insight>The geometric insight</a></li><li><a href=#the-power-of-quadratic-convergence>The power of quadratic convergence</a></li><li><a href=#the-computational-cost>The computational cost</a></li><li><a href=#when-newtons-method-can-fail>When Newton&rsquo;s method can fail</a></li></ul></li><li><a href=#looking-ahead-the-bridge-between-methods>Looking ahead: the bridge between methods</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=unconstrained-optimization---basics>Unconstrained optimization - basics
<a class=anchor href=#unconstrained-optimization---basics>#</a></h1><p>We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve:
$$
\underset{\mathbf{x}\in\mathbb{R}^d}{\operatorname{argmin}} f(\mathbf{x}).
$$</p><p>Let us try to characterizes the nature of the solutions under this setup.</p><h2 id=what-is-a-solution->What is a solution ?
<a class=anchor href=#what-is-a-solution->#</a></h2><figure id="figure-%!s(int=2)-1"><img src=%20../../../../../../tikZ/local_global_minima/main.svg alt="Local vs global" width=600px><figcaption><p><strong>Figure 2.1: </strong>Local and global minimum can coexist.</p></figcaption></figure><p>Generally, we would be happiest if we found a global minimizer of $f$ , a point where the
function attains its least value. A formal definition is :</p><div id="definition-%!s(int=2)-1" class=theorem-box><p class=theorem-title><strong>Definition 2.1 (Global minimizer)</strong></p><div class=theorem-content>A point $\mathbf{x}^\star$ is a <strong>global minimizer</strong> if $f(\mathbf{x}^\star)\leq f(\mathbf{x})$,
where $\mathbf{x}$ ranges over all of $\mathbb{R}^d$ (or at least over the domain of interest to the modeler).</div></div><p>The global minimizer can be difficult to find, because our knowledge of $f$ is usually only local.
Since our algorithm does not visit many points (we hope!), we usually do not have a good
picture of the overall shape of $f$ , and we can never be sure that the function does not take a
sharp dip in some region that has not been sampled by the algorithm. Most algorithms are
able to find only a local minimizer, which is a point that achieves the smallest value of f in
its neighborhood. Formally, we say:</p><div id="definition-%!s(int=2)-2" class=theorem-box><p class=theorem-title><strong>Definition 2.2 (Local minimizer)</strong></p><div class=theorem-content>A point $\mathbf{x}^\star$ is a <strong>local minimizer</strong> if $\exists r>0,\, f(\mathbf{x}^\star)\leq f(\mathbf{x})$, $\forall \mathbf{x}\in\mathcal{B}(\mathbf{x}^\star, r)$.</div></div><p>A point that satisfies
this definition is sometimes called a <strong>weak local minimizer</strong>. Alternatively, when $f(\mathbf{x}^\star)&lt;f(\mathbf{x})$, we say that the minimum is a <strong>strict local minimizer</strong>.</p><h2 id=taylors-theorem>Taylor&rsquo;s theorem
<a class=anchor href=#taylors-theorem>#</a></h2><p>From the definitions given above, it might seem that the only way to find out whether
a point $\mathbf{x}^\star$ is a local minimum is to examine all the points in its immediate vicinity, to make
sure that none of them has a smaller function value. When the function $f$ is smooth, however,
there are much more efficient and practical ways to identify local minima. In particular, if $f$
is twice continuously differentiable, we may be able to tell that $\mathbf{x}^\star$ is a local minimizer (and
possibly a strict local minimizer) by examining just the gradient $\nabla f (\mathbf{x}^\star)$ and the Hessian
$\nabla^2 f (\mathbf{x}^\star)$.
The mathematical tool used to study minimizers of smooth functions is Taylor’s the-
orem. Because this theorem is central to our analysis we state it now.</p><p><div id=taylor_theory class=theorem-box><p class=theorem-title><strong>Theorem 2.1 (Taylor's theorem)</strong></p><div class=theorem-content><p>Suppose that $f:\mathbb{R}^d\mapsto\mathbb{R}$ is continuously differentiable and that we have $\mathbf{p}\in\mathbb{R}^d $. The we have :
\begin{equation}
f(\mathbf{x}+\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x}+t\mathbf{p})^\mathrm{T}\mathbf{p},
\end{equation}
for some $t\in [0,1]$.</p><p>Moreover, if $f$ is twice continuously differentiable, we have :
\begin{equation}
f(\mathbf{x}+\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^\mathrm{T}\mathbf{p} + \frac{1}{2}\mathbf{p}^\mathrm{T}\nabla^2 f(\mathbf{x}+t\mathbf{p})\mathbf{p}),
\end{equation}
for some $t\in [0,1]$.</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>See any calculus book</div><div class=qed>■</div></div></p><p>Note that in this formulation, the definition is exact and the $t$ scalar is usually unknown. The interest lies in skeching proofs. In practical matters, we rather use the following approximation:<div id=taylor_approximation class=theorem-box><p class=theorem-title><strong>Theorem 2.2 (Taylor's approximation)</strong></p><div class=theorem-content><p>First order approximation:
\begin{equation}
f(\mathbf{x}+\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^\mathrm{T}\mathbf{p} + o(\lVert\mathbf{p}\rVert),
\end{equation}</p><p>Second-order approximation:</p><p>\begin{equation}
f(\mathbf{x}+\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^\mathrm{T}\mathbf{p} + \frac{1}{2}\mathbf{p}^\mathrm{T}\nabla^2 f(\mathbf{x})\mathbf{p} + o(\lVert\mathbf{p}\rVert^2),
\end{equation}</p><p>where $o(\lVert\mathbf{p}\rVert)$ and $o(\lVert\mathbf{p}\rVert^2)$ represent terms that grow slower than $\lVert\mathbf{p}\rVert$ and $\lVert\mathbf{p}\rVert^2$ respectively as $\lVert\mathbf{p}\rVert \to 0$.</p></div></div></p><h2 id=sufficient-and-necessary-conditions-for-local-minima>Sufficient and necessary conditions for local minima
<a class=anchor href=#sufficient-and-necessary-conditions-for-local-minima>#</a></h2><p>Let us consider a local minimum and see how they can be characterized to later design appropriate solution finding methods. The first well-known result is as follows:<div id=first-order_necessary class=theorem-box><p class=theorem-title><strong>Theorem 2.3 (First-order necessary conditions)</strong></p><div class=theorem-content>if $\mathbf{x}^\star$ is a local minimize, and $f$ is continuously differentiable in a neighborhood of $\mathbf{x}^\star$, then $\nabla f(\mathbf{x}^\star) = \mathbf{0}$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Suppose for contradiction that $\nabla f(\mathbf{x}^\star) \neq 0$, and define vector $\mathbf{p}=-\nabla f(\mathbf{x}^\star)$ such that by construction $\mathbf{p}^\mathrm{T}\nabla f(\mathbf{x}^\star) = - \lVert f(\mathbf{x}^\star) \rVert^2 &lt; 0$.</p><p>Since $f$ is a continuous function, we can define a scalar $T>0$ such that $\forall t\in [0,T[$, we still have:
$$
\mathbf{p}^\mathrm{T}f(\mathbf{x}+t\mathbf{p}) &lt; 0.
$$</p><p>Using <a href=#taylor_theory>Theorem 2.1</a> first-order result, we can write:
$$
f(\mathbf{x}^\star+t\mathbf{p}) = f(\mathbf{x}^\star) + t\mathbf{p}^\mathrm{T}\nabla f(\mathbf{x}^\star+\overline{t}\mathbf{p}),
$$
for some $\overline{t}\in[0,T[$ and any $t\in[0,T[$. Given previous inequality, we obtain:
$$
f(\mathbf{x}^\star+t\mathbf{p}) &lt; f(\mathbf{x}^\star),
$$
which contradicts the fact that $\mathbf{x}^\star$ is a local minimizer.</p></div><div class=qed>■</div></div></p><p>Henceforth, we will call <strong>stationary point</strong>, any $\mathbf{x}$ such that $\nabla f(\mathbf{x}) = 0$.</p><p>For the next result we recall that a matrix $\mathbf{B}$ is positive definite if $\mathbf{p}^\mathrm{T} \mathbf{B} \mathbf{p}>0$ for all $\mathbf{p} \neq \mathbf{0}$, and positive semidefinite if $\mathbf{p}^\mathrm{T} \mathbf{B} \mathbf{p} \geq 0$ for all $\mathbf{p}$.</p><div id="theorem-%!s(int=2)-4" class=theorem-box><p class=theorem-title><strong>Theorem 2.4 (Second-order necessary conditions)</strong></p><div class=theorem-content>If $\mathbf{x}^\star$ is a local minimizer of $f$ and $\nabla^2 f$ is continuous in an open neighborhood of $\mathbf{x}^\star$, then $\nabla f\left(\mathbf{x}^\star\right)=0$ and $\nabla^2 f\left(\mathbf{x}^\star\right)$ is positive semidefinite.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Proof. We know from <a href=#first-order_necessary>Theorem 2.3</a> that $\nabla f\left(\mathbf{x}^\star\right)=0$. For contradiction, assume that $\nabla^2 f\left(\mathbf{x}^\star\right)$ is not positive semidefinite. Then we can choose a vector $\mathbf{p}$ such that $\mathbf{p}^T \nabla^2 f\left(\mathbf{x}^\star\right) \mathbf{p}&lt;0$, and because $\nabla^2 f$ is continuous near $\mathbf{x}^\star$, there is a scalar $T>0$ such that $\mathbf{p}^\mathrm{T} \nabla^2 f\left(\mathbf{x}^*+\overline{t} \mathbf{p}\right) \mathbf{p}&lt;0$ for all $t \in[0, T[$.</p><p>By doing a Taylor series expansion around $\mathbf{x}^\star$, we have for all $\bar{t} \in[0, T[$ and some $t \in[0, \bar{t}]$ that</p><p>$$
f\left(\mathbf{x}^\star+\bar{t} \mathbf{p}\right) = f\left(\mathbf{x}^\star\right)+\bar{t} \mathbf{p}^\mathrm{T} \nabla f\left(\mathbf{x}^\star\right)+\frac{1}{2} \bar{t}^2 \mathbf{p}^\mathrm{T} \nabla^2 f\left(\mathbf{x}^\star+t \mathbf{p}\right) \mathbf{p}&lt;f\left(\mathbf{x}^\star\right) .
$$</p><p>As in <a href=#first-order_necessary>Theorem 2.3</a>, we have found a direction from $\mathbf{x}^\star$ along which $f$ is decreasing, and so again, $\mathbf{x}^\star$ is not a local minimizer.</p></div><div class=qed>■</div></div><p>We now describe sufficient conditions, which are conditions on the derivatives of $f$ at the point $\mathbf{z}^\star$ that guarantee that $\mathbf{x}^\star$ is a local minimizer.</p><p><div id=second-order_sufficient class=theorem-box><p class=theorem-title><strong>Theorem 2.5 (Second-Order Sufficient Conditions)</strong></p><div class=theorem-content>Suppose that $\nabla^2 f$ is continuous in an open neighborhood of $\mathbf{x}^\star$ and that $\nabla f\left(\mathbf{x}^\star\right)=0$ and $\nabla^2 f\left(\mathbf{x}^\star\right)$ is positive definite. Then $\mathbf{x}^\star$ is a strict local minimizer of $f$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Because the Hessian is continuous and positive definite at $\mathbf{x}^\star$, we can choose a radius $r>0$ so that $\nabla^2 f(x)$ remains positive definite for all $x$ in the open ball $\mathcal{D}=\left\{\mathbf{z} \mid\left\lVert\mathbf{z}-\mathbf{x}^\star\right\rVert&lt;\right.$ $r\}$. Taking any nonzero vector $p$ with $\lVert\mathbf{p}\rVert&lt;r$, we have $\mathbf{x}^\star+\mathbf{p} \in \mathcal{D}$ and so</p><p>$$
\begin{aligned}
f\left(\mathbf{x}^\star+p\right) & =f\left(\mathbf{x}^\star\right)+\mathbf{p}^\mathrm{T} \nabla f\left(\mathbf{x}^\star\right)+\frac{1}{2} \mathbf{p}^\mathrm{T} \nabla^2 f(\mathbf{z}) \mathbf{p} \
& =f\left(\mathbf{x}^\star\right)+\frac{1}{2} \mathbf{p}^\mathrm{T} \nabla^2 f(\mathbf{z}) \mathbf{p}
\end{aligned}
$$</p><p>where $\mathbf{z}=\mathbf{x}^\star+t \mathbf{p}$ for some $t \in(0,1)$. Since $\mathbf{z} \in \mathcal{D}$, we have $\mathbf{p}^{\mathrm{T}} \nabla^2 f(\mathbf{z}) \mathbf{p}>0$, and therefore $f\left(\mathbf{x}^\star+\mathbf{p}\right)>f\left(\mathbf{x}^\star\right)$, giving the result.</p></div><div class=qed>■</div></div></p><p>Note that the second-order sufficient conditions of <a href=#second-order_sufficient>Theorem 2.5</a> guarantee something stronger than the necessary conditions discussed earlier; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: A point $\mathbf{x}^\star$ may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function $f(x)=x^4$, for which the point $x^\star=0$ is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite).
These results, which are based on elementary calculus, provide the foundations for unconstrained optimization algorithms. In one way or another, all algorithms seek a point where $\nabla f(\cdot)$ vanishes.</p><h2 id=the-need-for-algorithms>The need for algorithms
<a class=anchor href=#the-need-for-algorithms>#</a></h2><p>On question we might ask here is why do we need algorithms to find local minima? After all, we have just shown that if $\nabla f(\mathbf{x}^\star)=0$, then $\mathbf{x}^\star$ is a local minimizer. The answer is that in practice, we do not always have the luxury to know the exact solution to $\nabla f(\mathbf{x})=0$. Moreover, we can&rsquo;t always compute the Hessian matrix to check the second-order conditions.</p><p>Thus, to circumvent the need to solve analytically the equations $\nabla f(\mathbf{x})=0$, we will design algorithms that iteratively update a point $\mathbf{x}$ until it converges to a local minimizer. The algorithms will be based on the properties of the gradient and Hessian, and will use the information they provide to guide the search for a local minimum. When hessian is not computable or too expensive to compute, we will use the gradient only, and the algorithms will be called <strong>gradient-based methods</strong>. When the Hessian is available, we will use it to accelerate convergence, and the algorithms will be called <strong>Newton methods</strong>. As a between between these methods lie <strong>quasi-Newton methods</strong>, which use an approximation of the Hessian to guide the search for a local minimum.</p><p>But before we dive in more complicated algorithms, let us consider the most obvious approaches and try to understand their limitations.</p><h2 id=steepest-descent-approach>Steepest-descent approach
<a class=anchor href=#steepest-descent-approach>#</a></h2><p>The first algorithm we consider is the so-called <strong>Steepest-descent</strong> algorithm which involves choosing an initial point $\mathbf{x}_0$ and compute a series of subsequent points with the following formula:</p><p>\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \nabla f(\mathbf{x}_k),
\label{eq: steepest descent}
\end{equation}</p><p>where $\alpha_k$ are a series of scalar values called <strong>step-size</strong> (or learning rate in machine learning context).</p><p>The intuition behind this approach is beautifully simple yet profound. Imagine yourself standing on a mountainside in thick fog, trying to find the bottom of the valley. Since you can&rsquo;t see the overall landscape, the most sensible strategy is to feel the slope beneath your feet and take a step in the direction that descends most steeply. This is precisely what the steepest-descent algorithm does mathematically.</p><p>The negative gradient $-\nabla f(\mathbf{x}_k)$ points in the direction of steepest decrease of the function at point $\mathbf{x}_k$. This isn&rsquo;t just a convenient mathematical fact—it&rsquo;s the fundamental geometric property that makes gradient-based optimization possible. By moving in this direction, we ensure that we&rsquo;re making the most aggressive local progress toward reducing the function value.</p><div class=center-container><div class=center-content><figure id=succesful_unconstained_optim><img src=%20../../../../../../tikZ/succesful_unconstained_optim/main.svg alt="Zig zag" width=400px><figcaption><p><strong>Figure 2.2: </strong>Optimization with steepest descent</p></figcaption></figure></div></div><h3 id=understanding-the-algorithm-step-by-step>Understanding the algorithm step by step
<a class=anchor href=#understanding-the-algorithm-step-by-step>#</a></h3><p>Let&rsquo;s walk through what happens in each iteration of steepest descent. Starting from point $\mathbf{x}_k$, we compute the gradient $\nabla f(\mathbf{x}_k)$. This vector tells us which direction the function increases most rapidly. Since we want to minimize, we go in the opposite direction: $-\nabla f(\mathbf{x}_k)$.</p><p>The step size $\alpha_k$ determines how far we travel in this direction. Think of it as the length of your stride as you walk down the mountain. The choice of step size involves a fundamental trade-off: too small and you make painfully slow progress; too large and you might overshoot the valley bottom or even start climbing uphill again.</p><h3 id=why-steepest-descent-can-struggle>Why steepest descent can struggle
<a class=anchor href=#why-steepest-descent-can-struggle>#</a></h3><div class=center-container><div class=center-content><figure id=zigzag><img src=%20../../../../../../tikZ/zigzag_valley_optimization/main.svg alt="Zig zag" width=400px><figcaption><p><strong>Figure 2.3: </strong>Problem of stepsize</p></figcaption></figure></div></div><p>Here&rsquo;s where the method reveals its first major limitation. Consider a function that looks like a long, narrow valley—mathematically, this corresponds to a function with a large condition number. Steepest descent exhibits what we call &ldquo;zigzag behavior&rdquo; in such cases as illustrated in Figure
<a href=#zigzag>2.3</a>
.</p><p>Picture this scenario: you&rsquo;re in a narrow canyon, and the steepest direction points toward one wall rather than down the canyon. You take steps toward that wall, then the gradient changes direction and points toward the other wall. Instead of walking efficiently down the canyon, you find yourself bouncing back and forth between the walls, making very slow progress toward your destination.</p><p>This zigzag pattern occurs because steepest descent is fundamentally myopic. At each step, it only considers the immediate local slope and ignores the broader geometric structure of the function. The algorithm doesn&rsquo;t &ldquo;remember&rdquo; where it came from or &ldquo;anticipate&rdquo; where the function is heading.</p><h3 id=convergence-properties>Convergence properties
<a class=anchor href=#convergence-properties>#</a></h3><p>Despite these limitations, steepest descent does have reliable convergence properties. Under reasonable mathematical conditions—essentially requiring that the function is well-behaved and doesn&rsquo;t have any pathological features—the algorithm will eventually reach a stationary point where the gradient vanishes.</p><p>The convergence is what we call &ldquo;linear,&rdquo; meaning that the error decreases by a constant factor at each iteration. While this sounds reasonable, it can be frustratingly slow in practice, especially for poorly conditioned problems where that constant factor is very close to one.</p><h2 id=newton-method>Newton method
<a class=anchor href=#newton-method>#</a></h2><p>If steepest descent is like navigating with only your immediate sense of slope, Newton&rsquo;s method is like having a detailed topographic map of your local surroundings. This method incorporates not just information about which way is downhill, but also how the slope itself is changing—what we call the curvature of the function.</p><h3 id=the-mathematical-foundation>The mathematical foundation
<a class=anchor href=#the-mathematical-foundation>#</a></h3><p>Newton&rsquo;s method emerges from a clever idea: instead of trying to minimize the original function directly, let&rsquo;s create a simpler approximation and minimize that instead. We use the second-order Taylor approximation around our current point $\mathbf{x}_k$:</p><p>$$f(\mathbf{x}_k + \mathbf{p}) \approx f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^T\mathbf{p} + \frac{1}{2}\mathbf{p}^T\nabla^2 f(\mathbf{x}_k)\mathbf{p}$$</p><p>This quadratic approximation captures both the slope (first-order term) and the curvature (second-order term) at our current location. The brilliant insight is that quadratic functions are easy to minimize—we simply set the gradient of the approximation equal to zero and solve for the optimal step $\mathbf{p}$.</p><p>Taking the gradient of our quadratic model and setting it to zero gives us:
$$\nabla f(\mathbf{x}_k) + \nabla^2 f(\mathbf{x}_k)\mathbf{p} = \mathbf{0}$$</p><p>Solving for the Newton step:
$$\mathbf{p}_k = -[\nabla^2 f(\mathbf{x}_k)]^{-1}\nabla f(\mathbf{x}_k)$$</p><p>The complete Newton iteration becomes:
$$\mathbf{x}_{k+1} = \mathbf{x}_k - [\nabla^2 f(\mathbf{x}_k)]^{-1}\nabla f(\mathbf{x}_k)$$</p><div class=center-container><div class=center-content><figure id=newton_approx><img src=%20../../../../../../tikZ/newton_approx/main.svg alt="Zig zag" width=400px><figcaption><p><strong>Figure 2.4: </strong>Newton optimization step</p></figcaption></figure></div></div><h3 id=the-geometric-insight>The geometric insight
<a class=anchor href=#the-geometric-insight>#</a></h3><p>What makes Newton&rsquo;s method so powerful becomes clear when we think geometrically. The Hessian matrix $\nabla^2 f(\mathbf{x}_k)$ encodes information about how the gradient changes in different directions. If the function curves sharply in one direction and gently in another, the Hessian &ldquo;knows&rdquo; this and adjusts the step accordingly.</p><p>Consider our narrow valley example again. While steepest descent keeps pointing toward the valley walls, Newton&rsquo;s method recognizes the elongated shape of the valley and naturally takes larger steps along the valley floor and smaller steps perpendicular to it. This geometric awareness eliminates the zigzag behavior that plagues steepest descent.</p><p>For quadratic functions, this geometric understanding leads to a remarkable property: Newton&rsquo;s method finds the exact minimum in a single step, regardless of how poorly conditioned the function might be. This happens because our second-order approximation is exact for quadratic functions.</p><h3 id=the-power-of-quadratic-convergence>The power of quadratic convergence
<a class=anchor href=#the-power-of-quadratic-convergence>#</a></h3><p>Near a solution that satisfies our second-order sufficient conditions, Newton&rsquo;s method exhibits quadratic convergence. This technical term describes an almost magical property: the number of correct digits in your answer roughly doubles with each iteration.</p><p>To appreciate this, consider what linear convergence means: if you have one correct digit, you need about three more iterations to get two correct digits. But with quadratic convergence, if you have one correct digit, the next iteration gives you two, then four, then eight. The improvement accelerates dramatically as you approach the solution.</p><p>This rapid convergence makes Newton&rsquo;s method incredibly efficient for high-precision optimization, which is why it forms the backbone of many sophisticated algorithms.</p><h3 id=the-computational-cost>The computational cost
<a class=anchor href=#the-computational-cost>#</a></h3><p>Newton&rsquo;s method&rsquo;s power comes with a price. At each iteration, we must compute the Hessian matrix, which requires evaluating all second partial derivatives of our function. For a function of $d$ variables, this means computing and storing $d(d+1)/2$ distinct second derivatives.</p><p>Even more expensive is solving the linear system $\nabla^2 f(\mathbf{x}_k)\mathbf{p}_k = -\nabla f(\mathbf{x}_k)$ at each iteration. For general matrices, this requires roughly $d^3/3$ arithmetic operations, which becomes prohibitive as the dimension grows.</p><h3 id=when-newtons-method-can-fail>When Newton&rsquo;s method can fail
<a class=anchor href=#when-newtons-method-can-fail>#</a></h3><p>Pure Newton&rsquo;s method isn&rsquo;t foolproof. The Hessian matrix might not be positive definite away from the minimum, which means our quadratic model might not have a minimum—it could have a maximum or a saddle point instead. In such cases, the Newton step might point in completely the wrong direction.</p><p>Additionally, if we start too far from a minimum, the quadratic approximation might be a poor representation of the true function, leading to steps that actually increase the function value.</p><h2 id=looking-ahead-the-bridge-between-methods>Looking ahead: the bridge between methods
<a class=anchor href=#looking-ahead-the-bridge-between-methods>#</a></h2><p>The contrasting strengths and weaknesses of steepest descent and Newton&rsquo;s method naturally lead to interesting questions. Can we capture some of Newton&rsquo;s geometric insight without the full computational burden? Can we ensure the global reliability of gradient methods while achieving faster local convergence?</p><p>These questions motivate more sophisticated approaches. Quasi-Newton methods, which we&rsquo;ll explore in later chapters, build approximations to the Hessian using only gradient information. Methods like BFGS achieve superlinear convergence—faster than linear but not quite quadratic—while requiring much less computation than full Newton steps.</p><p>Similarly, trust region methods and linesearch strategies, which we&rsquo;ll study later, provide systematic ways to ensure that our algorithms make reliable progress even when our local approximations aren&rsquo;t perfect.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#what-is-a-solution->What is a solution ?</a></li><li><a href=#taylors-theorem>Taylor&rsquo;s theorem</a></li><li><a href=#sufficient-and-necessary-conditions-for-local-minima>Sufficient and necessary conditions for local minima</a></li><li><a href=#the-need-for-algorithms>The need for algorithms</a></li><li><a href=#steepest-descent-approach>Steepest-descent approach</a><ul><li><a href=#understanding-the-algorithm-step-by-step>Understanding the algorithm step by step</a></li><li><a href=#why-steepest-descent-can-struggle>Why steepest descent can struggle</a></li><li><a href=#convergence-properties>Convergence properties</a></li></ul></li><li><a href=#newton-method>Newton method</a><ul><li><a href=#the-mathematical-foundation>The mathematical foundation</a></li><li><a href=#the-geometric-insight>The geometric insight</a></li><li><a href=#the-power-of-quadratic-convergence>The power of quadratic convergence</a></li><li><a href=#the-computational-cost>The computational cost</a></li><li><a href=#when-newtons-method-can-fail>When Newton&rsquo;s method can fail</a></li></ul></li><li><a href=#looking-ahead-the-bridge-between-methods>Looking ahead: the bridge between methods</a></li></ul></nav></div></aside></main></body></html>