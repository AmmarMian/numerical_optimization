<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Unconstrained optimization - basics
  #

We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve:
$$
\underset{\mathbf{x}\in\mathbb{R}^d}{\operatorname{argmin}} f(\mathbf{x}).
$$
Let us try to characterizes the nature of the solutions under this setup.

  What is a solution ?
  #










  
    
      Figure 3.1:Local and global minimum can coexist.
  


Generally, we would be happiest if we found a global minimizer of $f$ , a point where the
function attains its least value. A formal definition is :"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="3. Unconstrained optimization : basics"><meta property="og:description" content="Unconstrained optimization - basics # We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve: $$ \underset{\mathbf{x}\in\mathbb{R}^d}{\operatorname{argmin}} f(\mathbf{x}). $$
Let us try to characterizes the nature of the solutions under this setup.
What is a solution ? # Figure 3.1:Local and global minimum can coexist.
Generally, we would be happiest if we found a global minimizer of $f$ , a point where the function attains its least value. A formal definition is :"><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>3. Unconstrained optimization : basics | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.665f8ff0f1a6d15cf7c14837995b39df162fe67ec3f847201ccf1c23762f5785.js integrity="sha256-Zl+P8PGm0Vz3wUg3mVs53xYv5n7D+EcgHM8cI3YvV4U=" crossorigin=anonymous></script><script>const chapterNum=3;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\\\[","\\\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>2. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/ class=active>3. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_trustregions/>5. Unconstrained optimization : trust region</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>6. Constrained optimization</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>1. Proximal methods</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/stochastic/>2. Stochastic optimization</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression with gradient</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>3. Unconstrained optimization : basics</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#what-is-a-solution->What is a solution ?</a></li><li><a href=#taylors-theorem>Taylor&rsquo;s theorem</a></li><li><a href=#sufficient-and-necessary-conditions-for-local-minima>Sufficient and necessary conditions for local minima</a></li><li><a href=#the-need-for-algorithms>The need for algorithms</a></li><li><a href=#steepest-descent-approach>Steepest-descent approach</a></li><li><a href=#newton-method>Newton method</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=unconstrained-optimization---basics>Unconstrained optimization - basics
<a class=anchor href=#unconstrained-optimization---basics>#</a></h1><p>We hereby consider problems without any constraints on the set of admissible solutions, i.e we aim to solve:
$$
\underset{\mathbf{x}\in\mathbb{R}^d}{\operatorname{argmin}} f(\mathbf{x}).
$$</p><p>Let us try to characterizes the nature of the solutions under this setup.</p><h2 id=what-is-a-solution->What is a solution ?
<a class=anchor href=#what-is-a-solution->#</a></h2><figure id="figure-%!s(int=3)-1"><img src=%20../../../../../../tikZ/local_global_minima/main.svg alt="Local vs global" width=600px><figcaption><p><strong>Figure 3.1:</strong>Local and global minimum can coexist.</p></figcaption></figure><p>Generally, we would be happiest if we found a global minimizer of $f$ , a point where the
function attains its least value. A formal definition is :</p><div id="definition-%!s(int=3)-1" class=theorem-box><p class=theorem-title><strong>Definition 3.1 (Global minimizer)</strong></p><div class=theorem-content>A point $\mathbf{x}^\star$ is a <strong>global minimizer</strong> if $f(\mathbf{x}^\star)\leq f(\mathbf{x})$,
where $\mathbf{x}$ ranges over all of $\mathbb{R}^d$ (or at least over the domain of interest to the modeler).</div></div><p>The global minimizer can be difficult to find, because our knowledge of $f$ is usually only local.
Since our algorithm does not visit many points (we hope!), we usually do not have a good
picture of the overall shape of $f$ , and we can never be sure that the function does not take a
sharp dip in some region that has not been sampled by the algorithm. Most algorithms are
able to find only a local minimizer, which is a point that achieves the smallest value of f in
its neighborhood. Formally, we say:</p><div id="definition-%!s(int=3)-2" class=theorem-box><p class=theorem-title><strong>Definition 3.2 (Local minimizer)</strong></p><div class=theorem-content>A point $\mathbf{x}^\star$ is a <strong>local minimizer</strong> if $\exists r>0,\, f(\mathbf{x}^\star)\leq f(\mathbf{x})$, $\forall \mathbf{x}\in\mathcal{B}(\mathbf{x}^\star, r)$.</div></div><p>A point that satisfies
this definition is sometimes called a <strong>weak local minimizer</strong>. Alternatively, when $f(\mathbf{x}^\star)&lt;f(\mathbf{x})$, we say that the minimum is a <strong>strict local minimizer</strong>.</p><h2 id=taylors-theorem>Taylor&rsquo;s theorem
<a class=anchor href=#taylors-theorem>#</a></h2><p>From the definitions given above, it might seem that the only way to find out whether
a point $\mathbf{x}^\star$ is a local minimum is to examine all the points in its immediate vicinity, to make
sure that none of them has a smaller function value. When the function $f$ is smooth, however,
there are much more efficient and practical ways to identify local minima. In particular, if $f$
is twice continuously differentiable, we may be able to tell that $\mathbf{x}^\star$ is a local minimizer (and
possibly a strict local minimizer) by examining just the gradient $\nabla f (\mathbf{x}^\star)$ and the Hessian
$\nabla^2 f (\mathbf{x}^\star)$.
The mathematical tool used to study minimizers of smooth functions is Taylor’s the-
orem. Because this theorem is central to our analysis we state it now.</p><p><div id=taylor_theory class=theorem-box><p class=theorem-title><strong>Theorem 3.1 (Taylor's theorem)</strong></p><div class=theorem-content><p>Suppose that $f:\mathbb{R}^d\mapsto\mathbb{R}$ is continuously differentiable and that we have $\mathbf{p}\in\mathbb{R}^d $. The we have :
\begin{equation}
f(\mathbf{x}+\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x}+t\mathbf{p})^\mathrm{T}\mathbf{p},
\end{equation}
for some $t\in [0,1]$.</p><p>Moreover, if $f$ is twice continuously differentiable, we have :
\begin{equation}
f(\mathbf{x}+\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^\mathrm{T}\mathbf{p} + \frac{1}{2}\mathbf{p}^\mathrm{T}\nabla^2 f(\mathbf{x}+t\mathbf{p})\mathbf{p}),
\end{equation}
for some $t\in [0,1]$.</p></div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content>See any calculus book</div><div class=qed>■</div></div></p><p>Note that in this formulation, the definition is exact and the $t$ scalar is usually unknown. The interest lies in skeching proofs. In practical matters, we rather use the following approximation:<div id=taylor_approximation class=theorem-box><p class=theorem-title><strong>Theorem 3.2 (Taylor's approximation)</strong></p><div class=theorem-content><p>First order approximation:
\begin{equation}
f(\mathbf{x}+\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^\mathrm{T}\mathbf{p} + o(\lVert\mathbf{p}\rVert),
\end{equation}</p><p>Second-order approximation:</p><p>\begin{equation}
f(\mathbf{x}+\mathbf{p}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^\mathrm{T}\mathbf{p} + \frac{1}{2}\mathbf{p}^\mathrm{T}\nabla^2 f(\mathbf{x})\mathbf{p} + o(\lVert\mathbf{p}\rVert^2),
\end{equation}</p><p>where $o(\lVert\mathbf{p}\rVert)$ and $o(\lVert\mathbf{p}\rVert^2)$ represent terms that grow slower than $\lVert\mathbf{p}\rVert$ and $\lVert\mathbf{p}\rVert^2$ respectively as $\lVert\mathbf{p}\rVert \to 0$.</p></div></div></p><h2 id=sufficient-and-necessary-conditions-for-local-minima>Sufficient and necessary conditions for local minima
<a class=anchor href=#sufficient-and-necessary-conditions-for-local-minima>#</a></h2><p>Let us consider a local minimum and see how they can be characterized to later design appropriate solution finding methods. The first well-known result is as follows:<div id=first-order_necessary class=theorem-box><p class=theorem-title><strong>Theorem 3.3 (First-order necessary conditions)</strong></p><div class=theorem-content>if $\mathbf{x}^\star$ is a local minimize, and $f$ is continuously differentiable in a neighborhood of $\mathbf{x}^\star$, then $\nabla f(\mathbf{x}^\star) = \mathbf{0}$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Suppose for contradiction that $\nabla f(\mathbf{x}^\star) \neq 0$, and define vector $\mathbf{p}=-\nabla f(\mathbf{x}^\star)$ such that by construction $\mathbf{p}^\mathrm{T}\nabla f(\mathbf{x}^\star) = - \lVert f(\mathbf{x}^\star) \rVert^2 &lt; 0$.</p><p>Since $f$ is a continuous function, we can define a scalar $T>0$ such that $\forall t\in [0,T[$, we still have:
$$
\mathbf{p}^\mathrm{T}f(\mathbf{x}+t\mathbf{p}) &lt; 0.
$$</p><p>Using <a href=#taylor_theory>Theorem 3.1</a> first-order result, we can write:
$$
f(\mathbf{x}^\star+t\mathbf{p}) = f(\mathbf{x}^\star) + t\mathbf{p}^\mathrm{T}\nabla f(\mathbf{x}^\star+\overline{t}\mathbf{p}),
$$
for some $\overline{t}\in[0,T[$ and any $t\in[0,T[$. Given previous inequality, we obtain:
$$
f(\mathbf{x}^\star+t\mathbf{p}) &lt; f(\mathbf{x}^\star),
$$
which contradicts the fact that $\mathbf{x}^\star$ is a local minimizer.</p></div><div class=qed>■</div></div></p><p>Henceforth, we will call <strong>stationary point</strong>, any $\mathbf{x}$ such that $\nabla f(\mathbf{x}) = 0$.</p><p>For the next result we recall that a matrix $\mathbf{B}$ is positive definite if $\mathbf{p}^\mathrm{T} \mathbf{B} \mathbf{p}>0$ for all $\mathbf{p} \neq \mathbf{0}$, and positive semidefinite if $\mathbf{p}^\mathrm{T} \mathbf{B} \mathbf{p} \geq 0$ for all $\mathbf{p}$.</p><div id="theorem-%!s(int=3)-4" class=theorem-box><p class=theorem-title><strong>Theorem 3.4 (Second-order necessary conditions)</strong></p><div class=theorem-content>If $\mathbf{x}^\star$ is a local minimizer of $f$ and $\nabla^2 f$ is continuous in an open neighborhood of $\mathbf{x}^\star$, then $\nabla f\left(\mathbf{x}^\star\right)=0$ and $\nabla^2 f\left(\mathbf{x}^\star\right)$ is positive semidefinite.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Proof. We know from <a href=#first-order_necessary>Theorem 3.3</a> that $\nabla f\left(\mathbf{x}^\star\right)=0$. For contradiction, assume that $\nabla^2 f\left(\mathbf{x}^\star\right)$ is not positive semidefinite. Then we can choose a vector $\mathbf{p}$ such that $\mathbf{p}^T \nabla^2 f\left(\mathbf{x}^\star\right) \mathbf{p}&lt;0$, and because $\nabla^2 f$ is continuous near $\mathbf{x}^\star$, there is a scalar $T>0$ such that $\mathbf{p}^\mathrm{T} \nabla^2 f\left(\mathbf{x}^*+\overline{t} \mathbf{p}\right) \mathbf{p}&lt;0$ for all $t \in[0, T[$.</p><p>By doing a Taylor series expansion around $\mathbf{x}^\star$, we have for all $\bar{t} \in[0, T[$ and some $t \in[0, \bar{t}]$ that</p><p>$$
f\left(\mathbf{x}^\star+\bar{t} \mathbf{p}\right) = f\left(\mathbf{x}^\star\right)+\bar{t} \mathbf{p}^\mathrm{T} \nabla f\left(\mathbf{x}^\star\right)+\frac{1}{2} \bar{t}^2 \mathbf{p}^\mathrm{T} \nabla^2 f\left(\mathbf{x}^\star+t \mathbf{p}\right) \mathbf{p}&lt;f\left(\mathbf{x}^\star\right) .
$$</p><p>As in <a href=#first-order_necessary>Theorem 3.3</a>, we have found a direction from $\mathbf{x}^\star$ along which $f$ is decreasing, and so again, $\mathbf{x}^\star$ is not a local minimizer.</p></div><div class=qed>■</div></div><p>We now describe sufficient conditions, which are conditions on the derivatives of $f$ at the point $\mathbf{z}^\star$ that guarantee that $\mathbf{x}^\star$ is a local minimizer.</p><p><div id=second-order_sufficient class=theorem-box><p class=theorem-title><strong>Theorem 3.5 (Second-Order Sufficient Conditions)</strong></p><div class=theorem-content>Suppose that $\nabla^2 f$ is continuous in an open neighborhood of $\mathbf{x}^\star$ and that $\nabla f\left(\mathbf{x}^\star\right)=0$ and $\nabla^2 f\left(\mathbf{x}^\star\right)$ is positive definite. Then $\mathbf{x}^\star$ is a strict local minimizer of $f$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Because the Hessian is continuous and positive definite at $\mathbf{x}^\star$, we can choose a radius $r>0$ so that $\nabla^2 f(x)$ remains positive definite for all $x$ in the open ball $\mathcal{D}=\left\{\mathbf{z} \mid\left\lVert\mathbf{z}-\mathbf{x}^\star\right\rVert&lt;\right.$ $r\}$. Taking any nonzero vector $p$ with $\lVert\mathbf{p}\rVert&lt;r$, we have $\mathbf{x}^\star+\mathbf{p} \in \mathcal{D}$ and so</p><p>$$
\begin{aligned}
f\left(\mathbf{x}^\star+p\right) & =f\left(\mathbf{x}^\star\right)+\mathbf{p}^\mathrm{T} \nabla f\left(\mathbf{x}^\star\right)+\frac{1}{2} \mathbf{p}^\mathrm{T} \nabla^2 f(\mathbf{z}) \mathbf{p} \
& =f\left(\mathbf{x}^\star\right)+\frac{1}{2} \mathbf{p}^\mathrm{T} \nabla^2 f(\mathbf{z}) \mathbf{p}
\end{aligned}
$$</p><p>where $\mathbf{z}=\mathbf{x}^\star+t \mathbf{p}$ for some $t \in(0,1)$. Since $\mathbf{z} \in \mathcal{D}$, we have $\mathbf{p}^{\mathrm{T}} \nabla^2 f(\mathbf{z}) \mathbf{p}>0$, and therefore $f\left(\mathbf{x}^\star+\mathbf{p}\right)>f\left(\mathbf{x}^\star\right)$, giving the result.</p></div><div class=qed>■</div></div></p><p>Note that the second-order sufficient conditions of <a href=#second-order_sufficient>Theorem 3.5</a> guarantee something stronger than the necessary conditions discussed earlier; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: A point $\mathbf{x}^\star$ may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function $f(x)=x^4$, for which the point $x^\star=0$ is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite).
When the objective function is convex, local and global minimizers are simple to characterize.</p><p><div id=local_global_convex class=theorem-box><p class=theorem-title><strong>Theorem 3.6</strong></p><div class=theorem-content>When $f$ is convex, any local minimizer $\mathbf{x}^\star$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $\mathbf{x}^\star$ is a global minimizer of $f$.</div></div><div class=proof-box><p class=proof-title><strong>Proof</strong></p><div class=proof-content><p>Suppose that $\mathbf{x}^\star$ is a local but not a global minimizer. Then we can find a point $\mathbf{z} \in \mathbb{R}^n$ with $f(\mathbf{z})&lt;f\left(\mathbf{x}^\star\right)$. Consider the line segment that joins $\mathbf{x}^\star$ to $\mathbf{z}$, that is,
\begin{equation}
\mathbf{x}=\lambda \mathbf{z}+(1-\lambda) \mathbf{x}^\star, \quad \text { for some } \lambda \in(0,1]
\label{eq:line_segment}
\end{equation}
By the convexity property for $f$, we have
\begin{equation}
f(\mathbf{x}) \leq \lambda f(\mathbf{z})+(1-\lambda) f\left(\mathbf{x}^\star\right)&lt;f\left(\mathbf{x}^\star\right)
\label{eq:convexity}
\end{equation}</p><p>Any neighborhood $\mathcal{N}$ of $\mathbf{x}^\star$ contains a piece of the line segment \eqref{eq:line_segment}, so there will always be points $\mathbf{x} \in \mathcal{N}$ at which \eqref{eq:convexity} is satisfied. Hence, $\mathbf{x}^\star$ is not a local minimizer.
For the second part of the theorem, suppose that $\mathbf{x}^\star$ is not a global minimizer and choose $\mathbf{z}$ as above. Then, from convexity, we have</p><p>\begin{equation}
\begin{aligned}
\nabla f\left(\mathbf{x}^\star\right)^{\mathrm{T}}\left(\mathbf{z}-\mathbf{x}^\star\right) & =\left.\frac{d}{d \lambda} f\left(\mathbf{x}^\star+\lambda\left(\mathbf{z}-\mathbf{x}^\star\right)\right)\right|_{\lambda=0} \\
& =\lim _{\lambda \downarrow 0} \frac{f\left(\mathbf{x}^\star+\lambda\left(\mathbf{z}-\mathbf{x}^\star\right)\right)-f\left(\mathbf{x}^\star\right)}{\lambda} \\
& \leq \lim _{\lambda \downarrow 0} \frac{\lambda f(\mathbf{z})+(1-\lambda) f\left(\mathbf{x}^\star\right)-f\left(\mathbf{x}^\star\right)}{\lambda} \\
& =f(\mathbf{z})-f\left(\mathbf{x}^\star\right)&lt;0
\end{aligned}
\end{equation}</p><p>Therefore, $\nabla f\left(\mathbf{x}^\star\right) \neq 0$, and so $\mathbf{x}^\star$ is not a stationary point.</p></div><div class=qed>■</div></div></p><p>These results, which are based on elementary calculus, provide the foundations for unconstrained optimization algorithms. In one way or another, all algorithms seek a point where $\nabla f(\cdot)$ vanishes.</p><h2 id=the-need-for-algorithms>The need for algorithms
<a class=anchor href=#the-need-for-algorithms>#</a></h2><h2 id=steepest-descent-approach>Steepest-descent approach
<a class=anchor href=#steepest-descent-approach>#</a></h2><h2 id=newton-method>Newton method
<a class=anchor href=#newton-method>#</a></h2></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#what-is-a-solution->What is a solution ?</a></li><li><a href=#taylors-theorem>Taylor&rsquo;s theorem</a></li><li><a href=#sufficient-and-necessary-conditions-for-local-minima>Sufficient and necessary conditions for local minima</a></li><li><a href=#the-need-for-algorithms>The need for algorithms</a></li><li><a href=#steepest-descent-approach>Steepest-descent approach</a></li><li><a href=#newton-method>Newton method</a></li></ul></nav></div></aside></main></body></html>