<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Unconstrained optimization -  Second-order methods
  #

We have seen in the previous chapter that first-order methods, such as steepest descent, are often used to find a local minimum of a function $f(\mathbf{x})$. However, these methods can be slow to converge, especially when the function has ill-conditioned Hessian or when the initial guess is far from the solution. Second-order methods, which use information about the curvature of the function, can provide faster convergence rates."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/docs/lectures/advanced/unconstrained_newton/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="1. Unconstrained optimization : Second-order "><meta property="og:description" content="Unconstrained optimization - Second-order methods # We have seen in the previous chapter that first-order methods, such as steepest descent, are often used to find a local minimum of a function $f(\mathbf{x})$. However, these methods can be slow to converge, especially when the function has ill-conditioned Hessian or when the initial guess is far from the solution. Second-order methods, which use information about the curvature of the function, can provide faster convergence rates."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>1. Unconstrained optimization : Second-order | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/docs/lectures/advanced/unconstrained_newton/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.2f95a8412d3c4ad3086cd3b0e485d6c419c4581efb1ce467f255a757541f5712.js integrity="sha256-L5WoQS08StMIbNOw5IXWxBnEWB77HORn8lWnV1QfVxI=" crossorigin=anonymous></script><script>const chapterNum=1;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>6. Constrained optimization</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/ class=active>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>1. Unconstrained optimization : Second-order</h3><label for=toc-control><img src=/numerical_optimization/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#search-directions>Search directions</a></li><li><a href=#step-size-selection>Step-size selection</a></li><li><a href=#convergence-of-newton-methods>Convergence of Newton methods</a></li><li><a href=#rate-of-convergence>Rate of convergence</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=unconstrained-optimization----second-order-methods>Unconstrained optimization - Second-order methods
<a class=anchor href=#unconstrained-optimization----second-order-methods>#</a></h1><p>We have seen in the previous chapter that first-order methods, such as steepest descent, are often used to find a local minimum of a function $f(\mathbf{x})$. However, these methods can be slow to converge, especially when the function has ill-conditioned Hessian or when the initial guess is far from the solution. Second-order methods, which use information about the curvature of the function, can provide faster convergence rates.</p><h2 id=search-directions>Search directions
<a class=anchor href=#search-directions>#</a></h2><p>Another important search direction-perhaps the most important one of all-is the Newton direction. This direction is derived from the second-order Taylor series approximation to $f\left(\mathbf{x}_k+\mathbf{p}\right)$, which is
$$
f\left(\mathbf{x}_k+\mathbf{p}\right) \approx f_k+\mathbf{p}^{\mathrm{T}} \nabla f_k+\frac{1}{2} \mathbf{p}^{\mathrm{T}} \nabla^2 f_k \mathbf{p} \stackrel{\text { def }}{=} m_k(\mathbf{p})
$$</p><p>Assuming for the moment that $\nabla^2 f_k$ is positive definite, we obtain the Newton direction by finding the vector $\mathbf{p}$ that minimizes $m_k(\mathbf{p})$. By simply setting the derivative of $m_k(\mathbf{p})$ to zero, we obtain the following explicit formula:</p><p>\begin{equation}
\mathbf{p}_k^{\mathrm{N}}=-\nabla^2 f_k^{-1} \nabla f_k
\label{eq:newton_direction}
\end{equation}</p><p>The Newton direction is reliable when the difference between the true function $f\left(\mathbf{x}_k+ \mathbf{p}\right)$ and its quadratic model $m_k(\mathbf{p})$ is not too large. By comparing \eqref{eq:newton_direction} with traditional Taylor expansion, we see that the only difference between these functions is that the matrix $\nabla^2 f\left(\mathbf{x}_k+t \mathbf{p}\right)$ in the third term of the expansion has been replaced by $\nabla^2 f_k=\nabla^2 f\left(\mathbf{x}_k\right)$. If $\nabla^2 f(\cdot)$ is sufficiently smooth, this difference introduces a perturbation of only $O\left(\lVert\mathbf{p}\rVert^3\right)$ into the expansion, so that when $\lVert\mathbf{p}\rVert$ is small, the approximation $f\left(\mathbf{x}_k+\mathbf{p}\right) \approx m_k(\mathbf{p})$ is very accurate indeed.</p><p>The Newton direction can be used in a line search method when $\nabla^2 f_k$ is positive definite, for in this case we have</p><p>$$
\nabla f_k^{\mathrm{T}} \mathbf{p}_k^{\mathrm{N}}=-\mathbf{p}_k^{\mathrm{N} \mathrm{T}} \nabla^2 f_k \mathbf{p}_k^{\mathrm{N}} \leq-\sigma_k\lVert\mathbf{p}_k^{\mathrm{N}}\rVert^2
$$</p><p>for some $\sigma_k>0$. Unless the gradient $\nabla f_k$ (and therefore the step $\mathbf{p}_k^N$) is zero, we have that $\nabla f_k^{\mathrm{T}} \mathbf{p}_k^{\mathrm{N}}&lt;0$, so the Newton direction is a descent direction. Unlike the steepest descent direction, there is a &ldquo;natural&rdquo; step length of 1 associated with the Newton direction. Most
line search implementations of Newton&rsquo;s method use the unit step $\alpha=1$ where possible and adjust this step length only when it does not produce a satisfactory reduction in the value of $f$.</p><p>When $\nabla^2 f_k$ is not positive definite, the Newton direction may not even be defined, since $\nabla^2 f_k^{-1}$ may not exist. Even when it is defined, it may not satisfy the descent property $\nabla f_k^{\mathrm{T}} \mathbf{p}_k^{\mathrm{N}}&lt;0$, in which case it is unsuitable as a search direction. In these situations, line search methods modify the definition of $\mathbf{p}_k$ to make it satisfy the downhill condition while retaining the benefit of the second-order information contained in $\nabla^2 f_k$.</p><p>Methods that use the Newton direction have a fast rate of local convergence, typically quadratic. When a neighborhood of the solution is reached, convergence to high accuracy often occurs in just a few iterations. The main drawback of the Newton direction is the need for the Hessian $\nabla^2 f(\mathbf{x})$. Explicit computation of this matrix of second derivatives is sometimes, though not always, a cumbersome, error-prone, and expensive process.</p><p>Quasi-Newton search directions provide an attractive alternative in that they do not require computation of the Hessian and yet still attain a superlinear rate of convergence. In place of the true Hessian $\nabla^2 f_k$, they use an approximation $\mathbf{B}_k$, which is updated after each step to take account of the additional knowledge gained during the step. The updates make use of the fact that changes in the gradient $\mathbf{g}$ provide information about the second derivative of $f$ along the search direction. By using the expression from our statement of Taylor&rsquo;s theorem, we have by adding and subtracting the term $\nabla^2 f(\mathbf{x}) \mathbf{p}$ that</p><p>$$
\nabla f(\mathbf{x}+\mathbf{p})=\nabla f(\mathbf{x})+\nabla^2 f(\mathbf{x}) \mathbf{p}+\int_0^1\left[\nabla^2 f(\mathbf{x}+t \mathbf{p})-\nabla^2 f(\mathbf{x})\right] \mathbf{p} d t
$$</p><p>Because $\nabla f(\cdot)$ is continuous, the size of the final integral term is $o(\lVert\mathbf{p}\rVert)$. By setting $\mathbf{x}=\mathbf{x}_k$ and $\mathbf{p}=\mathbf{x}_{k+1}-\mathbf{x}_k$, we obtain</p><p>$$
\nabla f_{k+1}=\nabla f_k+\nabla^2 f_{k+1}\left(\mathbf{x}_{k+1}-\mathbf{x}_k\right)+o\left(\lVert\mathbf{x}_{k+1}-\mathbf{x}_k\rVert\right)
$$</p><p>When $\mathbf{x}_k$ and $\mathbf{x}_{k+1}$ lie in a region near the solution $\mathbf{x}^*$, within which $\nabla f$ is positive definite, the final term in this expansion is eventually dominated by the $\nabla^2 f_k\left(\mathbf{x}_{k+1}-\mathbf{x}_k\right)$ term, and we can write</p><p>$$
\nabla^2 f_{k+1}\left(\mathbf{x}_{k+1}-\mathbf{x}_k\right) \approx \nabla f_{k+1}-\nabla f_k
$$</p><p>We choose the new Hessian approximation $\mathbf{B}_{k+1}$ so that it mimics this property of the true Hessian, that is, we require it to satisfy the following condition, known as the secant equation:</p><p>\begin{equation}
\mathbf{B}_{k+1} \mathbf{s}_k=\mathbf{y}_k
\label{eq:secant_equation}
\end{equation}</p><p>where</p><p>$$
\mathbf{s}_k=\mathbf{x}_{k+1}-\mathbf{x}_k, \quad \mathbf{y}_k=\nabla f_{k+1}-\nabla f_k
$$</p><p>Typically, we impose additional requirements on $\mathbf{B}_{k+1}$, such as symmetry (motivated by symmetry of the exact Hessian), and a restriction that the difference between successive approximation $\mathbf{B}_k$ to $\mathbf{B}_{k+1}$ have low rank. The initial approximation $\mathbf{B}_0$ must be chosen by the user.</p><p>Two of the most popular formulae for updating the Hessian approximation $\mathbf{B}_k$ are the symmetric-rank-one (SR1) formula, defined by</p><p>\begin{equation}
\mathbf{B}_{k+1}=\mathbf{B}_k+\frac{\left(\mathbf{y}_k-\mathbf{B}_k \mathbf{s}_k\right)\left(\mathbf{y}_k-\mathbf{B}_k \mathbf{s}_k\right)^{\mathrm{T}}}{\left(\mathbf{y}_k-\mathbf{B}_k \mathbf{s}_k\right)^{\mathrm{T}} \mathbf{s}_k}
\label{eq:sr1_formula}
\end{equation}</p><p>and the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno, which is defined by</p><p>\begin{equation}
\mathbf{B}_{k+1}=\mathbf{B}_k-\frac{\mathbf{B}_k \mathbf{s}_k \mathbf{s}_k^{\mathrm{T}} \mathbf{B}_k}{\mathbf{s}_k^{\mathrm{T}} \mathbf{B}_k \mathbf{s}_k}+\frac{\mathbf{y}_k \mathbf{y}_k^{\mathrm{T}}}{\mathbf{y}_k^{\mathrm{T}} \mathbf{s}_k}
\label{eq:bfgs_formula}
\end{equation}</p><p>Note that the difference between the matrices $\mathbf{B}_k$ and $\mathbf{B}_{k+1}$ is a rank-one matrix in the case of \eqref{eq:sr1_formula}, and a rank-two matrix in the case of \eqref{eq:bfgs_formula}. Both updates satisfy the secant equation and both maintain symmetry. One can show that BFGS update \eqref{eq:bfgs_formula} generates positive definite approximations whenever the initial approximation $\mathbf{B}_0$ is positive definite and $\mathbf{s}_k^{\mathrm{T}} \mathbf{y}_k>0$.</p><p>The quasi-Newton search direction is given by using $\mathbf{B}_k$ in place of the exact Hessian in the formula \eqref{eq:newton_direction}, that is,</p><p>\begin{equation}
\mathbf{p}_k=-\mathbf{B}_k^{-1} \nabla f_k
\label{eq:quasi_newton_direction}
\end{equation}</p><p>Some practical implementations of quasi-Newton methods avoid the need to factorize $\mathbf{B}_k$ at each iteration by updating the inverse of $\mathbf{B}_k$, instead of $\mathbf{B}_k$ itself. In fact, the equivalent formula for \eqref{eq:sr1_formula} and \eqref{eq:bfgs_formula}, applied to the inverse approximation $\mathbf{H}_k \stackrel{\text { def }}{=} \mathbf{B}_k^{-1}$, is</p><p>\begin{equation}
\mathbf{H}_{k+1}=\left(\mathbf{I}-\rho_k \mathbf{s}_k \mathbf{y}_k^{\mathrm{T}}\right) \mathbf{H}_k\left(\mathbf{I}-\rho_k \mathbf{y}_k \mathbf{s}_k^{\mathrm{T}}\right)+\rho_k \mathbf{s}_k \mathbf{s}_k^{\mathrm{T}}, \quad \rho_k=\frac{1}{\mathbf{y}_k^{\mathrm{T}} \mathbf{s}_k}
\label{eq:inverse_bfgs}
\end{equation}</p><p>Calculation of $\mathbf{p}_k$ can then be performed by using the formula $\mathbf{p}_k=-\mathbf{H}_k \nabla f_k$. This can be implemented as a matrix-vector multiplication, which is typically simpler than the factorization/back-substitution procedure that is needed to implement the formula \eqref{eq:quasi_newton_direction}.</p><h2 id=step-size-selection>Step-size selection
<a class=anchor href=#step-size-selection>#</a></h2><p>Contrarily to the steepest descent, Newton methods have a &ldquo;natural&rdquo; step size of 1 associated with the Newton direction. This is because the Newton direction is derived from the second-order Taylor series approximation, which is designed to minimize the quadratic model of the function. However, in practice, it is often necessary to adjust this step size to ensure sufficient decrease in the function value.</p><p>When using a line search method, we can set $\alpha_k=1$ and check if this step size leads to a sufficient decrease in the function value. If it does not, we can use a backtracking line search to find a suitable step size that satisfies the Armijo condition. The Armijo condition ensures that the step size leads to a sufficient decrease in the function value, which is crucial for convergence of the method.</p><h2 id=convergence-of-newton-methods>Convergence of Newton methods
<a class=anchor href=#convergence-of-newton-methods>#</a></h2><p>As in first-order methods, we make use of Zoutendijk&rsquo;s condition, that still apllies.</p><p>Consider now the Newton-like method with $\mathbf{p}_k = -\mathbf{B}_k^{-1} \nabla f_k$ and assume that the matrices $\mathbf{B}_k$ are positive definite with a uniformly bounded condition number. That is, there is a constant $M$ such that</p><p>\begin{equation}
\|\mathbf{B}_k\|\|\mathbf{B}_k^{-1}\| \leq M, \quad \text { for all } k .
\label{eq:condition_bound}
\end{equation}</p><p>It is easy to show from the definition that</p><p>$$
\cos \theta_k \geq 1 / M
$$</p><p>By combining this bound with <a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch//#eqn-id%3Aeq%3Azoutendijk_condition>(4.16)</a> we find that</p><p>$$
\lim _{k \rightarrow \infty}\|\nabla f_k\|=0
$$</p><p>Therefore, we have shown that Newton and quasi-Newton methods are globally convergent if the matrices $\mathbf{B}_k$ have a bounded condition number and are positive definite (which is needed to ensure that $\mathbf{p}_k$ is a descent direction), and if the step lengths satisfy the Wolfe conditions.</p><p>For some algorithms, such as conjugate gradient methods, we will not be able to prove the limit <a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>(4.18)</a>, but only the weaker result</p><p>\begin{equation}
\liminf _{k \rightarrow \infty}\|\nabla f_k\|=0
\label{eq:weak_convergence}
\end{equation}</p><p>In other words, just a subsequence of the gradient norms $\|\nabla f_{k_j}\|$ converges to zero, rather than the whole sequence. This result, too, can be proved by using Zoutendijk&rsquo;s condition <a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch//#eqn-id%3Aeq%3Azoutendijk_condition>(4.16)</a>, but instead of a constructive proof, we outline a proof by contradiction. Suppose that \eqref{eq:weak_convergence} does not hold, so that the gradients remain bounded away from zero, that is, there exists $\gamma>0$ such that</p><p>$$
\|\nabla f_k\| \geq \gamma, \quad \text { for all } k \text { sufficiently large. }
$$</p><p>Then from <a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch//#eqn-id%3Aeq%3Azoutendijk_condition>(4.16)</a> we conclude that</p><p>$$
\cos \theta_k \rightarrow 0
$$</p><p>that is, the entire sequence $\{\cos \theta_k\}$ converges to 0. To establish \eqref{eq:weak_convergence}, therefore, it is enough to show that a subsequence $\{\cos \theta_{k_j}\}$ is bounded away from zero.</p><h2 id=rate-of-convergence>Rate of convergence
<a class=anchor href=#rate-of-convergence>#</a></h2><p>We refer the reader to the textbook:</p><blockquote><p>&ldquo;Numerical Optimization&rdquo; by Nocedal and Wright, 2nd edition, Springer, 2006, pages 47-51,
Peculiarly, see pages 51-53.</p></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#search-directions>Search directions</a></li><li><a href=#step-size-selection>Step-size selection</a></li><li><a href=#convergence-of-newton-methods>Convergence of Newton methods</a></li><li><a href=#rate-of-convergence>Rate of convergence</a></li></ul></nav></div></aside></main></body></html>