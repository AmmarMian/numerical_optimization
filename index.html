<!doctype html><html lang=fr dir=ltr><head><meta name=generator content="Hugo 0.142.0"><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Numerical Optimization: theory and applications
  #



     

  




  

  

     

  Description
  #

This page regroups information, lecture documents and practical labs for a course given at doctoral school SIE on numerical optimisation. This version concerns the year 2025. You can fin on the left, all the course materials with solutions to exercises and implementation of programs in python."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://ammarmian.github.io/numerical_optimization/"><meta property="og:site_name" content="Numerical optimization"><meta property="og:title" content="Introduction"><meta property="og:description" content="Numerical Optimization: theory and applications # Description # This page regroups information, lecture documents and practical labs for a course given at doctoral school SIE on numerical optimisation. This version concerns the year 2025. You can fin on the left, all the course materials with solutions to exercises and implementation of programs in python."><meta property="og:locale" content="fr"><meta property="og:type" content="website"><title>Introduction | Numerical optimization</title>
<link rel=icon href=/numerical_optimization/favicon.png><link rel=manifest href=/numerical_optimization/manifest.json><link rel=canonical href=http://ammarmian.github.io/numerical_optimization/><link rel=stylesheet href=/numerical_optimization/book.min.a8206ca0e1caaf803f4ef219929830ceb3f1ae72968a1975640344969b7a9c50.css integrity="sha256-qCBsoOHKr4A/TvIZkpgwzrPxrnKWihl1ZANElpt6nFA=" crossorigin=anonymous><script defer src=/numerical_optimization/fuse.min.js></script><script defer src=/numerical_optimization/en.search.min.c6ecd86286e3664fb198a09ef85d0f7ded9aa4abbdc4b6bde689adda153aff2e.js integrity="sha256-xuzYYobjZk+xmKCe+F0Pfe2apKu9xLa95omt2hU6/y4=" crossorigin=anonymous></script><link rel=alternate type=application/rss+xml href=http://ammarmian.github.io/numerical_optimization/index.xml title="Numerical optimization"><script>const chapterNum=0;MathJax={section:chapterNum,loader:{load:["[tex]/tagformat"]},tex:{packages:{"[+]":["tagformat"]},inlineMath:[["\\\\(","\\\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEnvironments:!0,processRefs:!0,tags:"ams",tagformat:{number:e=>MathJax.config.section+"."+e,id:e=>"eqn-id:"+e}},startup:{ready(){MathJax.startup.defaultReady(),MathJax.startup.input[0].preFilters.add(({math:e})=>{e.inputData.recompile&&(MathJax.config.section=e.inputData.recompile.section)}),MathJax.startup.input[0].postFilters.add(({math:e})=>{e.inputData.recompile&&(e.inputData.recompile.section=MathJax.config.section)})}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/numerical_optimization/><span>Numerical optimization</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li class=book-section-flat><a href=/numerical_optimization/docs/lectures/>Lectures</a><ul><li><a href=/numerical_optimization/docs/lectures/1_introduction/>Introduction</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/>I - Fundamentals</a><ul><li><a href=/numerical_optimization/docs/lectures/fundamentals/optimization_problems/>1. Optimization problems</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_optimization/>2. Unconstrained optimization : basics</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/convexity/>3. Convexity theory</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/unconstrained_linesearch/>4. Unconstrained optimization : linesearch</a></li><li><a href=/numerical_optimization/docs/lectures/fundamentals/constrained_optimisation/>5. Constrained optimization - Introduction</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/advanced/>II - Advanced problems</a><ul><li><a href=/numerical_optimization/docs/lectures/advanced/unconstrained_newton/>1. Unconstrained optimization : Second-order</a></li><li><a href=/numerical_optimization/docs/lectures/advanced/proximal_methods/>2. Proximal methods</a></li></ul></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/>III - Machine Learning problems</a><ul><li><a href=/numerical_optimization/docs/lectures/machine_learning/perceptron/>1. From Linear regression to perceptron</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/svm/>2. Support Vector Machine</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/neural_networks/>3. Neural Networks</a></li><li><a href=/numerical_optimization/docs/lectures/machine_learning/modern/>4. Modern trends</a></li></ul></li><li><input type=checkbox id=section-28bd40dd904095dea2d407e437b9446e class=toggle>
<label for=section-28bd40dd904095dea2d407e437b9446e class="flex justify-between"><a href=/numerical_optimization/docs/lectures/reminders/>Reminders</a></label><ul><li><a href=/numerical_optimization/docs/lectures/reminders/linear_algebra/>Linear Algebra</a></li><li><a href=/numerical_optimization/docs/lectures/reminders/differentiation/>Differentiation</a></li></ul></li></ul></li><li class=book-section-flat><a href=/numerical_optimization/docs/practical_labs/>Practical labs</a><ul><li><a href=/numerical_optimization/docs/practical_labs/linear_regression/>I - Linear Regression models</a></li><li><a href=/numerical_optimization/docs/practical_labs/remote_sensing/>II - Remote Sensing Project</a></li><li><a href=/numerical_optimization/docs/practical_labs/mnist/>III - Digit recognition</a></li><li><a href=/numerical_optimization/docs/practical_labs/environment/>Lab environment</a></li><li><a href=/numerical_optimization/docs/practical_labs/backtracking/>Backtracking memo</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/numerical_optimization/svg/menu.svg class=book-icon alt=Menu></label><h3>Introduction</h3><label for=toc-control></label></div></header><article class="markdown book-article"><h1 id=numerical-optimization-theory-and-applications>Numerical Optimization: theory and applications
<a class=anchor href=#numerical-optimization-theory-and-applications>#</a></h1><div class="book-columns flex flex-wrap"><div class="flex-even markdown-inner" style=flex-grow:1><div class=center-container><div class=center-content><div style=margin-left:-100px><figure><img src=./tikZ/main_illustration/main.svg alt="Himmelblau's Function" width=400px></figure></div></div></div></div><div class="flex-even markdown-inner" style=flex-grow:2><h2 id=description>Description
<a class=anchor href=#description>#</a></h2><p>This page regroups information, lecture documents and practical labs for a course given at doctoral school SIE on numerical optimisation. This version concerns the year 2025. You can fin on the left, all the course materials with solutions to exercises and implementation of programs in python.</p></div></div><h2 id=course-objectives>Course Objectives
<a class=anchor href=#course-objectives>#</a></h2><p>This course teaches an overview of modern optimization methods, for applications in inverse problems, machine learning and data science. Alternating between mathematical theory of optimization and practical lab sessions and projects, the course aims at providing doctoral students with the knowledge to solve common problems through numerical optimizations.</p><p>At the end of this course, the students should:</p><ul><li>Be able to recognize convex optimization problems that arise in scientific fields and design appropriate cost functions</li><li>Have an understanding of how such problems are solved, and gain some experience in solving them</li><li>Have knowledge of the underlying tools behind training of machine learning models</li><li>Be able to implement backpropagation and stochastic optimization algorithms for large models</li></ul><h2 id=program>Program
<a class=anchor href=#program>#</a></h2><h3 id=part-i---fundamental-theory-week-1>Part I - Fundamental Theory (Week 1)
<a class=anchor href=#part-i---fundamental-theory-week-1>#</a></h3><p>The course will first start with fundamental mathematical concepts of optimization and convex optimization:</p><ul><li>Formulating an optimization problem</li><li>Reminders of linear algebra and formulating a constrained optimization problem</li><li>Reminders on differentiability</li><li>Convexity theory</li><li>Gradient Methods</li><li>Second-order methods</li></ul><table><thead><tr><th>Session</th><th>Duration</th><th>Content</th><th>Date</th><th>Room</th><th>Slides</th></tr></thead><tbody><tr><td>CM1</td><td>1.5h</td><td>Introduction, Linear algebra and Differentiation reminders, and exercices</td><td>2 June 2025 10am</td><td>B-120</td><td><a href=./slides/01_introduction/main.pdf>Slides</a></td></tr><tr><td>CM2</td><td>1.5h</td><td>Steepest descent algorithm, Newton method and convexity</td><td>2 June 2025 1.15pm</td><td>B-120</td><td><a href=./slides/02_unconstrained_basics/main.pdf>Slides</a></td></tr><tr><td>TD1</td><td>1.5h</td><td>Application to linear regression (1/2)</td><td>2 June 2025 3pm</td><td>C-213</td><td></td></tr><tr><td>CM3</td><td>1.5h</td><td>Linesearch algorithms and their convergence</td><td>3 June 2025 10am</td><td>B-120</td><td><a href=./slides/03_unconstrained_linesearch/main.pdf>Slides</a></td></tr><tr><td>TD2</td><td>1.5h</td><td>Linesearch in linear regression (2/2)</td><td>3 June 2025 1.15pm</td><td>C-213</td><td></td></tr><tr><td>CM4</td><td>1.5h</td><td>Constrained optimization : linear programming and lagrangian methods</td><td>3 June 2025 3pm</td><td>B-120</td><td><a href=./slides/04_constrained_optim/main.pdf>Slides</a></td></tr></tbody></table><h3 id=part-ii---application-to-image--remote-sensing-problems-week-1>Part II - Application to Image / Remote Sensing Problems (Week 1)
<a class=anchor href=#part-ii---application-to-image--remote-sensing-problems-week-1>#</a></h3><p>Then we will apply those concepts in practice in inverse problem solving with examples in image denoising problems in remote sensing:</p><ul><li>Formulating an unconstrained optimization problem and solving it for a practical example</li><li>Numerical implementation in python</li></ul><p>This lab will be supervised by <a href=https://y-mhiri.github.io>Yassine Mhiri</a>.</p><table><thead><tr><th>Session</th><th>Duration</th><th>Content</th><th>Date</th><th>Room</th></tr></thead><tbody><tr><td>TP</td><td>4h</td><td>Implementation of inverse problems for image processing</td><td>4 June 2025 1pm</td><td>C-213</td></tr></tbody></table><h3 id=part-iii---advanced-topics-week-2>Part III - Advanced topics (Week 2)
<a class=anchor href=#part-iii---advanced-topics-week-2>#</a></h3><p>We then move on to more complicated problems:</p><ul><li>Newton with linesearch and quasi-Newton methods</li><li>Proximal algorithms</li></ul><table><thead><tr><th>Session</th><th>Duration</th><th>Content</th><th>Date</th><th>Room</th></tr></thead><tbody><tr><td>CM5</td><td>1.5h</td><td>Newton / Quasi Newton methods</td><td>12 June 2025 10am</td><td>B-120</td></tr><tr><td>CM6</td><td>1.5h</td><td>Proximal methods</td><td>12 June 2025 1.15pm</td><td>B-120</td></tr><tr><td>TD3</td><td>1.5h</td><td>Use of proximal methods in image processing</td><td>12 June 2025 3pm</td><td>C-213</td></tr></tbody></table><h3 id=part-iv---application-to-machine-learning-week-3>Part IV - Application to Machine Learning (Week 3)
<a class=anchor href=#part-iv---application-to-machine-learning-week-3>#</a></h3><p>We finally apply all those concepts with a focus on machine learning training process:</p><ul><li>Stochastic optimization</li><li>Shallow models: Optimization for linear/logistic regression, support vector machine, perceptron and MLP</li><li>Deep models: Formulating the back propagation algorithm for common layers</li></ul><table><thead><tr><th>Session</th><th>Duration</th><th>Content</th><th>Date</th><th>Room</th></tr></thead><tbody><tr><td>Lecture</td><td>CM7</td><td>1.5h</td><td>Stochastic optimisation</td><td>12 June 2025 1.15pm</td></tr><tr><td>Lecture</td><td>CM8</td><td>1.5h</td><td>Optimization for shallow models : from perceptron to MLP to CNN</td><td>16 June 2025 3pm</td></tr><tr><td>Exercise</td><td>TD4</td><td>1.5h</td><td>Implementing backpropagation for neural networks</td><td>17 June 2025 1.15pm</td></tr><tr><td>Lecture</td><td>CM9</td><td>1.5h</td><td>Optimization for deep models : Adam and SGD</td><td>17 June 2025 3pm</td></tr></tbody></table><h2 id=prerequisites>Prerequisites
<a class=anchor href=#prerequisites>#</a></h2><p>Students should have a decent understanding of basic linear algebra and differentiability of functions over several variables (some reminders will be given at the beginning still). Some capacity to program in Python is desired with knowledge of NumPy.</p><h2 id=teaching-method>Teaching Method
<a class=anchor href=#teaching-method>#</a></h2><p>Lectures will alternate between theoretical lectures, mathematical exercises and practical implementation in Python of the algorithms studied. A mini-project in Remote Sensing will also be given to illustrate on a real-world problem.</p><h2 id=ressources>Ressources
<a class=anchor href=#ressources>#</a></h2><p>The course is based on several resources, including:</p><ul><li><em>Numerical Optimization</em> by J. Nocedal and S. Wright, for linesearch and Newton methods</li><li><em>Proximal Algorithms</em> monograph by N. Parikh and S. Boyd, for proximal methods</li><li><em>Deep Learning</em> by I. Goodfellow, Y. Bengio and A. Courville, for backpropagation and stochastic optimization</li></ul><h2 id=registration>Registration
<a class=anchor href=#registration>#</a></h2><p>Register through Adum and/or by email to <a href=mailto:ammar.mian@univ-smb.fr>ammar.mian@univ-smb.fr</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>